{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:18.389123Z",
     "start_time": "2019-02-14T20:48:18.382107Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview-/-requirements\" data-toc-modified-id=\"Overview-/-requirements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview / requirements</a></span></li><li><span><a href=\"#Overhead\" data-toc-modified-id=\"Overhead-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overhead</a></span></li><li><span><a href=\"#Choose-parameters\" data-toc-modified-id=\"Choose-parameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Choose parameters</a></span></li><li><span><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hammond's-newdic\" data-toc-modified-id=\"Hammond's-newdic-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Hammond's newdic</a></span><ul class=\"toc-item\"><li><span><a href=\"#SUBTLEX\" data-toc-modified-id=\"SUBTLEX-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>SUBTLEX</a></span></li></ul></li><li><span><a href=\"#COCA\" data-toc-modified-id=\"COCA-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>COCA</a></span></li><li><span><a href=\"#IPhOD\" data-toc-modified-id=\"IPhOD-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>IPhOD</a></span></li></ul></li><li><span><a href=\"#IPhOD-processing...\" data-toc-modified-id=\"IPhOD-processing...-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>IPhOD processing...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-data-that-definitely-won't-end-up-in-the-lexicon-distribution...\" data-toc-modified-id=\"Remove-data-that-definitely-won't-end-up-in-the-lexicon-distribution...-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Remove data that definitely won't end up in the lexicon distribution...</a></span></li><li><span><a href=\"#Remove-words-with-triphones-and-diphones-that-a-channel-distribution-isn't-definable-for\" data-toc-modified-id=\"Remove-words-with-triphones-and-diphones-that-a-channel-distribution-isn't-definable-for-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Remove words with triphones and diphones that a channel distribution isn't definable for</a></span></li><li><span><a href=\"#Choose-wordforms-for-the-IPhOD-lexicon-distribution\" data-toc-modified-id=\"Choose-wordforms-for-the-IPhOD-lexicon-distribution-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Choose wordforms for the IPhOD lexicon distribution</a></span></li><li><span><a href=\"#Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-IPhOD\" data-toc-modified-id=\"Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-IPhOD-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Calculate frequencies, probabilities, and informativities of phonological wordforms in IPhOD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-probability-mass-among-phonological-realizations-of-the-same-orthographic-word\" data-toc-modified-id=\"Split-probability-mass-among-phonological-realizations-of-the-same-orthographic-word-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Split probability mass among phonological realizations of the same orthographic word</a></span></li><li><span><a href=\"#Calculate-probability-of-a-phonological-word-(marginalizing-over-orthographic-wordforms)\" data-toc-modified-id=\"Calculate-probability-of-a-phonological-word-(marginalizing-over-orthographic-wordforms)-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Calculate probability of a phonological word (marginalizing over orthographic wordforms)</a></span></li></ul></li><li><span><a href=\"#Define-representation-for-export\" data-toc-modified-id=\"Define-representation-for-export-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Define representation for export</a></span></li><li><span><a href=\"#Export-/-import\" data-toc-modified-id=\"Export-/-import-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Export / import</a></span><ul class=\"toc-item\"><li><span><a href=\"#...import...\" data-toc-modified-id=\"...import...-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>...import...</a></span></li></ul></li></ul></li><li><span><a href=\"#Hammond's-newdic-processing...\" data-toc-modified-id=\"Hammond's-newdic-processing...-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Hammond's newdic processing...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-clutter...\" data-toc-modified-id=\"Remove-clutter...-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Remove clutter...</a></span></li><li><span><a href=\"#Remove-words-with-triphones-that-a-channel-distribution-isn't-definable-for\" data-toc-modified-id=\"Remove-words-with-triphones-that-a-channel-distribution-isn't-definable-for-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Remove words with triphones that a channel distribution isn't definable for</a></span></li><li><span><a href=\"#Choose-wordforms-for-the-Hammond-lexicon-distribution\" data-toc-modified-id=\"Choose-wordforms-for-the-Hammond-lexicon-distribution-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Choose wordforms for the Hammond lexicon distribution</a></span></li><li><span><a href=\"#Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-Hammond's-newdic\" data-toc-modified-id=\"Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-Hammond's-newdic-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Calculate frequencies, probabilities, and informativities of phonological wordforms in Hammond's newdic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-up-probability-of-orthographic-words-in-SUBTLEX\" data-toc-modified-id=\"Look-up-probability-of-orthographic-words-in-SUBTLEX-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Look up probability of orthographic words in SUBTLEX</a></span></li><li><span><a href=\"#Look-up-probability-of-orthographic-words-in-IPhOD\" data-toc-modified-id=\"Look-up-probability-of-orthographic-words-in-IPhOD-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Look-up probability of orthographic words in IPhOD</a></span></li><li><span><a href=\"#Look-up-COCA-unigram-frequencies\" data-toc-modified-id=\"Look-up-COCA-unigram-frequencies-6.4.3\"><span class=\"toc-item-num\">6.4.3&nbsp;&nbsp;</span>Look up COCA unigram frequencies</a></span></li><li><span><a href=\"#Map-each-remaining-phonological-wordform-in-Hammond's-newdic-to-a-frequency/probability-(marginalizing-over-orthographic-wordforms)\" data-toc-modified-id=\"Map-each-remaining-phonological-wordform-in-Hammond's-newdic-to-a-frequency/probability-(marginalizing-over-orthographic-wordforms)-6.4.4\"><span class=\"toc-item-num\">6.4.4&nbsp;&nbsp;</span>Map each remaining phonological wordform in Hammond's newdic to a frequency/probability (marginalizing over orthographic wordforms)</a></span></li><li><span><a href=\"#Set-a-minimum-probability-threshold-for-inclusion\" data-toc-modified-id=\"Set-a-minimum-probability-threshold-for-inclusion-6.4.5\"><span class=\"toc-item-num\">6.4.5&nbsp;&nbsp;</span>Set a minimum probability threshold for inclusion</a></span></li></ul></li><li><span><a href=\"#Define-representation-for-export\" data-toc-modified-id=\"Define-representation-for-export-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Define representation for export</a></span></li><li><span><a href=\"#Export...\" data-toc-modified-id=\"Export...-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Export...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import...\" data-toc-modified-id=\"Import...-6.6.1\"><span class=\"toc-item-num\">6.6.1&nbsp;&nbsp;</span>Import...</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview / requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the ultimate goal of these notebooks is to process data for a word recognition model that can map from a segmental transcription of the incrementally produced prefix of a speaker's intended wordform to a listener's beliefs about what the speaker's actual intended wordform is. (See other notebooks - especially the collection documenting the model implementation -  for more details.) \n",
    "\n",
    "This requires a lexicon of transcribed wordforms, the ability to assign a prior probability to each wordform, and a model of coarticulation and noise/errors in the listening process. Where previous notebooks in this collection have each only transformed what is structurally the same dataset, this notebook transforms those structures into one of the two types of inputs to the word recognition model - a prior distribution over wordforms (a lexicon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, I am running Python 3.6.5, Jupyter 5.5.0, and otherwise Anaconda 5.2. More specifically, this notebook assumes the current working directory contains\n",
    " - a copy of Hammond's mysterious 'newdic' transcribed lexicon of English http://dingo.sbs.arizona.edu/~hammond/lsasummer11/newdic, processed by two previous notebooks ('Notebook 1b', `Notebook 2b`)\n",
    "    - `Hammond_newdic_IPA_aligned.csv`\n",
    " - a copy of the COCA unigrams data for American English: `.\\COCA\\1-gram\\w1.txt`\n",
    " - a copy of the $\\text{SUBTLEX}_{\\text{US}}$ database file `SUBTLEX-US frequency list with PoS information text version.txt` (available from e.g. https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus).\n",
    " - a copy of the data associated with IPhOD (available from http://www.iphod.com/), processed by two previous notebooks (`Notebook 1b`, `Notebook 2b`):\n",
    "   - `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv`\n",
    " - files from `Notebook 2b` and `Notebook 3a` indicating what stressed and unstressed triphones are in each of the two lexicons and what stressed and unstressed stimuli triphones are constructable from gating data aligned with each of the two lexicons:\n",
    "  - These outputs of `Notebook 2a` are required:\n",
    "     - `Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `IPhOD-aligned_destressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `IPhOD-aligned_stressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `Hammond-aligned_destressed stimuli illegal diphones.txt`\n",
    "     - `IPhOD-aligned_destressed stimuli illegal diphones.txt`\n",
    "     - `IPhOD-aligned_stressed stimuli illegal diphones.txt`\n",
    "\n",
    "This last set of files are required because use of either lexicon with a triphone noise model requires words to be removed if they contain triphones whose channel distribution can't be estimated or modeled from the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can produce twelve .json files, each containing a list of dictionaries mapping wordforms to frequencies:\n",
    " 1. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 2. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json`\n",
    " 3. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 4. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_noBadTriphones_phonWordToProb.json`\n",
    " 5. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 6. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_hasBadTriphones_phonWordToProb.json`\n",
    " 7. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 8. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_hasBadTriphones_phonWordToProb.json`\n",
    " 9. `Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 10. `Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json`\n",
    " 11. `Hammond_newdic_IPA_aligned_destressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 12. `Hammond_newdic_IPA_aligned_destressed_hasBadTriphones_phonWordToProb.json`\n",
    "\n",
    " \n",
    "That is, for each of two lexicon datasources there are two choices with respect to removing wordforms containing triphones that the aligned triphone channel distribution cannot model, two choices with respect to exporting probabilities or negative log probabilities, and for IPhOD there are two choices with respect to stress (stressed wordform transcriptions vs. destressed wordform transcriptions). (**FIXME/TODO:** For the time being I do not have a stressed version of the aligned Hammond lexicon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:21.846045Z",
     "start_time": "2019-02-14T20:48:21.840004Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:22.179615Z",
     "start_time": "2019-02-14T20:48:22.177400Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def union(Ss):\n",
    "    return reduce(set.union, Ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:22.621288Z",
     "start_time": "2019-02-14T20:48:22.615444Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def getRandomKey(a_dict, printKey = False):\n",
    "    randKey = random.choice(list(a_dict.keys()))\n",
    "    if printKey:\n",
    "        print('Random key: {0}'.format(randKey))\n",
    "    return randKey\n",
    "\n",
    "def testRandomKey(a_dict, printKey = True, printVal = True):\n",
    "    randKey = getRandomKey(a_dict)\n",
    "    if printKey:\n",
    "        print('Random key: {0}'.format(randKey))\n",
    "    if printVal:\n",
    "        print('value ⟶ {0}'.format(a_dict[randKey]))\n",
    "    return {'key': randKey, 'val': a_dict[randKey]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.006771Z",
     "start_time": "2019-02-14T20:48:22.998494Z"
    }
   },
   "outputs": [],
   "source": [
    "def tupleToDottedString(pair): \n",
    "    return '.'.join(pair)\n",
    "\n",
    "def dottedStringToTuple(s): \n",
    "    return tuple(s.split('.'))\n",
    "\n",
    "t2ds = tupleToDottedString\n",
    "ds2t = dottedStringToTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.401888Z",
     "start_time": "2019-02-14T20:48:23.396703Z"
    }
   },
   "outputs": [],
   "source": [
    "def importSeqs(seq_fn):\n",
    "    phoneSeqsAsStr = []\n",
    "    with open(seq_fn, 'r') as the_file:\n",
    "        for row in the_file:\n",
    "            phoneSeqsAsStr.append(row.rstrip('\\r\\n'))\n",
    "    return set(phoneSeqsAsStr)\n",
    "\n",
    "def exportSeqs(seq_fn, seqs):\n",
    "    with open(seq_fn, 'w') as the_file:\n",
    "        for seq in seqs:\n",
    "            the_file.write(seq + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.735404Z",
     "start_time": "2019-02-14T20:48:23.729993Z"
    }
   },
   "outputs": [],
   "source": [
    "leftEdge = '⋊'\n",
    "rightEdge = '⋉'\n",
    "edgeSymbols = {leftEdge, rightEdge}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:50:51.739991Z",
     "start_time": "2019-02-14T20:50:51.736208Z"
    }
   },
   "outputs": [],
   "source": [
    "def padInputSequenceWithBoundaries(inputSeq):\n",
    "    temp = list(dottedStringToTuple(inputSeq))\n",
    "    temp = tuple([leftEdge] + temp + [rightEdge])\n",
    "    return tupleToDottedString(temp)\n",
    "\n",
    "def trimBoundariesFromSequence(seq):\n",
    "    temp = list(dottedStringToTuple(seq))\n",
    "    if temp[0] == leftEdge:\n",
    "        temp = temp[1:]\n",
    "    if temp[-1] == rightEdge:\n",
    "        temp = temp[:-1]\n",
    "    return tupleToDottedString(tuple(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:24.606535Z",
     "start_time": "2019-02-14T20:48:24.596523Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import takewhile, product\n",
    "\n",
    "def dsToKfactors(k, ds):\n",
    "    seq = ds2t(ds)\n",
    "    l = len(seq)\n",
    "    if k > l:\n",
    "        return tuple()\n",
    "    kFactor_start_indices = takewhile(lambda pair: pair[0] <= l-k, enumerate(seq))\n",
    "    kFactors = tuple(seq[index[0]:index[0]+k] for index in kFactor_start_indices)\n",
    "    return set(map(t2ds, kFactors))\n",
    "\n",
    "def dsTo2factors(ds):\n",
    "    return dsToKfactors(2, ds)\n",
    "def dsTo3factors(ds):\n",
    "    return dsToKfactors(3, ds)\n",
    "\n",
    "def lexiconToKfactors(DSs, k):\n",
    "    myDsToKfactors = lambda ds: dsToKfactors(k, ds)\n",
    "    return union(map(set, map(myDsToKfactors, DSs)))\n",
    "\n",
    "def lexiconTo2factors(DSs):\n",
    "    return union(map(set, map(dsTo2factors, DSs)))\n",
    "def lexiconTo3factors(DSs):\n",
    "    return union(map(set, map(dsTo3factors, DSs)))\n",
    "\n",
    "\n",
    "def compareKfactors(DSs_A, DSs_B, k):\n",
    "    A = lexiconToKfactors(DSs_A, k)\n",
    "    B = lexiconToKfactors(DSs_B, k)\n",
    "    return {\"A == B\":A == B, \"A - B\": A - B, \"B - A\": B - A}\n",
    "\n",
    "def sameKfactors(DSs_A, DSs_B, k):\n",
    "    return compareKfactors(DSs_A, DSs_B, k)[\"A == B\"]\n",
    "\n",
    "def hasIllicitKfactors(W, illicit_k_factors):\n",
    "    if type(W) == str:      \n",
    "        # gather the k-factors into an immutable data structure\n",
    "        illicit_kfs = tuple(illicit_k_factors)\n",
    "        # get the set of k-factor lengths (values of k) among the illicit_kfs\n",
    "        illicit_factor_lengths = set([len(ds2t(kf)) for kf in illicit_kfs])\n",
    "        # map each k to the set of k-factors of dotted string ds\n",
    "        kFactorSets = {kf_l:dsToKfactors(kf_l, W) for kf_l in illicit_factor_lengths}\n",
    "        illegal_kfactors_discovered = tuple(ikf for ikf in illicit_kfs if ikf in kFactorSets[len(ds2t(ikf))])\n",
    "        if illegal_kfactors_discovered == tuple():\n",
    "            return False\n",
    "        return illegal_kfactors_discovered\n",
    "    else:\n",
    "        myFunc = lambda w: hasIllicitKfactors(w, illicit_k_factors)\n",
    "        results = tuple(map(myFunc, W))\n",
    "        if not any(results):\n",
    "            return False\n",
    "        return set(t2ds(each) for each in results if each != False)\n",
    "\n",
    "def sigmaK(sigma, k):\n",
    "    return product(sigma, repeat=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:25.226458Z",
     "start_time": "2019-02-14T20:48:25.221062Z"
    }
   },
   "outputs": [],
   "source": [
    "my_epsilon = 1e-13\n",
    "\n",
    "def norm(dist):\n",
    "    return sum(dist.values())\n",
    "\n",
    "def norms(dists):\n",
    "    return map(norm, dists)\n",
    "\n",
    "def isNormalized(dist, epsilon = None):\n",
    "    if epsilon == None:\n",
    "        epsilon = my_epsilon\n",
    "    return abs(norm(dist) - 1) < my_epsilon\n",
    "\n",
    "def areNormalized(dists, epsilon = None):\n",
    "    if epsilon == None:\n",
    "        epsilon = my_epsilon\n",
    "    return all(map(lambda k: isNormalized(dists[k]), dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:25.665831Z",
     "start_time": "2019-02-14T20:48:25.660317Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "\n",
    "def exportProbDist(fn, dist):\n",
    "    with codecs.open(fn, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dist, f, ensure_ascii = False, indent = 4)\n",
    "        \n",
    "def importProbDist(fn):\n",
    "    with open(fn, encoding='utf-8') as data_file:\n",
    "        dist_in = json.loads(data_file.read())\n",
    "    return dist_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:26.310348Z",
     "start_time": "2019-02-14T20:48:26.300407Z"
    }
   },
   "outputs": [],
   "source": [
    "diphone_analyses = ('destressed stimuli', 'stressed stimuli', 'destressed response')\n",
    "\n",
    "def importNphoneAnalysis(N, which_align):\n",
    "    assert which_align in {'unaligned', 'Hammond-aligned', 'IPhOD-aligned'}\n",
    "    assert N in {1,2,3}\n",
    "\n",
    "    which_infix = {1:'',\n",
    "                   2:'',\n",
    "                   3:'diphone-based'}[N]\n",
    "    which_suffix = {1:{'licit':'',\n",
    "                       'illicit':''},\n",
    "                    2:{'licit':'',\n",
    "                       'illicit':'illegal'},\n",
    "                    3:{'licit':'constructible',\n",
    "                       'illicit':'illegal'}}[N]\n",
    "    which_n = {1:'uniphones',\n",
    "               2:'diphones',\n",
    "               3:'triphones'}[N]\n",
    "    file_ext = '.txt'\n",
    "\n",
    "    which_licit = {1:('licit',),\n",
    "                   2:('licit', 'illicit'),\n",
    "                   3:('licit', 'illicit')}[N]\n",
    "\n",
    "    which_stress_which_diph = diphone_analyses\n",
    "\n",
    "    analysis = dict()\n",
    "    for each_licit in which_licit:\n",
    "#         print('each_licit = {0}'.format(each_licit))\n",
    "        analysis[each_licit] = dict()\n",
    "        for each_stress_each_diph in which_stress_which_diph:\n",
    "#             print('each_stress_each_diph = {0}'.format(each_stress_each_diph))\n",
    "            my_suff = ' '.join([each for each in [each_stress_each_diph, which_infix, which_suffix[each_licit], which_n] if each != ''])\n",
    "            analysis_fn = which_align + '_' + my_suff + file_ext\n",
    "#             analysis_fn = which_align + '_' + ' '.join([each_stress_each_diph, which_infix, which_suffix[each_licit], which_n]) + file_ext\n",
    "            print('Importing: ' + analysis_fn)\n",
    "            analysis[each_licit][each_stress_each_diph] = importSeqs(analysis_fn)\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:26.839321Z",
     "start_time": "2019-02-14T20:48:26.833448Z"
    }
   },
   "outputs": [],
   "source": [
    "def project_dict(the_dict, keys_to_keep):\n",
    "    new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "    return new_dict\n",
    "\n",
    "def edit_dict(the_dict, the_key, the_new_value):\n",
    "    '''\n",
    "    Composable (because it returns a value) but stateful(= in-place) dictionary update.\n",
    "    '''\n",
    "    the_dict.update({the_key: the_new_value})\n",
    "    return the_dict\n",
    "\n",
    "def modify_dict(the_dict, the_key, the_new_value):\n",
    "    '''\n",
    "    Composable and (naively-implemented) non-mutating dictionary update.\n",
    "    '''\n",
    "    new_dict = {k:the_dict[k] for k in the_dict}\n",
    "    new_dict.update({the_key: the_new_value})\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:27.386353Z",
     "start_time": "2019-02-14T20:48:27.382006Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import localtime, strftime\n",
    "def stamp():\n",
    "    return strftime('%H:%M:%S', localtime())\n",
    "\n",
    "def processDataWProgressUpdates(f, data):\n",
    "    print('Start @ {0}'.format(stamp()))\n",
    "    l = len(data)\n",
    "    benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "    benchmarkIndices = [round(each/100.0 * l) for each in benchmarkPercentages]\n",
    "    for i, d in enumerate(data):\n",
    "        if i in benchmarkIndices:\n",
    "            print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, l, i/l, d, stamp()))\n",
    "        f(d)\n",
    "    print('Finish @ {0}'.format(stamp()))\n",
    "        \n",
    "def constructDictWProgressUpdates(f, data, a_dict):\n",
    "    def g(d):\n",
    "        a_dict.update({d:f(d)})\n",
    "    processDataWProgressUpdates(g, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one of the two lines below uncommented to determine whether or not wordforms with unmodelable triphones will be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:29.798881Z",
     "start_time": "2019-02-14T20:48:29.793300Z"
    }
   },
   "outputs": [],
   "source": [
    "# which_filter = 'hasBadTriphones'\n",
    "which_filter = 'noBadTriphones'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one of the two lines uncommented to determine whether wordforms in the output files will have stressed or unstressed representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:30.492846Z",
     "start_time": "2019-02-14T20:48:30.486974Z"
    }
   },
   "outputs": [],
   "source": [
    "which_stress = 'destressed'\n",
    "# which_stress = 'stressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:30.840578Z",
     "start_time": "2019-02-14T20:48:30.832268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed_noBadTriphones'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which = which_stress + '_' + which_filter\n",
    "which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:31.599827Z",
     "start_time": "2019-02-14T20:48:31.597018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:31.967077Z",
     "start_time": "2019-02-14T20:48:31.959422Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hammond's newdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:32.929586Z",
     "start_time": "2019-02-14T20:48:32.807783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hammond-aligned_destressed_fourCousins.json\r\n",
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      " Hammond-aligned_destressed_oneCousins.json\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat012X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3X_hat012Y012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012X012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pYX.json'\r\n",
      "'Hammond-aligned_destressed pX0i.json'\r\n",
      "'Hammond-aligned_destressed pXi.json'\r\n",
      "'Hammond-aligned_destressed pXiPrimeX0i.json'\r\n",
      "'Hammond-aligned_destressed pX.json'\r\n",
      "'Hammond-aligned_destressed pXjX.json'\r\n",
      "'Hammond-aligned_destressed pxX0i.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      " Hammond-aligned_destressed_threeCousins.json\r\n",
      " Hammond-aligned_destressed_twoCousins.json\r\n",
      "'Hammond-aligned_destressed x0iToWs.json'\r\n",
      " Hammond-aligned_destressed_zeroCousins.json\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:33.262818Z",
     "start_time": "2019-02-14T20:48:33.257982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_fn = 'Hammond_newdic_IPA_aligned.csv'\n",
    "hammond_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:34.097688Z",
     "start_time": "2019-02-14T20:48:34.022902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['Transcription', 'stressInfoA', 'stressInfoB', 'Orthography', 'Frequency', 'PoSs'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'ə'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_newdic = []\n",
    "with open(hammond_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        hammond_newdic.append(row)\n",
    "\n",
    "len(hammond_newdic)\n",
    "hammond_newdic[0].keys()\n",
    "hammond_newdic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:34.537359Z",
     "start_time": "2019-02-14T20:48:34.447568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing: Hammond-aligned_destressed stimuli diphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli illegal diphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli illegal diphones.txt\n",
      "Importing: Hammond-aligned_destressed response illegal diphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphone-based illegal triphones.txt\n"
     ]
    }
   ],
   "source": [
    "diphoneAnalysis_h = importNphoneAnalysis(2, 'Hammond-aligned')\n",
    "triphoneAnalysis_h = importNphoneAnalysis(3, 'Hammond-aligned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBTLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:36.304146Z",
     "start_time": "2019-02-14T20:48:35.871970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['Word', 'FREQcount', 'CDcount', 'FREQlow', 'Cdlow', 'SUBTLWF', 'Lg10WF', 'SUBTLCD', 'Lg10CD', 'Dom_PoS_SUBTLEX', 'Freq_dom_PoS_SUBTLEX', 'Percentage_dom_PoS', 'All_PoS_SUBTLEX', 'All_freqs_SUBTLEX'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Word', 'a'),\n",
       "             ('FREQcount', '1041179'),\n",
       "             ('CDcount', '8382'),\n",
       "             ('FREQlow', '976941'),\n",
       "             ('Cdlow', '8380'),\n",
       "             ('SUBTLWF', '20415.27'),\n",
       "             ('Lg10WF', '6.0175'),\n",
       "             ('SUBTLCD', '99.93'),\n",
       "             ('Lg10CD', '3.9234'),\n",
       "             ('Dom_PoS_SUBTLEX', 'Article'),\n",
       "             ('Freq_dom_PoS_SUBTLEX', '993445'),\n",
       "             ('Percentage_dom_PoS', '0.96'),\n",
       "             ('All_PoS_SUBTLEX',\n",
       "              'Article.Adverb.Letter.To.Noun.Preposition.Adjective'),\n",
       "             ('All_freqs_SUBTLEX', '993445.33186.6441.744.257.52.5')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtlex_us_db_fn = 'SUBTLEX-US frequency list with PoS information text version.txt'\n",
    "subtlex = []\n",
    "with open(subtlex_us_db_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        subtlex.append(row)\n",
    "\n",
    "len(subtlex)\n",
    "subtlex[0].keys()\n",
    "subtlex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:36.720476Z",
     "start_time": "2019-02-14T20:48:36.668054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWords_SX = [r['Word'] for r in subtlex]\n",
    "len(orthWords_SX)\n",
    "len(set(orthWords_SX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:06:05.829215Z",
     "start_time": "2019-02-14T19:00:06.374684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start @ 11:00:06\n",
      "743 | 743/74286 = 0.010001884608136122 | adjournment | 11:00:10\n",
      "1486 | 1486/74286 = 0.020003769216272244 | alec | 11:00:13\n",
      "2229 | 2229/74286 = 0.030005653824408368 | angleworm | 11:00:17\n",
      "3714 | 3714/74286 = 0.04999596155399402 | attaching | 11:00:24\n",
      "7429 | 7429/74286 = 0.10000538459467463 | boulders | 11:00:42\n",
      "14857 | 14857/74286 = 0.1999973077026627 | cpu | 11:01:18\n",
      "22286 | 22286/74286 = 0.30000269229733734 | establishes | 11:01:53\n",
      "29714 | 29714/74286 = 0.3999946154053254 | having | 11:02:29\n",
      "37143 | 37143/74286 = 0.5 | licked | 11:03:05\n",
      "44572 | 44572/74286 = 0.6000053845946747 | ontario | 11:03:41\n",
      "52000 | 52000/74286 = 0.6999973077026627 | raffia | 11:04:17\n",
      "59429 | 59429/74286 = 0.8000026922973373 | skag | 11:04:53\n",
      "66857 | 66857/74286 = 0.8999946154053253 | topmost | 11:05:29\n",
      "70572 | 70572/74286 = 0.950004038446006 | ust | 11:05:47\n",
      "71315 | 71315/74286 = 0.9600059230541421 | vigilantes | 11:05:51\n",
      "72057 | 72057/74286 = 0.9699943461755917 | warrior | 11:05:55\n",
      "72800 | 72800/74286 = 0.9799962307837278 | whithersoever | 11:05:58\n",
      "73543 | 73543/74286 = 0.9899981153918639 | worm | 11:06:02\n",
      "Finish @ 11:06:05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49719560.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rowWword(w): \n",
    "    return [r for r in subtlex if r['Word'] == w][0]\n",
    "# orthwordToSXF = {w:rowWword(w)['FREQcount'] for w in orthWords_SX}\n",
    "\n",
    "orthwordToSXF = dict()\n",
    "def wToSXF(w):\n",
    "    return float(rowWword(w)['FREQcount'])\n",
    "\n",
    "#13m on an Intel i7-5650U (MacBook Air)\n",
    "constructDictWProgressUpdates(wToSXF, orthWords_SX, orthwordToSXF)\n",
    "\n",
    "sum(orthwordToSXF.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:40.511013Z",
     "start_time": "2019-02-14T20:48:40.505223Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:40.900822Z",
     "start_time": "2019-02-14T20:48:40.892102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:41.765918Z",
     "start_time": "2019-02-14T20:48:41.411667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram\tREADME\n",
      "w1cs_c.txt  w1cs.txt  w1.txt\n",
      "Single words occurring three times or more in the\n",
      "Corpus of Contemporary American English (http://corpus.byu.edu/coca)\n",
      "Not case sensitive; no part of speech\n",
      "\n",
      "\n",
      "For more information on full word frequency and n-grams lists, see:\n",
      "http://www.wordfrequency.info\n",
      "http://www.ngrams.info\n",
      "\n",
      "\n",
      "freq\tword1\t\n",
      "-----\t-----\n",
      "\n",
      "\n",
      "9738579\ta\n",
      "40\ta&e\n",
      "3\ta&j\n",
      "395\ta&m\n",
      "7\ta&m-commerce\n",
      "16\ta&m-corpus\n"
     ]
    }
   ],
   "source": [
    "!ls COCA\n",
    "!ls COCA/1-gram\n",
    "!head -20 ./COCA/1-gram/w1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:42.588588Z",
     "start_time": "2019-02-14T20:48:42.217863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Single words occurring three times or more in the',),\n",
       " ('Corpus of Contemporary American English (http://corpus.byu.edu/coca)',),\n",
       " ('Not case sensitive; no part of speech',),\n",
       " ('',),\n",
       " ('',),\n",
       " ('For more information on full word frequency and n-grams lists, see:',),\n",
       " ('http://www.wordfrequency.info',),\n",
       " ('http://www.ngrams.info',),\n",
       " ('',),\n",
       " ('',),\n",
       " ('freq', 'word1', ''),\n",
       " ('-----', '-----'),\n",
       " ('',),\n",
       " ('',),\n",
       " ('9738579', 'a'),\n",
       " ('40', 'a&e'),\n",
       " ('3', 'a&j'),\n",
       " ('395', 'a&m'),\n",
       " ('7', 'a&m-commerce'),\n",
       " ('16', 'a&m-corpus')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_unigrams_rel_path = './COCA/1-gram/'\n",
    "coca_unigrams_fn = 'w1.txt'\n",
    "\n",
    "coca_unigrams = []\n",
    "\n",
    "with open(coca_unigrams_rel_path + coca_unigrams_fn, 'r') as the_file:\n",
    "    for row in the_file:\n",
    "        coca_unigrams.append(tuple(row.rstrip('\\r\\n').split('\\t')))\n",
    "\n",
    "coca_unigrams[0:20]\n",
    "coca_unigrams = coca_unigrams[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:43.302368Z",
     "start_time": "2019-02-14T20:48:43.296643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('40', 'a&e'),\n",
       " ('3', 'a&j'),\n",
       " ('395', 'a&m'),\n",
       " ('7', 'a&m-commerce'),\n",
       " ('16', 'a&m-corpus')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "486687"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_unigrams[:5]\n",
    "len(coca_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:43.981636Z",
     "start_time": "2019-02-14T20:48:43.975834Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:45.274020Z",
     "start_time": "2019-02-14T20:48:44.935764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a&e', '40')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(reversed(coca_unigrams[0]))\n",
    "\n",
    "coca_unigrams = [tuple(reversed(pair)) for pair in coca_unigrams]\n",
    "coca_unigrams = list(map(lambda pair: (pair[0], float(pair[1])),\n",
    "                         coca_unigrams))\n",
    "coca_unigrams = dict(coca_unigrams)\n",
    "coca_unigram_counts = Counter(coca_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:45.572147Z",
     "start_time": "2019-02-14T20:48:45.566943Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:46.102274Z",
     "start_time": "2019-02-14T20:48:46.099211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'detente'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "316.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_orthWord = choice(hammond_newdic)['Orthography']\n",
    "random_orthWord\n",
    "coca_unigram_counts[random_orthWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:48.345055Z",
     "start_time": "2019-02-14T20:48:48.338162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'410,428,505.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(sum(coca_unigram_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPhOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:20:03.254507Z",
     "start_time": "2019-02-14T19:20:03.136126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words_IPA.csv       IPhOD2_Words.txt\r\n",
      "IPhOD2_Words_IPA_prob.csv  IPhODv2.0_REALS.zip\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:20:05.705211Z",
     "start_time": "2019-02-14T19:20:05.701705Z"
    }
   },
   "outputs": [],
   "source": [
    "# my_lexicon_fn = 'IPhOD2_Words.txt'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA.csv'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA_prob.csv'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA_prob_caughtCotMerged.csv'\n",
    "my_lexicon_fn = 'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:20:06.509376Z",
     "start_time": "2019-02-14T19:20:06.502734Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-94b5a33299c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_lexicon_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmy_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv'"
     ]
    }
   ],
   "source": [
    "lexicon = []\n",
    "with open(my_lexicon_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        lexicon.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:20:07.067972Z",
     "start_time": "2019-02-14T19:20:07.061308Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e76132361cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lexicon[0].keys()\n",
    "lexicon[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:08.601176Z",
     "start_time": "2018-12-05T04:21:08.388030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing: IPhOD-aligned_destressed stimuli diphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli illegal diphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli illegal diphones.txt\n",
      "Importing: IPhOD-aligned_destressed response illegal diphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphone-based illegal triphones.txt\n"
     ]
    }
   ],
   "source": [
    "#FIXME load illegal diphones and triphones\n",
    "diphoneAnalysis_i = importNphoneAnalysis(2, 'IPhOD-aligned')\n",
    "triphoneAnalysis_i = importNphoneAnalysis(3, 'IPhOD-aligned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPhOD processing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a json file mapping phonological wordforms to frequencies or probabilities for a word recognition model we need to\n",
    " 1. Remove data we don't want or can't use in the word recognition model.\n",
    " 2. Choose a set of phonological wordforms (unstressed or stressed) to map to frequencies/probabilities.\n",
    " 3. Filter the phonological wordforms based on the illegal diphones and triphones.\n",
    " 3. Calculate frequencies/probabilities, dealing with the vagaries of IPhOD (and remembering to normalize):\n",
    "   - The wordform frequency of each entry is a corpus frequency of the associated orthographic word.\n",
    "   - Some distinct orthographic wordforms share a phonological wordform.\n",
    " 4. Phonological wordforms need to have word edge symbols added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data that definitely won't end up in the lexicon distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T01:44:32.951773Z",
     "start_time": "2018-12-05T01:44:32.929125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 123, 'Job': 'clerk'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def project_dict(the_dict, keys_to_keep):\n",
    "#     new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "#     return new_dict\n",
    "# project_dict({'Name':'Joe','ID':123,'Job':'clerk'},['Job','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:09.500381Z",
     "start_time": "2018-12-05T04:21:08.603749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim_lexicon = list(map(lambda d: project_dict(d, ['StTrn', 'UnTrn', 'Word', 'SFreq','Prob','Nlprob']), lexicon))\n",
    "slim_lexicon[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words with triphones and diphones that a channel distribution isn't definable for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:09.512299Z",
     "start_time": "2018-12-05T04:21:09.503734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16389"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illegalDiphones_i = diphoneAnalysis_i['illicit'][which_stress + ' stimuli']\n",
    "len(illegalDiphones_i)\n",
    "illegalTriphones_i = triphoneAnalysis_i['illicit'][which_stress + ' stimuli']\n",
    "len(illegalTriphones_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:09.521815Z",
     "start_time": "2018-12-05T04:21:09.514780Z"
    }
   },
   "outputs": [],
   "source": [
    "def badNphone(nphone, n, badNphones):\n",
    "    if n == 2:\n",
    "        return nphone in badNphones\n",
    "    if n == 3:\n",
    "        return nphone in badNphones\n",
    "    raise Exception('n must be 2 or 3.')\n",
    "\n",
    "def containsBadNphones(row, n, getNphones, badNphones):\n",
    "    phs = getNphones(row, n)\n",
    "    return any([badNphone(ph, n, badNphones) for ph in phs])\n",
    "\n",
    "def rowsWBadNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if containsBadNphones(r, n, getNphones, badNphones)]\n",
    "\n",
    "def observedBadNphones(rows, n, getNphones, badNphones):\n",
    "    return union([getNphones(r, n) for r in rows if containsBadNphones(r, n, getNphones, badNphones)])\n",
    "\n",
    "def onlyRowsWithGoodNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if not containsBadNphones(r, n, getNphones, badNphones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:09.533118Z",
     "start_time": "2018-12-05T04:21:09.525031Z"
    }
   },
   "outputs": [],
   "source": [
    "badDiphones = set()\n",
    "badTriphones = set()\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    badDiphones = set()\n",
    "    badTriphones = set()\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    badDiphones = set(illegalDiphones_i)\n",
    "    badTriphones = set(illegalTriphones_i)\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} instead\".format(which_filter))\n",
    "    \n",
    "def getNphones_i(IPhOD_row, n):\n",
    "#     strTrnTuple = dottedStringToTuple(IPhOD_row['StTrn'])\n",
    "#     unTrnTuple = dottedStringToTuple(IPhOD_row['UnTrn'])\n",
    "\n",
    "    strNFactors = dsToKfactors(n, IPhOD_row['StTrn'])\n",
    "    unstrNFactors = dsToKfactors(n, IPhOD_row['UnTrn'])\n",
    "#     print(unstrNFactors)\n",
    "    \n",
    "#     uniqueStrNphones = set(strNFactors)\n",
    "#     uniqueUnstrNphones = set(unstrNFactors)\n",
    "    if which_stress == 'destressed':\n",
    "        return unstrNFactors\n",
    "    elif which_stress == 'stressed':\n",
    "        return strNFactors\n",
    "    else:\n",
    "        raise Exception('Bad which_stress arg.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:10.287752Z",
     "start_time": "2018-12-05T04:21:09.536585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.034776975754210626"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.017018805955438324"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Aaron',\n",
       " 'aberrant',\n",
       " 'acerbic',\n",
       " 'actuarial',\n",
       " 'actuaries',\n",
       " 'actuary',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'aerial',\n",
       " 'aerials',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aerobatics',\n",
       " 'aerobic',\n",
       " 'aerobically',\n",
       " 'aerodromes',\n",
       " 'aerodynamic',\n",
       " 'aerodynamically',\n",
       " 'aerodynamics',\n",
       " 'Aerodyne',\n",
       " 'aeronautical',\n",
       " 'aeronautical',\n",
       " 'Aeronautics',\n",
       " 'aerosol',\n",
       " 'Aerosols',\n",
       " 'aerospace',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'aficionado',\n",
       " 'ageratum',\n",
       " 'agrarian',\n",
       " 'airbag',\n",
       " 'airbags',\n",
       " 'airbase',\n",
       " 'airboat',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aircrafts',\n",
       " 'aircrafts',\n",
       " 'Aircrew',\n",
       " 'airdrop',\n",
       " 'aired',\n",
       " 'Airedale',\n",
       " 'Airedales',\n",
       " 'airfare',\n",
       " 'airfares',\n",
       " 'airfield',\n",
       " 'airfields',\n",
       " 'airflow',\n",
       " 'airfoil',\n",
       " 'airhead',\n",
       " 'airing',\n",
       " 'airless',\n",
       " 'airlift',\n",
       " 'airlifted',\n",
       " 'airlifting',\n",
       " 'Airlifts',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airliners',\n",
       " 'Airlines',\n",
       " 'airlock',\n",
       " 'airlocks',\n",
       " 'airmail',\n",
       " 'Airman',\n",
       " 'airmen',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airship',\n",
       " 'airships',\n",
       " 'airspace',\n",
       " 'airspeed',\n",
       " 'airstrike',\n",
       " 'airstrikes',\n",
       " 'airstrip',\n",
       " 'airstrips',\n",
       " 'airtight',\n",
       " 'airtime',\n",
       " 'airwave',\n",
       " 'airwaves',\n",
       " 'airway',\n",
       " 'Airways',\n",
       " 'airworthy',\n",
       " 'airy',\n",
       " 'Algerians',\n",
       " 'alphanumeric',\n",
       " 'Altair',\n",
       " 'America',\n",
       " 'America',\n",
       " 'American',\n",
       " 'American',\n",
       " 'Americana',\n",
       " 'Americanism',\n",
       " 'Americanization',\n",
       " 'Americanize',\n",
       " 'Americanized',\n",
       " 'Americans',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Americas',\n",
       " 'ancillary',\n",
       " 'Angeline',\n",
       " 'Antares',\n",
       " 'antiaircraft',\n",
       " 'antidisestablishmentarianism',\n",
       " 'anywhere',\n",
       " 'anywhere',\n",
       " 'apothecary',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'Apparently',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'aquarium',\n",
       " 'aquariums',\n",
       " 'Aquarius',\n",
       " 'Ara',\n",
       " 'Arab',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabic',\n",
       " 'Arable',\n",
       " 'Arabs',\n",
       " 'Arabs',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'Ariadne',\n",
       " 'arid',\n",
       " 'arid',\n",
       " 'Ariel',\n",
       " 'Aries',\n",
       " 'aristocracy',\n",
       " 'Aristotle',\n",
       " 'arithmetic',\n",
       " 'Arizona',\n",
       " 'armchair',\n",
       " 'armchairs',\n",
       " 'aromatic',\n",
       " 'arrant',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'arrow',\n",
       " 'arrow',\n",
       " 'Arrowhead',\n",
       " 'Arrowhead',\n",
       " 'arrowheads',\n",
       " 'arrowheads',\n",
       " 'arrowroot',\n",
       " 'arrows',\n",
       " 'arrows',\n",
       " 'Arthurian',\n",
       " 'ary',\n",
       " 'asparagus',\n",
       " 'atmospheric',\n",
       " 'atmospherics',\n",
       " 'austerity',\n",
       " 'authoritarian',\n",
       " 'authoritarianism',\n",
       " 'auxiliary',\n",
       " 'aviary',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'backstairs',\n",
       " 'Banbury',\n",
       " 'barbarian',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbaric',\n",
       " 'barbarity',\n",
       " 'barbarity',\n",
       " 'Barbera',\n",
       " 'Barbero',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bares',\n",
       " 'barest',\n",
       " 'baring',\n",
       " 'Barish',\n",
       " 'barite',\n",
       " 'baritone',\n",
       " 'baritones',\n",
       " 'barium',\n",
       " 'barometric',\n",
       " 'Baron',\n",
       " 'Baron',\n",
       " 'Baroness',\n",
       " 'baronet',\n",
       " 'baronet',\n",
       " 'baronets',\n",
       " 'baronets',\n",
       " 'barons',\n",
       " 'barons',\n",
       " 'barrack',\n",
       " 'barrack',\n",
       " 'barracks',\n",
       " 'barracks',\n",
       " 'barracuda',\n",
       " 'Barragan',\n",
       " 'Barre',\n",
       " 'barrel',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barreled',\n",
       " 'barreling',\n",
       " 'barreling',\n",
       " 'barrels',\n",
       " 'barrels',\n",
       " 'barren',\n",
       " 'barren',\n",
       " 'Barrens',\n",
       " 'Barret',\n",
       " 'barricade',\n",
       " 'barricade',\n",
       " 'barricaded',\n",
       " 'barricaded',\n",
       " 'barricades',\n",
       " 'barricades',\n",
       " 'barrier',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barriers',\n",
       " 'Barringer',\n",
       " 'Barrios',\n",
       " 'barrister',\n",
       " 'barrister',\n",
       " 'barristers',\n",
       " 'barristers',\n",
       " 'Barrow',\n",
       " 'Barrow',\n",
       " 'Barrows',\n",
       " 'Barrows',\n",
       " 'Barry',\n",
       " 'Barry',\n",
       " 'Bavarian',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'bearer',\n",
       " 'bearers',\n",
       " 'bearing',\n",
       " 'bearings',\n",
       " 'bearish',\n",
       " 'bears',\n",
       " 'beneficiaries',\n",
       " 'beneficiary',\n",
       " 'beret',\n",
       " 'beret',\n",
       " 'Berets',\n",
       " 'Berets',\n",
       " 'Bering',\n",
       " 'berries',\n",
       " 'Berrigan',\n",
       " 'Berry',\n",
       " 'Berryman',\n",
       " 'Beryl',\n",
       " 'Beware',\n",
       " 'beyond',\n",
       " 'beyond',\n",
       " 'bier',\n",
       " 'billionaire',\n",
       " 'billionaires',\n",
       " 'blackberries',\n",
       " 'blackberry',\n",
       " 'Blair',\n",
       " 'blare',\n",
       " 'blared',\n",
       " 'blares',\n",
       " 'blaring',\n",
       " 'Bluebeard',\n",
       " 'blueberries',\n",
       " 'blueberry',\n",
       " 'Bolero',\n",
       " 'bracero',\n",
       " 'Breweries',\n",
       " 'buccaneer',\n",
       " 'Buccaneers',\n",
       " 'budgetary',\n",
       " 'Bulgaria',\n",
       " 'Bulgarian',\n",
       " 'Bulgarians',\n",
       " 'burial',\n",
       " 'burials',\n",
       " 'buried',\n",
       " 'buries',\n",
       " 'Burroughs',\n",
       " 'bury',\n",
       " 'burying',\n",
       " 'caballero',\n",
       " 'cairn',\n",
       " 'cairns',\n",
       " 'caldera',\n",
       " 'caldera',\n",
       " 'canaries',\n",
       " 'canary',\n",
       " 'Canberra',\n",
       " 'Canterbury',\n",
       " 'capillaries',\n",
       " 'capillary',\n",
       " 'caramel',\n",
       " 'caramelize',\n",
       " 'caramelized',\n",
       " 'carat',\n",
       " 'carats',\n",
       " 'caravan',\n",
       " 'caravan',\n",
       " 'caravans',\n",
       " 'caravans',\n",
       " 'caravel',\n",
       " 'caraway',\n",
       " 'caraway',\n",
       " 'cardiopulmonary',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'carefree',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'caregiver',\n",
       " 'caregivers',\n",
       " 'careless',\n",
       " 'carelessly',\n",
       " 'carelessness',\n",
       " 'carer',\n",
       " 'cares',\n",
       " 'caretaker',\n",
       " 'caretakers',\n",
       " 'Carey',\n",
       " 'Caribbean',\n",
       " 'Caribe',\n",
       " 'caribou',\n",
       " 'caricature',\n",
       " 'caricaturist',\n",
       " 'Carillon',\n",
       " 'caring',\n",
       " 'Carioca',\n",
       " 'carob',\n",
       " 'Carol',\n",
       " 'Carol',\n",
       " 'Carole',\n",
       " 'Carole',\n",
       " 'Carolina',\n",
       " 'Carolinas',\n",
       " 'Caroline',\n",
       " 'carols',\n",
       " 'Carolyn',\n",
       " 'carotene',\n",
       " 'carousel',\n",
       " 'Carrel',\n",
       " 'carriage',\n",
       " 'carriage',\n",
       " 'carriages',\n",
       " 'carriages',\n",
       " 'carried',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrier',\n",
       " 'carriers',\n",
       " 'carriers',\n",
       " 'carries',\n",
       " 'carries',\n",
       " 'carrion',\n",
       " 'Carroll',\n",
       " 'Carroll',\n",
       " 'carrot',\n",
       " 'carrot',\n",
       " 'carrots',\n",
       " 'carrots',\n",
       " 'Carrousel',\n",
       " 'Carrow',\n",
       " 'carry',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'carrying',\n",
       " 'Caryl',\n",
       " 'cautionary',\n",
       " 'cavalier',\n",
       " 'cemeteries',\n",
       " 'cemetery',\n",
       " 'cemetery',\n",
       " 'centenary',\n",
       " 'Cera',\n",
       " 'cerebral',\n",
       " 'ceremonial',\n",
       " 'ceremonies',\n",
       " 'ceremony',\n",
       " 'Cervantes',\n",
       " 'chair',\n",
       " 'chaired',\n",
       " 'chairing',\n",
       " 'Chairman',\n",
       " 'chairmanship',\n",
       " 'chairmen',\n",
       " 'chairperson',\n",
       " 'chairs',\n",
       " 'chairwoman',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characteristically',\n",
       " 'characteristics',\n",
       " 'characterization',\n",
       " 'characterizations',\n",
       " 'characterize',\n",
       " 'characterized',\n",
       " 'characterizes',\n",
       " 'characterizing',\n",
       " 'characters',\n",
       " 'characters',\n",
       " 'chariot',\n",
       " 'chariots',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'charitable',\n",
       " 'charitably',\n",
       " 'charities',\n",
       " 'charities',\n",
       " 'charity',\n",
       " 'Charon',\n",
       " 'chemotherapy',\n",
       " 'Cher',\n",
       " 'Cherie',\n",
       " 'cherish',\n",
       " 'cherished',\n",
       " 'cherishes',\n",
       " 'cherishing',\n",
       " 'Cherokee',\n",
       " 'Cherokees',\n",
       " 'cherries',\n",
       " 'cherry',\n",
       " 'cherub',\n",
       " 'cherubs',\n",
       " 'Cheung',\n",
       " 'childbearing',\n",
       " 'childcare',\n",
       " 'Chimera',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'clairvoyance',\n",
       " 'clairvoyant',\n",
       " 'Clare',\n",
       " 'Clarence',\n",
       " 'Clarendon',\n",
       " 'clarification',\n",
       " 'clarifications',\n",
       " 'clarified',\n",
       " 'clarifies',\n",
       " 'clarify',\n",
       " 'clarifying',\n",
       " 'clarinet',\n",
       " 'clarinetist',\n",
       " 'Clarion',\n",
       " 'clarity',\n",
       " 'clarity',\n",
       " 'Clary',\n",
       " 'Cleric',\n",
       " 'clerical',\n",
       " 'clerical',\n",
       " 'clerics',\n",
       " 'coherently',\n",
       " 'cometary',\n",
       " 'commentaries',\n",
       " 'commentary',\n",
       " 'commissaries',\n",
       " 'commissary',\n",
       " 'companero',\n",
       " 'comparable',\n",
       " 'comparative',\n",
       " 'comparatively',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'compares',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'comparisons',\n",
       " 'Concerto',\n",
       " 'Concertos',\n",
       " 'concessionary',\n",
       " 'concierge',\n",
       " 'confectionary',\n",
       " 'confectionery',\n",
       " 'confrere',\n",
       " 'confreres',\n",
       " 'consortium',\n",
       " 'Constabulary',\n",
       " 'contemporaries',\n",
       " 'contemporary',\n",
       " 'contrariness',\n",
       " 'contrary',\n",
       " 'contrary',\n",
       " 'cookware',\n",
       " 'copyright',\n",
       " 'copyrighted',\n",
       " 'copyrights',\n",
       " 'copywriter',\n",
       " 'copywriters',\n",
       " 'corollary',\n",
       " 'coronary',\n",
       " 'Corsair',\n",
       " 'corticosteroids',\n",
       " 'counterrevolutionary',\n",
       " 'counterrevolutionary',\n",
       " 'counterterrorism',\n",
       " 'counterterrorism',\n",
       " 'Counterterrorist',\n",
       " 'Counterterrorist',\n",
       " 'cranberries',\n",
       " 'cranberry',\n",
       " 'culinary',\n",
       " 'curry',\n",
       " 'customarily',\n",
       " 'customary',\n",
       " 'dairies',\n",
       " 'dairy',\n",
       " 'dare',\n",
       " 'dared',\n",
       " 'Daredevil',\n",
       " 'daredevils',\n",
       " 'dares',\n",
       " 'daresay',\n",
       " 'daring',\n",
       " 'daycare',\n",
       " 'deathbed',\n",
       " 'debonair',\n",
       " 'declarant',\n",
       " 'declaratory',\n",
       " 'declare',\n",
       " 'declared',\n",
       " 'declares',\n",
       " 'declaring',\n",
       " 'Delaware',\n",
       " 'Delftware',\n",
       " 'demerit',\n",
       " 'demerits',\n",
       " 'denarii',\n",
       " 'Deoxyribonucleic',\n",
       " 'deregulate',\n",
       " 'deregulating',\n",
       " 'deregulation',\n",
       " 'derelict',\n",
       " 'dereliction',\n",
       " 'derelicts',\n",
       " 'derelicts',\n",
       " 'derivation',\n",
       " 'Derrick',\n",
       " 'derringer',\n",
       " 'Derry',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairs',\n",
       " 'deuterium',\n",
       " 'dewberry',\n",
       " 'dexterity',\n",
       " 'dexterity',\n",
       " 'dictionaries',\n",
       " 'dictionary',\n",
       " 'dietary',\n",
       " 'dignitaries',\n",
       " 'dignitary',\n",
       " 'dinnerware',\n",
       " 'directing',\n",
       " 'direction',\n",
       " 'directional',\n",
       " 'directionless',\n",
       " 'directions',\n",
       " 'directive',\n",
       " 'directives',\n",
       " 'directly',\n",
       " 'directness',\n",
       " 'director',\n",
       " 'Directorate',\n",
       " 'directorial',\n",
       " 'directories',\n",
       " 'directors',\n",
       " 'directorship',\n",
       " 'directory',\n",
       " 'directs',\n",
       " 'disappear',\n",
       " 'disappearance',\n",
       " 'disappearances',\n",
       " 'disappeared',\n",
       " 'disappearing',\n",
       " 'disappears',\n",
       " 'disciplinarian',\n",
       " 'disciplinary',\n",
       " 'discretionary',\n",
       " 'disparage',\n",
       " 'disparaged',\n",
       " 'disparages',\n",
       " 'disparaging',\n",
       " 'disparagingly',\n",
       " 'disparate',\n",
       " 'disparities',\n",
       " 'disparity',\n",
       " 'dispensary',\n",
       " 'disrepair',\n",
       " 'dissimilarity',\n",
       " 'diuretic',\n",
       " 'diuretics',\n",
       " 'diversionary',\n",
       " 'diversionary',\n",
       " 'doctrinaire',\n",
       " 'doer',\n",
       " 'doorknob',\n",
       " 'doorknobs',\n",
       " 'downstairs',\n",
       " 'dram',\n",
       " 'drams',\n",
       " 'dromedary',\n",
       " 'Drury',\n",
       " 'dysentery',\n",
       " 'earless',\n",
       " 'earmark',\n",
       " 'earmarked',\n",
       " 'earmarks',\n",
       " 'earmuff',\n",
       " 'earmuffs',\n",
       " 'earphone',\n",
       " 'earphones',\n",
       " 'earpiece',\n",
       " 'earpieces',\n",
       " 'earplug',\n",
       " 'earplugs',\n",
       " 'earring',\n",
       " 'earrings',\n",
       " 'ears',\n",
       " 'earshot',\n",
       " 'earthbound',\n",
       " 'earwax',\n",
       " 'egalitarian',\n",
       " 'elsewhere',\n",
       " 'Embarcadero',\n",
       " 'embarrass',\n",
       " 'embarrassed',\n",
       " 'embarrasses',\n",
       " 'embarrassing',\n",
       " 'embarrassingly',\n",
       " 'embarrassment',\n",
       " 'embarrassments',\n",
       " 'emeritus',\n",
       " 'emissaries',\n",
       " 'emissary',\n",
       " 'endear',\n",
       " 'endeared',\n",
       " 'endearing',\n",
       " 'engineered',\n",
       " 'ensnare',\n",
       " 'ensnared',\n",
       " 'ensnares',\n",
       " 'era',\n",
       " 'erasable',\n",
       " 'erase',\n",
       " 'erased',\n",
       " 'eraser',\n",
       " 'erasers',\n",
       " 'erases',\n",
       " 'erasing',\n",
       " 'Eric',\n",
       " 'Erica',\n",
       " 'Erika',\n",
       " 'Erin',\n",
       " 'erode',\n",
       " 'eroding',\n",
       " 'errand',\n",
       " 'errands',\n",
       " 'errant',\n",
       " 'erratically',\n",
       " 'erred',\n",
       " 'erring',\n",
       " 'erroneous',\n",
       " 'erroneously',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'errs',\n",
       " 'ersatz',\n",
       " 'ersatz',\n",
       " 'erudite',\n",
       " 'erudition',\n",
       " 'erupt',\n",
       " 'erupted',\n",
       " 'erupted',\n",
       " 'erupting',\n",
       " 'eruption',\n",
       " 'eruptions',\n",
       " 'eruptive',\n",
       " 'erupts',\n",
       " 'erupts',\n",
       " 'Escudero',\n",
       " 'esoteric',\n",
       " 'esoteric',\n",
       " 'estuary',\n",
       " 'Euro',\n",
       " 'eurodollar',\n",
       " 'everywhere',\n",
       " 'everywhere',\n",
       " 'evidentiary',\n",
       " 'evidentiary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'exclusionary',\n",
       " 'expeditionary',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimental',\n",
       " 'experimentally',\n",
       " 'experimentally',\n",
       " 'experimentation',\n",
       " 'experimented',\n",
       " 'experimenter',\n",
       " 'experimenting',\n",
       " 'experimenting',\n",
       " 'experiments',\n",
       " 'extramarital',\n",
       " 'extraordinaire',\n",
       " 'extraordinarily',\n",
       " 'extraordinary',\n",
       " 'extraordinary',\n",
       " 'eyewear',\n",
       " 'Eyrie',\n",
       " 'Fahrenheit',\n",
       " 'fair',\n",
       " 'Fairbanks',\n",
       " 'faire',\n",
       " 'fairer',\n",
       " 'fairest',\n",
       " 'fairground',\n",
       " 'fairgrounds',\n",
       " 'fairies',\n",
       " 'fairly',\n",
       " 'fairness',"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 13465 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasBadTriphs = rowsWBadNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "len(hasBadTriphs)\n",
    "len(lexicon)\n",
    "len(hasBadTriphs)/len(lexicon) #proportion of the lexicon (by type) that has bad triphs\n",
    "sum([float(entry['Prob']) for entry in hasBadTriphs]) #prob mass of bad triph words by token\n",
    "list(map(lambda r: r['Word'], hasBadTriphs[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:11.096365Z",
     "start_time": "2018-12-05T04:21:10.290212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03492504164353137"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.01714713558335328"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Aaron',\n",
       " 'aberrant',\n",
       " 'acerbic',\n",
       " 'actuarial',\n",
       " 'actuaries',\n",
       " 'actuary',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'Aer',\n",
       " 'aerial',\n",
       " 'aerials',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aerobatics',\n",
       " 'aerobic',\n",
       " 'aerobically',\n",
       " 'aerodromes',\n",
       " 'aerodynamic',\n",
       " 'aerodynamically',\n",
       " 'aerodynamics',\n",
       " 'Aerodyne',\n",
       " 'aeronautical',\n",
       " 'aeronautical',\n",
       " 'Aeronautics',\n",
       " 'aerosol',\n",
       " 'Aerosols',\n",
       " 'aerospace',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'aficionado',\n",
       " 'ageratum',\n",
       " 'agrarian',\n",
       " 'air',\n",
       " 'airbag',\n",
       " 'airbags',\n",
       " 'airbase',\n",
       " 'airboat',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aircrafts',\n",
       " 'aircrafts',\n",
       " 'Aircrew',\n",
       " 'airdrop',\n",
       " 'aired',\n",
       " 'Airedale',\n",
       " 'Airedales',\n",
       " 'airfare',\n",
       " 'airfares',\n",
       " 'airfield',\n",
       " 'airfields',\n",
       " 'airflow',\n",
       " 'airfoil',\n",
       " 'airhead',\n",
       " 'airing',\n",
       " 'airless',\n",
       " 'airlift',\n",
       " 'airlifted',\n",
       " 'airlifting',\n",
       " 'Airlifts',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airliners',\n",
       " 'Airlines',\n",
       " 'airlock',\n",
       " 'airlocks',\n",
       " 'airmail',\n",
       " 'Airman',\n",
       " 'airmen',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airship',\n",
       " 'airships',\n",
       " 'airspace',\n",
       " 'airspeed',\n",
       " 'airstrike',\n",
       " 'airstrikes',\n",
       " 'airstrip',\n",
       " 'airstrips',\n",
       " 'airtight',\n",
       " 'airtime',\n",
       " 'airwave',\n",
       " 'airwaves',\n",
       " 'airway',\n",
       " 'Airways',\n",
       " 'airworthy',\n",
       " 'airy',\n",
       " 'Algerians',\n",
       " 'alphanumeric',\n",
       " 'Altair',\n",
       " 'America',\n",
       " 'America',\n",
       " 'American',\n",
       " 'American',\n",
       " 'Americana',\n",
       " 'Americanism',\n",
       " 'Americanization',\n",
       " 'Americanize',\n",
       " 'Americanized',\n",
       " 'Americans',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Americas',\n",
       " 'ancillary',\n",
       " 'Angeline',\n",
       " 'Antares',\n",
       " 'antiaircraft',\n",
       " 'antidisestablishmentarianism',\n",
       " 'anywhere',\n",
       " 'anywhere',\n",
       " 'apothecary',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'Apparently',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'aquarium',\n",
       " 'aquariums',\n",
       " 'Aquarius',\n",
       " 'Ara',\n",
       " 'Arab',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabic',\n",
       " 'Arable',\n",
       " 'Arabs',\n",
       " 'Arabs',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'Ariadne',\n",
       " 'arid',\n",
       " 'arid',\n",
       " 'Ariel',\n",
       " 'Aries',\n",
       " 'aristocracy',\n",
       " 'Aristotle',\n",
       " 'arithmetic',\n",
       " 'Arizona',\n",
       " 'armchair',\n",
       " 'armchairs',\n",
       " 'aromatic',\n",
       " 'arrant',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'arrow',\n",
       " 'arrow',\n",
       " 'Arrowhead',\n",
       " 'Arrowhead',\n",
       " 'arrowheads',\n",
       " 'arrowheads',\n",
       " 'arrowroot',\n",
       " 'arrows',\n",
       " 'arrows',\n",
       " 'Arthurian',\n",
       " 'ary',\n",
       " 'asparagus',\n",
       " 'atmospheric',\n",
       " 'atmospherics',\n",
       " 'austerity',\n",
       " 'authoritarian',\n",
       " 'authoritarianism',\n",
       " 'auxiliary',\n",
       " 'aviary',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'backstairs',\n",
       " 'Banbury',\n",
       " 'barbarian',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbaric',\n",
       " 'barbarity',\n",
       " 'barbarity',\n",
       " 'Barbera',\n",
       " 'Barbero',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bares',\n",
       " 'barest',\n",
       " 'baring',\n",
       " 'Barish',\n",
       " 'barite',\n",
       " 'baritone',\n",
       " 'baritones',\n",
       " 'barium',\n",
       " 'barometric',\n",
       " 'Baron',\n",
       " 'Baron',\n",
       " 'Baroness',\n",
       " 'baronet',\n",
       " 'baronet',\n",
       " 'baronets',\n",
       " 'baronets',\n",
       " 'barons',\n",
       " 'barons',\n",
       " 'barrack',\n",
       " 'barrack',\n",
       " 'barracks',\n",
       " 'barracks',\n",
       " 'barracuda',\n",
       " 'Barragan',\n",
       " 'Barre',\n",
       " 'barrel',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barreled',\n",
       " 'barreling',\n",
       " 'barreling',\n",
       " 'barrels',\n",
       " 'barrels',\n",
       " 'barren',\n",
       " 'barren',\n",
       " 'Barrens',\n",
       " 'Barret',\n",
       " 'barricade',\n",
       " 'barricade',\n",
       " 'barricaded',\n",
       " 'barricaded',\n",
       " 'barricades',\n",
       " 'barricades',\n",
       " 'barrier',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barriers',\n",
       " 'Barringer',\n",
       " 'Barrios',\n",
       " 'barrister',\n",
       " 'barrister',\n",
       " 'barristers',\n",
       " 'barristers',\n",
       " 'Barrow',\n",
       " 'Barrow',\n",
       " 'Barrows',\n",
       " 'Barrows',\n",
       " 'Barry',\n",
       " 'Barry',\n",
       " 'Bavarian',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'bearer',\n",
       " 'bearers',\n",
       " 'bearing',\n",
       " 'bearings',\n",
       " 'bearish',\n",
       " 'bears',\n",
       " 'beneficiaries',\n",
       " 'beneficiary',\n",
       " 'beret',\n",
       " 'beret',\n",
       " 'Berets',\n",
       " 'Berets',\n",
       " 'Bering',\n",
       " 'berries',\n",
       " 'Berrigan',\n",
       " 'Berry',\n",
       " 'Berryman',\n",
       " 'Beryl',\n",
       " 'Beware',\n",
       " 'beyond',\n",
       " 'beyond',\n",
       " 'bier',\n",
       " 'billionaire',\n",
       " 'billionaires',\n",
       " 'blackberries',\n",
       " 'blackberry',\n",
       " 'Blair',\n",
       " 'blare',\n",
       " 'blared',\n",
       " 'blares',\n",
       " 'blaring',\n",
       " 'Bluebeard',\n",
       " 'blueberries',\n",
       " 'blueberry',\n",
       " 'Bolero',\n",
       " 'bracero',\n",
       " 'Breweries',\n",
       " 'buccaneer',\n",
       " 'Buccaneers',\n",
       " 'budgetary',\n",
       " 'Bulgaria',\n",
       " 'Bulgarian',\n",
       " 'Bulgarians',\n",
       " 'burial',\n",
       " 'burials',\n",
       " 'buried',\n",
       " 'buries',\n",
       " 'Burroughs',\n",
       " 'bury',\n",
       " 'burying',\n",
       " 'caballero',\n",
       " 'cairn',\n",
       " 'cairns',\n",
       " 'caldera',\n",
       " 'caldera',\n",
       " 'canaries',\n",
       " 'canary',\n",
       " 'Canberra',\n",
       " 'Canterbury',\n",
       " 'capillaries',\n",
       " 'capillary',\n",
       " 'caramel',\n",
       " 'caramelize',\n",
       " 'caramelized',\n",
       " 'carat',\n",
       " 'carats',\n",
       " 'caravan',\n",
       " 'caravan',\n",
       " 'caravans',\n",
       " 'caravans',\n",
       " 'caravel',\n",
       " 'caraway',\n",
       " 'caraway',\n",
       " 'cardiopulmonary',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'carefree',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'caregiver',\n",
       " 'caregivers',\n",
       " 'careless',\n",
       " 'carelessly',\n",
       " 'carelessness',\n",
       " 'carer',\n",
       " 'cares',\n",
       " 'caretaker',\n",
       " 'caretakers',\n",
       " 'Carey',\n",
       " 'Caribbean',\n",
       " 'Caribe',\n",
       " 'caribou',\n",
       " 'caricature',\n",
       " 'caricaturist',\n",
       " 'Carillon',\n",
       " 'caring',\n",
       " 'Carioca',\n",
       " 'carob',\n",
       " 'Carol',\n",
       " 'Carol',\n",
       " 'Carole',\n",
       " 'Carole',\n",
       " 'Carolina',\n",
       " 'Carolinas',\n",
       " 'Caroline',\n",
       " 'carols',\n",
       " 'Carolyn',\n",
       " 'carotene',\n",
       " 'carousel',\n",
       " 'Carrel',\n",
       " 'carriage',\n",
       " 'carriage',\n",
       " 'carriages',\n",
       " 'carriages',\n",
       " 'carried',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrier',\n",
       " 'carriers',\n",
       " 'carriers',\n",
       " 'carries',\n",
       " 'carries',\n",
       " 'carrion',\n",
       " 'Carroll',\n",
       " 'Carroll',\n",
       " 'carrot',\n",
       " 'carrot',\n",
       " 'carrots',\n",
       " 'carrots',\n",
       " 'Carrousel',\n",
       " 'Carrow',\n",
       " 'carry',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'carrying',\n",
       " 'Caryl',\n",
       " 'cautionary',\n",
       " 'cavalier',\n",
       " 'cemeteries',\n",
       " 'cemetery',\n",
       " 'cemetery',\n",
       " 'centenary',\n",
       " 'Cera',\n",
       " 'cerebral',\n",
       " 'ceremonial',\n",
       " 'ceremonies',\n",
       " 'ceremony',\n",
       " 'Cervantes',\n",
       " 'chair',\n",
       " 'chaired',\n",
       " 'chairing',\n",
       " 'Chairman',\n",
       " 'chairmanship',\n",
       " 'chairmen',\n",
       " 'chairperson',\n",
       " 'chairs',\n",
       " 'chairwoman',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characteristically',\n",
       " 'characteristics',\n",
       " 'characterization',\n",
       " 'characterizations',\n",
       " 'characterize',\n",
       " 'characterized',\n",
       " 'characterizes',\n",
       " 'characterizing',\n",
       " 'characters',\n",
       " 'characters',\n",
       " 'chariot',\n",
       " 'chariots',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'charitable',\n",
       " 'charitably',\n",
       " 'charities',\n",
       " 'charities',\n",
       " 'charity',\n",
       " 'Charon',\n",
       " 'chemotherapy',\n",
       " 'Cher',\n",
       " 'Cherie',\n",
       " 'cherish',\n",
       " 'cherished',\n",
       " 'cherishes',\n",
       " 'cherishing',\n",
       " 'Cherokee',\n",
       " 'Cherokees',\n",
       " 'cherries',\n",
       " 'cherry',\n",
       " 'cherub',\n",
       " 'cherubs',\n",
       " 'Cheung',\n",
       " 'childbearing',\n",
       " 'childcare',\n",
       " 'Chimera',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'clairvoyance',\n",
       " 'clairvoyant',\n",
       " 'Clare',\n",
       " 'Clarence',\n",
       " 'Clarendon',\n",
       " 'clarification',\n",
       " 'clarifications',\n",
       " 'clarified',\n",
       " 'clarifies',\n",
       " 'clarify',\n",
       " 'clarifying',\n",
       " 'clarinet',\n",
       " 'clarinetist',\n",
       " 'Clarion',\n",
       " 'clarity',\n",
       " 'clarity',\n",
       " 'Clary',\n",
       " 'Cleric',\n",
       " 'clerical',\n",
       " 'clerical',\n",
       " 'clerics',\n",
       " 'coherently',\n",
       " 'cometary',\n",
       " 'commentaries',\n",
       " 'commentary',\n",
       " 'commissaries',\n",
       " 'commissary',\n",
       " 'companero',\n",
       " 'comparable',\n",
       " 'comparative',\n",
       " 'comparatively',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'compares',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'comparisons',\n",
       " 'Concerto',\n",
       " 'Concertos',\n",
       " 'concessionary',\n",
       " 'concierge',\n",
       " 'confectionary',\n",
       " 'confectionery',\n",
       " 'confrere',\n",
       " 'confreres',\n",
       " 'consortium',\n",
       " 'Constabulary',\n",
       " 'contemporaries',\n",
       " 'contemporary',\n",
       " 'contrariness',\n",
       " 'contrary',\n",
       " 'contrary',\n",
       " 'cookware',\n",
       " 'copyright',\n",
       " 'copyrighted',\n",
       " 'copyrights',\n",
       " 'copywriter',\n",
       " 'copywriters',\n",
       " 'corollary',\n",
       " 'coronary',\n",
       " 'Corsair',\n",
       " 'corticosteroids',\n",
       " 'counterrevolutionary',\n",
       " 'counterrevolutionary',\n",
       " 'counterterrorism',\n",
       " 'counterterrorism',\n",
       " 'Counterterrorist',\n",
       " 'Counterterrorist',\n",
       " 'cranberries',\n",
       " 'cranberry',\n",
       " 'culinary',\n",
       " 'curry',\n",
       " 'customarily',\n",
       " 'customary',\n",
       " 'dairies',\n",
       " 'dairy',\n",
       " 'dare',\n",
       " 'dared',\n",
       " 'Daredevil',\n",
       " 'daredevils',\n",
       " 'dares',\n",
       " 'daresay',\n",
       " 'daring',\n",
       " 'daycare',\n",
       " 'deathbed',\n",
       " 'debonair',\n",
       " 'declarant',\n",
       " 'declaratory',\n",
       " 'declare',\n",
       " 'declared',\n",
       " 'declares',\n",
       " 'declaring',\n",
       " 'Delaware',\n",
       " 'Delftware',\n",
       " 'demerit',\n",
       " 'demerits',\n",
       " 'denarii',\n",
       " 'Deoxyribonucleic',\n",
       " 'deregulate',\n",
       " 'deregulating',\n",
       " 'deregulation',\n",
       " 'derelict',\n",
       " 'dereliction',\n",
       " 'derelicts',\n",
       " 'derelicts',\n",
       " 'derivation',\n",
       " 'Derrick',\n",
       " 'derringer',\n",
       " 'Derry',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairs',\n",
       " 'deuterium',\n",
       " 'dewberry',\n",
       " 'dexterity',\n",
       " 'dexterity',\n",
       " 'dictionaries',\n",
       " 'dictionary',\n",
       " 'dietary',\n",
       " 'dignitaries',\n",
       " 'dignitary',\n",
       " 'dinnerware',\n",
       " 'directing',\n",
       " 'direction',\n",
       " 'directional',\n",
       " 'directionless',\n",
       " 'directions',\n",
       " 'directive',\n",
       " 'directives',\n",
       " 'directly',\n",
       " 'directness',\n",
       " 'director',\n",
       " 'Directorate',\n",
       " 'directorial',\n",
       " 'directories',\n",
       " 'directors',\n",
       " 'directorship',\n",
       " 'directory',\n",
       " 'directs',\n",
       " 'disappear',\n",
       " 'disappearance',\n",
       " 'disappearances',\n",
       " 'disappeared',\n",
       " 'disappearing',\n",
       " 'disappears',\n",
       " 'disciplinarian',\n",
       " 'disciplinary',\n",
       " 'discretionary',\n",
       " 'disparage',\n",
       " 'disparaged',\n",
       " 'disparages',\n",
       " 'disparaging',\n",
       " 'disparagingly',\n",
       " 'disparate',\n",
       " 'disparities',\n",
       " 'disparity',\n",
       " 'dispensary',\n",
       " 'disrepair',\n",
       " 'dissimilarity',\n",
       " 'diuretic',\n",
       " 'diuretics',\n",
       " 'diversionary',\n",
       " 'diversionary',\n",
       " 'doctrinaire',\n",
       " 'doer',\n",
       " 'doorknob',\n",
       " 'doorknobs',\n",
       " 'downstairs',\n",
       " 'dram',\n",
       " 'drams',\n",
       " 'dromedary',\n",
       " 'Drury',\n",
       " 'dysentery',\n",
       " 'ear',\n",
       " 'earless',\n",
       " 'earmark',\n",
       " 'earmarked',\n",
       " 'earmarks',\n",
       " 'earmuff',\n",
       " 'earmuffs',\n",
       " 'earphone',\n",
       " 'earphones',\n",
       " 'earpiece',\n",
       " 'earpieces',\n",
       " 'earplug',\n",
       " 'earplugs',\n",
       " 'earring',\n",
       " 'earrings',\n",
       " 'ears',\n",
       " 'earshot',\n",
       " 'earthbound',\n",
       " 'earwax',\n",
       " 'egalitarian',\n",
       " 'elsewhere',\n",
       " 'Embarcadero',\n",
       " 'embarrass',\n",
       " 'embarrassed',\n",
       " 'embarrasses',\n",
       " 'embarrassing',\n",
       " 'embarrassingly',\n",
       " 'embarrassment',\n",
       " 'embarrassments',\n",
       " 'emeritus',\n",
       " 'emissaries',\n",
       " 'emissary',\n",
       " 'endear',\n",
       " 'endeared',\n",
       " 'endearing',\n",
       " 'engineered',\n",
       " 'ensnare',\n",
       " 'ensnared',\n",
       " 'ensnares',\n",
       " 'era',\n",
       " 'erasable',\n",
       " 'erase',\n",
       " 'erased',\n",
       " 'eraser',\n",
       " 'erasers',\n",
       " 'erases',\n",
       " 'erasing',\n",
       " 'ere',\n",
       " 'Eric',\n",
       " 'Erica',\n",
       " 'Erika',\n",
       " 'Erin',\n",
       " 'erode',\n",
       " 'eroding',\n",
       " 'err',\n",
       " 'errand',\n",
       " 'errands',\n",
       " 'errant',\n",
       " 'erratically',\n",
       " 'erred',\n",
       " 'erring',\n",
       " 'erroneous',\n",
       " 'erroneously',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'errs',\n",
       " 'ersatz',\n",
       " 'ersatz',\n",
       " 'erudite',\n",
       " 'erudition',\n",
       " 'erupt',\n",
       " 'erupted',\n",
       " 'erupted',\n",
       " 'erupting',\n",
       " 'eruption',\n",
       " 'eruptions',\n",
       " 'eruptive',\n",
       " 'erupts',\n",
       " 'erupts',\n",
       " 'Escudero',\n",
       " 'esoteric',\n",
       " 'esoteric',\n",
       " 'estuary',\n",
       " 'Euro',\n",
       " 'eurodollar',\n",
       " 'everywhere',\n",
       " 'everywhere',\n",
       " 'evidentiary',\n",
       " 'evidentiary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'exclusionary',\n",
       " 'expeditionary',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimental',\n",
       " 'experimentally',\n",
       " 'experimentally',\n",
       " 'experimentation',\n",
       " 'experimented',\n",
       " 'experimenter',\n",
       " 'experimenting',\n",
       " 'experimenting',\n",
       " 'experiments',\n",
       " 'extramarital',\n",
       " 'extraordinaire',\n",
       " 'extraordinarily',\n",
       " 'extraordinary',\n",
       " 'extraordinary',\n",
       " 'eyewear',\n",
       " 'Eyre',\n",
       " 'Eyrie',\n",
       " 'Fahrenheit',\n",
       " 'fair',\n",
       " 'Fairbanks',\n",
       " 'faire',\n",
       " 'fairer',\n",
       " 'fairest',\n",
       " 'fairground',\n",
       " 'fa"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 13396 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasBadDiphs = rowsWBadNphones(lexicon, 2, getNphones_i, badDiphones)\n",
    "len(hasBadDiphs)\n",
    "len(lexicon)\n",
    "len(hasBadDiphs)/len(lexicon) #proportion of the lexicon (by type) that has bad triphs\n",
    "sum([float(entry['Prob']) for entry in hasBadDiphs]) #prob mass of bad triph words by token\n",
    "list(map(lambda r: r['Word'], hasBadDiphs[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:11.103763Z",
     "start_time": "2018-12-05T04:21:11.099123Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME see how many words there are like this and what they're like, also check sum of probability of such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:12.721748Z",
     "start_time": "2018-12-05T04:21:11.107457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observedBadNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "len(observedBadNphones(lexicon, 3, getNphones_i, badTriphones))\n",
    "# observedBadNphones(lexicon, 2, getNphones_i, badDiphones)\n",
    "len(observedBadNphones(lexicon, 2, getNphones_i, badDiphones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:14.179779Z",
     "start_time": "2018-12-05T04:21:12.724757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52143"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noBadTriphs_IPhOD = onlyRowsWithGoodNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "noBadDiphsOrTriphs_IPhOD = onlyRowsWithGoodNphones(noBadTriphs_IPhOD, 2, getNphones_i, badDiphones)\n",
    "# [r for r in lexicon if not containsBadTriphones(r)]\n",
    "len(lexicon)\n",
    "len(noBadTriphs_IPhOD)\n",
    "len(noBadDiphsOrTriphs_IPhOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:21:14.188680Z",
     "start_time": "2018-12-05T04:21:14.182630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noBadTriphones'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_filter\n",
    "\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    lexicon_filtered = lexicon\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    lexicon_filtered = noBadTriphs_IPhOD\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} intsead\".format(which_filter))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose wordforms for the IPhOD lexicon distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we want to export a json file( = a list of dictionaries) mapping each phonological wordform to a number representing a frequency or probability or negative log probability of that phonological wordform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:07.590959Z",
     "start_time": "2018-12-05T04:30:07.583722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'UnTrn'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_stress\n",
    "\n",
    "if which_stress == 'destressed':\n",
    "    wordform_field = 'UnTrn'\n",
    "elif which_stress == 'stressed':\n",
    "    wordform_field = 'StTrn'\n",
    "else:\n",
    "    raise Exception(\"'which_stress' must be one of 'destressed' or 'stressed'; got {0} instead\".format(which_stress))\n",
    "\n",
    "wordform_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:07.967511Z",
     "start_time": "2018-12-05T04:30:07.910993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "49601"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2550"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonologicalWords = [entry[wordform_field] for entry in lexicon_filtered]\n",
    "len(phonologicalWords)\n",
    "len(set(phonologicalWords))\n",
    "len(phonologicalWords) - len(set(phonologicalWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate frequencies, probabilities, and informativities of phonological wordforms in IPhOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to frequency/probability/negative log probability: unfortunately, each entry represents a phonological representation of an orthographic word, and the probability is based on the orthographic word; multiple phonological representations share an orthographic representation (and therefore a frequency/probability/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:09.713225Z",
     "start_time": "2018-12-05T04:30:09.670489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46328"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWords = [entry['Word'] for entry in lexicon_filtered]\n",
    "len(orthWords)\n",
    "len(set(orthWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many orthographic words are associated with multiple phonological representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:10.747855Z",
     "start_time": "2018-12-05T04:30:10.744781Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:11.198549Z",
     "start_time": "2018-12-05T04:30:11.133705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "41202"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5126"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWordCounter = Counter(orthWords)\n",
    "sum(orthWordCounter.values())\n",
    "uniqueWords = {k for k in orthWordCounter if orthWordCounter[k] == 1}\n",
    "dupedWords = {k for k in orthWordCounter if orthWordCounter[k] > 1}\n",
    "len(uniqueWords)\n",
    "len(dupedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:11.532189Z",
     "start_time": "2018-12-05T04:30:11.504706Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dampening'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('Indx', '11815'),\n",
       "              ('NPhon', '8'),\n",
       "              ('NSyll', '3'),\n",
       "              ('Nlprob', '24.481167390012075'),\n",
       "              ('Prob', '4.270063040648297e-08'),\n",
       "              ('SCDcnt', '2'),\n",
       "              ('SFreq', '0.06'),\n",
       "              ('StTrn', 'd.æ1.m.p.ə0.n.ɪ0.ŋ'),\n",
       "              ('UnTrn', 'd.æ.m.p.ə.n.ɪ.ŋ'),\n",
       "              ('Word', 'dampening'),\n",
       "              ('strBPAV', '0.00741504'),\n",
       "              ('strCBPAV', '0.00661280'),\n",
       "              ('strCDEN', '2'),\n",
       "              ('strCLCPOSPAV', '0.08305788'),\n",
       "              ('strCPOSPAV', '0.05955620'),\n",
       "              ('strCTPAV', '0.00033685'),\n",
       "              ('strDENS', '1'),\n",
       "              ('strFBPAV', '0.00472760'),\n",
       "              ('strFDEN', '0.06'),\n",
       "              ('strFLCPOSPAV', '0.07730884'),\n",
       "              ('strFPOSPAV', '0.05451145'),\n",
       "              ('strFTPAV', '0.00020005'),\n",
       "              ('strLBPAV', '0.00709295'),\n",
       "              ('strLCPOSPAV', '0.09440462'),\n",
       "              ('strLDEN', '0.03'),\n",
       "              ('strLLCPOSPAV', '0.09070180'),\n",
       "              ('strLPOSPAV', '0.06493566'),\n",
       "              ('strLTPAV', '0.00028131'),\n",
       "              ('strPOSPAV', '0.06404622'),\n",
       "              ('strTPAV', '0.00029313'),\n",
       "              ('unsBPAV', '0.00795247'),\n",
       "              ('unsCBPAV', '0.00774625'),\n",
       "              ('unsCDEN', '2'),\n",
       "              ('unsCLCPOSPAV', '0.08551193'),\n",
       "              ('unsCPOSPAV', '0.06150736'),\n",
       "              ('unsCTPAV', '0.00037989'),\n",
       "              ('unsDENS', '1'),\n",
       "              ('unsFBPAV', '0.00617989'),\n",
       "              ('unsFDEN', '0.06'),\n",
       "              ('unsFLCPOSPAV', '0.07964048'),\n",
       "              ('unsFPOSPAV', '0.05674703'),\n",
       "              ('unsFTPAV', '0.00023422'),\n",
       "              ('unsLBPAV', '0.00783985'),\n",
       "              ('unsLCPOSPAV', '0.09734962'),\n",
       "              ('unsLDEN', '0.03'),\n",
       "              ('unsLLCPOSPAV', '0.09380975'),\n",
       "              ('unsLPOSPAV', '0.06753174'),\n",
       "              ('unsLTPAV', '0.00033894'),\n",
       "              ('unsPOSPAV', '0.06718647'),\n",
       "              ('unsTPAV', '0.00035747')]),\n",
       " OrderedDict([('Indx', '11816'),\n",
       "              ('NPhon', '7'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '24.481167390012075'),\n",
       "              ('Prob', '4.270063040648297e-08'),\n",
       "              ('SCDcnt', '2'),\n",
       "              ('SFreq', '0.06'),\n",
       "              ('StTrn', 'd.æ1.m.p.n.ɪ0.ŋ'),\n",
       "              ('UnTrn', 'd.æ.m.p.n.ɪ.ŋ'),\n",
       "              ('Word', 'dampening'),\n",
       "              ('strBPAV', '0.00406695'),\n",
       "              ('strCBPAV', '0.00382498'),\n",
       "              ('strCDEN', '15'),\n",
       "              ('strCLCPOSPAV', '0.08511888'),\n",
       "              ('strCPOSPAV', '0.05121748'),\n",
       "              ('strCTPAV', '0.00022221'),\n",
       "              ('strDENS', '3'),\n",
       "              ('strFBPAV', '0.00218137'),\n",
       "              ('strFDEN', '0.36'),\n",
       "              ('strFLCPOSPAV', '0.08159589'),\n",
       "              ('strFPOSPAV', '0.04686754'),\n",
       "              ('strFTPAV', '0.00012495'),\n",
       "              ('strLBPAV', '0.00383534'),\n",
       "              ('strLCPOSPAV', '0.08378962'),\n",
       "              ('strLDEN', '0.15'),\n",
       "              ('strLLCPOSPAV', '0.08340109'),\n",
       "              ('strLPOSPAV', '0.05337476'),\n",
       "              ('strLTPAV', '0.00020720'),\n",
       "              ('strPOSPAV', '0.05373736'),\n",
       "              ('strTPAV', '0.00020816'),\n",
       "              ('unsBPAV', '0.00431275'),\n",
       "              ('unsCBPAV', '0.00427003'),\n",
       "              ('unsCDEN', '15'),\n",
       "              ('unsCLCPOSPAV', '0.08818820'),\n",
       "              ('unsCPOSPAV', '0.05337555'),\n",
       "              ('unsCTPAV', '0.00022899'),\n",
       "              ('unsDENS', '3'),\n",
       "              ('unsFBPAV', '0.00287781'),\n",
       "              ('unsFDEN', '0.36'),\n",
       "              ('unsFLCPOSPAV', '0.08718543'),\n",
       "              ('unsFPOSPAV', '0.04988937'),\n",
       "              ('unsFTPAV', '0.00013859'),\n",
       "              ('unsLBPAV', '0.00418041'),\n",
       "              ('unsLCPOSPAV', '0.08622199'),\n",
       "              ('unsLDEN', '0.15'),\n",
       "              ('unsLLCPOSPAV', '0.08588417'),\n",
       "              ('unsLPOSPAV', '0.05596412'),\n",
       "              ('unsLTPAV', '0.00021460'),\n",
       "              ('unsPOSPAV', '0.05712759'),\n",
       "              ('unsTPAV', '0.00021108')])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duped_word_ex = list(dupedWords)[0]\n",
    "duped_word_ex\n",
    "some_entries = [entry for entry in lexicon_filtered if entry['Word'] == duped_word_ex]\n",
    "some_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split probability mass among phonological realizations of the same orthographic word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since frequencies are only available for orthographic wordforms, we'll assume that each of the $n$ phonological wordforms associated with the same orthographic wordform $o$ with frequency $f_o$ each have frequency $\\frac{f_o}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:13.360813Z",
     "start_time": "2018-12-05T04:30:13.357683Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# def edit_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable (because it returns a value) but stateful(= in-place) dictionary update.\n",
    "#     '''\n",
    "#     the_dict.update({the_key: the_new_value})\n",
    "#     return the_dict\n",
    "\n",
    "# def modify_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable and (naively-implemented) non-mutating dictionary update.\n",
    "#     '''\n",
    "#     new_dict = {k:the_dict[k] for k in the_dict}\n",
    "#     new_dict.update({the_key: the_new_value})\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:30:13.677577Z",
     "start_time": "2018-12-05T04:30:13.674857Z"
    }
   },
   "outputs": [],
   "source": [
    "# from time import localtime, strftime\n",
    "# def stamp():\n",
    "#     return strftime('%H:%M:%S', localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:28.733927Z",
     "start_time": "2018-12-05T04:30:16.353133Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 | 463/46328 = 0.009993956138836125 | lavishing | 20:30:23\n",
      "927 | 927/46328 = 0.02000949749611466 | pedaling | 20:30:30\n",
      "1390 | 1390/46328 = 0.030003453634950786 | living | 20:30:37\n",
      "2316 | 2316/46328 = 0.049991365912623036 | victory | 20:30:52\n",
      "4633 | 4633/46328 = 0.10000431704368848 | Baton | 20:31:31\n",
      "9266 | 9266/46328 = 0.20000863408737696 | Lancelot | 20:32:45\n",
      "13898 | 13898/46328 = 0.29999136591262304 | dysfunction | 20:33:59\n",
      "18531 | 18531/46328 = 0.3999956829563115 | November | 20:35:13\n",
      "23164 | 23164/46328 = 0.5 | waitresses | 20:36:25\n",
      "27797 | 27797/46328 = 0.6000043170436885 | cuddle | 20:37:38\n",
      "32430 | 32430/46328 = 0.700008634087377 | syndromes | 20:38:51\n",
      "37062 | 37062/46328 = 0.799991365912623 | shooed | 20:40:03\n",
      "41695 | 41695/46328 = 0.8999956829563115 | articulated | 20:41:16\n",
      "44012 | 44012/46328 = 0.950008634087377 | stinkers | 20:41:52\n",
      "44475 | 44475/46328 = 0.960002590226213 | needling | 20:42:00\n",
      "44938 | 44938/46328 = 0.9699965463650492 | vacancy | 20:42:07\n",
      "45401 | 45401/46328 = 0.9799905025038853 | roadway | 20:42:14\n",
      "45865 | 45865/46328 = 0.9900060438611639 | dioxide | 20:42:21\n"
     ]
    }
   ],
   "source": [
    "def getEntriesMatchingOrthWord(orthWord):\n",
    "    return [entry for entry in lexicon_filtered if entry['Word'] == orthWord]\n",
    "\n",
    "# orthWordToEntries = {orthWord:getEntriesMatchingOrthWord(orthWord) for orthWord in set(orthWords)}\n",
    "\n",
    "orthWordToEntries = dict()\n",
    "\n",
    "orthWordSet = set(orthWords)\n",
    "constructDictWProgressUpdates(getEntriesMatchingOrthWord, orthWordSet, orthWordToEntries)\n",
    "\n",
    "# orthWordSet = set(orthWords)\n",
    "# totalOrthWords = len(orthWordSet)\n",
    "# benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [round(each/100.0 * totalOrthWords) for each in benchmarkPercentages]\n",
    "# for i, orthWord in enumerate(orthWordSet):\n",
    "#     if i in benchmarkIndices:\n",
    "#         print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, totalOrthWords, i/totalOrthWords, orthWord, stamp()))\n",
    "\n",
    "#     orthWordToEntries[orthWord] = getEntriesMatchingOrthWord(orthWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:28.748314Z",
     "start_time": "2018-12-05T04:42:28.739263Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitOrthFreq(orth_word):\n",
    "#     matchingEntries = [entry for entry in slim_lexicon if entry['Word'] == orth_word]\n",
    "    matchingEntries = orthWordToEntries[orth_word]\n",
    "\n",
    "    if matchingEntries == []:\n",
    "        return 0.0\n",
    "    \n",
    "    orthFreq = matchingEntries[0]['SFreq']\n",
    "    assert(all(match['SFreq'] == orthFreq for match in matchingEntries))\n",
    "    \n",
    "    dividedFreq = float(orthFreq) / len(matchingEntries)\n",
    "    return dividedFreq\n",
    "\n",
    "def add_splitOrthFreqAnnotations(entry, inPlace = None):\n",
    "    if inPlace == None:\n",
    "        inPlace = False\n",
    "\n",
    "    splitFreq = splitOrthFreq(entry['Word'])\n",
    "#     print('word: {0}'.format(entry['Word']))\n",
    "    new_entry = dict()\n",
    "    if not inPlace:\n",
    "        new_entry = modify_dict(entry, 'split_SFreq', splitFreq)\n",
    "    else:\n",
    "        new_entry = edit_dict(entry, 'split_SFreq', splitFreq)\n",
    "#     print('new split freq: {0}'.format(splitFreq))\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:28.767248Z",
     "start_time": "2018-12-05T04:42:28.751203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Indx', '1'),\n",
       "             ('NPhon', '1'),\n",
       "             ('NSyll', '1'),\n",
       "             ('Nlprob', '6.10491267350873'),\n",
       "             ('Prob', '0.014529081648642661'),\n",
       "             ('SCDcnt', '8382'),\n",
       "             ('SFreq', '20415.27'),\n",
       "             ('StTrn', 'ə0'),\n",
       "             ('UnTrn', 'ə'),\n",
       "             ('Word', 'a'),\n",
       "             ('strBPAV', '0'),\n",
       "             ('strCBPAV', '0'),\n",
       "             ('strCDEN', '114167'),\n",
       "             ('strCLCPOSPAV', '0.10933994'),\n",
       "             ('strCPOSPAV', '0.03349570'),\n",
       "             ('strCTPAV', '0'),\n",
       "             ('strDENS', '21'),\n",
       "             ('strFBPAV', '0'),\n",
       "             ('strFDEN', '145035.32'),\n",
       "             ('strFLCPOSPAV', '0.21988359'),\n",
       "             ('strFPOSPAV', '0.06061931'),\n",
       "             ('strFTPAV', '0'),\n",
       "             ('strLBPAV', '0'),\n",
       "             ('strLCPOSPAV', '0.06666667'),\n",
       "             ('strLDEN', '77.56'),\n",
       "             ('strLLCPOSPAV', '0.07556101'),\n",
       "             ('strLPOSPAV', '0.03338777'),\n",
       "             ('strLTPAV', '0'),\n",
       "             ('strPOSPAV', '0.03588891'),\n",
       "             ('strTPAV', '0'),\n",
       "             ('unsBPAV', '0'),\n",
       "             ('unsCBPAV', '0'),\n",
       "             ('unsCDEN', '136251'),\n",
       "             ('unsCLCPOSPAV', '0.15987477'),\n",
       "             ('unsCPOSPAV', '0.04159586'),\n",
       "             ('unsCTPAV', '0'),\n",
       "             ('unsDENS', '26'),\n",
       "             ('unsFBPAV', '0'),\n",
       "             ('unsFDEN', '150377.45'),\n",
       "             ('unsFLCPOSPAV', '0.22760866'),\n",
       "             ('unsFPOSPAV', '0.07192868'),\n",
       "             ('unsFTPAV', '0'),\n",
       "             ('unsLBPAV', '0'),\n",
       "             ('unsLCPOSPAV', '0.07692308'),\n",
       "             ('unsLDEN', '91.03'),\n",
       "             ('unsLLCPOSPAV', '0.12570126'),\n",
       "             ('unsLPOSPAV', '0.03999447'),\n",
       "             ('unsLTPAV', '0'),\n",
       "             ('unsPOSPAV', '0.04449866'),\n",
       "             ('unsTPAV', '0')])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_filtered)\n",
    "lexicon_filtered[0]\n",
    "add_splitOrthFreqAnnotations(lexicon_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:28.774343Z",
     "start_time": "2018-12-05T04:42:28.770912Z"
    }
   },
   "outputs": [],
   "source": [
    "# filtered_splitFreqs = [add_splitOrthFreqAnnotations(entry) for entry in lexicon_filtered]\n",
    "# len(filtered_splitFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:30.011821Z",
     "start_time": "2018-12-05T04:42:28.777207Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitFreqs = list(map(add_splitOrthFreqAnnotations, lexicon_filtered))\n",
    "filtered_splitFreqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:30.044640Z",
     "start_time": "2018-12-05T04:42:30.014198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941881.0300000808\n"
     ]
    }
   ],
   "source": [
    "sumFreqs = sum([float(entry['split_SFreq']) for entry in filtered_splitFreqs])\n",
    "print(sumFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:30.052264Z",
     "start_time": "2018-12-05T04:42:30.047763Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow, isclose\n",
    "\n",
    "def add_splitOrthProbAnnotations(entry):\n",
    "    splitFreq = float(entry['split_SFreq'])\n",
    "    \n",
    "    splitProb = splitFreq / sumFreqs\n",
    "    splitNlprob = -1.0 * log2(splitProb)\n",
    "    \n",
    "    new_entry = modify_dict(entry, 'split_Prob', splitProb)\n",
    "    new_entry = modify_dict(new_entry, 'split_Nlprob', splitNlprob)\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:30.845811Z",
     "start_time": "2018-12-05T04:42:30.055131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635,\n",
       " 'split_Prob': 0.010837499296486653,\n",
       " 'split_Nlprob': 6.527824289979772}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitProbs = list(map(add_splitOrthProbAnnotations, filtered_splitFreqs))\n",
    "filtered_splitProbs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate probability of a phonological word (marginalizing over orthographic wordforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the $n$ phonological wordforms of a given orthographic wordform $o$ with frequency $o_f$ now has frequency $\\frac{o_f}{n}$, but what about different orthographic words that share a phonological wordform? Ultimately we want a probability distribution mapping phonological wordforms to frequencies/probabilities, so we need to map each phonological wordform to a sum of frequencies/probabilities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:42:30.856929Z",
     "start_time": "2018-12-05T04:42:30.848432Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWords = set(phonologicalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:55.030479Z",
     "start_time": "2018-12-05T04:42:30.859985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 | 496/49601 = 0.009999798391161469 | l.ɪ.n.tʃ.p.ɪ.n | 20:42:38\n",
      "992 | 992/49601 = 0.019999596782322937 | æ.v.ə.d | 20:42:46\n",
      "1488 | 1488/49601 = 0.029999395173484406 | æ.b.d.ʌ.k.t.s | 20:42:54\n",
      "2480 | 2480/49601 = 0.04999899195580734 | d.ɪ.g.r.i.d | 20:43:10\n",
      "4960 | 4960/49601 = 0.09999798391161469 | b.aɪ.p.ɑ.r.t.ɪ.z.ə.n | 20:43:49\n",
      "9920 | 9920/49601 = 0.19999596782322937 | ɪ.n.t.ɚ.ʌ.p.t.ɪ.d | 20:45:09\n",
      "14880 | 14880/49601 = 0.29999395173484406 | k.ə.n.s.ʌ.l.t.ɪ.ŋ | 20:46:31\n",
      "19840 | 19840/49601 = 0.39999193564645874 | r.ɛ.d.b.ɚ.d | 20:47:50\n",
      "24800 | 24800/49601 = 0.4999899195580734 | r.oʊ.l.ɚ.b.l.eɪ.d.ɪ.ŋ | 20:49:15\n",
      "29761 | 29761/49601 = 0.6000080643535413 | dʒ.u.l | 20:50:34\n",
      "34721 | 34721/49601 = 0.7000060482651559 | k.ə.n.t.ɪ.g.j.u.ə.s | 20:51:54\n",
      "39681 | 39681/49601 = 0.8000040321767706 | s.æ.t.ɚ.aɪ.z | 20:53:14\n",
      "44641 | 44641/49601 = 0.9000020160883853 | s.k.ɑ.r.k.i.p.ɪ.ŋ | 20:54:34\n",
      "47121 | 47121/49601 = 0.9500010080441926 | h.w.ɪ.p.s | 20:55:15\n",
      "47617 | 47617/49601 = 0.9600008064353541 | m.aɪ.n.ɚ.d | 20:55:23\n",
      "48113 | 48113/49601 = 0.9700006048265156 | s.t.r.ʌ.k.tʃ.ɚ | 20:55:31\n",
      "48609 | 48609/49601 = 0.980000403217677 | l.ɛ.g.ə.t | 20:55:39\n",
      "49105 | 49105/49601 = 0.9900002016088385 | r.ɪ.z.ɪ.s.t.ɪ.d | 20:55:47\n"
     ]
    }
   ],
   "source": [
    "def getEntriesMatchingPhonWord(phonWord):\n",
    "    return [entry for entry in filtered_splitProbs if entry[wordform_field] == phonWord]\n",
    "\n",
    "# phonWordToEntries = {phonWord:getEntriesMatchingPhonWord(phonWord) for phonWord in phonWords}\n",
    "\n",
    "phonWordToEntries = dict()\n",
    "\n",
    "constructDictWProgressUpdates(getEntriesMatchingPhonWord, phonWords, phonWordToEntries)\n",
    "\n",
    "# phonWords\n",
    "# totalPhonWords = len(phonWords)\n",
    "# benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [round(each/100.0 * totalPhonWords) for each in benchmarkPercentages]\n",
    "# for i, phonWord in enumerate(phonWords):\n",
    "#     if i in benchmarkIndices:\n",
    "#         print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, totalPhonWords, i/totalPhonWords, phonWord, stamp()))\n",
    "\n",
    "#     phonWordToEntries[phonWord] = getEntriesMatchingPhonWord(phonWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:55.046687Z",
     "start_time": "2018-12-05T04:55:55.036499Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPhonProb(phon_word):\n",
    "#     matchingEntries = [entry[field] for entry in slim_splitProbs if entry[field] == phonrep]\n",
    "    matchingEntries = phonWordToEntries[phon_word]\n",
    "    \n",
    "    if matchingEntries == []:\n",
    "        return 0.0\n",
    "    \n",
    "    prob = sum([entry['split_Prob'] for entry in matchingEntries])\n",
    "    return prob\n",
    "\n",
    "def add_phonProbs(entry, inPlace = None):\n",
    "    if inPlace == None:\n",
    "        inPlace = False\n",
    "    \n",
    "    transcription = entry[wordform_field]\n",
    "    prob = getPhonProb(transcription)\n",
    "    nlprob = -1.0 * log2(prob)\n",
    "    \n",
    "    new_entry = dict()\n",
    "    if not inPlace:\n",
    "        new_entry = modify_dict(entry, 'phon_Prob', prob)\n",
    "        new_entry = modify_dict(new_entry, 'phon_Nlprob', nlprob)\n",
    "    else:\n",
    "        new_entry = edit_dict(entry, 'phon_Prob', prob)\n",
    "        new_entry = edit_dict(new_entry, 'phon_Nlprob', nlprob)\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.211879Z",
     "start_time": "2018-12-05T04:55:55.050783Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635,\n",
       " 'split_Prob': 0.010837499296486653,\n",
       " 'split_Nlprob': 6.527824289979772,\n",
       " 'phon_Prob': 0.010837499296486653,\n",
       " 'phon_Nlprob': 6.527824289979772}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitprobs_phonprobs = list(map(add_phonProbs, filtered_splitProbs))\n",
    "filtered_splitprobs_phonprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.250565Z",
     "start_time": "2018-12-05T04:55:56.214388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Indx': '41164',\n",
       "  'NPhon': '5',\n",
       "  'NSyll': '2',\n",
       "  'Nlprob': '19.5269710796252',\n",
       "  'Prob': '1.3237195426009723e-06',\n",
       "  'SCDcnt': '85',\n",
       "  'SFreq': '1.86',\n",
       "  'StTrn': 'r.ɪ1.dʒ.ə0.d',\n",
       "  'UnTrn': 'r.ɪ.dʒ.ə.d',\n",
       "  'Word': 'rigid',\n",
       "  'strBPAV': '0.00216083',\n",
       "  'strCBPAV': '0.00214780',\n",
       "  'strCDEN': '171',\n",
       "  'strCLCPOSPAV': '0.08322913',\n",
       "  'strCPOSPAV': '0.05567915',\n",
       "  'strCTPAV': '0.00004278',\n",
       "  'strDENS': '4',\n",
       "  'strFBPAV': '0.00115820',\n",
       "  'strFDEN': '4',\n",
       "  'strFLCPOSPAV': '0.08932192',\n",
       "  'strFPOSPAV': '0.04696634',\n",
       "  'strFTPAV': '0.00001926',\n",
       "  'strLBPAV': '0.00242575',\n",
       "  'strLCPOSPAV': '0.07578718',\n",
       "  'strLDEN': '1.09',\n",
       "  'strLLCPOSPAV': '0.07965779',\n",
       "  'strLPOSPAV': '0.05789941',\n",
       "  'strLTPAV': '0.00006205',\n",
       "  'strPOSPAV': '0.05668689',\n",
       "  'strTPAV': '0.00005763',\n",
       "  'unsBPAV': '0.00400750',\n",
       "  'unsCBPAV': '0.00400524',\n",
       "  'unsCDEN': '171',\n",
       "  'unsCLCPOSPAV': '0.09365356',\n",
       "  'unsCPOSPAV': '0.06351975',\n",
       "  'unsCTPAV': '0.00008944',\n",
       "  'unsDENS': '4',\n",
       "  'unsFBPAV': '0.00268055',\n",
       "  'unsFDEN': '4',\n",
       "  'unsFLCPOSPAV': '0.10282063',\n",
       "  'unsFPOSPAV': '0.05599098',\n",
       "  'unsFTPAV': '0.00004332',\n",
       "  'unsLBPAV': '0.00437706',\n",
       "  'unsLCPOSPAV': '0.08172226',\n",
       "  'unsLDEN': '1.09',\n",
       "  'unsLLCPOSPAV': '0.08696793',\n",
       "  'unsLPOSPAV': '0.06745590',\n",
       "  'unsLTPAV': '0.00012714',\n",
       "  'unsPOSPAV': '0.06806723',\n",
       "  'unsTPAV': '0.00012880',\n",
       "  'split_SFreq': 0.93,\n",
       "  'split_Prob': 9.873858485077679e-07,\n",
       "  'split_Nlprob': 19.94988269609624,\n",
       "  'phon_Prob': 9.873858485077679e-07,\n",
       "  'phon_Nlprob': 19.94988269609624},\n",
       " {'Indx': '41165',\n",
       "  'NPhon': '5',\n",
       "  'NSyll': '2',\n",
       "  'Nlprob': '19.5269710796252',\n",
       "  'Prob': '1.3237195426009723e-06',\n",
       "  'SCDcnt': '85',\n",
       "  'SFreq': '1.86',\n",
       "  'StTrn': 'r.ɪ1.dʒ.ɪ0.d',\n",
       "  'UnTrn': 'r.ɪ.dʒ.ɪ.d',\n",
       "  'Word': 'rigid',\n",
       "  'strBPAV': '0.00227069',\n",
       "  'strCBPAV': '0.00199256',\n",
       "  'strCDEN': '114',\n",
       "  'strCLCPOSPAV': '0.09081155',\n",
       "  'strCPOSPAV': '0.04768333',\n",
       "  'strCTPAV': '0.00001793',\n",
       "  'strDENS': '3',\n",
       "  'strFBPAV': '0.00232474',\n",
       "  'strFDEN': '2.61',\n",
       "  'strFLCPOSPAV': '0.08851489',\n",
       "  'strFPOSPAV': '0.03718920',\n",
       "  'strFTPAV': '0.00000912',\n",
       "  'strLBPAV': '0.00214621',\n",
       "  'strLCPOSPAV': '0.07950475',\n",
       "  'strLDEN': '0.71',\n",
       "  'strLLCPOSPAV': '0.08022137',\n",
       "  'strLPOSPAV': '0.04534423',\n",
       "  'strLTPAV': '0.00002834',\n",
       "  'strPOSPAV': '0.04302293',\n",
       "  'strTPAV': '0.00003058',\n",
       "  'unsBPAV': '0.00417202',\n",
       "  'unsCBPAV': '0.00365626',\n",
       "  'unsCDEN': '114',\n",
       "  'unsCLCPOSPAV': '0.10311375',\n",
       "  'unsCPOSPAV': '0.05736822',\n",
       "  'unsCTPAV': '0.00007939',\n",
       "  'unsDENS': '3',\n",
       "  'unsFBPAV': '0.00327493',\n",
       "  'unsFDEN': '2.61',\n",
       "  'unsFLCPOSPAV': '0.10352536',\n",
       "  'unsFPOSPAV': '0.04740790',\n",
       "  'unsFTPAV': '0.00003851',\n",
       "  'unsLBPAV': '0.00411429',\n",
       "  'unsLCPOSPAV': '0.08591874',\n",
       "  'unsLDEN': '0.71',\n",
       "  'unsLLCPOSPAV': '0.08827808',\n",
       "  'unsLPOSPAV': '0.05682895',\n",
       "  'unsLTPAV': '0.00012177',\n",
       "  'unsPOSPAV': '0.05703602',\n",
       "  'unsTPAV': '0.00013901',\n",
       "  'split_SFreq': 0.93,\n",
       "  'split_Prob': 9.873858485077679e-07,\n",
       "  'split_Nlprob': 19.94988269609624,\n",
       "  'phon_Prob': 9.873858485077679e-07,\n",
       "  'phon_Nlprob': 19.94988269609624}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findEntrySatisfying(pred, entries):\n",
    "    return [entry for entry in entries if pred(entry)]\n",
    "def findEntryMatchingOrth(orthword, entries):\n",
    "    return findEntrySatisfying(lambda entry: entry['Word'] == orthword, entries)\n",
    "findEntryMatchingOrth('rigid', filtered_splitprobs_phonprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define representation for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T19:39:22.499309Z",
     "start_time": "2019-02-14T19:39:22.494146Z"
    }
   },
   "outputs": [],
   "source": [
    "# # dottedStringToTuple = lambda ds: tuple(ds.split('.'))\n",
    "# # tupleToDottedString = lambda t: '.'.join(t)\n",
    "\n",
    "# # leftEdge = '⋊'\n",
    "# # rightEdge = '⋉'\n",
    "# # edgeSymbols = set([leftEdge, rightEdge])\n",
    "\n",
    "# def padInputSequenceWithBoundaries(inputSeq):\n",
    "#     temp = list(dottedStringToTuple(inputSeq))\n",
    "#     temp = tuple([leftEdge] + temp + [rightEdge])\n",
    "#     return tupleToDottedString(temp)\n",
    "\n",
    "# def trimBoundariesFromSequence(seq):\n",
    "#     temp = list(dottedStringToTuple(seq))\n",
    "#     if temp[0] == leftEdge:\n",
    "#         temp = temp[1:]\n",
    "#     if temp[-1] == rightEdge:\n",
    "#         temp = temp[:-1]\n",
    "#     return tupleToDottedString(tuple(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.265250Z",
     "start_time": "2018-12-05T04:55:56.261933Z"
    }
   },
   "outputs": [],
   "source": [
    "padWordsWithBoundaries = True\n",
    "# padWordsWithBoundaries = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.521903Z",
     "start_time": "2018-12-05T04:55:56.269089Z"
    }
   },
   "outputs": [],
   "source": [
    "my_phonWords = set()\n",
    "if padWordsWithBoundaries:\n",
    "    padded_phonWords = set(map(padInputSequenceWithBoundaries, phonWords))\n",
    "    assert(set(map(trimBoundariesFromSequence, padded_phonWords)) == phonWords)\n",
    "    my_phonWords = padded_phonWords\n",
    "else:\n",
    "    my_phonWords = phonWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.698730Z",
     "start_time": "2018-12-05T04:55:56.524286Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToProb = {padInputSequenceWithBoundaries(phonWord):getPhonProb(phonWord) for phonWord in phonWords}\n",
    "else:\n",
    "    phonWordToProb = {phonWord:getPhonProb(phonWord) for phonWord in phonWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.710959Z",
     "start_time": "2018-12-05T04:55:56.700966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.ɪ.l.k.t.⋉'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.990014503211299e-07"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(phonWordToProb.keys())[0]\n",
    "phonWordToProb[ list(phonWordToProb.keys())[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.914505Z",
     "start_time": "2018-12-05T04:55:56.713476Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToNlprob = {padInputSequenceWithBoundaries(phonWord):-1.0 * log2(getPhonProb(phonWord)) for phonWord in phonWords}\n",
    "else:\n",
    "    phonWordToNlprob = {phonWord:-1.0 * log2(getPhonProb(phonWord)) for phonWord in phonWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.927585Z",
     "start_time": "2018-12-05T04:55:56.916942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.ɪ.l.k.t.⋉'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20.934452655526634"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(phonWordToNlprob.keys())[0]\n",
    "phonWordToNlprob[ list(phonWordToNlprob.keys())[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export / import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.934638Z",
     "start_time": "2018-12-05T04:55:56.930342Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isNormalized(phonWordToProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:56.944424Z",
     "start_time": "2018-12-05T04:55:56.938110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericmeinhardt/Downloads/c2-jnA'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.101541Z",
     "start_time": "2018-12-05T04:55:56.946788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD-aligned_destressed_pseudocount0 f3_Y0Y1_X0X1.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0 f6_Y0Y1_X0X1.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0 p3YX.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0 p6YX.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0 pYX.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 f3_Y0Y1_X0X1.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 f6_Y0Y1_X0X1.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 p3Y1X01.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 p3YX.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 p6YX.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 pY1X0X1X2.json\r\n",
      "IPhOD-aligned_destressed_pseudocount0.01 pYX.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.116851Z",
     "start_time": "2018-12-05T04:55:57.108260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = my_lexicon_fn[:-4]\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.123351Z",
     "start_time": "2018-12-05T04:55:57.119571Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.130110Z",
     "start_time": "2018-12-05T04:55:57.126578Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.136775Z",
     "start_time": "2018-12-05T04:55:57.133363Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.474116Z",
     "start_time": "2018-12-05T04:55:57.140038Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.796412Z",
     "start_time": "2018-12-05T04:55:57.477058Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_nlprob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToNlprob, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.805065Z",
     "start_time": "2018-12-05T04:55:57.798863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericmeinhardt/Downloads/c2-jnA'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.937995Z",
     "start_time": "2018-12-05T04:55:57.807853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *rob.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:57.956500Z",
     "start_time": "2018-12-05T04:55:57.943872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericmeinhardt/Downloads/c2-jnA'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.086430Z",
     "start_time": "2018-12-05T04:55:57.959125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*rob.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.096494Z",
     "start_time": "2018-12-05T04:55:58.089989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = my_lexicon_fn[:-4] #defined way up above...\n",
    "my_filename_stem\n",
    "\n",
    "my_fn_suffix_filter = '_' + which\n",
    "\n",
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.103071Z",
     "start_time": "2018-12-05T04:55:58.099310Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.191910Z",
     "start_time": "2018-12-05T04:55:58.106697Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToProb_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.202959Z",
     "start_time": "2018-12-05T04:55:58.194384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.ɪ.l.k.t.⋉'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToProb_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.217337Z",
     "start_time": "2018-12-05T04:55:58.206327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.990014503211299e-07"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.990014503211299e-07"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb_in[test_k]\n",
    "phonWordToProb[test_k]\n",
    "phonWordToProb_in[test_k] == phonWordToProb[test_k]\n",
    "assert(phonWordToProb_in[test_k] == phonWordToProb[test_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.226040Z",
     "start_time": "2018-12-05T04:55:58.219750Z"
    }
   },
   "outputs": [],
   "source": [
    "def matchAt(key, dA, dB):\n",
    "    return (key in dA and key in dB) and dA[key] == dB[key]\n",
    "\n",
    "def dicts_match(dA, dB):\n",
    "    allKeys = set.union(set(dA.keys()), set(dB.keys))\n",
    "    missingFromB = {k for k in allKeys if k not in set(dB.keys)}\n",
    "    missingFromA = {k for k in allKeys if k not in set(dA.keys)}\n",
    "    missingKeys = set.union(missingFromB, missingFromA)\n",
    "    \n",
    "    sharedKeys = allKeys - missingKeys\n",
    "    differentValues = {k for k in sharedKeys if not dA[key] == dB[key]}\n",
    "    mismatches = set.union(missingKeys, differentValues)\n",
    "    allMatch = mismatches == set()\n",
    "    return allMatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.280313Z",
     "start_time": "2018-12-05T04:55:58.229090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatchingEntries = [entry for entry in my_phonWords if not phonWordToProb_in[entry] == phonWordToProb[entry]]\n",
    "len(mismatchingEntries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.294957Z",
     "start_time": "2018-12-05T04:55:58.283162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.990014503211299e-07"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb = phonWordToProb_in\n",
    "phonWordToProb[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.360146Z",
     "start_time": "2018-12-05T04:55:58.297546Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToNlprob_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_nlprob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToNlprob_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.370105Z",
     "start_time": "2018-12-05T04:55:58.362491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.ɪ.l.k.t.⋉'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToNlprob_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.384309Z",
     "start_time": "2018-12-05T04:55:58.372820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.934452655526634"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20.934452655526634"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToNlprob_in[test_k]\n",
    "phonWordToNlprob[test_k]\n",
    "phonWordToNlprob_in[test_k] == phonWordToNlprob[test_k]\n",
    "assert(phonWordToNlprob_in[test_k] == phonWordToNlprob[test_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.434955Z",
     "start_time": "2018-12-05T04:55:58.386965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatchingEntries = [entry for entry in my_phonWords if not phonWordToNlprob_in[entry] == phonWordToNlprob[entry]]\n",
    "len(mismatchingEntries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.448136Z",
     "start_time": "2018-12-05T04:55:58.437679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.934452655526634"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToNlprob = phonWordToNlprob_in\n",
    "phonWordToNlprob[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:55:58.580643Z",
     "start_time": "2018-12-05T04:55:58.450517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *rob.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hammond's newdic processing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a json file mapping phonological wordforms to frequencies or probabilities for a word recognition model we need to\n",
    " 1. Remove any entries we don't want or can't use in the word recognition model.\n",
    " 2. Choose a set of phonological wordforms (unstressed or stressed) to map to frequencies/probabilities.\n",
    " 3. Calculate frequencies/probabilities, dealing with the vagaries of homophony (and remembering to normalize ):\n",
    "   - The wordform frequency of each entry is a corpus frequency of the associated orthographic word.\n",
    "   - Some distinct orthographic wordforms share a phonological wordform.\n",
    " 4. Phonological wordforms need to have word edge symbols added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove clutter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:02.578254Z",
     "start_time": "2019-02-14T20:49:02.573111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'ə'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_newdic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:06:54.158665Z",
     "start_time": "2018-12-05T05:06:54.155855Z"
    }
   },
   "outputs": [],
   "source": [
    "# def project_dict(the_dict, keys_to_keep):\n",
    "#     new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "#     return new_dict\n",
    "# project_dict({'Name':'Joe','ID':123,'Job':'clerk'},['Job','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:07.725203Z",
     "start_time": "2019-02-14T20:49:07.696421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'ə', 'Orthography': 'a', 'Frequency': '23178'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim_hammond = list(map(lambda d: project_dict(d,['Orthography','Transcription','Frequency']), hammond_newdic))\n",
    "len(slim_hammond)\n",
    "slim_hammond[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words with triphones that a channel distribution isn't definable for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:09.954001Z",
     "start_time": "2019-02-14T20:49:09.948353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16389"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illegalDiphones_h = diphoneAnalysis_h['illicit'][which_stress + ' stimuli']\n",
    "len(illegalDiphones_h)\n",
    "illegalTriphones_h = triphoneAnalysis_h['illicit'][which_stress + ' stimuli']\n",
    "len(illegalTriphones_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:10.469463Z",
     "start_time": "2019-02-14T20:49:10.460801Z"
    }
   },
   "outputs": [],
   "source": [
    "badDiphones = set()\n",
    "badTriphones = set()\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    badDiphones = set()\n",
    "    badTriphones = set()\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    badDiphones = set(illegalDiphones_h)\n",
    "    badTriphones = set(illegalTriphones_h)\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} instead\".format(which_filter))\n",
    "\n",
    "def getNphones_h(hammond_row, n):\n",
    "#     strNFactors = dsToKfactors(n, hammond_row['Transcription'])\n",
    "    unstrNFactors = dsToKfactors(n, hammond_row['Transcription'])\n",
    "#     print(unstrNFactors)\n",
    "    \n",
    "#     uniqueStrNphones = set(strNFactors)\n",
    "#     uniqueUnstrNphones = set(unstrNFactors)\n",
    "    if which_stress == 'destressed':\n",
    "        return unstrNFactors\n",
    "    elif which_stress == 'stressed':\n",
    "        raise Exception('Stressed Hammond transcriptions not calculated yet.')\n",
    "    else:\n",
    "        raise Exception('Bad which_stress arg.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:11.190925Z",
     "start_time": "2019-02-14T20:49:11.183823Z"
    }
   },
   "outputs": [],
   "source": [
    "def badNphone(nphone, n, badNphones):\n",
    "    if n == 2:\n",
    "        return nphone in badNphones\n",
    "    if n == 3:\n",
    "        return nphone in badNphones\n",
    "    raise Exception('n must be 2 or 3.')\n",
    "\n",
    "def containsBadNphones(row, n, getNphones, badNphones):\n",
    "    phs = getNphones(row, n)\n",
    "    return any([badNphone(ph, n, badNphones) for ph in phs])\n",
    "\n",
    "def rowsWBadNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if containsBadNphones(r, n, getNphones, badNphones)]\n",
    "\n",
    "def observedBadNphones(rows, n, getNphones, badNphones):\n",
    "    return union([getNphones(r, n) for r in rows if containsBadNphones(r, n, getNphones, badNphones)])\n",
    "\n",
    "def onlyRowsWithGoodNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if not containsBadNphones(r, n, getNphones, badNphones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:11.753227Z",
     "start_time": "2019-02-14T20:49:11.663897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03743342892257272"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadTriphs = rowsWBadNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "len(hasBadTriphs)\n",
    "len(slim_hammond)\n",
    "len(hasBadTriphs)/len(slim_hammond) #proportion of the lexicon (by type) that has bad triphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:12.375728Z",
     "start_time": "2019-02-14T20:49:12.282162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03763826300696436"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadDiphs = rowsWBadNphones(slim_hammond, 2, getNphones_h, badDiphones)\n",
    "len(hasBadDiphs)\n",
    "len(slim_hammond)\n",
    "len(hasBadDiphs)/len(slim_hammond) #proportion of the lexicon (by type) that has bad triphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:12.779959Z",
     "start_time": "2019-02-14T20:49:12.775077Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME see how many words there are like this and what they're like, also check sum of probability of such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:13.500195Z",
     "start_time": "2019-02-14T20:49:13.306261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1233"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observedBadNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "len(observedBadNphones(slim_hammond, 3, getNphones_h, badTriphones))\n",
    "# observedBadNphones(slim_hammond, 2, getNphones_h, badDiphones)\n",
    "len(observedBadNphones(slim_hammond, 2, getNphones_h, badDiphones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:14.014152Z",
     "start_time": "2019-02-14T20:49:13.799913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18797"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noBadTriphs_Hammond = onlyRowsWithGoodNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "noBadDiphsOrTriphs_Hammond = onlyRowsWithGoodNphones(noBadTriphs_Hammond, 2, getNphones_h, badDiphones)\n",
    "\n",
    "len(slim_hammond)\n",
    "len(noBadTriphs_Hammond)\n",
    "len(noBadDiphsOrTriphs_Hammond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:14.247250Z",
     "start_time": "2019-02-14T20:49:14.236131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noBadTriphones'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_filter\n",
    "\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    hammond_filtered = slim_hammond\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    hammond_filtered = noBadDiphsOrTriphs_Hammond\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} intsead\".format(which_filter))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose wordforms for the Hammond lexicon distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we want to export a json file( = a list of dictionaries) mapping each phonological wordform to a number representing a frequency or probability or negative log probability of that phonological wordform. Accordingly, we must pick a phonological wordform representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:16.803895Z",
     "start_time": "2019-02-14T20:49:16.797281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Transcription'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_stress\n",
    "\n",
    "if which_stress == 'destressed':\n",
    "    wordform_field = 'Transcription'\n",
    "elif which_stress == 'stressed':\n",
    "#     wordform_field = 'StTrn'\n",
    "    raise Exception(\"No stressed transcription is currently supported for Hammond's newdic.\")\n",
    "else:\n",
    "    raise Exception(\"'which_stress' must be one of 'destressed' or 'stressed'; got {0} instead\".format(which_stress))\n",
    "\n",
    "wordform_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:17.464251Z",
     "start_time": "2019-02-14T20:49:17.454865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18295"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonologicalWords_h = [entry[wordform_field] for entry in hammond_filtered]\n",
    "len(phonologicalWords_h)\n",
    "len(set(phonologicalWords_h))\n",
    "len(phonologicalWords_h) - len(set(phonologicalWords_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate frequencies, probabilities, and informativities of phonological wordforms in Hammond's newdic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We either want a mapping from transcription to probability (by some measure) or from transcription to negative log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up probability of orthographic words in SUBTLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:05.719482Z",
     "start_time": "2018-12-05T05:07:05.710402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'æ.b.ɚ.ɪ.dʒ.ɪ.n.l',\n",
       " 'Orthography': 'aboriginal',\n",
       " 'Frequency': '1'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entry = hammond_filtered[55]\n",
    "test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:06.402273Z",
     "start_time": "2018-12-05T05:07:06.395907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthwordToSXF[test_entry['Orthography']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:07.256485Z",
     "start_time": "2018-12-05T05:07:07.253349Z"
    }
   },
   "outputs": [],
   "source": [
    "def getHammondOrthSFreq(hammond_orthWord):\n",
    "    if hammond_orthWord in orthwordToSXF:\n",
    "        return orthwordToSXF[hammond_orthWord]\n",
    "    return None\n",
    "#     raise Exception('Orthographic word must be in SUBTLEX!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:08.249692Z",
     "start_time": "2018-12-05T05:07:08.237389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_words = [entry['Orthography'] for entry in hammond_filtered]\n",
    "len(hammond_filtered)\n",
    "len(hammond_words)\n",
    "len(set(hammond_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:09.023487Z",
     "start_time": "2018-12-05T05:07:09.019754Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_words = set(hammond_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:09.809039Z",
     "start_time": "2018-12-05T05:07:09.711117Z"
    }
   },
   "outputs": [],
   "source": [
    "# This maps each orthographic word in hammond_filtered to a SUBTLEX frequency taken from IPhOD\n",
    "wordToSFreq = {w:getHammondOrthSFreq(w) for w in hammond_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:10.681617Z",
     "start_time": "2018-12-05T05:07:10.661089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16386"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2407"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definedSFwords = {w:float(wordToSFreq[w]) for w in wordToSFreq if wordToSFreq[w] != None}\n",
    "len(hammond_words)\n",
    "len(definedSFwords)\n",
    "len(hammond_words) - len(definedSFwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:14.712153Z",
     "start_time": "2018-12-05T05:07:11.562393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16386"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SFdefined_hammond_words = list(definedSFwords.keys())\n",
    "filtered_hammond_SFdefined = [entry for entry in hammond_filtered if entry['Orthography'] in SFdefined_hammond_words]\n",
    "len(hammond_filtered)\n",
    "len(filtered_hammond_SFdefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:14.720955Z",
     "start_time": "2018-12-05T05:07:14.714871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40454906.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumFreqs = sum(definedSFwords.values())\n",
    "sumFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:14.865701Z",
     "start_time": "2018-12-05T05:07:14.862205Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow\n",
    "wordToProb = lambda orthword: definedSFwords[orthword] / sumFreqs\n",
    "wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:07:15.515633Z",
     "start_time": "2018-12-05T05:07:15.512538Z"
    }
   },
   "outputs": [],
   "source": [
    "entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hammond_filtered_freqaligned = filtered_hammond_SFdefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-up probability of orthographic words in IPhOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPhOD's frequency estimates are better than whatever's associated with Hammond's newdic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:10:12.702409Z",
     "start_time": "2018-11-19T10:10:12.693908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'ə.b.ʌ.v', 'Orthography': 'above', 'Frequency': '301'}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_entry = hammond_filtered[55]\n",
    "# test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:10:12.730484Z",
     "start_time": "2018-11-19T10:10:12.703229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('Indx', '131'),\n",
       "              ('NPhon', '4'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '14.811101320914501'),\n",
       "              ('Prob', '3.4786780237814795e-05'),\n",
       "              ('SCDcnt', '1778'),\n",
       "              ('SFreq', '48.88'),\n",
       "              ('StTrn', 'ə0.b.ʌ1.v'),\n",
       "              ('UnTrn', 'ə.b.ʌ.v'),\n",
       "              ('Word', 'above'),\n",
       "              ('strBPAV', '0.00164573'),\n",
       "              ('strCBPAV', '0.00147589'),\n",
       "              ('strCDEN', '7'),\n",
       "              ('strCLCPOSPAV', '0.01880416'),\n",
       "              ('strCPOSPAV', '0.01557133'),\n",
       "              ('strCTPAV', '0.00002837'),\n",
       "              ('strDENS', '2'),\n",
       "              ('strFBPAV', '0.00310272'),\n",
       "              ('strFDEN', '0.16'),\n",
       "              ('strFLCPOSPAV', '0.04887563'),\n",
       "              ('strFPOSPAV', '0.02609009'),\n",
       "              ('strFTPAV', '0.00001221'),\n",
       "              ('strLBPAV', '0.00150352'),\n",
       "              ('strLCPOSPAV', '0.01205451'),\n",
       "              ('strLDEN', '0.07'),\n",
       "              ('strLLCPOSPAV', '0.01306863'),\n",
       "              ('strLPOSPAV', '0.01572887'),\n",
       "              ('strLTPAV', '0.00001380'),\n",
       "              ('strPOSPAV', '0.01710418'),\n",
       "              ('strTPAV', '0.00001059'),\n",
       "              ('unsBPAV', '0.00327491'),\n",
       "              ('unsCBPAV', '0.00279985'),\n",
       "              ('unsCDEN', '7'),\n",
       "              ('unsCLCPOSPAV', '0.02534145'),\n",
       "              ('unsCPOSPAV', '0.02232539'),\n",
       "              ('unsCTPAV', '0.00037722'),\n",
       "              ('unsDENS', '2'),\n",
       "              ('unsFBPAV', '0.00414321'),\n",
       "              ('unsFDEN', '0.16'),\n",
       "              ('unsFLCPOSPAV', '0.05298109'),\n",
       "              ('unsFPOSPAV', '0.03131507'),\n",
       "              ('unsFTPAV', '0.00018279'),\n",
       "              ('unsLBPAV', '0.00295668'),\n",
       "              ('unsLCPOSPAV', '0.01917614'),\n",
       "              ('unsLDEN', '0.07'),\n",
       "              ('unsLLCPOSPAV', '0.02015231'),\n",
       "              ('unsLPOSPAV', '0.02419034'),\n",
       "              ('unsLTPAV', '0.00039288'),\n",
       "              ('unsPOSPAV', '0.02687065'),\n",
       "              ('unsTPAV', '0.00046897')])]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# findEntryMatchingOrth(test_entry['Orthography'], lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:10:12.742421Z",
     "start_time": "2018-11-19T10:10:12.731298Z"
    }
   },
   "outputs": [],
   "source": [
    "# def getHammondOrthSFreq(hammond_orthWord):\n",
    "#     matchingEntries = findEntryMatchingOrth(hammond_orthWord, lexicon)\n",
    "#     if len(matchingEntries) == 0:\n",
    "#         return None\n",
    "#     matchingSFreq = matchingEntries[0]['SFreq'] #all entries with the same orthography have the same SUBTLEX frequency measure\n",
    "#     return matchingSFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:10:12.759092Z",
     "start_time": "2018-11-19T10:10:12.743288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17211"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17211"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17211"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hammond_words = [entry['Orthography'] for entry in hammond_filtered]\n",
    "# len(hammond_filtered)\n",
    "# len(hammond_words)\n",
    "# len(set(hammond_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:10:12.768789Z",
     "start_time": "2018-11-19T10:10:12.759921Z"
    }
   },
   "outputs": [],
   "source": [
    "# hammond_words = set(hammond_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:38.040310Z",
     "start_time": "2018-11-19T10:10:12.769691Z"
    }
   },
   "outputs": [],
   "source": [
    "# This maps each orthographic word in hammond_filtered to a SUBTLEX frequency taken from IPhOD\n",
    "# wordToSFreq = {w:getHammondOrthSFreq(w) for w in hammond_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:38.042793Z",
     "start_time": "2018-11-19T10:13:38.041438Z"
    }
   },
   "outputs": [],
   "source": [
    "# wordToSFreq2 = dict()\n",
    "# total = len(hammond_words)\n",
    "# benchmarks = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [b/100.0 * total for b in benchmarks]\n",
    "# for i,w in enumerate(hammond_words):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to remove every word that we can't align with IPhOD to have a meaningful probability distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:38.057510Z",
     "start_time": "2018-11-19T10:13:38.043617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17211"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11883"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5328"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definedSFwords = {w:float(wordToSFreq[w]) for w in wordToSFreq if wordToSFreq[w] != None}\n",
    "# len(hammond_words)\n",
    "# len(definedSFwords)\n",
    "# len(hammond_words) - len(definedSFwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:39.562799Z",
     "start_time": "2018-11-19T10:13:38.058386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17211"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11883"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SFdefined_hammond_words = list(definedSFwords.keys())\n",
    "# filtered_hammond_SFdefined = [entry for entry in hammond_filtered if entry['Orthography'] in SFdefined_hammond_words]\n",
    "# len(hammond_filtered)\n",
    "# len(filtered_hammond_SFdefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:39.565887Z",
     "start_time": "2018-11-19T10:13:39.563696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735239.2000000038"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sumFreqs = sum(definedSFwords.values())\n",
    "# sumFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:39.571067Z",
     "start_time": "2018-11-19T10:13:39.566693Z"
    }
   },
   "outputs": [],
   "source": [
    "# from math import log, log2, pow\n",
    "# wordToProb = lambda orthword: definedSFwords[orthword] / sumFreqs\n",
    "# wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T10:13:39.575132Z",
     "start_time": "2018-11-19T10:13:39.571934Z"
    }
   },
   "outputs": [],
   "source": [
    "# entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "# entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hammond_filtered_freqaligned = filtered_hammond_SFdefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up COCA unigram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:28.575953Z",
     "start_time": "2019-02-14T20:49:28.572357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'æ.b.ɚ.ɪ.dʒ.ɪ.n.l',\n",
       " 'Orthography': 'aboriginal',\n",
       " 'Frequency': '1'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entry = hammond_filtered[55]\n",
    "test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:30.629546Z",
     "start_time": "2019-02-14T20:49:30.622326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_orth_words = [row['Orthography'] for row in hammond_filtered]\n",
    "len(hammond_orth_words)\n",
    "len(set(hammond_orth_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:32.182419Z",
     "start_time": "2019-02-14T20:49:32.147974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18180"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hammond_filtered)\n",
    "hammond_filtered_COCA_alignable = [row for row in hammond_filtered if row['Orthography'] in coca_unigram_counts]\n",
    "len(hammond_filtered_COCA_alignable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:32.471093Z",
     "start_time": "2019-02-14T20:49:32.459203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a&e', 'a&j', 'a&m', 'a&m-commerce', 'a&m-corpus']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coca_unigram_counts.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:34.077698Z",
     "start_time": "2019-02-14T20:49:34.075310Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_filtered_freqaligned = hammond_filtered_COCA_alignable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:34.528531Z",
     "start_time": "2019-02-14T20:49:34.517473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'307,863,432.0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammandOrthWordToCOCAFreq = Counter({w:coca_unigram_counts[w]\n",
    "                                     for w in hammond_orth_words})\n",
    "sumFreqs = sum(hammandOrthWordToCOCAFreq.values())\n",
    "'{:,}'.format( sumFreqs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:36.920173Z",
     "start_time": "2019-02-14T20:49:36.917409Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow\n",
    "wordToProb = lambda orthword: hammandOrthWordToCOCAFreq[orthword] / sumFreqs\n",
    "wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:37.788144Z",
     "start_time": "2019-02-14T20:49:37.785607Z"
    }
   },
   "outputs": [],
   "source": [
    "entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map each remaining phonological wordform in Hammond's newdic to a frequency/probability (marginalizing over orthographic wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:49:40.853914Z",
     "start_time": "2019-02-14T20:49:40.843512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transcription'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['l.u.m.ə.n.ɪ.s',\n",
       " 'v.ʌ.l.tʃ.ɚ',\n",
       " 'ə.k.j.u.t',\n",
       " 'k.l.ɑ.p',\n",
       " 'n.ɛ.k.s.ɪ.s',\n",
       " 'h.ɑ.s.p.ɪ.s',\n",
       " 'b.i.θ.ɪ.ŋ.k',\n",
       " 'b.ə.l.aɪ',\n",
       " 'k.æ.t.ə.m.ɚ.æ.n',\n",
       " 'b.ə.l.ɪ.s.t.ɪ.k']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform_field\n",
    "\n",
    "phonWords_h = set([entry[wordform_field] for entry in hammond_filtered_freqaligned])\n",
    "len(phonWords_h)\n",
    "list(phonWords_h)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:50:05.699487Z",
     "start_time": "2019-02-14T20:49:53.565851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17262"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['b.i.t.l',\n",
       " 'l.ɛ.v.i',\n",
       " 's.ɛ.l',\n",
       " 's.t.r.eɪ.t',\n",
       " 'θ.r.u',\n",
       " 'p.ʊ.r',\n",
       " 'k.ɑ.m.p.l.ə.m.ɛ.n.t.ɚ.i',\n",
       " 'f.oʊ.r.θ',\n",
       " 'p.ɚ.l',\n",
       " 'k.u.l.i']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 401, 3: 36, 4: 2})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entriesMatchingPhonWord(phonWord):\n",
    "    return [entry for entry in hammond_filtered_freqaligned if entry[wordform_field] == phonWord]\n",
    "phonWord_h_To_entries = {phonWord:entriesMatchingPhonWord(phonWord) for phonWord in phonWords_h}\n",
    "\n",
    "unambiguousPhonWords = [w for w in phonWords_h if len(phonWord_h_To_entries[w]) == 1]\n",
    "ambiguousPhonWords = [w for w in phonWords_h if len(phonWord_h_To_entries[w]) > 1]\n",
    "len(phonWords_h)\n",
    "len(unambiguousPhonWords)\n",
    "len(ambiguousPhonWords)\n",
    "list(ambiguousPhonWords)[:10]\n",
    "orthWordCounts = [len(phonWord_h_To_entries[w]) for w in ambiguousPhonWords]\n",
    "Counter(orthWordCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:50:11.952657Z",
     "start_time": "2019-02-14T20:50:11.944713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Transcription': 'k.j.u', 'Orthography': 'cue', 'Frequency': '1'},\n",
       " {'Transcription': 'k.j.u', 'Orthography': 'q', 'Frequency': '7'},\n",
       " {'Transcription': 'k.j.u', 'Orthography': 'queue', 'Frequency': '0'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1.0105129991534688e-05, 1.755973408365044e-05, 2.387422225579555e-06]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.005228630076468e-05"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.005228630076468e-05"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginalizes over orthographic words with phonological transcription phonWord\n",
    "def phonWord_to_marginalProb(phonWord):\n",
    "    matchingEntries = phonWord_h_To_entries[phonWord]\n",
    "    pPhonWord = sum(map(entryToProb, matchingEntries))\n",
    "    return pPhonWord\n",
    "phonWord_h_To_entries['k.j.u']\n",
    "list(map(entryToProb, phonWord_h_To_entries['k.j.u']))\n",
    "sum(list(map(entryToProb, phonWord_h_To_entries['k.j.u'])))\n",
    "phonWord_to_marginalProb('k.j.u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a minimum probability threshold for inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:51:06.480658Z",
     "start_time": "2019-02-14T20:51:06.439928Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb = {padInputSequenceWithBoundaries(w):phonWord_to_marginalProb(w) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:51:07.415547Z",
     "start_time": "2019-02-14T20:51:07.409816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('⋊.l.u.m.ə.n.ɪ.s.⋉', 5.6680976648113246e-06)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_h = list(phonWordToProb.items())\n",
    "len(pW_h)\n",
    "pW_h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:51:08.703533Z",
     "start_time": "2019-02-14T20:51:08.695691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('⋊.d.ɑ.m.ə.n.ɪ.r.⋉', 9.744580512569612e-09),\n",
       " ('⋊.p.ɚ.d.ʊ.r.ə.b.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.æ.k.s.ɛ.p.t.eɪ.ʃ.ɪ.n.⋉', 9.744580512569612e-09),\n",
       " ('⋊.f.r.ʌ.k.t.ə.f.aɪ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.p.r.ə.m.i.θ.i.ə.m.⋉', 9.744580512569612e-09),\n",
       " ('⋊.k.aʊ.n.t.ɪ.ŋ.h.aʊ.s.⋉', 9.744580512569612e-09),\n",
       " ('⋊.f.æ.m.ɪ.ʃ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ə.d.ʌ.l.t.ɚ.ə.n.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.r.ɔɪ.s.t.ɚ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.m.ɛ.g.ə.s.aɪ.k.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.f.ɑ.n.d.u.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ɛ.f.ɚ.v.ɛ.s.⋉', 9.744580512569612e-09),\n",
       " ('⋊.v.æ.p.ə.d.l.i.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ə.θ.ɚ.s.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.æ.ŋ.g.l.w.ɚ.m.⋉', 9.744580512569612e-09),\n",
       " ('⋊.v.ʌ.l.k.ə.n.aɪ.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.l.æ.k.r.ə.m.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ə.m.eɪ.n.⋉', 9.744580512569612e-09),\n",
       " ('⋊.k.ɑ.r.m.ɪ.n.ɪ.t.ɪ.v.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ɪ.t.ɚ.b.i.ə.m.⋉', 9.744580512569612e-09)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowToHi = sorted(pW_h, key=lambda pair: pair[1], reverse=False)\n",
    "lowToHi[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:51:13.647206Z",
     "start_time": "2019-02-14T20:51:13.643835Z"
    }
   },
   "outputs": [],
   "source": [
    "def trimEdges(phonWord):\n",
    "    return t2ds(ds2t(phonWord)[1:-1])\n",
    "\n",
    "def phonWordToOrthWords_Hammond(phonWord):\n",
    "    w_trimmed = trimEdges(phonWord)\n",
    "    matchingEntries = [entry for entry in hammond_filtered_freqaligned if entry[wordform_field] == w_trimmed]\n",
    "    matchingOrthWords = set([entry['Orthography'] for entry in matchingEntries])\n",
    "    return matchingOrthWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:51:24.212667Z",
     "start_time": "2019-02-14T20:51:14.672249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current minimum probability threshold: 1e-08\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 9967\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 17589\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.ʌ.n.d.ɪ.s.i.v.⋉ \t {'undeceive'} \t 1.299277e-08\n",
      "\t⋊.v.ʌ.l.g.ɚ.aɪ.z.⋉ \t {'vulgarize'} \t 1.299277e-08\n",
      "\t⋊.s.ɪ.ŋ.k.ə.p.eɪ.t.⋉ \t {'syncopate'} \t 1.299277e-08\n",
      "\t⋊.ɪ.n.k.ʌ.l.p.ə.b.l.⋉ \t {'inculpable'} \t 1.299277e-08\n",
      "\t⋊.v.aɪ.æ.t.ɪ.k.m.⋉ \t {'viaticum'} \t 1.299277e-08\n",
      "\t⋊.ə.dʒ.ʊ.r.⋉ \t {'adjure'} \t 1.299277e-08\n",
      "\t⋊.t.æ.b.j.ʊ.l.eɪ.t.ɚ.⋉ \t {'tabulator'} \t 1.299277e-08\n",
      "\t⋊.v.ɛ.n.tʃ.ɚ.ə.s.⋉ \t {'venturous'} \t 1.299277e-08\n",
      "\t⋊.k.ɑ.r.ə.g.eɪ.t.⋉ \t {'corrugate'} \t 1.299277e-08\n",
      "\t⋊.g.l.ɪ.s.t.ɚ.⋉ \t {'glister'} \t 1.299277e-08\n",
      "\t⋊.d.ɪ.ʃ.ɛ.v.l.⋉ \t {'dishevel'} \t 1.299277e-08\n",
      "\t⋊.p.r.oʊ.n.ʌ.n.s.i.ə.m.ɛ.n.t.oʊ.⋉ \t {'pronunciamento'} \t 1.299277e-08\n",
      "\t⋊.p.oʊ.l.t.⋉ \t {'poult'} \t 1.299277e-08\n",
      "\t⋊.j.ʌ.ŋ.k.ɚ.⋉ \t {'younker'} \t 1.299277e-08\n",
      "\t⋊.k.ʌ.m.f.ə.t.⋉ \t {'comfit'} \t 1.299277e-08\n",
      "\t⋊.k.æ.m.p.s.t.u.l.⋉ \t {'campstool'} \t 1.299277e-08\n",
      "\t⋊.m.ə.l.oʊ.d.i.ə.n.⋉ \t {'melodeon'} \t 1.299277e-08\n",
      "\t⋊.t.ɛ.t.æ.n.ɪ.k.⋉ \t {'tetanic'} \t 1.299277e-08\n",
      "\t⋊.ɪ.m.ɪ.t.ɪ.g.ə.b.l.⋉ \t {'immitigable'} \t 1.299277e-08\n",
      "\t⋊.æ.p.ɚ.t.eɪ.n.⋉ \t {'appertain'} \t 1.299277e-08\n",
      " \n",
      "Current minimum probability threshold: 1e-07\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 9417\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 15662\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.f.ə.l.æ.n.d.ɚ.⋉ \t {'philander'} \t 1.006940e-07\n",
      "\t⋊.s.ɪ.s.ə.r.oʊ.n.i.⋉ \t {'cicerone'} \t 1.006940e-07\n",
      "\t⋊.b.ʊ.l.r.ʌ.ʃ.⋉ \t {'bulrush'} \t 1.006940e-07\n",
      "\t⋊.ə.l.ɪ.t.⋉ \t {'alit'} \t 1.006940e-07\n",
      "\t⋊.ʌ.n.d.ɚ.b.ɪ.d.⋉ \t {'underbid'} \t 1.006940e-07\n",
      "\t⋊.l.oʊ.dʒ.ɪ.ʃ.ɪ.n.⋉ \t {'logician'} \t 1.006940e-07\n",
      "\t⋊.k.ə.n.d.u.s.⋉ \t {'conduce'} \t 1.006940e-07\n",
      "\t⋊.f.ʊ.l.s.k.æ.p.⋉ \t {'foolscap'} \t 1.006940e-07\n",
      "\t⋊.n.ɑ.b.d.⋉ \t {'knobbed'} \t 1.006940e-07\n",
      "\t⋊.p.w.ɪ.s.n.s.⋉ \t {'puissance'} \t 1.006940e-07\n",
      "\t⋊.k.ə.n.f.æ.b.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉ \t {'confabulation'} \t 1.006940e-07\n",
      "\t⋊.d.ɪ.s.p.ə.z.ɛ.s.⋉ \t {'dispossess'} \t 1.006940e-07\n",
      "\t⋊.d.ɪ.b.ɪ.l.ɪ.t.eɪ.t.⋉ \t {'debilitate'} \t 1.006940e-07\n",
      "\t⋊.k.æ.v.l.⋉ \t {'cavil'} \t 1.006940e-07\n",
      "\t⋊.ɪ.m.p.æ.s.ɪ.v.ə.t.i.⋉ \t {'impassivity'} \t 1.006940e-07\n",
      "\t⋊.ɪ.n.f.l.æ.m.ə.b.l.⋉ \t {'inflammable'} \t 1.006940e-07\n",
      "\t⋊.h.j.u.m.ə.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉ \t {'humanization'} \t 1.006940e-07\n",
      "\t⋊.v.ɪ.ʃ.i.s.w.ɑ.z.⋉ \t {'vichyssoise'} \t 1.006940e-07\n",
      "\t⋊.n.ʌ.b.ɪ.n.⋉ \t {'nubbin'} \t 1.006940e-07\n",
      "\t⋊.m.æ.l.ə.d.r.ɔɪ.t.⋉ \t {'maladroit'} \t 1.006940e-07\n",
      " \n",
      "Current minimum probability threshold: 5e-07\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 8463\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 12565\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.ɪ.n.f.ɚ.m.ə.t.i.⋉ \t {'infirmity'} \t 5.002218e-07\n",
      "\t⋊.k.ə.n.t.r.aɪ.v.ə.n.s.⋉ \t {'contrivance'} \t 5.002218e-07\n",
      "\t⋊.m.æ.d.h.aʊ.s.⋉ \t {'madhouse'} \t 5.002218e-07\n",
      "\t⋊.ə.l.æ.k.r.ə.t.i.⋉ \t {'alacrity'} \t 5.002218e-07\n",
      "\t⋊.m.æ.g.p.aɪ.⋉ \t {'magpie'} \t 5.002218e-07\n",
      "\t⋊.ɪ.n.k.ɑ.n.t.n.ɪ.n.t.⋉ \t {'incontinent'} \t 5.002218e-07\n",
      "\t⋊.d.aɪ.ɚ.ə.s.t.⋉ \t {'diarist'} \t 5.002218e-07\n",
      "\t⋊.ə.f.ɪ.ʃ.ɪ.s.⋉ \t {'officious'} \t 5.002218e-07\n",
      "\t⋊.ɪ.n.k.w.ɪ.z.ɪ.t.ɚ.⋉ \t {'inquisitor'} \t 5.002218e-07\n",
      "\t⋊.k.w.aɪ.n.aɪ.n.⋉ \t {'quinine'} \t 5.002218e-07\n",
      "\t⋊.k.æ.v.l.k.eɪ.d.⋉ \t {'cavalcade'} \t 5.002218e-07\n",
      "\t⋊.æ.p.r.ə.b.eɪ.ʃ.ɪ.n.⋉ \t {'approbation'} \t 5.002218e-07\n",
      "\t⋊.p.ɑ.p.j.ʊ.l.ɚ.aɪ.z.⋉ \t {'popularize'} \t 5.002218e-07\n",
      "\t⋊.k.r.aʊ.t.⋉ \t {'kraut'} \t 5.002218e-07\n",
      "\t⋊.p.ə.t.ɛ.n.tʃ.i.æ.l.ɪ.t.i.⋉ \t {'potentiality'} \t 5.002218e-07\n",
      "\t⋊.p.ɚ.æ.m.ə.d.l.⋉ \t {'pyramidal'} \t 5.002218e-07\n",
      "\t⋊.m.oʊ.l.t.⋉ \t {'molt'} \t 5.002218e-07\n",
      "\t⋊.m.ə.r.i.n.oʊ.⋉ \t {'merino'} \t 5.002218e-07\n",
      "\t⋊.s.ɛ.n.t.r.ə.l.aɪ.z.⋉ \t {'centralize'} \t 5.002218e-07\n",
      "\t⋊.t.æ.k.t.ɪ.ʃ.ɪ.n.⋉ \t {'tactician'} \t 5.002218e-07\n",
      " \n",
      "Current minimum probability threshold: 1e-06\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 7791\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 10633\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.s.ʊ.p.aɪ.n.⋉ \t {'supine'} \t 1.000444e-06\n",
      "\t⋊.v.ə.v.eɪ.ʃ.ɪ.s.⋉ \t {'vivacious'} \t 1.000444e-06\n",
      "\t⋊.k.ɑ.m.p.oʊ.t.⋉ \t {'compote'} \t 1.000444e-06\n",
      "\t⋊.t.r.ɪ.p.t.ɪ.k.⋉ \t {'triptych'} \t 1.000444e-06\n",
      "\t⋊.æ.m.ɪ.k.ə.b.l.⋉ \t {'amicable'} \t 1.000444e-06\n",
      "\t⋊.g.r.ɪ.s.t.⋉ \t {'grist'} \t 1.000444e-06\n",
      "\t⋊.d.ɛ.v.ə.s.t.eɪ.t.⋉ \t {'devastate'} \t 1.000444e-06\n",
      "\t⋊.k.l.oʊ.ð.⋉ \t {'clothe'} \t 1.000444e-06\n",
      "\t⋊.w.ɑ.l.ə.p.⋉ \t {'wallop'} \t 1.000444e-06\n",
      "\t⋊.r.ɪ.v.oʊ.k.⋉ \t {'revoke'} \t 1.000444e-06\n",
      "\t⋊.p.ɪ.k.ɪ.ŋ.z.⋉ \t {'pickings'} \t 1.000444e-06\n",
      "\t⋊.r.ɛ.t.r.oʊ.g.r.eɪ.d.⋉ \t {'retrograde'} \t 1.000444e-06\n",
      "\t⋊.ɪ.n.t.r.ə.m.j.ʊ.r.l.⋉ \t {'intramural'} \t 1.000444e-06\n",
      "\t⋊.d.ɪ.m.ə.n.u.ʃ.ɪ.n.⋉ \t {'diminution'} \t 1.000444e-06\n",
      "\t⋊.k.ɑ.n.f.ə.s.k.eɪ.t.⋉ \t {'confiscate'} \t 1.000444e-06\n",
      "\t⋊.ɪ.m.p.r.ɑ.b.ə.b.l.i.⋉ \t {'improbably'} \t 1.000444e-06\n",
      "\t⋊.ə.g.eɪ.p.⋉ \t {'agape'} \t 1.000444e-06\n",
      "\t⋊.v.ɚ.m.l.⋉ \t {'vermeil'} \t 1.003692e-06\n",
      "\t⋊.m.ʌ.t.n.⋉ \t {'mutton'} \t 1.003692e-06\n",
      "\t⋊.t.ɪ.n.s.l.⋉ \t {'tinsel'} \t 1.003692e-06\n",
      " \n",
      "Current minimum probability "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 5000 exceeded with 6135 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def readableEntry(phonWordProbPair):\n",
    "    print('\\t{0} \\t {1} \\t {2:4e}'.format(phonWordProbPair[0], phonWordToOrthWords_Hammond(phonWordProbPair[0]), phonWordProbPair[1]))\n",
    "\n",
    "lowToHi_Ws = [pair[0] for pair in lowToHi]\n",
    "lowToHi_triphones = lexiconTo3factors(lowToHi_Ws)\n",
    "\n",
    "minProbThresholds = [1.0e-08, 1.0e-07, 5.0e-07, 1.0e-06, 1.0e-05]\n",
    "for minProb in minProbThresholds:\n",
    "    lowToHi_aboveThreshold = [each for each in lowToHi if each[1] >= minProb]\n",
    "    lowToHi_aT_triphones = lexiconTo3factors([pair[0] for pair in lowToHi_aboveThreshold])\n",
    "    \n",
    "    print('Current minimum probability threshold: {0}'.format(minProb))\n",
    "    print('\\t|triphones| before vs. after cut-off: {0} vs. {1}'.format(len(lowToHi_triphones), len(lowToHi_aT_triphones)))\n",
    "    print('\\t|Ws| before vs. after cut-off: {0} vs. {1}'.format(len(lowToHi), len(lowToHi_aboveThreshold)))\n",
    "    print('Below are the 20 least probable phonological wordforms with probability above the threshold:')\n",
    "    for entry in lowToHi_aboveThreshold[:20]:\n",
    "#         print(entry)\n",
    "        readableEntry(entry)\n",
    "#     dict(lowToHi_aboveThreshold[:20])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5e-07` seems like a reasonable compromise between not having too many weird words that will skew recognition probabilities and not losing too many wordforms or triphones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:16.687976Z",
     "start_time": "2019-02-14T20:54:16.669551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990899341367702"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability mass of words lost due to thresholding = 0.0009100658632298364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minProb = 5e-07\n",
    "lowToHi_aboveThreshold = [each for each in lowToHi if each[1] >= minProb]\n",
    "thresholdedLexicon = Counter(dict(lowToHi_aboveThreshold))\n",
    "sumFreqs = sum(thresholdedLexicon.values())\n",
    "sumFreqs\n",
    "print('Probability mass of words lost due to thresholding = {0}'.format(1.0 - sumFreqs))\n",
    "\n",
    "thresholdedLexicon = {k:thresholdedLexicon[k]/sumFreqs for k in thresholdedLexicon}\n",
    "sum(thresholdedLexicon.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:24.942343Z",
     "start_time": "2019-02-14T20:54:24.938244Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWord_to_marginalProb = lambda w: thresholdedLexicon[padInputSequenceWithBoundaries(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:26.788381Z",
     "start_time": "2019-02-14T20:54:26.758454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12565"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12565"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phonWords_h)\n",
    "len(thresholdedLexicon)\n",
    "phonWords_h = {w for w in phonWords_h if padInputSequenceWithBoundaries(w) in thresholdedLexicon}\n",
    "len(phonWords_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define representation for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:34.175434Z",
     "start_time": "2019-02-14T20:54:34.173277Z"
    }
   },
   "outputs": [],
   "source": [
    "padWordsWithBoundaries = True\n",
    "# padWordsWithBoundaries = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:44.506713Z",
     "start_time": "2019-02-14T20:54:44.474052Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToProb = {padInputSequenceWithBoundaries(w):phonWord_to_marginalProb(w) for w in phonWords_h}\n",
    "else:\n",
    "    phonWordToProb = {w:phonWord_to_marginalProb(w) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:46.960518Z",
     "start_time": "2019-02-14T20:54:46.927102Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToNlProb = {padInputSequenceWithBoundaries(w):-1 * log2(phonWord_to_marginalProb(w)) for w in phonWords_h}\n",
    "else:\n",
    "    phonWordToNlProb = {w:-1 * log2(phonWord_to_marginalProb(w)) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:48.815346Z",
     "start_time": "2019-02-14T20:54:48.812824Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isNormalized(phonWordToProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:50.415708Z",
     "start_time": "2019-02-14T20:54:50.411804Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:50.994529Z",
     "start_time": "2019-02-14T20:54:50.988916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:51.986472Z",
     "start_time": "2019-02-14T20:54:51.867516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hammond-aligned_destressed_fourCousins.json\r\n",
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      " Hammond-aligned_destressed_oneCousins.json\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat012X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3X_hat012Y012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012X012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pYX.json'\r\n",
      "'Hammond-aligned_destressed pX0i.json'\r\n",
      "'Hammond-aligned_destressed pXi.json'\r\n",
      "'Hammond-aligned_destressed pXiPrimeX0i.json'\r\n",
      "'Hammond-aligned_destressed pX.json'\r\n",
      "'Hammond-aligned_destressed pXjX.json'\r\n",
      "'Hammond-aligned_destressed pxX0i.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      " Hammond-aligned_destressed_threeCousins.json\r\n",
      " Hammond-aligned_destressed_twoCousins.json\r\n",
      "'Hammond-aligned_destressed x0iToWs.json'\r\n",
      " Hammond-aligned_destressed_zeroCousins.json\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:52.837601Z",
     "start_time": "2019-02-14T20:54:52.832549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = hammond_fn[:-4]\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:53.924301Z",
     "start_time": "2019-02-14T20:54:53.920163Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:54.426877Z",
     "start_time": "2019-02-14T20:54:54.422778Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:54.839436Z",
     "start_time": "2019-02-14T20:54:54.834182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:55.368821Z",
     "start_time": "2019-02-14T20:54:55.234329Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:55.679331Z",
     "start_time": "2019-02-14T20:54:55.669923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:56.436543Z",
     "start_time": "2019-02-14T20:54:56.320261Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToNlProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:57.285560Z",
     "start_time": "2019-02-14T20:54:57.280643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:57.923735Z",
     "start_time": "2019-02-14T20:54:57.805219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hammond-aligned_destressed_fourCousins.json\r\n",
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      " Hammond-aligned_destressed_oneCousins.json\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat012X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3X_hat012Y012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012X012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pYX.json'\r\n",
      "'Hammond-aligned_destressed pX0i.json'\r\n",
      "'Hammond-aligned_destressed pXi.json'\r\n",
      "'Hammond-aligned_destressed pXiPrimeX0i.json'\r\n",
      "'Hammond-aligned_destressed pX.json'\r\n",
      "'Hammond-aligned_destressed pXjX.json'\r\n",
      "'Hammond-aligned_destressed pxX0i.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      " Hammond-aligned_destressed_threeCousins.json\r\n",
      " Hammond-aligned_destressed_twoCousins.json\r\n",
      "'Hammond-aligned_destressed x0iToWs.json'\r\n",
      " Hammond-aligned_destressed_zeroCousins.json\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:54:59.802853Z",
     "start_time": "2019-02-14T20:54:59.797680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:00.224431Z",
     "start_time": "2019-02-14T20:55:00.102533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hammond-aligned_destressed_fourCousins.json\r\n",
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      " Hammond-aligned_destressed_oneCousins.json\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat012X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3X_hat012Y012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X012_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Xhat1X1_np.npy'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y012X012_np.dat'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY012.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.01 pYX.json'\r\n",
      "'Hammond-aligned_destressed pX0i.json'\r\n",
      "'Hammond-aligned_destressed pXi.json'\r\n",
      "'Hammond-aligned_destressed pXiPrimeX0i.json'\r\n",
      "'Hammond-aligned_destressed pX.json'\r\n",
      "'Hammond-aligned_destressed pXjX.json'\r\n",
      "'Hammond-aligned_destressed pxX0i.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      " Hammond-aligned_destressed_threeCousins.json\r\n",
      " Hammond-aligned_destressed_twoCousins.json\r\n",
      "'Hammond-aligned_destressed x0iToWs.json'\r\n",
      " Hammond-aligned_destressed_zeroCousins.json\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:00.438246Z",
     "start_time": "2019-02-14T20:55:00.433372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = hammond_fn[:-4] #'Hammond_newdic_IPA_aligned'\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:01.699437Z",
     "start_time": "2019-02-14T20:55:01.695321Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:02.017498Z",
     "start_time": "2019-02-14T20:55:02.011671Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:02.562522Z",
     "start_time": "2019-02-14T20:55:02.556774Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:02.901156Z",
     "start_time": "2019-02-14T20:55:02.896428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:03.429322Z",
     "start_time": "2019-02-14T20:55:03.415969Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToProb_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:04.785028Z",
     "start_time": "2019-02-14T20:55:04.779667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:05.206426Z",
     "start_time": "2019-02-14T20:55:05.083159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n"
     ]
    }
   ],
   "source": [
    "ls Hammond_newdic*prob*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:05.951211Z",
     "start_time": "2019-02-14T20:55:05.939840Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToNlprob_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToNlprob_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:06.511571Z",
     "start_time": "2019-02-14T20:55:06.506149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.l.u.m.ə.n.ɪ.s.⋉'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToProb_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:10.294695Z",
     "start_time": "2019-02-14T20:55:10.288050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.673260705712797e-06"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5.673260705712797e-06"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb_in[test_k]\n",
    "phonWordToProb[test_k]\n",
    "phonWordToProb_in[test_k] == phonWordToProb[test_k]\n",
    "assert(phonWordToProb_in[test_k] == phonWordToProb[test_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:55:13.080475Z",
     "start_time": "2019-02-14T20:55:13.065785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.427390407025747"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'phonWordToNlprob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-43111d7823fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mphonWordToNlprob_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mphonWordToNlprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mphonWordToNlprob_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mphonWordToNlProb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonWordToNlprob_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mphonWordToNlProb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phonWordToNlprob' is not defined"
     ]
    }
   ],
   "source": [
    "phonWordToNlprob_in[test_k]\n",
    "phonWordToNlprob[test_k]\n",
    "phonWordToNlprob_in[test_k] == phonWordToNlProb[test_k]\n",
    "assert(phonWordToNlprob_in[test_k] == phonWordToNlProb[test_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "134px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
