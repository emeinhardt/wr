{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:02.819669Z",
     "start_time": "2019-08-07T18:30:02.814866Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li><li><span><a href=\"#Watermark\" data-toc-modified-id=\"Watermark-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Watermark</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-load-data\" data-toc-modified-id=\"Imports-/-load-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load/extract-sanity-checking-data\" data-toc-modified-id=\"Load/extract-sanity-checking-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load/extract sanity-checking data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segmental-lexicon,-prefixes,-inventory,-and-triphones\" data-toc-modified-id=\"Segmental-lexicon,-prefixes,-inventory,-and-triphones-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Segmental lexicon, prefixes, inventory, and triphones</a></span></li><li><span><a href=\"#Triphone-channel-distribution-and-channel-alphabet\" data-toc-modified-id=\"Triphone-channel-distribution-and-channel-alphabet-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Triphone channel distribution and channel alphabet</a></span></li><li><span><a href=\"#Preview-and-postview-distributions-and-alphabets\" data-toc-modified-id=\"Preview-and-postview-distributions-and-alphabets-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Preview and postview distributions and alphabets</a></span></li><li><span><a href=\"#Corpus-contexts\" data-toc-modified-id=\"Corpus-contexts-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Corpus contexts</a></span></li><li><span><a href=\"#Conversion-to-one-hot-vectors-/-sequences-thereof\" data-toc-modified-id=\"Conversion-to-one-hot-vectors-/-sequences-thereof-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Conversion to one-hot vectors / sequences thereof</a></span></li></ul></li><li><span><a href=\"#Load-segmental-sequence-channel-matrices-$p(Y_0^k|X_0^k)$-and-$p(Y_0^f|X_0^f)$\" data-toc-modified-id=\"Load-segmental-sequence-channel-matrices-$p(Y_0^k|X_0^k)$-and-$p(Y_0^f|X_0^f)$-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Load segmental sequence channel matrices $p(Y_0^k|X_0^k)$ and $p(Y_0^f|X_0^f)$</a></span></li><li><span><a href=\"#Load-contextual-distribution-on-segmental-wordforms-$p(W|C)$\" data-toc-modified-id=\"Load-contextual-distribution-on-segmental-wordforms-$p(W|C)$-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Load contextual distribution on segmental wordforms $p(W|C)$</a></span></li><li><span><a href=\"#Load-lexicon-metadata\" data-toc-modified-id=\"Load-lexicon-metadata-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Load lexicon metadata</a></span></li></ul></li><li><span><a href=\"#Slow,-but-sanity-checking-calculations\" data-toc-modified-id=\"Slow,-but-sanity-checking-calculations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Slow, but sanity-checking calculations</a></span><ul class=\"toc-item\"><li><span><a href=\"#$p(C_0^i-|-X_0^i;-X_{i+1})$\" data-toc-modified-id=\"$p(C_0^i-|-X_0^i;-X_{i+1})$-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>$p(C_0^i | X_0^i; X_{i+1})$</a></span></li><li><span><a href=\"#$p(Y_0^i|X_0^k)$\" data-toc-modified-id=\"$p(Y_0^i|X_0^k)$-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>$p(Y_0^i|X_0^k)$</a></span></li><li><span><a href=\"#$p(X_0^f|C)$\" data-toc-modified-id=\"$p(X_0^f|C)$-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>$p(X_0^f|C)$</a></span></li><li><span><a href=\"#$p(Y_0^i-|-c)$\" data-toc-modified-id=\"$p(Y_0^i-|-c)$-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>$p(Y_0^i | c)$</a></span></li><li><span><a href=\"#$p(Y_0^i|x_0^k)$-sampler\" data-toc-modified-id=\"$p(Y_0^i|x_0^k)$-sampler-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>$p(Y_0^i|x_0^k)$ sampler</a></span></li><li><span><a href=\"#$p(\\widehat{X_0^f}|Y_0^{i},-C)$\" data-toc-modified-id=\"$p(\\widehat{X_0^f}|Y_0^{i},-C)$-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>$p(\\widehat{X_0^f}|Y_0^{i}, C)$</a></span></li><li><span><a href=\"#$\\widehat{p}(\\widehat{X_0^f}|X_0^i;X_{i+1},-C)$\" data-toc-modified-id=\"$\\widehat{p}(\\widehat{X_0^f}|X_0^i;X_{i+1},-C)$-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>$\\widehat{p}(\\widehat{X_0^f}|X_0^i;X_{i+1}, C)$</a></span></li></ul></li><li><span><a href=\"#Definition-of-core-calculation\" data-toc-modified-id=\"Definition-of-core-calculation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Definition of core calculation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Channel-sampler\" data-toc-modified-id=\"Channel-sampler-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Channel sampler</a></span></li><li><span><a href=\"#Sample-parallel-posterior-estimator\" data-toc-modified-id=\"Sample-parallel-posterior-estimator-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Sample-parallel posterior estimator</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Testing</a></span></li><li><span><a href=\"#Parallelization-over-samples-and-contexts\" data-toc-modified-id=\"Parallelization-over-samples-and-contexts-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Parallelization over samples and contexts</a></span></li><li><span><a href=\"#Exact/full-wordform-length-variant-functions\" data-toc-modified-id=\"Exact/full-wordform-length-variant-functions-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Exact/full wordform-length variant functions</a></span></li><li><span><a href=\"#Calculation-volume-organization-for-Case-1\" data-toc-modified-id=\"Calculation-volume-organization-for-Case-1-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Calculation volume organization for Case 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Slices:-Calculate-results-for-groups-of-reconstructed-wordforms-by-(input-prefix,-context)\" data-toc-modified-id=\"Slices:-Calculate-results-for-groups-of-reconstructed-wordforms-by-(input-prefix,-context)-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Slices: Calculate results for groups of reconstructed wordforms by (input prefix, context)</a></span></li><li><span><a href=\"#Blocks:-Calculate-results-for-groups-of-input-prefixes-by-context\" data-toc-modified-id=\"Blocks:-Calculate-results-for-groups-of-input-prefixes-by-context-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Blocks: Calculate results for groups of input prefixes by context</a></span></li></ul></li><li><span><a href=\"#Calculation-volume-organization-2\" data-toc-modified-id=\"Calculation-volume-organization-2-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Calculation volume organization 2</a></span></li><li><span><a href=\"#Calculation-volume-organization-3\" data-toc-modified-id=\"Calculation-volume-organization-3-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Calculation volume organization 3</a></span><ul class=\"toc-item\"><li><span><a href=\"#Slices:-Calculate-$\\{p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)\\}$-for-all-$W$-and-for-a-single-given-$c$\" data-toc-modified-id=\"Slices:-Calculate-$\\{p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)\\}$-for-all-$W$-and-for-a-single-given-$c$-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>Slices: Calculate $\\{p(\\widehat{W} = w^* | W = w^*, C = c)\\}$ for all $W$ and for a single given $c$</a></span></li><li><span><a href=\"#Slices:-Calculate-$\\{p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)\\}$-for-all-$c$\" data-toc-modified-id=\"Slices:-Calculate-$\\{p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)\\}$-for-all-$c$-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>Slices: Calculate $\\{p(\\widehat{W} = w^* | W = w^*, C = c)\\}$ for all $c$</a></span></li></ul></li></ul></li><li><span><a href=\"#Calculate-distribution\" data-toc-modified-id=\"Calculate-distribution-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculate distribution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Number-of-calculations-necessary-under-different-calculation-criteria\" data-toc-modified-id=\"Number-of-calculations-necessary-under-different-calculation-criteria-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Number of calculations necessary under different calculation criteria</a></span></li><li><span><a href=\"#Determine-dimensions-and-cells-to-be-calculated-vs.-ignored\" data-toc-modified-id=\"Determine-dimensions-and-cells-to-be-calculated-vs.-ignored-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Determine dimensions and cells to be calculated vs. ignored</a></span><ul class=\"toc-item\"><li><span><a href=\"#Option:-Only-calculate-full-source-wordforms-rather-than-all-source-prefixes?\" data-toc-modified-id=\"Option:-Only-calculate-full-source-wordforms-rather-than-all-source-prefixes?-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Option: Only calculate full source wordforms rather than all source prefixes?</a></span></li><li><span><a href=\"#Suboption:-Given-$\\neg-r$-and-a-full-source-wordform-$w^*$,-only-calculate-posterior-probability-$p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)$,-$\\forall-w^*$?\" data-toc-modified-id=\"Suboption:-Given-$\\neg-r$-and-a-full-source-wordform-$w^*$,-only-calculate-posterior-probability-$p(\\widehat{W}-=-w^*-|-W-=-w^*,-C-=-c)$,-$\\forall-w^*$?-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Suboption: Given $\\neg r$ and a full source wordform $w^*$, only calculate posterior probability $p(\\widehat{W} = w^* | W = w^*, C = c)$, $\\forall w^*$?</a></span></li><li><span><a href=\"#Suboption:-Restrict-calculations-only-to-(source-sequence,-reconstruction-sequence)-pairs-within-edit-distance-$k$?\" data-toc-modified-id=\"Suboption:-Restrict-calculations-only-to-(source-sequence,-reconstruction-sequence)-pairs-within-edit-distance-$k$?-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Suboption: Restrict calculations only to (source sequence, reconstruction sequence) pairs within edit distance $k$?</a></span></li><li><span><a href=\"#Altogether...\" data-toc-modified-id=\"Altogether...-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Altogether...</a></span></li></ul></li><li><span><a href=\"#Case-1:-$r-\\land-(0-<-k-\\leq-4)$\" data-toc-modified-id=\"Case-1:-$r-\\land-(0-<-k-\\leq-4)$-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Case 1: $r \\land (0 &lt; k \\leq 4)$</a></span></li><li><span><a href=\"#Case-2:-$\\neg-r-\\land-(0-<-k-\\leq-4)-\\land-\\neg-e$\" data-toc-modified-id=\"Case-2:-$\\neg-r-\\land-(0-<-k-\\leq-4)-\\land-\\neg-e$-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Case 2: $\\neg r \\land (0 &lt; k \\leq 4) \\land \\neg e$</a></span></li><li><span><a href=\"#Case-3:-$\\neg-r-\\land-e$\" data-toc-modified-id=\"Case-3:-$\\neg-r-\\land-e$-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Case 3: $\\neg r \\land e$</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a choice of parameters $\\epsilon$ and $n$, and given\n",
    " - wordform channel matrices $p(Y_0^f|X_0^f)$\n",
    " - a contextual distribution on segmental wordforms $p(X_0^f|C)$\n",
    " - segmental lexicon metadata pre-calculating $k$-cousins/$k$-spheres up to $k=4$\n",
    " \n",
    "Calculate\n",
    "\n",
    "$$\\hat{p}(\\hat{X}_0^f = x_0^{'f}|X_0^f = x_0^{*f}, c) = \\frac{1}{n} \\sum\\limits_{y_0^f \\in S} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c)$$\n",
    " where \n",
    "  - edit distance $d(x_0^{'f}, x_0^{*f}) \\leq 4$\n",
    "  - $S = $ a set of $n$ samples from $p(Y_0^f|x_0^{*f})$. In practice an $n \\approx 50$ seems to result in estimates that are within $10^{-6}$ of the true estimate. \n",
    "  - $p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) = \\frac{p(y_0^f|x_0^{'f})p(x_0^{'f}|c)}{p(y_0^f | c)}$\n",
    "  - $p(y_0^f| c) = \\sum\\limits_{v', x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|v')p(v'|c) = \\sum\\limits_{x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.006526Z",
     "start_time": "2019-08-07T18:30:02.847957Z"
    }
   },
   "outputs": [],
   "source": [
    "import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.086233Z",
     "start_time": "2019-08-07T18:30:03.011743Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.176530Z",
     "start_time": "2019-08-07T18:30:03.092886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2019-08-07T11:30:03-07:00\n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.7.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-51-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "host name  : kotoba\n",
      "Git hash   : 902da5f0b863a6746f6e304260628021cec732ac\n"
     ]
    }
   ],
   "source": [
    "%watermark -ihmuvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.363960Z",
     "start_time": "2019-08-07T18:30:03.177879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\r\n",
      "CPU op-mode(s):      32-bit, 64-bit\r\n",
      "Byte Order:          Little Endian\r\n",
      "CPU(s):              12\r\n",
      "On-line CPU(s) list: 0-11\r\n",
      "Thread(s) per core:  2\r\n",
      "Core(s) per socket:  6\r\n",
      "Socket(s):           1\r\n",
      "NUMA node(s):        1\r\n",
      "Vendor ID:           GenuineIntel\r\n",
      "CPU family:          6\r\n",
      "Model:               158\r\n",
      "Model name:          Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\r\n",
      "Stepping:            10\r\n",
      "CPU MHz:             4110.557\r\n",
      "CPU max MHz:         4600.0000\r\n",
      "CPU min MHz:         800.0000\r\n",
      "BogoMIPS:            6384.00\r\n",
      "Virtualization:      VT-x\r\n",
      "L1d cache:           32K\r\n",
      "L1i cache:           32K\r\n",
      "L2 cache:            256K\r\n",
      "L3 cache:            12288K\r\n",
      "NUMA node0 CPU(s):   0-11\r\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.477058Z",
     "start_time": "2019-08-07T18:30:03.365489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            31G        1.7G         22G        3.6M        7.2G         29G\r\n",
      "Swap:          2.0G        1.2G        834M\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.611255Z",
     "start_time": "2019-08-07T18:30:03.483143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version:        410.104\r\n",
      "srcversion:     3B812B02678A6B43A294F17\r\n"
     ]
    }
   ],
   "source": [
    "!modinfo nvidia | grep version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.624838Z",
     "start_time": "2019-08-07T18:30:03.617464Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.799802Z",
     "start_time": "2019-08-07T18:30:03.632989Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.807166Z",
     "start_time": "2019-08-07T18:30:03.801239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd(); repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:03.883869Z",
     "start_time": "2019-08-07T18:30:03.808248Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#p(Y_0^k|X_0^k) - from step 4e\n",
    "c = ''\n",
    "# c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle'\n",
    "c = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle'\n",
    "\n",
    "#p(Y_0^f|X_0^f) - from step 4e\n",
    "f = ''\n",
    "f = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_wordform_index.pickle'\n",
    "\n",
    "#p(X_0^f|C) - from step 4c\n",
    "w = ''\n",
    "# w = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy'\n",
    "w = 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C.npy'\n",
    "\n",
    "# LTR metadata directory - contains pre-computed k-cousin/k-sphere information\n",
    "m = ''\n",
    "# m = 'LTR_Buckeye_aligned_w_GD_AmE_destressed'\n",
    "m = 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed'\n",
    "\n",
    "# output filepath prefix for pW_WC\n",
    "o = ''\n",
    "# o = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_WC'\n",
    "# o = 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_WC'\n",
    "o = 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts'\n",
    "\n",
    "# for sanity checking...\n",
    "\n",
    "# the sorted prefixes of W here correspond to the W dimension of file 'w'\n",
    "p = ''\n",
    "# p = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "p = 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "# the sorted conditioning triphones and outcome response uniphones correspond to\n",
    "# the dimensions of file 'c'\n",
    "t = ''\n",
    "# t = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "t = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X012.json'\n",
    "\n",
    "# the sorted projected contexts here correspond to the C dimension of file 'w'\n",
    "s = ''\n",
    "# s = 'LD_Fisher_vocab_in_Buckeye_contexts/LM_filtered_buckeye_contexts.txt'\n",
    "s = 'LD_Fisher_vocab_in_swbd2003_contexts/LM_filtered_swbd2003_contexts.txt'\n",
    "\n",
    "\n",
    "# if True, then run notebook 'benchmark' cells\n",
    "x = ''\n",
    "\n",
    "# calculation parameters\n",
    "\n",
    "#samples per (reconstructed target wordform, source prefix, context) triple\n",
    "n = ''\n",
    "n = '50'\n",
    "\n",
    "#maximum edit distance for reconstructed target, source prefix 'cousins' to calculate\n",
    "k = ''\n",
    "k = '2'\n",
    "\n",
    "#batch size - currently only interpretable/used for case 3 (see below)\n",
    "# if using a GPU with around 8GB of memory, use a batch size < 300\n",
    "# if using a CPU and parallelizing, use a larger batch size (500-750).\n",
    "# In either case, using all available memory will *not* in general\n",
    "# maximize performance.\n",
    "b = ''\n",
    "b = '200'\n",
    "\n",
    "#parallelize batches over processes?\n",
    "l = ''\n",
    "l = 'True'\n",
    "\n",
    "#use GPU? (overrides parallelize option)\n",
    "g = ''\n",
    "g = 'True'\n",
    "\n",
    "#if 'False', only calculate p(\\hat{W}|W = w, c), i.e. don't calculate p(\\hat{W}|P = p, c)\n",
    "r = ''\n",
    "r = 'False' \n",
    "\n",
    "#if r='False' and e='True', only calculate p(\\hat{W} = w*| W = w*, c) ∀w ∈ W\n",
    "e = ''\n",
    "e = 'True' \n",
    "\n",
    "#the (optional) beginning and end of the block of complete wordforms this notebook should calculate\n",
    "wStart = ''\n",
    "wEnd = ''\n",
    "# wStart = '0'\n",
    "# wEnd = '2'\n",
    "\n",
    "#the (optional) beginning and end of the block of contexts this notebook should calculate\n",
    "# cStart = ''\n",
    "# cEnd = ''\n",
    "# cStart = '0'\n",
    "# cEnd = '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.022232Z",
     "start_time": "2019-08-07T18:30:03.885772Z"
    }
   },
   "outputs": [],
   "source": [
    "file_params = (c, f, w, m, p, t, s)\n",
    "for fp in file_params:\n",
    "    if not path.exists(fp):\n",
    "        print(f\"Parameter path does not exist: {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.110411Z",
     "start_time": "2019-08-07T18:30:04.027341Z"
    }
   },
   "outputs": [],
   "source": [
    "if x == '' or x == 'False':\n",
    "    benchmark = False\n",
    "elif x == 'True':\n",
    "    benchmark = True\n",
    "else:\n",
    "    raise Exception(f\"x arg must be one of {'', 'True', 'False'}, got {x} instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.185017Z",
     "start_time": "2019-08-07T18:30:04.115357Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = path.dirname(o)\n",
    "if not path.exists(output_dir):\n",
    "    print('Making output path {0}'.format(output_dir))\n",
    "    makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.297028Z",
     "start_time": "2019-08-07T18:30:04.190027Z"
    }
   },
   "outputs": [],
   "source": [
    "if n == '':\n",
    "    n = 50\n",
    "else:\n",
    "    n = int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.385032Z",
     "start_time": "2019-08-07T18:30:04.301831Z"
    }
   },
   "outputs": [],
   "source": [
    "if k == '':\n",
    "    k = 2\n",
    "else:\n",
    "    k = int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.459958Z",
     "start_time": "2019-08-07T18:30:04.386177Z"
    }
   },
   "outputs": [],
   "source": [
    "if b == '':\n",
    "    b = None\n",
    "else:\n",
    "    b = int(b)\n",
    "    \n",
    "assert 0 < b, f\"Batch size argument b = {b} must be greater than 0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.560297Z",
     "start_time": "2019-08-07T18:30:04.464454Z"
    }
   },
   "outputs": [],
   "source": [
    "if l == '' or l == 'True':\n",
    "    l = True\n",
    "elif l == 'False':\n",
    "    l = False\n",
    "else:\n",
    "    raise Exception(f\"l must be one of {'', 'True', 'False'}, got {l} instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.643513Z",
     "start_time": "2019-08-07T18:30:04.565377Z"
    }
   },
   "outputs": [],
   "source": [
    "if g == '' or g == 'True':\n",
    "    g = True\n",
    "elif g == 'False':\n",
    "    g = False\n",
    "else:\n",
    "    raise Exception(f\"g must be one of {'', 'True', 'False'}, got {g} instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.718420Z",
     "start_time": "2019-08-07T18:30:04.648303Z"
    }
   },
   "outputs": [],
   "source": [
    "if r == '' or r == 'False':\n",
    "    r = False\n",
    "#     raise Exception('Assuming only full wordforms have been produced is not supported currently.')\n",
    "elif r == 'True':\n",
    "    r = True\n",
    "else:\n",
    "    raise Exception(f\"r must be one of {'','True','False'}, got '{r}' instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.793942Z",
     "start_time": "2019-08-07T18:30:04.723310Z"
    }
   },
   "outputs": [],
   "source": [
    "if not r and (e == '' or e == 'True'):\n",
    "    e = True\n",
    "#     raise Exception('Only calculating the posterior probability of the actual (correct) full intended wordform is not supported currently.')\n",
    "elif r and (e == '' or e == 'True'):\n",
    "    raise Exception(\"e can only be True if r is False\")\n",
    "elif e == 'False':\n",
    "    e = False\n",
    "else:\n",
    "    raise Exception(f\"e must be one of {'','True','False'}, got '{e}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.868559Z",
     "start_time": "2019-08-07T18:30:04.798805Z"
    }
   },
   "outputs": [],
   "source": [
    "if b is not None:\n",
    "    if r:\n",
    "        raise Exception(f'batch size b = {b} only currently interpretable and supported when r is False and e is True.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:04.976203Z",
     "start_time": "2019-08-07T18:30:04.873608Z"
    }
   },
   "outputs": [],
   "source": [
    "# postview_fp = path.join(path.dirname(t), \"p6Y0X01.json\"); postview_fp\n",
    "\n",
    "# preview_fp = path.join(path.dirname(t), \"p3Y1X01.json\"); preview_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:05.050889Z",
     "start_time": "2019-08-07T18:30:04.988661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wEnd provided. Will be set to 1 less than the length of the number of contexts in the metadata.\n"
     ]
    }
   ],
   "source": [
    "# cStart = int(cStart)\n",
    "# cEnd = int(cEnd)\n",
    "\n",
    "# assert cStart <= cEnd, f\"First context id of block to calculate be <= the context id of the end of the block, got {cStart} and {cEnd} instead.\"\n",
    "\n",
    "if wStart == '':\n",
    "    wStart = 0\n",
    "else:\n",
    "    wStart = int(wStart)\n",
    "if wEnd == '':\n",
    "    wEnd = None\n",
    "    print('No wEnd provided. Will be set to 1 less than the length of the number of contexts in the metadata.')\n",
    "else:\n",
    "    wEnd = int(wEnd)\n",
    "\n",
    "if wEnd is not None:\n",
    "    assert wStart <= wEnd, f\"First word id of block to calculate must be <= the word id of the end of the block, got {wStart} and {wEnd} instead.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:05.163670Z",
     "start_time": "2019-08-07T18:30:05.054478Z"
    }
   },
   "outputs": [],
   "source": [
    "consolidation_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports / load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:05.281871Z",
     "start_time": "2019-08-07T18:30:05.168534Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.017316Z",
     "start_time": "2019-08-07T18:30:05.286642Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.301273Z",
     "start_time": "2019-08-07T18:30:06.018288Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.312724Z",
     "start_time": "2019-08-07T18:30:06.305008Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.402558Z",
     "start_time": "2019-08-07T18:30:06.314694Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_gui, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.478985Z",
     "start_time": "2019-08-07T18:30:06.407391Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "# J = 16\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.604704Z",
     "start_time": "2019-08-07T18:30:06.480098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Disabling 'parallelize' flag...\n",
      "GeForce GTX 1080\n",
      "Total Memory: 8513.978368\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    if g and l:\n",
    "        print(\"Disabling 'parallelize' flag...\")\n",
    "        l = False\n",
    "    \n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    total_mem_MB = torch.cuda.get_device_properties(device).total_memory / 1e6\n",
    "    print('Total Memory: {0}'.format(total_mem_MB) )\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.get_device_name(1))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')\n",
    "else:\n",
    "    raise Exception(f\"g set to 'True' but torch cannot find a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.753022Z",
     "start_time": "2019-08-07T18:30:06.606713Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:06.863909Z",
     "start_time": "2019-08-07T18:30:06.758059Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "\n",
    "my_type = my_ft\n",
    "# my_type = my_dt\n",
    "\n",
    "torch.set_default_tensor_type(my_type)\n",
    "\n",
    "my_tt = torch.float32\n",
    "# my_tt = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/extract sanity-checking data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to make queries, spot checks, and sanity checks. That means we want to be able to reference\n",
    " 1. the set of strings constituting segmental wordforms and prefixes\n",
    " 2. the source and channel alphabets\n",
    " 3. the channel distribution's conditioning triphones ∩ the triphones in the lexicon = the triphones in the lexicon\n",
    " 4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmental wordforms were necessary for \n",
    " - the lexicon metadata calculation (step 4b)\n",
    " - the contextual distribution on segmental wordforms (step 4c)\n",
    " - the definition of the segmental sequence channel matrices (step 4d)\n",
    " \n",
    "What does each use as input? (Pass that to this notebook.)\n",
    "\n",
    "Each notebook uses ...pW_V.json (or something slightly downstream of that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmental lexicon, prefixes, inventory, and triphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:07.010145Z",
     "start_time": "2019-08-07T18:30:06.866373Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract \n",
    " - `orthographic vocabulary`\n",
    " - `segmental vocabulary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:07.751625Z",
     "start_time": "2019-08-07T18:30:07.011278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9411"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vs = set(pW_V.keys())\n",
    "Ws = union(mapValues(lambda dist: set(conditions(dist)), \n",
    "                     pW_V).values())\n",
    "len(Vs)\n",
    "len(Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:07.979695Z",
     "start_time": "2019-08-07T18:30:07.752592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{4: 5,\n",
       " 5: 173,\n",
       " 6: 1266,\n",
       " 7: 1790,\n",
       " 8: 1646,\n",
       " 9: 1328,\n",
       " 10: 1052,\n",
       " 11: 828,\n",
       " 12: 508,\n",
       " 13: 329,\n",
       " 14: 150,\n",
       " 15: 66,\n",
       " 16: 24,\n",
       " 17: 6,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:08.008963Z",
     "start_time": "2019-08-07T18:30:07.980749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordLengthRange = sorted(list(range(min(wordlengthsInclEdges),\n",
    "                                    max(wordlengthsInclEdges)+1)))\n",
    "wordLengthRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:08.098261Z",
     "start_time": "2019-08-07T18:30:08.009861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:08.281394Z",
     "start_time": "2019-08-07T18:30:08.103140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 9172,\n",
       " 5: 9167,\n",
       " 6: 8994,\n",
       " 7: 7728,\n",
       " 8: 5938,\n",
       " 9: 4292,\n",
       " 10: 2964,\n",
       " 11: 1912,\n",
       " 12: 1084,\n",
       " 13: 576,\n",
       " 14: 247,\n",
       " 15: 97,\n",
       " 16: 31,\n",
       " 17: 7,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mappings between `segmental wordforms` and `segmental prefixes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:15.824140Z",
     "start_time": "2019-08-07T18:30:08.282454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9172/9172 [00:07<00:00, 1228.24it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79727"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_relation = set(union({(w,p) for p in getPrefixes(w)} for w in tqdm(Ws)))\n",
    "len(prefix_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract \n",
    " - `segmental prefixes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:15.841170Z",
     "start_time": "2019-08-07T18:30:15.825264Z"
    }
   },
   "outputs": [],
   "source": [
    "Ps = set(map(lambda pair: pair[1],\n",
    "             prefix_relation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \n",
    " - `sorted` versions of the `segmental vocabulary` and `segmental prefixes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:16.027649Z",
     "start_time": "2019-08-07T18:30:15.842232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "42231"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "Ps_t = tuple(sorted(list(Ps)))\n",
    "num_wordforms = len(Ws_t)\n",
    "num_prefixes = len(Ps_t)\n",
    "\n",
    "num_wordforms\n",
    "num_prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mappings between `orthographic wordforms` and `segmental wordforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:21.640498Z",
     "start_time": "2019-08-07T18:30:16.028994Z"
    }
   },
   "outputs": [],
   "source": [
    "v_to_Ws = mapValues(lambda dist: set(conditions(dist)),\n",
    "                    pW_V)\n",
    "V_W_relation = {(v,w) \n",
    "                for v in v_to_Ws \n",
    "                for w in v_to_Ws[v]}\n",
    "w_to_Vs = {w:{v for v in Vs if (v,w) in V_W_relation}\n",
    "           for w in Ws}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:21.645259Z",
     "start_time": "2019-08-07T18:30:21.641763Z"
    }
   },
   "outputs": [],
   "source": [
    "if not benchmark: \n",
    "    del pW_V\n",
    "    del Vs\n",
    "    del v_to_Ws\n",
    "    del V_W_relation\n",
    "    del w_to_Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract\n",
    " - `segmental inventory`\n",
    " - `triphones` in the `segmental lexicon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:21.849609Z",
     "start_time": "2019-08-07T18:30:21.646254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'aɪ',\n",
       " 'aʊ',\n",
       " 'b',\n",
       " 'd',\n",
       " 'dʒ',\n",
       " 'eɪ',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'oʊ',\n",
       " 'p',\n",
       " 's',\n",
       " 't',\n",
       " 'tʃ',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'z',\n",
       " 'æ',\n",
       " 'ð',\n",
       " 'ŋ',\n",
       " 'ɑ',\n",
       " 'ɔɪ',\n",
       " 'ə',\n",
       " 'ɚ',\n",
       " 'ɛ',\n",
       " 'ɪ',\n",
       " 'ɹ',\n",
       " 'ʃ',\n",
       " 'ʊ',\n",
       " 'ʌ',\n",
       " 'ʒ',\n",
       " 'θ',\n",
       " '⋉',\n",
       " '⋊'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_alphabet = lexiconToInventory(Ws)\n",
    "Xs = source_alphabet\n",
    "len(Xs)\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:21.898037Z",
     "start_time": "2019-08-07T18:30:21.850910Z"
    }
   },
   "outputs": [],
   "source": [
    "Xs_t = tuple(sorted(Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:22.774605Z",
     "start_time": "2019-08-07T18:30:21.899500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7412"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['k.ə.m',\n",
       " 'g.aʊ.n',\n",
       " 'p.ɚ.b',\n",
       " 's.g.ʌ',\n",
       " '⋊.g.i',\n",
       " 'l.i.ʃ',\n",
       " 'h.u.i',\n",
       " 't.aɪ.d',\n",
       " 'z.æ.s',\n",
       " 'b.oʊ.h']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_triphones = lexiconTo3factors(Ws)\n",
    "len(lexicon_triphones)\n",
    "list(lexicon_triphones)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:22.779349Z",
     "start_time": "2019-08-07T18:30:22.775706Z"
    }
   },
   "outputs": [],
   "source": [
    "X012s = lexicon_triphones\n",
    "X012s_t = tuple(sorted(list(X012s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:22.905704Z",
     "start_time": "2019-08-07T18:30:22.780231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triphs_with_LE = {triph for triph in lexicon_triphones if leftEdge in triph}\n",
    "len(triphs_with_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:22.996152Z",
     "start_time": "2019-08-07T18:30:22.910419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triphs_with_RE = {triph for triph in lexicon_triphones if rightEdge in triph}\n",
    "len(triphs_with_RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.260448Z",
     "start_time": "2019-08-07T18:30:23.000712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['oʊ.m', 'n.u', 'u.k', 'd.n', 'ɹ.tʃ', 'ɑ.v', 'θ.ɪ', 'aɪ.g', 'θ.æ', 'oʊ.g']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_diphones = lexiconTo2factors(Ws)\n",
    "len(lexicon_diphones)\n",
    "list(lexicon_diphones)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.263676Z",
     "start_time": "2019-08-07T18:30:23.261585Z"
    }
   },
   "outputs": [],
   "source": [
    "X01s = lexicon_diphones\n",
    "X01s_t = tuple(sorted(list(X01s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.342015Z",
     "start_time": "2019-08-07T18:30:23.264528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X01s_with_LE = {diph for diph in X01s if leftEdge in diph}\n",
    "len(X01s_with_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.416264Z",
     "start_time": "2019-08-07T18:30:23.344201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X01s_with_RE = {diph for diph in X01s if rightEdge in diph}\n",
    "len(X01s_with_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triphone channel distribution and channel alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.693142Z",
     "start_time": "2019-08-07T18:30:23.417455Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(t))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.698149Z",
     "start_time": "2019-08-07T18:30:23.694195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7412"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_triphones = conditions(p3Y1X012)\n",
    "len(channel_triphones)\n",
    "\n",
    "assert all(triph in channel_triphones for triph in lexicon_triphones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.798065Z",
     "start_time": "2019-08-07T18:30:23.699075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{triph for triph in lexicon_triphones if triph not in channel_triphones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.883077Z",
     "start_time": "2019-08-07T18:30:23.802685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'aɪ',\n",
       " 'aʊ',\n",
       " 'b',\n",
       " 'd',\n",
       " 'dʒ',\n",
       " 'eɪ',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'oʊ',\n",
       " 'p',\n",
       " 's',\n",
       " 't',\n",
       " 'tʃ',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'z',\n",
       " 'æ',\n",
       " 'ð',\n",
       " 'ŋ',\n",
       " 'ɑ',\n",
       " 'ɔɪ',\n",
       " 'ɚ',\n",
       " 'ɛ',\n",
       " 'ɪ',\n",
       " 'ɹ',\n",
       " 'ʃ',\n",
       " 'ʊ',\n",
       " 'ʌ',\n",
       " 'ʒ',\n",
       " 'θ',\n",
       " '⋉'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_alphabet = outcomes(p3Y1X012)\n",
    "len(channel_alphabet)\n",
    "channel_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:23.951088Z",
     "start_time": "2019-08-07T18:30:23.887578Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1s = channel_alphabet\n",
    "Y1s_t = tuple(sorted(Y1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.084367Z",
     "start_time": "2019-08-07T18:30:23.955874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ə', '⋊'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_lexicon_inventory_but_not_in_channel_inventory = source_alphabet - channel_alphabet\n",
    "in_lexicon_inventory_but_not_in_channel_inventory\n",
    "\n",
    "assert in_lexicon_inventory_but_not_in_channel_inventory == {leftEdge, rightEdge} or in_lexicon_inventory_but_not_in_channel_inventory == {'ə', leftEdge, rightEdge} or in_lexicon_inventory_but_not_in_channel_inventory == {'ə', leftEdge}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview and postview distributions and alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.175116Z",
     "start_time": "2019-08-07T18:30:24.089653Z"
    }
   },
   "outputs": [],
   "source": [
    "# preview_fp\n",
    "# postview_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.273289Z",
     "start_time": "2019-08-07T18:30:24.179941Z"
    }
   },
   "outputs": [],
   "source": [
    "# p3Y1X01 = condDistsAsProbDists(importProbDist(preview_fp))\n",
    "\n",
    "# p6Y0X01 = condDistsAsProbDists(importProbDist(postview_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.350999Z",
     "start_time": "2019-08-07T18:30:24.278020Z"
    }
   },
   "outputs": [],
   "source": [
    "# assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "# assert uniformOutcomes(p6Y0X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.426024Z",
     "start_time": "2019-08-07T18:30:24.355756Z"
    }
   },
   "outputs": [],
   "source": [
    "# preview_outcomes = outcomes(p3Y1X01)\n",
    "\n",
    "# postview_outcomes = outcomes(p6Y0X01)\n",
    "\n",
    "# assert preview_outcomes == Y1s\n",
    "\n",
    "# assert postview_outcomes == Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.501241Z",
     "start_time": "2019-08-07T18:30:24.430962Z"
    }
   },
   "outputs": [],
   "source": [
    "# preview_channel_diphones = conditions(p3Y1X01)\n",
    "# len(preview_channel_diphones)\n",
    "\n",
    "# # missing_preview_conditions = {diph for diph in lexicon_diphones if (diph not in preview_channel_diphones) and (leftEdge not in diph)}\n",
    "# # missing_preview_conditions\n",
    "\n",
    "# # p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "\n",
    "# # #add 𝚺⋉ to preview conditions\n",
    "# # # for each 𝛔⋉ ∈ 𝚺⋉, ensure p(Y_1 = ⋉|𝛔⋉) = 1.0\n",
    "# # for c in missing_preview_conditions:\n",
    "# #     p3Y1X01[c] = {o:1.0 if ds2t(o)[1] == rightEdge else 0.0 for o in Y1s}\n",
    "\n",
    "# # # add ⋉ to preview outcomes\n",
    "# # for c in preview_channel_diphones\n",
    "    \n",
    "# # p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "\n",
    "# assert all(diph in preview_channel_diphones for diph in lexicon_diphones if not (leftEdge in diph or rightEdge in diph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.584397Z",
     "start_time": "2019-08-07T18:30:24.506258Z"
    }
   },
   "outputs": [],
   "source": [
    "# postview_channel_diphones = conditions(p6Y0X01)\n",
    "# len(postview_channel_diphones)\n",
    "\n",
    "# assert all(diph in postview_channel_diphones for diph in lexicon_diphones if not (leftEdge in diph or rightEdge in diph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.656492Z",
     "start_time": "2019-08-07T18:30:24.589267Z"
    }
   },
   "outputs": [],
   "source": [
    "# p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "# p6Y0X01 = condProbDistAsDicts(p6Y0X01)\n",
    "\n",
    "# p3Y1X01 = {c:p3Y1X01[c] for c in lexicon_diphones if not (leftEdge in c or rightEdge in c)}\n",
    "# p6Y0X01 = {c:p6Y0X01[c] for c in lexicon_diphones if not (leftEdge in c or rightEdge in c)}\n",
    "\n",
    "# assert areNormalized(p3Y1X01)\n",
    "# assert areNormalized(p6Y0X01)\n",
    "# assert uniformOutcomes(p3Y1X01)\n",
    "# assert uniformOutcomes(p6Y0X01)\n",
    "\n",
    "# p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "# p6Y0X01 = condDistsAsProbDists(p6Y0X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.790557Z",
     "start_time": "2019-08-07T18:30:24.682735Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(conditions(p3Y1X01))\n",
    "# len(conditions(p6Y0X01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:24.884231Z",
     "start_time": "2019-08-07T18:30:24.802703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y012s = set(map(t2ds, sigmaK(set.union(Y1s, edgeSymbols), 3)))\n",
    "# len(set.union(Y1s, edgeSymbols))\n",
    "# len(Y012s)\n",
    "# Y012s_t = tuple(sorted(list(Y012s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.016461Z",
     "start_time": "2019-08-07T18:30:24.889022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106295"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('<rem> and doctor',\n",
       " '<rem> and electric',\n",
       " '<rem> and especially',\n",
       " '<rem> and even',\n",
       " '<rem> and for',\n",
       " '<rem> and he')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = importSeqs(s, list)\n",
    "len(Cs)\n",
    "\n",
    "Cs_t = tuple(sorted(Cs))\n",
    "Cs_t[123:129]\n",
    "\n",
    "assert tuple(Cs) == Cs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.074659Z",
     "start_time": "2019-08-07T18:30:25.017510Z"
    }
   },
   "outputs": [],
   "source": [
    "if not benchmark:\n",
    "    del Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.161137Z",
     "start_time": "2019-08-07T18:30:25.077354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wEnd not provided; setting wEnd to 1 less than the number of segmental wordforms in the metadata = 9172\n"
     ]
    }
   ],
   "source": [
    "if wEnd is None:\n",
    "    print(f'wEnd not provided; setting wEnd to 1 less than the number of segmental wordforms in the metadata = {len(Ws_t)}')\n",
    "    wEnd = len(Ws_t) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.292469Z",
     "start_time": "2019-08-07T18:30:25.165885Z"
    }
   },
   "outputs": [],
   "source": [
    "if wEnd > len(Ws_t):\n",
    "    raise Exception(f'wEnd = {wEnd} must be less than the number of segmental wordforms ({len(Ws_t)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.401667Z",
     "start_time": "2019-08-07T18:30:25.297198Z"
    }
   },
   "outputs": [],
   "source": [
    "if wEnd < wStart:\n",
    "    raise Exception(f\"wStart must be <= wEnd, got {wStart} and {wEnd} instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to one-hot vectors / sequences thereof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.483172Z",
     "start_time": "2019-08-07T18:30:25.403730Z"
    }
   },
   "outputs": [],
   "source": [
    "# construct what you need to convert to/from one-hot representations\n",
    "\n",
    "# look at segment sequence channel matrix notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.584756Z",
     "start_time": "2019-08-07T18:30:25.488393Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.701256Z",
     "start_time": "2019-08-07T18:30:25.589938Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.822058Z",
     "start_time": "2019-08-07T18:30:25.702415Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.949515Z",
     "start_time": "2019-08-07T18:30:25.826998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3756, 1449])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 7412)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7412,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.113104Z",
     "start_time": "2019-08-07T18:30:25.951176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ð'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ð'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.t.u.p'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.t.u.p'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'t.u.p'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 39)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 39)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y1s_RE = outcomes(p3Y1X01)\n",
    "# len(Y1s_RE)\n",
    "# Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "# print(Y1s_RE - Y1s)\n",
    "\n",
    "# Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "# Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "# Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "Y1OHmap = seqsToOneHotMap(Y1s)\n",
    "# OHY1REmap = oneHotToSeqMap(Y1s_RE)\n",
    "OHY1map = oneHotToSeqMap(Y1s)\n",
    "\n",
    "def ymap(y):\n",
    "#     return Y1REmap[y]\n",
    "    return Y1map[y]\n",
    "\n",
    "def y0kMap(y0k):\n",
    "    return np.array(list(map(ymap, ds2t(y0k)[1:])))\n",
    "\n",
    "def channelSeqOHs2ds(y0k_OHs, addLeftEdge = False):\n",
    "    if not addLeftEdge:\n",
    "#         return t2ds(tuple( map(OHY1REmap, tuple(y0k_OHs) ) ))\n",
    "        return t2ds(tuple( map(OHY1map, tuple(y0k_OHs) ) ))\n",
    "#     return leftEdge + '.' + t2ds(tuple( map(OHY1REmap, tuple(y0k_OHs) ) ))\n",
    "    return leftEdge + '.' + t2ds(tuple( map(OHY1map, tuple(y0k_OHs) ) ))\n",
    "\n",
    "def channelSeqds2OHs(y0k):\n",
    "    y0k_t = ds2t(y0k)\n",
    "    if leftEdge == y0k_t[0]:\n",
    "        y1k_t = y0k_t[1:]\n",
    "    y1k_t = y0k_t[1:]\n",
    "#     return np.array([Y1REOHmap[ yj ] for yj in y1k_t]) #shape should be (_, 39)\n",
    "    return np.array([Y1OHmap[ yj ] for yj in y1k_t]) #shape should be (_, 38)\n",
    "\n",
    "def y0kOHmap(y0k):\n",
    "##     y0k_t = ds2t(y0k)\n",
    "## #     y0k_indices = y0kMap(y0k) #np.array(list(map(lambda y1: Y1map[y1], y0k_t[1:])))\n",
    "##     y1k = t2ds(y0k_t[1:])\n",
    "##     y0k_OHs = dsToUniphoneOHs(y1k, Y1REOHmap)\n",
    "#     return dsToUniphoneOHs(t2ds(ds2t(y0k)[1:]), Y1REOHmap)\n",
    "    y0k_t = ds2t(y0k) #let l = len(y0k_t)\n",
    "    if y0k_t[0] == leftEdge:\n",
    "        y1k_t = y0k_t[1:]\n",
    "#         return np.array([Y1REOHmap[ yj ] for yj in y1k_t]) #shape should be (l-1, 39)\n",
    "        return np.array([Y1OHmap[ yj ] for yj in y1k_t]) #shape should be (l-1, 38)\n",
    "#     return np.array([Y1REOHmap[ yj ] for yj in y0k_t]) #shape should be (l, 39)\n",
    "    return np.array([Y1OHmap[ yj ] for yj in y0k_t]) #shape should be (l, 38)\n",
    "\n",
    "\n",
    "# list(Y1s_RE)[0]\n",
    "list(Y1s)[0]\n",
    "# OHY1REmap( Y1REOHmap[ list(Y1s_RE)[0] ] )\n",
    "OHY1map( Y1OHmap[ list(Y1s)[0] ] )\n",
    "\n",
    "channelSeqds2OHs(leftEdge + '.'+ 't.u.p')\n",
    "\n",
    "channelSeqOHs2ds( channelSeqds2OHs(leftEdge + '.'+ 't.u.p') , True)\n",
    "\n",
    "channelSeqOHs2ds(y0kOHmap(leftEdge + '.' + 't.u.p'), True)\n",
    "channelSeqOHs2ds(y0kOHmap('t.u.p'))\n",
    "len(ds2t(leftEdge + '.' + 't.u.p'))\n",
    "len(ds2t('t.u.p'))\n",
    "y0kOHmap('t.u.p').shape\n",
    "y0kOHmap(leftEdge + '.' + 't.u.p').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.296363Z",
     "start_time": "2019-08-07T18:30:26.114515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 7412)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.298955Z",
     "start_time": "2019-08-07T18:30:26.297408Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.378303Z",
     "start_time": "2019-08-07T18:30:26.300005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.p.ɹ.ɑ.p.ə.g.æ.n.d.ə.⋉.⋉'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.d.ʌ'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws_t))\n",
    "random_source_wordform\n",
    "\n",
    "random_source_prefix = choice(list(Ps_t))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.475938Z",
     "start_time": "2019-08-07T18:30:26.380495Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.550715Z",
     "start_time": "2019-08-07T18:30:26.480828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.tʃ.d.dʒ.aɪ.g.v.ʃ.j.ɛ.tʃ.k.p'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.649660Z",
     "start_time": "2019-08-07T18:30:26.552254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.v.aɪ.b.ɹ.eɪ.ʃ.ɪ.n.⋉.⋉'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.l.u.k.ɹ'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.æ.tʃ.p.w'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps_t))\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge or len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)) or len(ds2t(random_channel_prefix)) + 1 > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps_t))\n",
    "    random_source_wordform = choice(list(Ws_t))\n",
    "    random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "# while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "# #     random_source_prefix = getRandomKey(pX0i)\n",
    "#     random_source_prefix = choice(list(Ps))\n",
    "# # random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "# while len(ds2t(random_channel_prefix)) + 1 > len(ds2t(random_source_wordform)):\n",
    "#     random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "\n",
    "random_source_wordform\n",
    "random_source_prefix\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.764232Z",
     "start_time": "2019-08-07T18:30:26.654176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds2t(random_channel_prefix))\n",
    "len(ds2t(random_source_prefix))\n",
    "len(ds2t(random_source_wordform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.871786Z",
     "start_time": "2019-08-07T18:30:26.769088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.l.u', 'l.u.k', 'u.k.ɹ')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.l.u.k.ɹ'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:26.951488Z",
     "start_time": "2019-08-07T18:30:26.876329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7097, 2146, 4116)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load segmental sequence channel matrices $p(Y_0^k|X_0^k)$ and $p(Y_0^f|X_0^f)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index $i$ of `pY0k_X0ks` is a tensor of shape $|W| \\times |Y_1| \\times (i-2)$: for each wordform $w$ in $W$, there is a length $i-2$ stack of vectors giving the channel distribution for the length $i$ prefix (including the left word edge symbol and the upcoming segment) of $w$. (The stack is of length $i-2$ because there are $i-2$ triphones associated with a length $i$ source prefix.) Where necessary and appropriate (viz. when the wordform's length is incongruent w.r.t. the channel string), a wordform's channel matrix has been padded with columns of 0s to reflect the fact that the relevant source prefix could never have given rise to a channel string of that length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pY0f_X0fs` is an analogous tensor with the same shape, but intended for use in contexts where only wordforms whose length exactly matches the length of a channel string is worth considering as a cause of the channel string. (Accordingly, it has more zero vectors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:30.736709Z",
     "start_time": "2019-08-07T18:30:26.956464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks = pickle.load(open(c, 'rb'))\n",
    "len(pY0k_X0ks)\n",
    "pY0k_X0ks[10].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:34.828183Z",
     "start_time": "2019-08-07T18:30:30.737846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0f_X0fs = pickle.load(open(f, 'rb'))\n",
    "len(pY0f_X0fs)\n",
    "pY0f_X0fs[10].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:34.833500Z",
     "start_time": "2019-08-07T18:30:34.829356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005723328,\n",
       " 0.008584992,\n",
       " 0.011446656,\n",
       " 0.01430832,\n",
       " 0.017169984,\n",
       " 0.020031648,\n",
       " 0.022893312,\n",
       " 0.025754976,\n",
       " 0.02861664,\n",
       " 0.031478304,\n",
       " 0.034339968,\n",
       " 0.037201632,\n",
       " 0.040063296,\n",
       " 0.04292496,\n",
       " 0.045786624,\n",
       " 0.048648288]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005723328,\n",
       " 0.008584992,\n",
       " 0.011446656,\n",
       " 0.01430832,\n",
       " 0.017169984,\n",
       " 0.020031648,\n",
       " 0.022893312,\n",
       " 0.025754976,\n",
       " 0.02861664,\n",
       " 0.031478304,\n",
       " 0.034339968,\n",
       " 0.037201632,\n",
       " 0.040063296,\n",
       " 0.04292496,\n",
       " 0.045786624,\n",
       " 0.048648288]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FIXME\n",
    "list(map(lambda stack: stack.nbytes / 1e9 if stack is not None else None,\n",
    "         pY0k_X0ks))\n",
    "list(map(lambda stack: stack.nbytes / 1e9 if stack is not None else None,\n",
    "         pY0f_X0fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.015577Z",
     "start_time": "2019-08-07T18:30:34.834508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks[0].dtype\n",
    "pY0f_X0fs[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.087921Z",
     "start_time": "2019-08-07T18:30:35.018345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{4: 9172,\n",
       " 5: 9167,\n",
       " 6: 8994,\n",
       " 7: 7728,\n",
       " 8: 5938,\n",
       " 9: 4292,\n",
       " 10: 2964,\n",
       " 11: 1912,\n",
       " 12: 1084,\n",
       " 13: 576,\n",
       " 14: 247,\n",
       " 15: 97,\n",
       " 16: 31,\n",
       " 17: 7,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges\n",
    "wordlengthsInclEdges\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.173925Z",
     "start_time": "2019-08-07T18:30:35.088981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ws_t)\n",
    "len(Y1s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.279012Z",
     "start_time": "2019-08-07T18:30:35.178868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9172, 39, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks[0].shape\n",
    "pY0k_X0ks[1].shape\n",
    "pY0k_X0ks[2].shape\n",
    "pY0k_X0ks[3].shape\n",
    "\n",
    "pY0f_X0fs[0].shape\n",
    "pY0f_X0fs[1].shape\n",
    "pY0f_X0fs[2].shape\n",
    "pY0f_X0fs[3].shape\n",
    "pY0f_X0fs[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.348239Z",
     "start_time": "2019-08-07T18:30:35.283673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65264711, 0.02141693],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.02545656, 0.00855777],\n",
       "       [0.02545656, 0.58122414],\n",
       "       [0.00682531, 0.00821919],\n",
       "       [0.06235012, 0.00855777],\n",
       "       [0.00682531, 0.00833257],\n",
       "       [0.00700978, 0.02552273],\n",
       "       [0.00682531, 0.00844442],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.0051651 , 0.00734595],\n",
       "       [0.00682531, 0.00844442],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00844442],\n",
       "       [0.00700978, 0.01630829],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00844442],\n",
       "       [0.00682531, 0.00844442],\n",
       "       [0.00700978, 0.05787536],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00682531, 0.00844442],\n",
       "       [0.00664084, 0.00832952],\n",
       "       [0.00682531, 0.00833257],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00682531, 0.00821919],\n",
       "       [0.00700978, 0.00734595],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00682531, 0.00844442],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.02905399],\n",
       "       [0.00682531, 0.00821919],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00700978, 0.00855777],\n",
       "       [0.00682531, 0.00833257],\n",
       "       [0.00700978, 0.00844442],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks[4][0]\n",
    "pY0f_X0fs[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.427840Z",
     "start_time": "2019-08-07T18:30:35.352880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 7412)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.514889Z",
     "start_time": "2019-08-07T18:30:35.432569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9172, 39, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9172, 39, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks[4].shape\n",
    "pY0f_X0fs[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.591989Z",
     "start_time": "2019-08-07T18:30:35.519434Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_CM_for(x0f_WE, total_prefix_length, exact = False):\n",
    "    '''\n",
    "    total_prefix_length includes \n",
    "     - the \"⋊\" symbol,\n",
    "     - the actually and completely produced prefix,\n",
    "     - the next (i.e. upcoming) segment.\n",
    "     \n",
    "    Ex:\n",
    "      w = '⋊.aɪ.d.i.l.aɪ.z.⋉'\n",
    "      |'⋊.aɪ.d.i.l.aɪ.z.⋉'| = 8\n",
    "      l = 5\n",
    "    =>\n",
    "      'total_prefix' = '⋊.aɪ.d.i.l'\n",
    "      |'⋊.aɪ.d.i.l'| = 5\n",
    "      The associated channel matrix is of shape (|Y1s|, l-2 = 5-2 = 3).\n",
    "    '''\n",
    "    if not exact:\n",
    "        return pY0k_X0ks[total_prefix_length][Ws_t.index(x0f_WE)]\n",
    "    else:\n",
    "        return pY0f_X0fs[total_prefix_length][Ws_t.index(x0f_WE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:35.733091Z",
     "start_time": "2019-08-07T18:30:35.596779Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l.aɪ.z.⋉.⋉'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('aɪ.d.i', 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('⋊.aɪ.d.i', 4)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('⋊.aɪ.d.i.l', 5)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65264711, 0.02141693, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.58122414, 0.00693832],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.06235012, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.02552273, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.73787478],\n",
       "       [0.0051651 , 0.00734595, 0.00603577],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.01630829, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.05787536, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00664084, 0.00832952, 0.00684392],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00734595, 0.00603577],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.01339964],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.02905399, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('⋊.aɪ.d', 'aɪ.d.i', 'd.i.l')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6923, 5, 498)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65264711, 0.02141693, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.58122414, 0.00693832],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.06235012, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.02552273, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.73787478],\n",
       "       [0.0051651 , 0.00734595, 0.00603577],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.01630829, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.05787536, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00664084, 0.00832952, 0.00684392],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00734595, 0.00603577],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.01339964],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.02905399, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_0 = Ws_t[0]; w_0\n",
    "w_0_produced_prefix_noLE = t2ds(ds2t(Ws_t[0])[1:3+1]); w_0_produced_prefix_noLE, len(ds2t(w_0_produced_prefix_noLE))\n",
    "w_0_produced_prefix = t2ds(ds2t(Ws_t[0])[0:3+1]); w_0_produced_prefix, len(ds2t(w_0_produced_prefix))\n",
    "w_0_produced_prefix_plus_next = t2ds(ds2t(Ws_t[0])[0:3+2]); w_0_produced_prefix_plus_next, len(ds2t(w_0_produced_prefix_plus_next))\n",
    "\n",
    "pY0k_X0ks[5][0].shape\n",
    "# pY0k_X0ks[3][0] #assume the produced prefix is of length i+1=3+1=4 (not including left word edge)\n",
    "pY0k_X0ks[5][0] # assume the produced prefix is of length i-2=3 (not including left word edge)\n",
    "                # assume the produced prefix is of length i-1=4 (including left word edge)\n",
    "                # assume the produced prefix + upcoming segment is of length i=5 (including left word edge)\n",
    "\n",
    "w_0_whole_prefix = t2ds(ds2t(Ws_t[0])[0:3+2]); w_0_whole_prefix\n",
    "w_0_whole_prefix_3factors = sourcePrefixToTriphones(w_0_whole_prefix); w_0_whole_prefix_3factors\n",
    "w_0_whole_prefix_3factor_indices = sourcePrefixToTriphoneIndices(w_0_whole_prefix); w_0_whole_prefix_3factor_indices\n",
    "w_0_prefix_CM = np.vstack([p3Y1X012_np[:,source_triph_idx] for source_triph_idx in w_0_whole_prefix_3factor_indices]).T; w_0_prefix_CM\n",
    "w_0_prefix_CM.shape\n",
    "\n",
    "assert np.allclose( w_0_prefix_CM, pY0k_X0ks[5][0] )\n",
    "assert np.allclose( pY0k_X0ks[5][0], retrieve_CM_for(Ws_t[0], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    " - `|pY0k_X0ks|` = $max_l = |w|. ∄w'. |w'| > |w|$ (around 16-18, usually) \n",
    " - `pY0k_X0ks[i]` = the lexical channel matrices for 'total prefixes' of length `i` (i.e. incl. word edges and upcoming segment)\n",
    " - `pY0k_X0ks[i]` =  the lexical channel matrices for prefixes of length `i-1` (i.e. *not* incl. word edges, but incl. upcoming segment)\n",
    " - `|pY0k_X0ks[i]|` = `(|W|, |Y1|, i-2)` = (segmental wordforms, Y1s, number of triphones)\n",
    " \n",
    "(The same statements hold wrt `pY0fX0f`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load contextual distribution on segmental wordforms $p(W|C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:51.604623Z",
     "start_time": "2019-08-07T18:30:35.734859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9172, 106295)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7.79950192"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_C = np.load(w)\n",
    "pW_C.shape\n",
    "pW_C.dtype\n",
    "pW_C.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:51.607898Z",
     "start_time": "2019-08-07T18:30:51.605808Z"
    }
   },
   "outputs": [],
   "source": [
    "if pW_C.shape[1] != len(Cs_t):\n",
    "    raise Exception(f'# of contexts in metadata file does not match context dimension of pW_C matrix: {len(Cs_t)} vs. {pW_C.shape[1]}\\n\\t{s}\\n\\t{w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:51.793254Z",
     "start_time": "2019-08-07T18:30:51.608834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "106295"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ws_t)\n",
    "len(Cs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:51.874450Z",
     "start_time": "2019-08-07T18:30:51.795909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a couple of'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2.52036749e-10, 1.50634197e-08, 2.52036749e-10, ...,\n",
       "       5.54693797e-10, 4.15967189e-08, 6.22123621e-07])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs_t[Cs_t.index('a couple of')]\n",
    "pW_C[:,Cs_t.index('a couple of')]\n",
    "isNormalized_np(pW_C[:,Cs_t.index('a couple of')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:51.978017Z",
     "start_time": "2019-08-07T18:30:51.879099Z"
    }
   },
   "outputs": [],
   "source": [
    "def pW_C_lookup(w=None,c=None):\n",
    "    if w is None and c is None:\n",
    "        raise Exception('Must specify at least one of a context string or segmental wordform string')\n",
    "    if w is None:\n",
    "        my_pW_c = pW_C[:,Cs_t.index(c)]\n",
    "        my_pW_c_as_dict = dict(zip(Ws_t, my_pW_c))\n",
    "        assert isNormalized(my_pW_c_as_dict, epsilon = 1e-6)\n",
    "        return ProbDist(my_pW_c_as_dict)\n",
    "    if w is not None and c is not None:\n",
    "        my_pw_c = pW_C[Ws_t.index(w), Cs_t.index(c)]\n",
    "        return my_pw_c\n",
    "    if c is None:\n",
    "        my_pw_C = pW_C[Ws_t.index(w), :]\n",
    "        my_pw_C_as_dict = dict(zip(Cs_t, my_pw_C))\n",
    "        return my_pw_C_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.163676Z",
     "start_time": "2019-08-07T18:30:51.983176Z"
    }
   },
   "outputs": [],
   "source": [
    "my_dist = pW_C_lookup(c='a couple of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.254012Z",
     "start_time": "2019-08-07T18:30:52.164660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('⋊.t.aɪ.m.z.⋉.⋉', 0.7017068824069426),\n",
       " ('⋊.ð.ɛ.m.⋉.⋉', 0.058586970250377776),\n",
       " ('⋊.ð.ə.⋉.⋉', 0.05467041905649899),\n",
       " ('⋊.ʌ.ð.ɚ.⋉.⋉', 0.03664541602803625),\n",
       " ('⋊.m.aɪ.⋉.⋉', 0.028159696907432537),\n",
       " ('⋊.ð.oʊ.z.⋉.⋉', 0.020634293629461325),\n",
       " ('⋊.h.ʌ.n.d.ɹ.ə.d.⋉.⋉', 0.01748920205729206),\n",
       " ('⋊.l.aɪ.k.⋉.⋉', 0.014006091693604115),\n",
       " ('⋊.g.ʊ.d.⋉.⋉', 0.005842215226429354),\n",
       " ('⋊.m.ɪ.l.i.ə.n.⋉.⋉', 0.005304906857139398)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.373279Z",
     "start_time": "2019-08-07T18:30:52.258713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who had just'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "97052"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('⋊.h.æ.d.⋉.⋉', 0.2686310263917649),\n",
       " ('⋊.h.ɚ.d.⋉.⋉', 0.14325012966281095),\n",
       " ('⋊.k.ʌ.m.⋉.⋉', 0.11577348613569036),\n",
       " ('⋊.b.ɪ.n.⋉.⋉', 0.0559933476208487),\n",
       " ('⋊.g.ɑ.t.n.⋉.⋉', 0.03877476492689467),\n",
       " ('⋊.l.aɪ.k.⋉.⋉', 0.035802490943047514),\n",
       " ('⋊.j.u.⋉.⋉', 0.025379457450672833),\n",
       " ('⋊.ð.ə.⋉.⋉', 0.02293632913758328),\n",
       " ('⋊.ə.b.aʊ.t.⋉.⋉', 0.02014622808818711),\n",
       " ('⋊.t.u.⋉.⋉', 0.018663050189945213)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_context = choice(Cs_t); random_context\n",
    "Cs_t.index(random_context)\n",
    "pW_C_lookup(c=random_context).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lexicon metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.426445Z",
     "start_time": "2019-08-07T18:30:52.374331Z"
    }
   },
   "outputs": [],
   "source": [
    "cousin_fn_map = {i:'{0}cousinsOf.npz'.format(i) \n",
    "                 for i in range(5)}\n",
    "sphere_fn_map = {i:'{0}spheresOf.npz'.format(i) \n",
    "                 for i in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.507011Z",
     "start_time": "2019-08-07T18:30:52.427997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0cousinsOf.npz',\n",
       " 1: '1cousinsOf.npz',\n",
       " 2: '2cousinsOf.npz',\n",
       " 3: '3cousinsOf.npz',\n",
       " 4: '4cousinsOf.npz'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cousin_fn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.580106Z",
     "start_time": "2019-08-07T18:30:52.511925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0spheresOf.npz',\n",
       " 1: '1spheresOf.npz',\n",
       " 2: '2spheresOf.npz',\n",
       " 3: '3spheresOf.npz',\n",
       " 4: '4spheresOf.npz'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sphere_fn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.706137Z",
     "start_time": "2019-08-07T18:30:52.584617Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all(fn in listdir(m) for fn in cousin_fn_map.values())\n",
    "assert all(fn in listdir(m) for fn in sphere_fn_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:52.800047Z",
     "start_time": "2019-08-07T18:30:52.710887Z"
    }
   },
   "outputs": [],
   "source": [
    "chdir(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:56.809948Z",
     "start_time": "2019-08-07T18:30:52.806073Z"
    }
   },
   "outputs": [],
   "source": [
    "cousin_mats = mapValues(sparse.load_npz, cousin_fn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.023918Z",
     "start_time": "2019-08-07T18:30:56.811111Z"
    }
   },
   "outputs": [],
   "source": [
    "sphere_mats = mapValues(sparse.load_npz, sphere_fn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.026478Z",
     "start_time": "2019-08-07T18:30:57.024918Z"
    }
   },
   "outputs": [],
   "source": [
    "chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$th-cousin/sphere matrix $m$ is a matrix of shape $|P| \\times |W|$, where $m_{i,j}$ = 1 iff prefix $p_i$ is in a $k$-cousin/sphere relation to wordform $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.102729Z",
     "start_time": "2019-08-07T18:30:57.027522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42231"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ps_t)\n",
    "len(Ws_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.177707Z",
     "start_time": "2019-08-07T18:30:57.103651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <COO: shape=(42231, 9172), dtype=uint8, nnz=79727, fill_value=0>,\n",
       " 1: <COO: shape=(42231, 9172), dtype=uint8, nnz=1000076, fill_value=0>,\n",
       " 2: <COO: shape=(42231, 9172), dtype=uint8, nnz=9307972, fill_value=0>,\n",
       " 3: <COO: shape=(42231, 9172), dtype=uint8, nnz=39821468, fill_value=0>,\n",
       " 4: <COO: shape=(42231, 9172), dtype=uint8, nnz=71449640, fill_value=0>}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.001355359, 1: 0.017001292, 2: 0.158235524, 3: 0.676964956, 4: 1.21464388}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cousin_mats\n",
    "mapValues(lambda m: m.nbytes / 1e9,\n",
    "          cousin_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.278004Z",
     "start_time": "2019-08-07T18:30:57.178558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <COO: shape=(42231, 9172), dtype=uint8, nnz=9172, fill_value=0>,\n",
       " 1: <COO: shape=(42231, 9172), dtype=uint8, nnz=30652, fill_value=0>,\n",
       " 2: <COO: shape=(42231, 9172), dtype=uint8, nnz=370148, fill_value=0>,\n",
       " 3: <COO: shape=(42231, 9172), dtype=uint8, nnz=2062370, fill_value=0>,\n",
       " 4: <COO: shape=(42231, 9172), dtype=uint8, nnz=3207990, fill_value=0>}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.000155924, 1: 0.000521084, 2: 0.006292516, 3: 0.03506029, 4: 0.05453583}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sphere_mats\n",
    "mapValues(lambda m: m.nbytes / 1e9,\n",
    "          sphere_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.426163Z",
     "start_time": "2019-08-07T18:30:57.278833Z"
    }
   },
   "outputs": [],
   "source": [
    "# c1.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.536051Z",
     "start_time": "2019-08-07T18:30:57.431050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42229, 6949)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.θ.ʌ.n.d.ɚ.⋉'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.w.ʌ.n.d.ɚ.⋉.⋉'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = cousin_mats[1]\n",
    "(c1.coords[0][-2], c1.coords[1][-2])\n",
    "Ps_t[c1.coords[0][-2]]\n",
    "Ws_t[c1.coords[1][-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.618645Z",
     "start_time": "2019-08-07T18:30:57.540735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.712108Z",
     "start_time": "2019-08-07T18:30:57.623491Z"
    }
   },
   "outputs": [],
   "source": [
    "# segmental_wordforms = importSeqs(path.join(m, 'LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V_Transcriptions.txt'))\n",
    "# len(segmental_wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.787117Z",
     "start_time": "2019-08-07T18:30:57.716895Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(segmental_wordforms)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.862204Z",
     "start_time": "2019-08-07T18:30:57.792007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ws_t = tuple(sorted(list(map(padInputSequenceWithBoundaries,\n",
    "#                              segmental_wordforms))))\n",
    "# len(Ws_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:57.937115Z",
     "start_time": "2019-08-07T18:30:57.867208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ps = union(map(lambda w: getPrefixes(padInputSequenceWithBoundaries(w)), segmental_wordforms))\n",
    "# len(Ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.052600Z",
     "start_time": "2019-08-07T18:30:57.942099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ps_t = tuple(sorted(list(Ps)))\n",
    "# len(Ps_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow, but sanity-checking calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(C_0^i | X_0^i; X_{i+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_i = (Y_{i-1}^{x_i}, Y_i^{x_i}, Y_{i+1}^{x_i})$\n",
    "\n",
    "$p(y_{i-1}, y_i; y_{i+1} | x_{i-1}^i; x_{i+1}) = p(y_{i-1} | x_{i-1}, x_i;) p(y_i | x_{i-1}^i ; x_{i+1}) p(y_{i+1} | x_i ; x_{i+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(c_0^i | x_0^i; x_{i+1}) = \\prod\\limits_{j=0}^{j=i} p(c_i | x_{i-1}^i ; x_{i+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.136245Z",
     "start_time": "2019-08-07T18:30:58.053648Z"
    }
   },
   "outputs": [],
   "source": [
    "# def pC1_X012(y012, x012):\n",
    "#     x012_t = ds2t(x012)\n",
    "#     y012_t = ds2t(y012)\n",
    "    \n",
    "#     x0, x1, x2 = x012_t[0], x012_t[1], x012_t[2]\n",
    "#     x01 = (x0, x1)\n",
    "#     x12 = (x1, x2)\n",
    "    \n",
    "#     y0, y1, y2 = y012_t[0], y012_t[1], y012_t[2]\n",
    "    \n",
    "#     if x0 == leftEdge:\n",
    "#         if y0 == leftEdge:\n",
    "#             left_term = 1.0\n",
    "#         else:\n",
    "#             left_term = 0.0\n",
    "#     else:\n",
    "#         if y0 == leftEdge:\n",
    "#             left_term = 0.0\n",
    "#         else:\n",
    "#             left_term = p6Y0X01[x01][y0]\n",
    "    \n",
    "#     center_term = p3Y1X012[x012][y1]\n",
    "    \n",
    "#     if x1 == rightEdge:\n",
    "#         if y1 == rightEdge:\n",
    "#             right_term = 1.0\n",
    "#         else:\n",
    "#             right_term = 0.0\n",
    "#     else:\n",
    "#         if y1 == rightEdge:\n",
    "#             right_term = 0.0\n",
    "#         else:\n",
    "#             right_term = p3Y1X01[x12][y2]\n",
    "    \n",
    "#     terms = (left_term, center_term, right_term)\n",
    "    \n",
    "#     result = prod(terms)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def pC0iX0k(c0i, x0k):\n",
    "#     xp_t = ds2t(x0k)\n",
    "#     three_factor_seq = dsToKfactorSequence(3, x0k)\n",
    "    \n",
    "#     inputs = zip(c0i, three_factor_seq)\n",
    "    \n",
    "#     terms = starmap(pC1_X012, inputs)\n",
    "    \n",
    "#     result = prod(terms)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.209991Z",
     "start_time": "2019-08-07T18:30:58.139226Z"
    }
   },
   "outputs": [],
   "source": [
    "# s = leftEdge + '.i.p.i.' + rightEdge; s\n",
    "# dsToKfactorSequence(3, s)\n",
    "# threeFactorSequenceToDS(dsToKfactorSequence(3, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.285081Z",
     "start_time": "2019-08-07T18:30:58.210948Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(Y012s)\n",
    "# len(X012s)\n",
    "# len(Y012s) * len(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.363432Z",
     "start_time": "2019-08-07T18:30:58.286088Z"
    }
   },
   "outputs": [],
   "source": [
    "# # takes 5.5m on sidious and 45GB\n",
    "# square = set(product(Y012s, X012s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.438313Z",
     "start_time": "2019-08-07T18:30:58.364520Z"
    }
   },
   "outputs": [],
   "source": [
    "# def pY012_X012_calc(y012, x012):\n",
    "#     return (x012, pC1_X012(y012, x012))\n",
    "\n",
    "# pY012_X012 = dict(par(delayed(pY012_X012_calc)(y012, x012) for y012, x012 in product(Y012s, X012s)))\n",
    "\n",
    "# # def pY012_X012_calc(y012_x012_pair):\n",
    "# #     y012, x012 = y012_x012_pair\n",
    "# #     return (x012, pC1_X012(y012, x012))\n",
    "\n",
    "# # pY012_X012 = dict(par(delayed(pY012_X012_calc)(y012_x012_pair) for y012_x012_pair in product(Y012s, X012s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(Y_0^i|X_0^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.514967Z",
     "start_time": "2019-08-07T18:30:58.439344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pY0k_X0ks)\n",
    "sorted(wordlengthsInclEdges)\n",
    "assert len(pY0k_X0ks)-1 == sorted(wordlengthsInclEdges)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.622640Z",
     "start_time": "2019-08-07T18:30:58.515898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "42231"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ws)\n",
    "len(Ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.702539Z",
     "start_time": "2019-08-07T18:30:58.623868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0k_X0ks[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.777551Z",
     "start_time": "2019-08-07T18:30:58.703582Z"
    }
   },
   "outputs": [],
   "source": [
    "def pY0iX0k(y0i, x0k, debug = False):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    yp_t = ds2t(y0i) #\"y prefix\"\n",
    "#     assert xp_t[0] == leftEdge\n",
    "#     assert yp_t[0] == leftEdge\n",
    "    if len(xp_t) < 3:\n",
    "        raise Exception('|x0k| must be ≥ 3.')\n",
    "    if (len(yp_t)+1 != len(xp_t)) and not ((yp_t[-1] == rightEdge) and len(yp_t) == len(xp_t)):\n",
    "        raise Exception('|y0i| must = |x0k|-1, or |y0k| == |x0k| and y_k == ⋉.')\n",
    "#     if len(yp_t) != len(xp_t):\n",
    "#         raise Exception('Lengths of y0i and x0k must match.')\n",
    "    # if len(xp_t) == 1:\n",
    "    #     if xp_t[0] != leftEdge:\n",
    "    #         raise Exception('First symbol of x0k must be left word edge symbol.')    \n",
    "    #     if yp_t[0] == leftEdge:\n",
    "    #         return 1.0\n",
    "    #     else:\n",
    "    #         return 0.0\n",
    "    if yp_t[-1] == rightEdge:\n",
    "        #if y_{f+1} == ⋉, then \n",
    "        # the only strings x_0^f that could generate it\n",
    "        # have exactly the same length and also end with ⋉\n",
    "        if len(yp_t) != len(xp_t):\n",
    "            return 0.0\n",
    "        \n",
    "        x_k = xp_t[-1]\n",
    "        if x_k != rightEdge:\n",
    "            return 0.0\n",
    "        \n",
    "        #if |x_0^f| == |y_0^f| ∧ (y_{f+1} == x_{f+1} == ⋉),\n",
    "        # then remove y_{f+1} and proceed with the calculation\n",
    "        # as normal...\n",
    "        y_k = rightEdge\n",
    "        yp_t = yp_t[:-1]\n",
    "    elif xp_t[-1] == rightEdge:\n",
    "        # if x_k = ⋉ and y_k != ⋉, then the probability of that channel string is 0.0...\n",
    "        return 0.0\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    # yk = yp_t[-1]\n",
    "    # xik_ds = t2ds((xi, xk))\n",
    "    # preview_dist = p3Y1X01[xik_ds]\n",
    "#     if yk not in preview_dist:\n",
    "#         print('y0i: {0}\\nx0k: {1}\\nxi: {2}\\nxk: {3}\\nyk: {4}\\nxik: {5}'.format(y0i, x0k, xi, xk, yk, xik_ds))\n",
    "#     preview_term = preview_dist[yk]\n",
    "    \n",
    "#     if debug:\n",
    "#         print('xi = {0}\\nxk = {1}\\nyk = {2}'.format(xi, xk, yk))\n",
    "#         print('preview term = {0}'.format(preview_term))\n",
    "    \n",
    "#     x0i_t = xp_t[:-1] #prefix of intended wordform that has actually been produced\n",
    "    # y0i_t = yp_t[:-1]\n",
    "#     x0i_ds = t2ds(x0i_t)\n",
    "#     y0i_ds = t2ds(y0i_t)\n",
    "\n",
    "#     if debug:\n",
    "#         x0i_t = xp_t[:-1] #prefix of intended wordform that has actually been produced\n",
    "#         x0i_ds = t2ds(x0i_t)\n",
    "#         y0i_ds = t2ds(y0i_t)\n",
    "#         print('produced prefix x0i = {0}'.format(x0i_ds))\n",
    "#         print('perceived prefix y0i = {0}'.format(y0i_ds))\n",
    "    \n",
    "#     x012s = dsToKfactorSequence(3, x0i_ds)\n",
    "#     x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    x012s = dsToKfactorSequence(3, x0k)\n",
    "    # y1s = y0i_t[1:]\n",
    "    y1s = yp_t[1:]\n",
    "#     pairs = list(zip(x012s, y1s))\n",
    "#     produced_terms = tuple([p3Y1X012[x012][y1] for x012, y1 in pairs])\n",
    "    pairs = zip(x012s, y1s)\n",
    "    produced_terms = (p3Y1X012[x012][y1] for x012, y1 in pairs)\n",
    "    \n",
    "#     result = preview_term * prod(produced_terms)\n",
    "    result = prod(produced_terms)\n",
    "\n",
    "#     if debug:\n",
    "#         print('x012s: {0}'.format(x012s))\n",
    "#         print('y1s: {0}'.format(y1s))\n",
    "#         print('main terms: {0}'.format(produced_terms))\n",
    "#         print('end result = {0}'.format(result))\n",
    "    \n",
    "    if debug:\n",
    "#         return result, [p3Y1X012[x012] for x012 in x012s]\n",
    "        return result, np.array([[p3Y1X012[x012][k] for k in sorted(p3Y1X012[x012].keys())] for x012 in x012s]).T\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.884306Z",
     "start_time": "2019-08-07T18:30:58.778659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l.aɪ.z.⋉.⋉'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l.aɪ'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65264711, 0.02141693, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.00855777, 0.00703146],\n",
       "       [0.02545656, 0.58122414, 0.00693832],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.06235012, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.02552273, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.73787478],\n",
       "       [0.0051651 , 0.00734595, 0.00603577],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.01630829, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.05787536, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00664084, 0.00832952, 0.00684392],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00734595, 0.00603577],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00844442, 0.00693832],\n",
       "       [0.00700978, 0.00855777, 0.01339964],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.02905399, 0.00703146],\n",
       "       [0.00682531, 0.00821919, 0.00675326],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00700978, 0.00855777, 0.00703146],\n",
       "       [0.00682531, 0.00833257, 0.00684642],\n",
       "       [0.00700978, 0.00844442, 0.00693832],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_0\n",
    "# w_0_prefix = \n",
    "w_0_3 = t2ds(ds2t(w_0)[:3+2]); w_0_3# + 1\n",
    "w_0_4 = t2ds(ds2t(w_0)[:4+2]); w_0_4# + 1\n",
    "pY0k_X0ks[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:58.967011Z",
     "start_time": "2019-08-07T18:30:58.885285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1983147788326225"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.65264711, 0.02141693, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.02545656, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.02545656, 0.58122414, 0.00693832, 0.00713029],\n",
       "       [0.00682531, 0.00821919, 0.00675326, 0.00694011],\n",
       "       [0.06235012, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00833257, 0.00684642, 0.00703584],\n",
       "       [0.00700978, 0.02552273, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00700978, 0.00855777, 0.73787478, 0.007226  ],\n",
       "       [0.0051651 , 0.00734595, 0.00603577, 0.00620276],\n",
       "       [0.00682531, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.70851712],\n",
       "       [0.00700978, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00700978, 0.01630829, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.02155085],\n",
       "       [0.00700978, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00682531, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00700978, 0.05787536, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.02155085],\n",
       "       [0.00682531, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00664084, 0.00832952, 0.00684392, 0.00703327],\n",
       "       [0.00682531, 0.00833257, 0.00684642, 0.00703584],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00821919, 0.00675326, 0.00694011],\n",
       "       [0.00700978, 0.00734595, 0.00603577, 0.00620276],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.00700978, 0.00855777, 0.01339964, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.02905399, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00821919, 0.00675326, 0.00694011],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00700978, 0.00855777, 0.00703146, 0.007226  ],\n",
       "       [0.00682531, 0.00833257, 0.00684642, 0.00703584],\n",
       "       [0.00700978, 0.00844442, 0.00693832, 0.00713029],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0iX0k(w_0_3, w_0_4)\n",
    "pY0iX0k(w_0_3, w_0_4, True)[1]\n",
    "assert np.allclose( pY0k_X0ks[6][0], pY0iX0k(w_0_3, w_0_4, True)[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.046865Z",
     "start_time": "2019-08-07T18:30:58.968248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.14201210183923127"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0iX0k('⋊.k.ɑ.n.t', '⋊.k.ɑ.n.t.⋉')\n",
    "pY0iX0k('⋊.k.ɑ.n.t.⋉', '⋊.k.ɑ.n.t.⋉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.149910Z",
     "start_time": "2019-08-07T18:30:59.049556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25068286913932425"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0034938697256087312"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0iX0k('⋊.k.ɑ.n', '⋊.k.ɑ.n.t')\n",
    "pY0iX0k('⋊.k.ɑ.m', '⋊.k.ɑ.n.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.248379Z",
     "start_time": "2019-08-07T18:30:59.154741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t.u.p.l'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'t.u.p'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'t.u.p'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'t.u.p.l.?.?'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trimToMatch(y0i, x0f):\n",
    "    x0f_t = ds2t(x0f)\n",
    "    y0i_t = ds2t(y0i)\n",
    "    l = len(y0i_t)\n",
    "    if len(x0f_t) < l+1:\n",
    "        raise Exception('|x0f| must ≥ |y0i| + 1.')\n",
    "    x0k_t = x0f_t[:l+1]\n",
    "    x0k = t2ds(x0k_t)\n",
    "    return x0k\n",
    "\n",
    "def trimToLength(l, x0f, pad=False, padSymbol='?'):\n",
    "    x0f_t = ds2t(x0f)\n",
    "#     y0i_t = ds2t(y0i)\n",
    "#     l = len(y0i_t)\n",
    "    if len(x0f_t) < l and not pad:\n",
    "        raise Exception('x0f must be at least as long as l.')\n",
    "    if len(x0f_t) < l and pad:\n",
    "        x0k_t = x0f_t[:l]\n",
    "        x0k_t_padded = tuple( list(x0k_t) + [padSymbol] * (l - len(x0k_t)) )\n",
    "        return t2ds(x0k_t_padded)\n",
    "    else:\n",
    "        x0k_t = x0f_t[:l]\n",
    "        x0k = t2ds(x0k_t)\n",
    "        return x0k\n",
    "    \n",
    "# trimToMatch('t.u.p', 't.u.p')\n",
    "trimToMatch('t.u.p', 't.u.p.l')\n",
    "trimToLength(3, 't.u.p')\n",
    "trimToLength(3, 't.u.p.l')\n",
    "trimToLength(6, 't.u.p.l', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.342916Z",
     "start_time": "2019-08-07T18:30:59.252883Z"
    }
   },
   "outputs": [],
   "source": [
    "def pY0iX0f(y0i, x0f, debug = False):\n",
    "    x0f_t = ds2t(x0f)\n",
    "    y0i_t = ds2t(y0i)\n",
    "    l = len(y0i_t)\n",
    "    if len(x0f_t) < l+1 and not ((y0i_t[-1] == rightEdge) and len(y0i_t) == len(x0f_t)):\n",
    "        raise Exception('|x0f| must ≥ |y0i| + 1 or |x0f| = |y0k| ∧ y_k = ⋉')\n",
    "    x0k_t = x0f_t[:l+1]\n",
    "    x0k = t2ds(x0k_t)\n",
    "#     print(y0i)\n",
    "#     print(x0k)\n",
    "    return pY0iX0k(y0i, x0k, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.457410Z",
     "start_time": "2019-08-07T18:30:59.347625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25068286913932425"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25068286913932425"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY0iX0f('⋊.k.ɑ.n', '⋊.k.ɑ.n.t')\n",
    "pY0iX0f('⋊.k.ɑ.n', '⋊.k.ɑ.n.t.⋉')\n",
    "pY0iX0f('⋊.k.ɑ.n.t', '⋊.k.ɑ.n.t.⋉')\n",
    "pY0iX0f('⋊.k.ɑ.n.t', '⋊.k.ɑ.n.t.⋉')\n",
    "pY0iX0f('⋊.k.ɑ.n.t.⋉', '⋊.k.ɑ.n.t.⋉.⋉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.546495Z",
     "start_time": "2019-08-07T18:30:59.461876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.d.ɪ.s.ɑ.ɹ.m.⋉.⋉'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.ŋ.ɔɪ.ʒ.ɪ'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.d.ɪ.s.ɑ.ɹ.m.⋉.⋉'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.ŋ.ɔɪ.ʒ.ɪ'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_x0f = choice(Ws_t); random_x0f\n",
    "lw = len(ds2t(random_x0f))\n",
    "while lw-1 <= 2:\n",
    "    random_x0f = getRandomKey(pW); random_x0f\n",
    "    lw = len(ds2t(random_x0f))\n",
    "\n",
    "random_y0i = randomPrefix(choice(range(2,lw-1)), alphabet=Y1s); random_y0i\n",
    "random_x0f\n",
    "random_y0i\n",
    "# pY0iX0f(random_y0i, random_x0f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(X_0^f|C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.660467Z",
     "start_time": "2019-08-07T18:30:59.551281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9172, 106295)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9172, 106295)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_C.shape\n",
    "(len(Ws_t), len(Cs_t))\n",
    "assert pW_C.shape[0] == len(Ws_t)\n",
    "assert pW_C.shape[1] == len(Cs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.738427Z",
     "start_time": "2019-08-07T18:30:59.662151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cs_t[Cs_t.index('a couple of')]\n",
    "\n",
    "# pX0f = pW_C[:,Cs_t.index('a couple of')]\n",
    "\n",
    "# pW = pW_C_lookup(c = Cs_t[Cs_t.index('a couple of')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.816053Z",
     "start_time": "2019-08-07T18:30:59.739460Z"
    }
   },
   "outputs": [],
   "source": [
    "def pX0f_C(w, c):\n",
    "    return pW_C[:, Cs_t.index(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(Y_0^i | c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:59.929103Z",
     "start_time": "2019-08-07T18:30:59.820952Z"
    }
   },
   "outputs": [],
   "source": [
    "def pXhat0fY0i_unnormalized(xhat0f, y0i, c):\n",
    "    likelihood = pY0iX0f(y0i, xhat0f)\n",
    "#     prior = pW[xhat0f]\n",
    "    prior = pW_C_lookup(xhat0f, c)\n",
    "    return likelihood * prior\n",
    "\n",
    "def pY0i(y0i, c):\n",
    "#     l = len(ds2t(y0i))\n",
    "    y0i_t = ds2t(y0i)\n",
    "    l = len(y0i_t)\n",
    "    if y0i_t[-1] == rightEdge:\n",
    "        possible_source_wordforms = {w for w in Ws if len(ds2t(w)) == l}\n",
    "    else:\n",
    "        possible_source_wordforms = {w for w in Ws if len(ds2t(w)) >= l+1}\n",
    "#     x0ksWithLengthl = prefixesWithLength(l)\n",
    "#     py0kx0ks = tuple(pXhat0fY0k_unnormalized(x0k, y0k) for x0k in x0ksWithLengthl)\n",
    "\n",
    "#     sufficiently_long_words = {w for w in Ws if len(ds2t(w)) >= l+1}\n",
    "#     print(len(sufficiently_long_words))\n",
    "    #line below is slower by about a third (pypy/kotoba)\n",
    "#     sufficiently_long_words = wordformsAtLeastLlong(l, True)\n",
    "\n",
    "    py0ix0fs = (pXhat0fY0i_unnormalized(x0f, y0i, c) for x0f in possible_source_wordforms)\n",
    "#     def foo(x0f):\n",
    "#         return pXhat0fY0k_unnormalized(x0f, y0k)\n",
    "    # there's no apparent benefit to using foo + generator\n",
    "#     py0kx0fs = (foo(x0f) for x0f in sufficiently_long_words)\n",
    "# Parallel(n_jobs=-1, verbose=50, batch_size=4, prefer=\"processes\", backend=\"multiprocessing\")\n",
    "#     py0kx0fs = Parallel(n_jobs=8, batch_size=4, verbose=50, prefer=\"threads\", backend=\"multiprocessing\")(delayed(pXhat0fY0k_unnormalized)(x0f, y0k) for x0f in sufficiently_long_words)\n",
    "#     py0kx0fs = Parallel(n_jobs=8, prefer=\"threads\")(delayed(pXhat0fY0k_unnormalized)(x0f, y0k) for x0f in sufficiently_long_words)\n",
    "    return sum(py0ix0fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.253300Z",
     "start_time": "2019-08-07T18:30:59.933833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like i bought'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-b9724145ccd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCs_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrandom_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpY0i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'⋊.k.ɑ.n.t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-b18365c89366>\u001b[0m in \u001b[0;36mpY0i\u001b[0;34m(y0i, c)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     py0kx0fs = Parallel(n_jobs=8, batch_size=4, verbose=50, prefer=\"threads\", backend=\"multiprocessing\")(delayed(pXhat0fY0k_unnormalized)(x0f, y0k) for x0f in sufficiently_long_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     py0kx0fs = Parallel(n_jobs=8, prefer=\"threads\")(delayed(pXhat0fY0k_unnormalized)(x0f, y0k) for x0f in sufficiently_long_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy0ix0fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-b18365c89366>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#     sufficiently_long_words = wordformsAtLeastLlong(l, True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpy0ix0fs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpXhat0fY0i_unnormalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx0f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossible_source_wordforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#     def foo(x0f):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         return pXhat0fY0k_unnormalized(x0f, y0k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-b18365c89366>\u001b[0m in \u001b[0;36mpXhat0fY0i_unnormalized\u001b[0;34m(xhat0f, y0i, c)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpY0iX0f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhat0f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     prior = pW[xhat0f]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpW_C_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxhat0f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-3b601af58d05>\u001b[0m in \u001b[0;36mpW_C_lookup\u001b[0;34m(w, c)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mProbDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_pW_c_as_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmy_pw_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpW_C\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWs_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCs_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmy_pw_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_context = choice(Cs_t); random_context\n",
    "pY0i('⋊.k.ɑ.n.t', random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.257568Z",
     "start_time": "2019-08-07T18:30:04.779Z"
    }
   },
   "outputs": [],
   "source": [
    "# {w for w in Ws if (w,'⋊.k.ɑ.n.t') in prefix_relation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.258343Z",
     "start_time": "2019-08-07T18:30:04.793Z"
    }
   },
   "outputs": [],
   "source": [
    "random_context = choice(Cs_t); random_context\n",
    "pXhat0fY0i_unnormalized('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(Y_0^i|x_0^k)$ sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.259226Z",
     "start_time": "2019-08-07T18:30:04.807Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_pY0iX0k(x0k, debug = False):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "#     yp_t = ds2t(y0i) #\"y prefix\"\n",
    "#     if len(yp_t) != len(xp_t):\n",
    "#         raise Exception('Lengths of y0i and x0i must match.')\n",
    "    if len(xp_t) == 1:\n",
    "        if xp_t[0] != leftEdge:\n",
    "            raise Exception('First symbol of x0k must be left word edge symbol.')\n",
    "        else:\n",
    "            return leftEdge\n",
    "    if len(xp_t) < 3:\n",
    "        raise Exception('|x0k| must be ≥ 3')\n",
    "    \n",
    "    # xi = xp_t[-2] #just-completed segment\n",
    "    # xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    # xik = t2ds((xi, xk))\n",
    "    # yk = sampleFrom(p3Y1X01[xik])\n",
    "#     yk = yp_t[-1]\n",
    "#     preview_term = p3Y1X01[t2ds((xi, xk))][yk]\n",
    "    \n",
    "#     if debug:\n",
    "#         print('xi = {0}\\nxk = {1}\\nyk = {2}'.format(xi, xk, yk))\n",
    "#         print('preview term = {0}'.format(preview_term))\n",
    "    \n",
    "#     x0i_t = xp_t[:-1] #prefix of intended wordform that has actually been produced\n",
    "#     y0i_t = yp_t[:-1]\n",
    "#     x0i_ds = t2ds(x0i_t)\n",
    "#     y0i_ds = t2ds(y0i_t)\n",
    "\n",
    "#     if debug:\n",
    "#         print('produced prefix x0i = {0}'.format(x0i_ds))\n",
    "#         print('perceived prefix y0i = {0}'.format(y0i_ds))\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "#     print('x012s: {0}'.format(x012s))\n",
    "#     y1s = y0i_t[1:]\n",
    "    y1s = [sampleFrom(p3Y1X012[x012]) for x012 in x012s]\n",
    "#     print('y1s: {0}'.format(y1s))\n",
    "#     pairs = list(zip(x012s, y1s))\n",
    "#     produced_terms = tuple([p3Y1X012[x012][y1] for x012, y1 in pairs])\n",
    "    y0i_t = tuple( [leftEdge] + y1s )\n",
    "#     print('y0i_t = {0}'.format(y0i_t))\n",
    "    y0i_ds = t2ds(y0i_t)\n",
    "#     print('y0i_ds = {0}'.format(y0i_ds))\n",
    "#     assert len(y0i_t) == len(x0i_t)\n",
    "    # yp_t = tuple([leftEdge] + y1s + [yk])\n",
    "    # y0i = t2ds(yp_t)\n",
    "    \n",
    "    if xp_t[-1] == rightEdge:\n",
    "        y0i_ds = y0i_ds + '.' + rightEdge\n",
    "    \n",
    "    if debug:\n",
    "#         sorted_outcomes = Y1s_t\n",
    "        return y0i_ds, np.array([[p3Y1X012[x012][k] for k in Y1s_t] for x012 in x012s]).T\n",
    "#         return y0i_ds, np.array([[p3Y1X012[x012][k] for k in sorted(p3Y1X012[x012].keys())] for x012 in x012s]).T\n",
    "#         print('x012s: {0}'.format(x012s))\n",
    "#         print('y1s: {0}'.format(tuple(y1s)))\n",
    "#         print('main terms: {0}'.format(produced_terms))\n",
    "\n",
    "    return y0i_ds\n",
    "#     return preview_term * prod(produced_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.259933Z",
     "start_time": "2019-08-07T18:30:04.819Z"
    }
   },
   "outputs": [],
   "source": [
    "'⋊.æ.k.t.ɪ.v.ə.t.i.⋉.⋉'\n",
    "activity_samples1 = [sample_pY0iX0k('⋊.æ.k.t.ɪ.v.ə.t.i.⋉.⋉') for each in range(20)]; activity_samples1\n",
    "activity_samples2 = [sample_pY0iX0k('⋊.æ.k.t.ɪ.v.ə.t.i.⋉') for each in range(20)]; activity_samples2\n",
    "activity_samples3 = [sample_pY0iX0k('⋊.æ.k.t.ɪ.v.ə.t.i') for each in range(5)]; activity_samples3\n",
    "del activity_samples1, activity_samples2, activity_samples3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.260759Z",
     "start_time": "2019-08-07T18:30:04.831Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefix\n",
    "random_channel_samples = [sample_pY0iX0k(random_source_prefix) for each in range(50)]; random_channel_samples\n",
    "del random_channel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.261671Z",
     "start_time": "2019-08-07T18:30:04.844Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_samples = [sample_pY0iX0k('⋊.k.ɑ.n.t') for each in range(50)]; cont_samples\n",
    "del cont_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.262402Z",
     "start_time": "2019-08-07T18:30:04.856Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_pY0iX0k('⋊.k.ɑ.n.t', debug=True)[1].shape\n",
    "sample_pY0iX0k('⋊.k.ɑ.n.t', debug=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(\\widehat{X_0^f}|Y_0^{i}, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\widehat{x_0^f}|y_0^{i}, c) = \\frac{p(y_0^{i} | \\widehat{x_0^f})p(\\widehat{x_0^f} | c)}{p(y_0^{i})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.263202Z",
     "start_time": "2019-08-07T18:30:04.876Z"
    }
   },
   "outputs": [],
   "source": [
    "def pXhat0fY0i_old(xhat0f, y0i, c):\n",
    "    numerator = pXhat0fY0i_unnormalized(xhat0f, y0i, c)\n",
    "    denominator = pY0i(y0i, c)\n",
    "    if denominator == 0.0:\n",
    "#         if numerator == 0.0:\n",
    "#             print('both numerator and denominator are 0')\n",
    "#         else:\n",
    "#             print('just the denominator is 0')\n",
    "        return 0\n",
    "    return numerator / denominator\n",
    "\n",
    "random_context = choice(Cs_t); random_context\n",
    "(random_source_wordform, random_channel_prefix, random_context)\n",
    "pXhat0fY0i_old(random_source_wordform, random_channel_prefix, random_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\widehat{p}(\\widehat{X_0^f}|X_0^i;X_{i+1}, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\widehat{x_0^f} | x_0^i;x_{i+1}, c) = \\sum\\limits_{y_0^{i}} p(\\widehat{x_0^f} | y_0^{i}, c)p(y_0^{i}|x_0^{i};x_{i+1}, c)$\n",
    "\n",
    "$\\widehat{p}(\\widehat{x_0^f}|x_0^i;x_{i+1}, c) = \\frac{1}{m}\\sum\\limits_{m \\text{ sampled } y_0^{i}\\text{s from } p(Y_0^{i} | x_0^{i};x_{i+1})} p(\\widehat{x_0^f} | y_0^{i}, c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.263936Z",
     "start_time": "2019-08-07T18:30:04.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# BASELINE = OLD\n",
    "def phatXhat0fX0k_baseline(xhat0f, x0k, c, m = 50, my_j=None):\n",
    "#     y0ks = tuple(sample_pY0kX0k(x0k) for each in range(n))\n",
    "#     terms = (pXhat0fY0k(xhat0f, y0k) for y0k in y0ks)\n",
    "\n",
    "#     foo = partial(pXhat0fY0i, xhat0f)\n",
    "#     def foo(y0i):\n",
    "#         return pXhat0fY0i(xhat0f, y0i)\n",
    "#     terms = (foo(y0i) for y0i in (sample_pY0iX0i(x0i) for each in range(n)))\n",
    "#     terms = Parallel(n_jobs=8, prefer=\"threads\")(delayed(foo)(y0i) for y0i in y0is)\n",
    "#     terms = Parallel(n_jobs=8, prefer=\"threads\")(delayed(foo)(y0i) for y0i in (sample_pY0iX0i(x0i) for each in range(n)))\n",
    "#     terms = Parallel(n_jobs=5, verbose=50, batch_size=10, prefer=\"threads\")(delayed(pXhat0fY0i)(xhat0f, y0i) for y0i in (sample_pY0iX0i(x0i) for each in range(n)))\n",
    "#     terms = Parallel(n_jobs=-1, verbose=50, batch_size=round(n/12), prefer=\"processes\", backend=\"multiprocessing\")(delayed(pXhat0fY0i)(xhat0f, y0i) for y0i in (sample_pY0iX0i(x0i) for each in range(n)))\n",
    "\n",
    "#     terms = Parallel(n_jobs=-1, batch_size=round(n/12), prefer=\"processes\", backend=\"multiprocessing\")(delayed(pXhat0fY0k_old)(xhat0f, y0k) for y0k in (sample_pY0kX0k(x0k) for each in range(n)))\n",
    "#     terms = Parallel(n_jobs=-1, batch_size='auto', prefer=\"processes\", backend=\"multiprocessing\")(delayed(pXhat0fY0i_old)(xhat0f, y0i) for y0i in (sample_pY0iX0k(x0k) for each in range(m)))\n",
    "    if my_j is None:\n",
    "        my_j = -1\n",
    "    terms = Parallel(n_jobs=my_j, batch_size='auto', prefer=\"processes\", backend=\"multiprocessing\")(delayed(pXhat0fY0i_old)(xhat0f, y0i, c) for y0i in (sample_pY0iX0k(x0k) for each in range(m)))\n",
    "#     terms = (pXhat0fY0k_old(xhat0f, y0k) for y0k in (sample_pY0kX0k(x0k) for each in range(n)))\n",
    "    s = sum(terms)\n",
    "    sbar = (1.0 * s) / m\n",
    "    return sbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.264665Z",
     "start_time": "2019-08-07T18:30:04.902Z"
    }
   },
   "outputs": [],
   "source": [
    "random_context = choice(Cs_t); random_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.265414Z",
     "start_time": "2019-08-07T18:30:04.913Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.266110Z",
     "start_time": "2019-08-07T18:30:04.923Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.266869Z",
     "start_time": "2019-08-07T18:30:04.934Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    {w for w in Ws if (w,'⋊.k.ɑ.n.t') in prefix_relation}\n",
    "    [f\"p(W' = {w}) → {pW[w]}\"\n",
    "     for w in sorted(list({w for w in Ws if (w,'⋊.k.ɑ.n.t') in prefix_relation}),\n",
    "                     key=lambda w:pW_C_lookup(w=w, c=random_context), reverse=True)]\n",
    "\n",
    "    [f\"p(W' = {w}|r = '⋊.k.ɑ.n.t') → {phatXhat0fX0k_baseline(w, '⋊.k.ɑ.n.t', random_context, 50, -1)}\"\n",
    "     for w in sorted(list({w for w in Ws if (w,'⋊.k.ɑ.n.t') in prefix_relation}),\n",
    "                     key=lambda w:phatXhat0fX0k_baseline(w, '⋊.k.ɑ.n.t', random_context, 50, -1), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.267548Z",
     "start_time": "2019-08-07T18:30:04.950Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    {w for w in Ws if (w,'⋊.k.ɑ.n') in prefix_relation}\n",
    "\n",
    "    [f\"p(W' = {w}) → {pW_C_lookup(w=w, c=random_context)}\"\n",
    "     for w in sorted(list({w for w in Ws if (w,'⋊.k.ɑ.n') in prefix_relation}),\n",
    "                     key=lambda w:pW_C_lookup(w=w, c=random_context), reverse=True)]\n",
    "\n",
    "    [f\"p(W' = {w}|r = '⋊.k.ɑ.n.t') → {phatXhat0fX0k_baseline(w, '⋊.k.ɑ.n.t', random_context, 50, -1)}\"\n",
    "     for w in sorted(list({w for w in Ws if (w,'⋊.k.ɑ.n') in prefix_relation}),\n",
    "                     key=lambda w:pW_C_lookup(w=w, c=random_context), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.268259Z",
     "start_time": "2019-08-07T18:30:04.966Z"
    }
   },
   "outputs": [],
   "source": [
    "'⋊.k.ɑ.n.t'\n",
    "len( ds2t('⋊.k.ɑ.n.t') )\n",
    "len( ds2t('⋊.k.ɑ.n.t') ) - 1\n",
    "retrieve_CM_for('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', 4)\n",
    "\n",
    "np.allclose( retrieve_CM_for('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', 4), \n",
    "             pY0k_X0ks[4][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of core calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.268961Z",
     "start_time": "2019-08-07T18:30:04.979Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.269677Z",
     "start_time": "2019-08-07T18:30:04.990Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform = choice(Ws_t)\n",
    "random_wordform_length = len(ds2t(random_wordform))\n",
    "random_wordform_idx = Ws_t.index(random_wordform)\n",
    "random_wordform, random_wordform_idx, random_wordform_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.270434Z",
     "start_time": "2019-08-07T18:30:05.008Z"
    }
   },
   "outputs": [],
   "source": [
    "type(pW_C)\n",
    "pW_C.shape\n",
    "pW_C[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.271228Z",
     "start_time": "2019-08-07T18:30:05.022Z"
    }
   },
   "outputs": [],
   "source": [
    "Cs_t[Cs_t.index('a couple of')]\n",
    "random_context = 'a couple of'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.271995Z",
     "start_time": "2019-08-07T18:30:05.032Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.272743Z",
     "start_time": "2019-08-07T18:30:05.042Z"
    }
   },
   "outputs": [],
   "source": [
    "# pX0f = pW_C[:,Cs_t.index('a couple of')]\n",
    "# pX0f_torch = torch.from_numpy(pX0f)\n",
    "pW_C_torch = torch.from_numpy(pW_C).type(my_tt)\n",
    "pW_C_torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.273480Z",
     "start_time": "2019-08-07T18:30:05.053Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pW_t(c_idx):\n",
    "    return pW_C_torch[:,c_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.274240Z",
     "start_time": "2019-08-07T18:30:05.067Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "    CMsByLengthByWordformIndex = pY0k_X0ks\n",
    "    CMsByLengthByWordformIndex_torch = [torch.from_numpy(each).type(my_tt) \n",
    "                                        for each in CMsByLengthByWordformIndex]\n",
    "    exactCMsByLengthByWordformIndex = pY0f_X0fs\n",
    "    exactCMsByLengthByWordformIndex_torch = [torch.tensor(each).type(my_tt) \n",
    "                                             for each in exactCMsByLengthByWordformIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.274979Z",
     "start_time": "2019-08-07T18:30:05.085Z"
    }
   },
   "outputs": [],
   "source": [
    "CMsByLengthByWordformIndex[10].dtype\n",
    "CMsByLengthByWordformIndex_torch[10].dtype\n",
    "\n",
    "exactCMsByLengthByWordformIndex[10].dtype\n",
    "exactCMsByLengthByWordformIndex_torch[10].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.275638Z",
     "start_time": "2019-08-07T18:30:05.097Z"
    }
   },
   "outputs": [],
   "source": [
    "len(CMsByLengthByWordformIndex_torch)\n",
    "len(exactCMsByLengthByWordformIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.276278Z",
     "start_time": "2019-08-07T18:30:05.113Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "random_wordform_idx\n",
    "random_wordform_length\n",
    "torch.equal(CMsByLengthByWordformIndex_torch[random_wordform_length][random_wordform_idx],\n",
    "            exactCMsByLengthByWordformIndex_torch[random_wordform_length][random_wordform_idx])\n",
    "CMsByLengthByWordformIndex_torch[random_wordform_length-1][random_wordform_idx]\n",
    "exactCMsByLengthByWordformIndex_torch[random_wordform_length-1][random_wordform_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.276923Z",
     "start_time": "2019-08-07T18:30:05.126Z"
    }
   },
   "outputs": [],
   "source": [
    "exactCMsByLengthByWordformIndex_torch[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.277685Z",
     "start_time": "2019-08-07T18:30:05.136Z"
    }
   },
   "outputs": [],
   "source": [
    "CMsByLengthByWordformIndex_torch[10].dtype\n",
    "exactCMsByLengthByWordformIndex_torch[10].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.278348Z",
     "start_time": "2019-08-07T18:30:05.146Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "ds2t(random_wordform)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.279138Z",
     "start_time": "2019-08-07T18:30:05.170Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "ds2t(random_wordform)\n",
    "random_wordform_idx\n",
    "random_wordform_length\n",
    "random_wordform_CM = CMsByLengthByWordformIndex_torch[random_wordform_length][random_wordform_idx]\n",
    "'--'\n",
    "retrieve_CM_for(random_wordform, len(ds2t(random_wordform)))\n",
    "np.allclose(retrieve_CM_for(random_wordform, len(ds2t(random_wordform))),\n",
    "            CMsByLengthByWordformIndex[random_wordform_length][random_wordform_idx])\n",
    "'--'\n",
    "CMsByLengthByWordformIndex_torch[random_wordform_length][random_wordform_idx].shape\n",
    "random_wordform_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.279817Z",
     "start_time": "2019-08-07T18:30:05.183Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.280510Z",
     "start_time": "2019-08-07T18:30:05.195Z"
    }
   },
   "outputs": [],
   "source": [
    "if not benchmark:\n",
    "    del pY0k_X0ks\n",
    "    del pY0f_X0fs\n",
    "    del CMsByLengthByWordformIndex\n",
    "    del exactCMsByLengthByWordformIndex\n",
    "    del pW_C\n",
    "    del pX0f_C\n",
    "    del p3Y1X012\n",
    "    if not r and e:\n",
    "        del cousin_mats\n",
    "        del sphere_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.281195Z",
     "start_time": "2019-08-07T18:30:05.207Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.281873Z",
     "start_time": "2019-08-07T18:30:05.221Z"
    }
   },
   "outputs": [],
   "source": [
    "# if not r:\n",
    "    def depthSampler2a_t(CM, m=1):\n",
    "        '''\n",
    "        Given \n",
    "         - a sequence of channel distributions (i.e. a pytorch tensor)\n",
    "           associated with some prefix x_0^i of the lexicon\n",
    "         - a number of samples m\n",
    "        this returns m samples from the channel distribution p(Y_0^i|x_0^i),\n",
    "        where each 'sample' (i.e. channel string) is a sequence of one hot vectors.\n",
    "        \n",
    "        Let l = i+1:\n",
    "        |depthSampler2a_t(|CM| = (|Y1|, l), m)| = (m, l, |Y1|)\n",
    "        '''\n",
    "        stack = torch.zeros((m, CM.shape[1], CM.shape[0]))\n",
    "#         print(f'{stack.dtype}')\n",
    "        for eachStack in torch.arange(m):\n",
    "            for i in torch.arange(CM.shape[1]):\n",
    "                stack[eachStack, i] = torch.distributions.Multinomial(1, CM[:,i]).sample()\n",
    "        return stack\n",
    "#         return torch.squeeze(stack)\n",
    "# else:\n",
    "#     def depthSampler2a_t(xCM, m=1):\n",
    "#         stack = torch.zeros((m, xCM.shape[1], xCM.shape[0]))\n",
    "#         for eachStack in torch.arange(m):\n",
    "#             for i in torch.arange(xCM.shape[1]):\n",
    "#                 stack[eachStack, i] = torch.distributions.Multinomial(1, xCM[:,i]).sample()\n",
    "#         return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.282533Z",
     "start_time": "2019-08-07T18:30:05.231Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform_CM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.283250Z",
     "start_time": "2019-08-07T18:30:05.249Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "sampled_channel_sequences = depthSampler2a_t(random_wordform_CM, m=2); sampled_channel_sequences\n",
    "channelSeqOHs2ds(sampled_channel_sequences.numpy()[0], False)\n",
    "channelSeqOHs2ds(sampled_channel_sequences.numpy()[0], True)\n",
    "sampled_channel_sequences.shape\n",
    "del sampled_channel_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.283920Z",
     "start_time": "2019-08-07T18:30:05.262Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "sampled_channel_sequences2 = depthSampler2a_t(random_wordform_CM, m=100)\n",
    "[channelSeqOHs2ds(each.numpy(), True) for each in sampled_channel_sequences2]\n",
    "del sampled_channel_sequences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.284628Z",
     "start_time": "2019-08-07T18:30:05.273Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_wordform\n",
    "    sampled_channel_sequences3 = [sample_pY0iX0k(random_wordform) for each in range(50)]; sampled_channel_sequences3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.285266Z",
     "start_time": "2019-08-07T18:30:05.283Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    len(ds2t('⋊.ɔɪ.ŋ.dʒ.l.aɪ.h'))\n",
    "    y0kOHmap('⋊.ɔɪ.ŋ.dʒ.l.aɪ.h')\n",
    "    y0kOHmap('⋊.ɔɪ.ŋ.dʒ.l.aɪ.h').shape\n",
    "\n",
    "    np.array(list(map(y0kOHmap, sampled_channel_sequences3))).shape\n",
    "    np.array(list(map(y0kOHmap, [sample_pY0iX0k(random_wordform) for each in range(50)])))\n",
    "    del sampled_channel_sequences3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.285945Z",
     "start_time": "2019-08-07T18:30:05.293Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_wordform_CM.dtype\n",
    "    type(sample_pY0iX0k(random_wordform, debug=True)[1])\n",
    "    sample_pY0iX0k(random_wordform, debug=True)[1].dtype\n",
    "\n",
    "    torch.equal(torch.tensor(sample_pY0iX0k(random_wordform, debug=True)[1]).type(my_tt), \n",
    "                random_wordform_CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.286638Z",
     "start_time": "2019-08-07T18:30:05.310Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform\n",
    "random_wordform_length\n",
    "random_wordform_CM.shape\n",
    "depthSampler2a_t(random_wordform_CM, m=50).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.287360Z",
     "start_time": "2019-08-07T18:30:05.324Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSampler2a_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.288106Z",
     "start_time": "2019-08-07T18:30:05.334Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSampler2a_t(random_wordform_CM, m=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.288886Z",
     "start_time": "2019-08-07T18:30:05.345Z"
    }
   },
   "outputs": [],
   "source": [
    "def depthSamplerB_t(CM, m=1):\n",
    "    '''\n",
    "    Given \n",
    "     - a sequence of channel distributions (i.e. a pytorch tensor)\n",
    "       associated with some prefix x_0^i of the lexicon\n",
    "     - a number of samples m\n",
    "    this returns m samples from the channel distribution p(Y_0^i|x_0^i),\n",
    "    where each 'sample' (i.e. channel string) is a sequence of one hot vectors.\n",
    "\n",
    "    Let l = i+1:\n",
    "    |depthSamplerB_t(|CM| = (|Y1|, l), m)| = (m, l, |Y1|)\n",
    "    '''\n",
    "    return torch.stack([torch.stack([torch.distributions.Multinomial(1, CM[:,i]).sample()\n",
    "                                      for i in torch.arange(CM.shape[1])]).type(torch.uint8) \n",
    "                        for eachStack in torch.arange(m)])\n",
    "#     stack = torch.zeros((m, CM.shape[1], CM.shape[0]))\n",
    "# #         print(f'{stack.dtype}')\n",
    "#     for eachStack in torch.arange(m):\n",
    "#         for i in torch.arange(CM.shape[1]):\n",
    "#             stack[eachStack, i] = torch.distributions.Multinomial(1, CM[:,i]).sample()\n",
    "#     return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.289510Z",
     "start_time": "2019-08-07T18:30:05.356Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerB_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.290143Z",
     "start_time": "2019-08-07T18:30:05.368Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit torch.distributions.Multinomial(100, random_wordform_CM[:,0]).sample().type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.290900Z",
     "start_time": "2019-08-07T18:30:05.380Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit torch.distributions.Multinomial(1, random_wordform_CM[:,0]).sample(sample_shape=(100,)).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.291700Z",
     "start_time": "2019-08-07T18:30:05.393Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit random_wordform_CM[:,0].multinomial(100, replacement=True) #already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.292413Z",
     "start_time": "2019-08-07T18:30:05.404Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatchSizes(size_to_batch, num_batches):\n",
    "    elements_per_batch = size_to_batch // num_batches\n",
    "    batch_sizes = elements_per_batch * torch.ones((num_batches,), dtype=torch.int64)\n",
    "    final_adjustment_batch_size = size_to_batch % num_batches\n",
    "    if final_adjustment_batch_size != 0:\n",
    "        batch_sizes = list(batch_sizes)\n",
    "        batch_sizes = batch_sizes + [final_adjustment_batch_size]\n",
    "        batch_sizes = torch.tensor(batch_sizes)\n",
    "    return batch_sizes\n",
    "\n",
    "getBatchSizes(100, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.293151Z",
     "start_time": "2019-08-07T18:30:05.414Z"
    }
   },
   "outputs": [],
   "source": [
    "def depthSamplerC_t(CM, m=1):\n",
    "# def depthSamplerC_t(CM, m=1, parallel=False, numBatches=-1):\n",
    "    '''\n",
    "    Given \n",
    "     - a sequence of channel distributions (i.e. a pytorch tensor)\n",
    "       associated with some prefix x_0^i of the lexicon\n",
    "     - a number of samples m\n",
    "    this returns m samples from the channel distribution p(Y_0^i|x_0^i),\n",
    "    where each 'sample' (i.e. channel string) is a sequence of one hot vectors.\n",
    "\n",
    "    Let l = i+1:\n",
    "    |depthSamplerC_t(|CM| = (|Y1|, l), m)| = (m, l, |Y1|)    \n",
    "    '''\n",
    "#     If parallel is True and numBatches is an int > 0, then the m samples will be \n",
    "#     generated in parallel, in numBatches batches via joblib. \n",
    "    \n",
    "    s = CM.shape[0]\n",
    "    l = CM.shape[1]\n",
    "    \n",
    "#     stampedNote('Generating samples...')\n",
    "    #each of the l rows contains m sampled Y1 indices\n",
    "    # where the samples in each row are drawn from the l'th column of CM\n",
    "    stack_of_sampled_Y1_indices = torch.multinomial(CM.t(), m, replacement=True) #:: (l, m)\n",
    "\n",
    "#     if not parallel:\n",
    "#         #each of the l rows contains m sampled Y1 indices\n",
    "#         # where the samples in each row are drawn from the l'th column of CM\n",
    "#         stack_of_sampled_Y1_indices = torch.multinomial(CM.t(), m, replacement=True) #:: (l, m)\n",
    "#     elif parallel and numBatches > 0:\n",
    "#         CM_t = CM.t()\n",
    "#         batch_sizes = getBatchSizes(m, numBatches)\n",
    "        \n",
    "#         stack_of_sampled_Y1_indices = torch.cat(par(delayed(torch.multinomial)(CM_t, batch_size, replacement=True)\n",
    "#                                                     for batch_size in batch_sizes), \n",
    "#                                                 dim=1) #:: (l, m)\n",
    "#     else:\n",
    "#         raise Exception(f'If parallel is True, numbatches must be > 0, got {numBatches} instead.')\n",
    "#     print(f'sample dtype: {stack_of_sampled_Y1_indices.dtype}')\n",
    "#     print(f'sample shape: {stack_of_sampled_Y1_indices.shape}')\n",
    "    \n",
    "#     stampedNote('Converting to OH vectors...')\n",
    "    #convert each of the l rows (of length m) into a cat'd sequence (of length s * m) of OH vectors\n",
    "    deck_of_stacks_of_OHs = torch.zeros((l, s * m), dtype=torch.uint8) # :: (l, s * m)\n",
    "    \n",
    "#     stampedNote('Scattering...')\n",
    "    # stack_of_sampled_Y1_indices[i] is a sequence of m Y1 indices, but\n",
    "    # WRT deck_of_stacks_of_OHs, stack_of_sampled_Y1_indices[i] * offset represents \n",
    "    #   a set of column indices to set to 1\n",
    "    offset = s * torch.arange(m)\n",
    "    deck_of_stacks_of_OHs.scatter_(1, # we are specifying m *columns* (hence 1) for each row\n",
    "                                   stack_of_sampled_Y1_indices + offset, # each row is a sequence of columns to be filled with...\n",
    "                                   1) #...a 1 at all column indices\n",
    "#     return deck_of_stacks_of_OHs\n",
    "\n",
    "    # PERFORMANCE NOTE: *this* is by far the slowest part of sample generation when split+stack is used rather than reshape\n",
    "#     stampedNote('Splitting...')\n",
    "    #split along axis=1 into s-sized chunks\n",
    "    deck_of_stacks_of_OHs = torch.stack( torch.split( deck_of_stacks_of_OHs, s, 1) ) # :: = (m, l, s) => each of the m (l,s) matrices is a channel sequence represented as l (s,) OH vectors\n",
    "    #2-6x faster at scale and simplistitc testing shows the result is the same as stack->split, but downstream results are *clearly wrong*\n",
    "#     deck_of_stacks_of_OHs2 = torch.reshape(deck_of_stacks_of_OHs, (m, l, s)) \n",
    "#     return deck_of_stacks_of_OHs, deck_of_stacks_of_OHs2\n",
    "#     stampedNote('Done.')\n",
    "#     deck_of_stacks_of_OHs = torch.reshape(deck_of_stacks_of_OHs, (m, l, s))\n",
    "    \n",
    "    return deck_of_stacks_of_OHs\n",
    "\n",
    "def depthSamplerD_t(CM, m=1, numBatches=1):\n",
    "    '''\n",
    "    Wrapper call for using joblib to parallelize sampling using\n",
    "    depthSamplerC_t. \n",
    "    \n",
    "    Performance gain comes from two properties of depthSamplerC_t\n",
    "        - the single-process nature of the pytorch sampling calls\n",
    "        - the poor scaling of the last step of depthSamplerC_t\n",
    "    and a balance of properties of m and numBatches\n",
    "        - m must be fairly large (>> 10^6)\n",
    "        - the number of batches must be appropriately sized.\n",
    "    \n",
    "    E.g. For \n",
    "      (machine, input data, m, numBatches) = (Quine, newdic, 5e6, 10)\n",
    "    depthSamplerD_t is about 1.5x faster than depthSamplerC_t.\n",
    "    '''\n",
    "    sample_batch_sizes = getBatchSizes(m, numBatches)\n",
    "    \n",
    "    sample_batches = tuple(par(delayed(depthSamplerC_t)(CM, m=batch_size)\n",
    "                               for batch_size in sample_batch_sizes))\n",
    "    combined_samples = torch.cat(sample_batches)\n",
    "    return combined_samples\n",
    "\n",
    "def depthSamplerE_t(CM, m=1):\n",
    "    return torch.distributions.Multinomial(1, CM.t()).sample(sample_shape=(m,)).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.293847Z",
     "start_time": "2019-08-07T18:30:05.425Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerC_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.294533Z",
     "start_time": "2019-08-07T18:30:05.436Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerE_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.295230Z",
     "start_time": "2019-08-07T18:30:05.446Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerC_t(random_wordform_CM, m=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.295962Z",
     "start_time": "2019-08-07T18:30:05.457Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerE_t(random_wordform_CM, m=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.296635Z",
     "start_time": "2019-08-07T18:30:05.467Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #42.6s / quine\n",
    "    #15.1s / kotoba\n",
    "    exampleSample = depthSamplerC_t(random_wordform_CM, m=5000000).shape\n",
    "    del exampleSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.297318Z",
     "start_time": "2019-08-07T18:30:05.478Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #28.1s / quine\n",
    "    #6.92s / kotoba\n",
    "    exampleSample = depthSamplerD_t(random_wordform_CM, m=5000000, numBatches=10).shape\n",
    "    del exampleSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.298028Z",
     "start_time": "2019-08-07T18:30:05.488Z"
    }
   },
   "outputs": [],
   "source": [
    "# 30.7s / quine, random_wordform_CM.shape = (39,6), using stack+split at end instead of reshape\n",
    "if benchmark:\n",
    "    %timeit depthSamplerC_t(random_wordform_CM, m=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.298684Z",
     "start_time": "2019-08-07T18:30:05.501Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_OH(size, hot_bit_idx):\n",
    "    locations = torch.tensor([hot_bit_idx], dtype=torch.long)\n",
    "    values = torch.ones(1, dtype=torch.uint8)\n",
    "    return torch.zeros(size, dtype=torch.uint8).scatter_(0,\n",
    "                                                         locations,\n",
    "                                                         values)\n",
    "\n",
    "def make_n_OHs(n, size, hot_bit_idx):\n",
    "    index_vector = torch.arange(n)\n",
    "    offsets = (size * index_vector)\n",
    "    locations = hot_bit_idx + offsets\n",
    "    # line below makes function 10-12x slower\n",
    "#     locations = torch.tensor([hot_bit_idx + i * size for i in torch.arange(n)], dtype=torch.long)\n",
    "    values = torch.ones(n, dtype=torch.uint8)\n",
    "    vstacked = torch.zeros(size*n, dtype=torch.uint8).scatter_(0,\n",
    "                                                               locations,\n",
    "                                                               values)\n",
    "\n",
    "    #two lines below are basically the same time/memory wise\n",
    "    OHs = torch.stack(torch.split(vstacked, size)) #mildly more memory efficient\n",
    "#     OHs = torch.reshape(vstacked, (n, size)) #slightly faster, but may risk weird copy vs. view behavior downstream\n",
    "    return OHs\n",
    "\n",
    "def make_OHs_helper(types_and_counts, size, m):\n",
    "    '''\n",
    "    types_and_counts :: (m, 2)\n",
    "    types_and_counts[i] = a column vector representing (type_idx, type_count), where\n",
    "                           - 0 <= type_idx <= size-1\n",
    "    '''\n",
    "    tc = types_and_counts\n",
    "#     index_vector = torch.arange(n)\n",
    "#     locations = hot_bit_idx + (size * index_vector)\n",
    "    num_types = tc.shape[0]\n",
    "    \n",
    "    #each index appears only in a 'chunk' of copies of itself...\n",
    "    # 'type_index' = hot_bit_idx in simpler function above...\n",
    "    type_index_of_each_OH_token = torch.cat([tc[i,0] * torch.ones(tc[i,1], dtype=torch.uint8) \n",
    "                                             for i in torch.arange(num_types)])\n",
    "    \n",
    "    shuffled_type_index_of_each_OH_token = type_index_of_each_OH_token\n",
    "    \n",
    "    OH_token_index_vector = torch.arange(m)\n",
    "    offsets = (size * OH_token_index_vector)\n",
    "    locations = shuffled_type_index_of_each_OH_token.type(torch.long) + offsets\n",
    "    \n",
    "#     locations = []\n",
    "#     for type_idx in tc[:,0]: #first column is the set of type indices\n",
    "#         for each in range(tc):\n",
    "#             locations.append(type_idx)\n",
    "#     locations = torch.tensor([tc[i,0] + j * size for j in torch.arange(tc[i,1])], dtype=torch.long)\n",
    "#     locations = torch.tensor([hot_bit_idx + i * size for i in torch.arange(m)], dtype=torch.long)\n",
    "    values = torch.ones(m, dtype=torch.uint8)\n",
    "    vstacked = torch.zeros(size*m, dtype=torch.uint8).scatter_(0,\n",
    "                                                               locations,\n",
    "                                                               values)\n",
    "    OHs = torch.stack(torch.split(vstacked, size)) #mildly more memory efficient\n",
    "#     OHs = torch.reshape(vstacked, (n, size)) #slightly faster, but may risk weird copy vs. view behavior downstream\n",
    "    return OHs\n",
    "\n",
    "def makeTypeCountMatrix(sampleVector):\n",
    "    type_indices = sampleVector.nonzero().squeeze().type(torch.uint8)\n",
    "    type_counts = torch.gather(sampleVector, 0, type_indices).type(torch.uint8)\n",
    "#     type_counts = torch.index_select(sampleVector, 0, type_indices)\n",
    "    return torch.stack((type_indices, type_counts)).t()\n",
    "\n",
    "def make_OHs(sampleVector):\n",
    "    '''\n",
    "    sampleVector is a sparse non-negative integer vector of shape (|Y1s|,).\n",
    "    \n",
    "    It represents a sample of size m from a distribution over Y1s, where \n",
    "     - the value at index i indicates the count of sampled objects associated with index i.\n",
    "    \n",
    "    This returns a uniformly shuffled stack of one-hot vectors representing the sample.\n",
    "    '''\n",
    "    my_size = sampleVector.shape[0]\n",
    "    m = sampleVector.sum().item()\n",
    "    sample_type_indices = sampleVector.nonzero().squeeze().type(torch.long)\n",
    "    sample_counts = torch.gather(sampleVector, 0, sample_type_indices).type(torch.long)\n",
    "#     sample_counts = torch.index_select(sampleVector, 0, sample_type_indices)\n",
    "    types_and_counts = torch.stack((sample_type_indices, sample_counts)).type(torch.uint8)\n",
    "    return types_and_counts\n",
    "\n",
    "# make_OH(39, 6).shape\n",
    "# make_n_OHs(2, 39, 6).shape\n",
    "make_n_OHs(20, 39, 6).shape\n",
    "# print('----')\n",
    "# make_n_OHs_alt(20, 39, 6).shape\n",
    "# make_OHs(ten_samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.299421Z",
     "start_time": "2019-08-07T18:30:05.513Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit make_n_OHs(100000, 39, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.300100Z",
     "start_time": "2019-08-07T18:30:05.524Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerB_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.300776Z",
     "start_time": "2019-08-07T18:30:05.535Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerB_t(random_wordform_CM, m=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.301527Z",
     "start_time": "2019-08-07T18:30:05.545Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerC_t(random_wordform_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.302244Z",
     "start_time": "2019-08-07T18:30:05.559Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit depthSamplerC_t(random_wordform_CM, m=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-parallel posterior estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.302954Z",
     "start_time": "2019-08-07T18:30:05.573Z"
    }
   },
   "outputs": [],
   "source": [
    "# import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.303773Z",
     "start_time": "2019-08-07T18:30:05.586Z"
    }
   },
   "outputs": [],
   "source": [
    "# if not r:\n",
    "#     def pXhat0fX0i_pxt(xhat0f_idx, x0k_CM=None, m = 50, x0k=None):\n",
    "    def pXhat0fX0i_pxt(xhat0f_idx, x0k_CM, c_idx, m = 50):\n",
    "        '''\n",
    "        If ⋊ x_0 x_1 ... x_i x_k denotes\n",
    "        the produced prefix r of the speaker's intended wordform w,\n",
    "         where\n",
    "           x_i = the last fully produced segment\n",
    "           k = i+1\n",
    "           x_k = the next segment\n",
    "           |⋊ x_0 x_1 ... x_i x_k| = l + 2\n",
    "           l = i+1 = the length of the produced prefix (not incl. \n",
    "                     the left word edge or the next segment)\n",
    "        \n",
    "        then\n",
    "        \n",
    "        x0k_CM :: (|Y1|, l) is the associated l-length stack of size |Y1| vectors\n",
    "         where x0k_CM[:,j] is the distribution p(Y_j|X0^k has been produced)\n",
    "           \n",
    "        xhat0f_idx is the index of a wordform w' s.t. \n",
    "        \n",
    "        pXhat0fX0i_pxt(xhat0f_idx, x0k_CM) ≈ p(w'|r)\n",
    "        '''\n",
    "        shape_info = False\n",
    "        pW = get_pW_t(c_idx)\n",
    "#         if x0k_CM is None and x0k is None:\n",
    "#             raise Exception('Must specify one of x0k_CM or x0k.')\n",
    "        \n",
    "        # Computation proceeds in two steps:\n",
    "        # calculate the denominator p(y0i) for m = 50 sampled y0i's\n",
    "        # calculate the numerator p(y0i|w')p(w') for m = 50 sampled y0i's\n",
    "        \n",
    "        \n",
    "    #     xhat0f_idx = Wmap[xhat0f]\n",
    "\n",
    "    #     l = len(ds2t(x0i))\n",
    "    #     x0k_CM = CMsByPrefixIndex[prefixMap[x0i]]\n",
    "\n",
    "    #     my_Q_l = CMsByLengthByWordformIndex[l - 2]\n",
    "\n",
    "        #Collect m samples from p(Y0i|X0k) = p(Y0i|r), \n",
    "        # where each sample is an l-length stack of one-hot vectors\n",
    "        # where each one-hot vector corresponds to a channel symbol:\n",
    "#         Y_prime = depthSampler2a_t(x0k_CM, m)#.float() #:: (m, l, |Y1|)\n",
    "        Y_prime = depthSamplerC_t(x0k_CM, m)#.float() #:: (m, l, |Y1|)\n",
    "#         if x0k_CM is not None:\n",
    "#             Y_prime = depthSampler2a_t(x0k_CM, m)#.float() #:: (m, l, |Y1|)\n",
    "#             if x0k is not None:\n",
    "# #                 print('foo')\n",
    "#                 print(type(x0k_CM))\n",
    "#                 print(x0k_CM.shape)\n",
    "#                 generated_CM = sample_pY0iX0k(x0k, debug=True)[1]\n",
    "#                 print(type(generated_CM))\n",
    "#                 print(generated_CM.shape)\n",
    "#                 generated_CM_t = torch.tensor(generated_CM)\n",
    "#                 assert torch.allclose(x0k_CM, torch.tensor(sample_pY0iX0k(x0k, debug=True)[1]))\n",
    "# #                 print('bar')\n",
    "#         else:\n",
    "#             Y_prime = torch.tensor(list(map(y0kOHmap, [sample_pY0iX0k(x0k) for each in range(m)])))\n",
    "#         print('baz')\n",
    "        l = Y_prime.shape[1]# + 1\n",
    "        if shape_info:\n",
    "            print(f'|Y_prime| = {Y_prime.shape} = (m, l, |Y1|)')\n",
    "\n",
    "        #Grab ALL the relevant-prefix-length channel matrices \n",
    "        # for every segmental wordform in the lexicon:\n",
    "#         my_Q_l = CMsByLengthByWordformIndex_torch[l - 3]#.float()\n",
    "#         my_Q_l = CMsByLengthByWordformIndex_torch[l - 1]#.float() #:: (|W|, |Y1|, l)\n",
    "        my_Q_l = CMsByLengthByWordformIndex_torch[l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "        if shape_info:\n",
    "            print(f'|Q_l| = {my_Q_l.shape} = (|W|=n, |Y1|, l)')\n",
    "    \n",
    "#         print(f'{Y_prime.dtype}, {my_Q_l.dtype}')\n",
    "    \n",
    "        # NORMALIZATION\n",
    "#         print(Y_prime.dtype)\n",
    "#         print(my_Q_l.dtype)\n",
    "#         V_prime = oe.contract('mli,kil->mkl', Y_prime.type(my_tt), my_Q_l, backend='torch')  # :: (m,n,l)\n",
    "        V_prime = torch.einsum('mli,kil->mkl', Y_prime.type(my_tt), my_Q_l)  # :: (m,n,l)\n",
    "#         print(V_prime.dtype)\n",
    "#         print(f\"|V'| = {V_prime.shape} = (m, n, l)\")\n",
    "        M_prime = torch.prod(V_prime, 2) # :: (m,n)\n",
    "#         print(f\"|M'| = {M_prime.shape} = (m, n)\")\n",
    "#         N_prime = torch.matmul(M_prime, pX0f_torch) # :: (m, ) <- prior probabilities of each of the m sampled channel prefixes\n",
    "        N_prime = torch.matmul(M_prime, pW)\n",
    "#         print(f\"|N'| = {N_prime.shape} = (m, )\")\n",
    "#         Z_prime = 1.0 / N_prime # :: (m, )\n",
    "        Z_prime = torch.ones(N_prime.shape, dtype=my_tt)\n",
    "        torch.div(input=Z_prime,\n",
    "                  other=N_prime,\n",
    "                  out=Z_prime)\n",
    "#         print(f\"|Z'| = {Z_prime.shape} = (m, )\")\n",
    "        if shape_info:\n",
    "            print(f\"|V'| = {V_prime.shape} = (m, n, l)\")\n",
    "            print(f\"|M'| = {M_prime.shape} = (m, n)\")\n",
    "            print(f\"|N'| = {N_prime.shape} = (m, )\")\n",
    "            print(f\"|Z'| = {Z_prime.shape} = (m, )\")\n",
    "\n",
    "        # NUMERATOR\n",
    "        L_w = my_Q_l[xhat0f_idx]#.float()\n",
    "#         print(L_w.dtype)\n",
    "        V_prime_w = torch.einsum('mij,ji->mi',Y_prime.type(my_tt), L_w)\n",
    "#         print(V_prime_w.dtype)\n",
    "#         print(f\"|V'_w| = {V_prime_w.shape} = (m, l)\")\n",
    "        O_w = torch.prod(V_prime_w, 1) # :: (m,) likelihoods of each of the m sampled channel prefixes\n",
    "#         print(f\"|O_w| = {O_w.shape} = (m, )\")\n",
    "#         U_w = pX0f_torch[xhat0f_idx] * O_w ## :: (m,) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "        U_w = pW[xhat0f_idx] * O_w ## :: (m,) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "#         print(f\"|U_w| = {U_w.shape} = (m, )\")\n",
    "        if shape_info:\n",
    "            print(f\"|V'_w| = {V_prime_w.shape} = (m, l)\")\n",
    "            print(f\"|O_w| = {O_w.shape} = (m, )\")\n",
    "            print(f\"|U_w| = {U_w.shape} = (m, )\")\n",
    "        \n",
    "        \n",
    "        if shape_info:\n",
    "            E = torch.dot(Z_prime, U_w) / m\n",
    "            print(f\"|E| = scalar: type(E) = {type(E)}\")\n",
    "        return torch.dot(Z_prime, U_w) / m\n",
    "\n",
    "    #     return torch.dot( 1.0 / torch.matmul(torch.prod(torch.einsum('mli,kil->mkl', Y_prime, my_Q_l), 2), pX0f_torch) , \n",
    "    #        pX0f_torch[xhat0f_idx] * torch.prod(torch.einsum('mij,ji->mi',Y_prime, L_w), 1) ) / m\n",
    "# else:\n",
    "#     def pXhat0fX0k_pxt(xhat0f_idx, x0i_xCM, m = 50):\n",
    "#     #     xhat0f_idx = Wmap[xhat0f]\n",
    "\n",
    "#     #     l = len(ds2t(x0i))\n",
    "#     #     x0i_xCM = xCMsByPrefixIndex[prefixMap[x0i]]\n",
    "\n",
    "#     #     my_Q_l = xCMsByLengthByWordformIndex[l - 2]\n",
    "\n",
    "#         Y_prime = depthSampler2a_t(x0i_xCM, m)#.float()\n",
    "#         l = Y_prime.shape[1] + 1\n",
    "\n",
    "#         my_Q_l = xCMsByLengthByWordformIndex_torch[l - 3]#.float()\n",
    "\n",
    "#         # NORMALIZATION\n",
    "#         V_prime = torch.einsum('mli,kil->mkl', Y_prime, my_Q_l)  # :: (m,n,l)\n",
    "#         M_prime = torch.prod(V_prime, 2) # :: (m,n)\n",
    "#         N_prime = torch.matmul(M_prime, pX0f_torch) # :: (m, 1) <- prior probabilities of each of the m sampled channel prefixes\n",
    "#         Z_prime = 1.0 / N_prime # :: (m, 1)\n",
    "\n",
    "#         # NUMERATOR\n",
    "#         L_w = my_Q_l[xhat0f_idx]#.float()\n",
    "#         V_prime_w = torch.einsum('mij,ji->mi',Y_prime, L_w)\n",
    "#         O_w = torch.prod(V_prime_w, 1) # :: (m,1) likelihoods of each of the m sampled channel prefixes\n",
    "#         U_w = pX0f_torch[xhat0f_idx] * O_w ## :: (m,1) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "\n",
    "#         return torch.dot(Z_prime, U_w) / m\n",
    "\n",
    "#     #     return torch.dot( 1.0 / torch.matmul(torch.prod(torch.einsum('mli,kil->mkl', Y_prime, my_Q_l), 2), pX0f_torch) , \n",
    "#     #        pX0f_torch[xhat0f_idx] * torch.prod(torch.einsum('mij,ji->mi',Y_prime, L_w), 1) ) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.304493Z",
     "start_time": "2019-08-07T18:30:05.597Z"
    }
   },
   "outputs": [],
   "source": [
    "def pXhat0fX0i_pxtn(xhat0f_idx, x0k_CM, c_idx, m = 50):\n",
    "    return pXhat0fX0i_pxt(xhat0f_idx, x0k_CM, c_idx, m = 50).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.305211Z",
     "start_time": "2019-08-07T18:30:05.609Z"
    }
   },
   "outputs": [],
   "source": [
    "def prefixTox0kCM(r):\n",
    "    source_wf = list(wordsWithPrefix(r, Ws))[0]\n",
    "    source_wf_idx = Ws_t.index(source_wf)\n",
    "    \n",
    "    r_t = ds2t(r)\n",
    "    total_x0k_length = len(r_t)# includes left edge symbol + upcoming segment\n",
    "#     prefix_length_noWE = total_x0k_length - 1 #includes upcoming segment\n",
    "#     prefix_length_produced_so_far = prefix_length_noWE - 1\n",
    "#     offset = 1\n",
    "#     x0k_cm = CMsByLengthByWordformIndex_torch[prefix_length_produced_so_far - offset][source_wf_idx]\n",
    "    x0k_cm = CMsByLengthByWordformIndex_torch[total_x0k_length][source_wf_idx]\n",
    "    return x0k_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.305969Z",
     "start_time": "2019-08-07T18:30:05.622Z"
    }
   },
   "outputs": [],
   "source": [
    "CMsByLengthByWordformIndex_torch[3].shape\n",
    "CMsByLengthByWordformIndex_torch[4].shape\n",
    "len(CMsByLengthByWordformIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.306669Z",
     "start_time": "2019-08-07T18:30:05.633Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Ws_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.307387Z",
     "start_time": "2019-08-07T18:30:05.645Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordformTox0kCM(w=None, w_idx=None):\n",
    "#     if w is None and w_idx is None:\n",
    "#         raise Exception('At least one argument must be specified.')\n",
    "    if w is None:\n",
    "        w = Ws_t[w_idx]\n",
    "        w_t = ds2t(w)\n",
    "    else:\n",
    "        w_t = ds2t(w)\n",
    "        w_idx = Ws_t.index(w)\n",
    "    \n",
    "    total_x0f_length = len(w_t) #includes both edge symbols\n",
    "#     word_length_noLE = total_x0f_length - 1 #still includes right edge symbol = \"upcoming segment\"\n",
    "#     word_length_noRE = word_length_noLE - 1\n",
    "#     offset = 2\n",
    "#     assert word_length_produced_so_far - offset >= 0 , f\"{word_length_produced_so_far} - {offset} < 0; w = {w}\"\n",
    "#     assert word_length_produced_so_far - offset < len(CMsByLengthByWordformIndex_torch), f\"{word_length_produced_so_far} - {offset} >= {len(CMsByLengthByWordformIndex_torch)}; w = {w}\"\n",
    "#     assert w_idx in range(CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset].shape[0]), f\"{w_idx}; w = {w}\"\n",
    "#     x0f_cm = CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset][w_idx]\n",
    "    x0f_cm = CMsByLengthByWordformIndex_torch[total_x0f_length][w_idx]\n",
    "    return x0f_cm\n",
    "\n",
    "# def wordformTox0fCM(w=None, w_idx=None):\n",
    "# #     if w is None and w_idx is None:\n",
    "# #         raise Exception('At least one argument must be specified.')\n",
    "#     if w is None:\n",
    "#         w = Ws_t[w_idx]\n",
    "#         w_t = ds2t(w)\n",
    "#     else:\n",
    "#         w_t = ds2t(w)\n",
    "#         w_idx = Ws_t.index(w)\n",
    "    \n",
    "#     total_x0f_length = len(w_t) #includes both edge symbols\n",
    "# #     word_length_noLE = total_x0f_length - 1 #still includes right edge symbol = \"upcoming segment\"\n",
    "# #     word_length_noRE = word_length_noLE - 1\n",
    "# #     offset = 2\n",
    "# #     assert word_length_produced_so_far - offset >= 0 , f\"{word_length_produced_so_far} - {offset} < 0; w = {w}\"\n",
    "# #     assert word_length_produced_so_far - offset < len(CMsByLengthByWordformIndex_torch), f\"{word_length_produced_so_far} - {offset} >= {len(CMsByLengthByWordformIndex_torch)}; w = {w}\"\n",
    "# #     assert w_idx in range(CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset].shape[0]), f\"{w_idx}; w = {w}\"\n",
    "# #     x0f_cm = CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset][w_idx]\n",
    "#     x0f_cm = exactCMsByLengthByWordformIndex_torch[total_x0f_length][w_idx]\n",
    "#     return x0f_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.308115Z",
     "start_time": "2019-08-07T18:30:05.666Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ds2t('⋊.m.ɪ.n.ɪ.s.t'))\n",
    "len(dsTo3factors('⋊.m.ɪ.n.ɪ.s.t'))\n",
    "prefixTox0kCM('⋊.m.ɪ.n.ɪ.s.t').shape\n",
    "\n",
    "len(ds2t('⋊.m.ɪ.n.ɪ.s.t.ɚ'))\n",
    "len(dsTo3factors('⋊.m.ɪ.n.ɪ.s.t.ɚ'))\n",
    "prefixTox0kCM('⋊.m.ɪ.n.ɪ.s.t.ɚ').shape\n",
    "\n",
    "len(ds2t('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉'))\n",
    "len(dsTo3factors('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉'))\n",
    "prefixTox0kCM('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉').shape\n",
    "\n",
    "len(ds2t('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉.⋉'))\n",
    "len(dsTo3factors('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉.⋉'))\n",
    "prefixTox0kCM('⋊.m.ɪ.n.ɪ.s.t.ɚ.⋉.⋉').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.308834Z",
     "start_time": "2019-08-07T18:30:05.676Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefix\n",
    "len(ds2t(random_source_prefix)) - 2\n",
    "prefixTox0kCM(random_source_prefix).shape\n",
    "prefixTox0kCM(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.309647Z",
     "start_time": "2019-08-07T18:30:05.688Z"
    }
   },
   "outputs": [],
   "source": [
    "# for convenience\n",
    "def pXhat0fX0i_pxtn_conv(w, r, c, m = 50):\n",
    "    w_idx = Ws_t.index(w)\n",
    "\n",
    "#     r_idx = Ps_t.index(r)\n",
    "    source_wf = list(wordsWithPrefix(r, Ws))[0]\n",
    "    source_wf_idx = Ws_t.index(source_wf)\n",
    "    \n",
    "    total_x0k_length = len(ds2t(r))# includes left edge symbol + upcoming segment\n",
    "#     prefix_length_noWE = total_x0k_length - 1 #includes upcoming segment\n",
    "#     prefix_length_produced_so_far = prefix_length_noWE - 1\n",
    "#     offset = 1\n",
    "#     x0k_cm = CMsByLengthByWordformIndex_torch[prefix_length_produced_so_far - offset][source_wf_idx]\n",
    "    x0k_cm = CMsByLengthByWordformIndex_torch[total_x0k_length][source_wf_idx]\n",
    "    \n",
    "    c_idx = Cs_t.index(c)\n",
    "    \n",
    "    return pXhat0fX0i_pxtn(w_idx, x0k_cm, c_idx, m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.310488Z",
     "start_time": "2019-08-07T18:30:05.702Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefix\n",
    "len(ds2t(random_source_prefix)) - 2\n",
    "CMsByLengthByWordformIndex_torch[len(ds2t(random_source_prefix))][Ws_t.index(list(wordsWithPrefix(random_source_prefix, Ws))[0])].shape\n",
    "CMsByLengthByWordformIndex_torch[len(ds2t(random_source_prefix))][Ws_t.index(list(wordsWithPrefix(random_source_prefix, Ws))[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.311213Z",
     "start_time": "2019-08-07T18:30:05.719Z"
    }
   },
   "outputs": [],
   "source": [
    "(random_source_wordform, random_source_prefix, random_context)\n",
    "pXhat0fX0i_pxtn_conv(random_source_wordform, random_source_prefix, random_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.311951Z",
     "start_time": "2019-08-07T18:30:05.738Z"
    }
   },
   "outputs": [],
   "source": [
    "# random_context = choice(Cs_t); random_context\n",
    "random_context = 'a couple of'\n",
    "random_context_idx = Cs_t.index(random_context); random_context_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.312626Z",
     "start_time": "2019-08-07T18:30:05.749Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform, random_wordform, random_context\n",
    "pXhat0fX0i_pxt(random_wordform_idx, \n",
    "               random_wordform_CM,\n",
    "               random_context_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.313288Z",
     "start_time": "2019-08-07T18:30:05.759Z"
    }
   },
   "outputs": [],
   "source": [
    "random_wordform, random_wordform, random_context\n",
    "pXhat0fX0i_pxt(random_wordform_idx, \n",
    "               random_wordform_CM,\n",
    "               random_context_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.313969Z",
     "start_time": "2019-08-07T18:30:05.777Z"
    }
   },
   "outputs": [],
   "source": [
    "Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')\n",
    "\n",
    "'⋊.k.ɑ.n.t'\n",
    "len( ds2t('⋊.k.ɑ.n.t') )\n",
    "len( ds2t('⋊.k.ɑ.n.t') ) - 2\n",
    "# retrieve_CM_for('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', 5)\n",
    "\n",
    "# np.allclose( retrieve_CM_for('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', 5), \n",
    "#              CMsByLengthByWordformIndex[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.314617Z",
     "start_time": "2019-08-07T18:30:05.793Z"
    }
   },
   "outputs": [],
   "source": [
    "pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'),\n",
    "               CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')],\n",
    "               Cs_t.index('a couple of'),\n",
    "               m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.315295Z",
     "start_time": "2019-08-07T18:30:05.804Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    f\"m = 10    → {sum([phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 10) for each in range(10)])/10}\"\n",
    "    f\"m = 50    → {sum([phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 50) for each in range(10)])/10}\"\n",
    "    f\"m = 100   → {sum([phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 100) for each in range(10)])/10}\"\n",
    "    f\"m = 250   → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 250)}\"\n",
    "    f\"m = 500   → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 500)}\"\n",
    "    f\"m = 1000  → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 1000)}\"\n",
    "    f\"m = 2500  → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 2500)}\"\n",
    "    f\"m = 5000  → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 5000)}\"\n",
    "    f\"m = 10000 → {phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉', '⋊.k.ɑ.n.t', random_context, 10000)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.316071Z",
     "start_time": "2019-08-07T18:30:05.814Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    f\"m = 50   → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 50)}\"\n",
    "    f\"m = 100  → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 100)}\"\n",
    "    f\"m = 200  → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 200)}\"\n",
    "    f\"m = 500  → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 500)}\"\n",
    "    f\"m = 1000 → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 1000)}\"\n",
    "    f\"m = 2000 → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 2000)}\"\n",
    "    f\"m = 5000 → {pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], random_context_idx, m = 5000)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.316688Z",
     "start_time": "2019-08-07T18:30:05.824Z"
    }
   },
   "outputs": [],
   "source": [
    "# formerly mean 75.9ms / quine\n",
    "# formerly mean 39.7ms / quine\n",
    "# mean 27.6 ms / quine\n",
    "# mean 3.73 ms / kotoba\n",
    "if benchmark:\n",
    "    %timeit pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], Cs_t.index('a couple of'),m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.317392Z",
     "start_time": "2019-08-07T18:30:05.835Z"
    }
   },
   "outputs": [],
   "source": [
    "# formerly mean 80ms on quine\n",
    "# formerly mean 41.8ms on quine\n",
    "# mean 27.3ms / quine\n",
    "# mean 3.35ms / kotoba\n",
    "if benchmark:\n",
    "    %timeit pXhat0fX0i_pxtn(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], Cs_t.index('a couple of'), m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.318039Z",
     "start_time": "2019-08-07T18:30:05.848Z"
    }
   },
   "outputs": [],
   "source": [
    "#formerly mean 221ms / quine\n",
    "# formerly mean 122ms / quine\n",
    "# formerly mean 69.4ms\n",
    "# mean 83ms / quine\n",
    "# mean 7.27ms / kotoba\n",
    "if benchmark:\n",
    "    %timeit pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], Cs_t.index('a couple of'), m = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportionally, what parts of the calculation take the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.318670Z",
     "start_time": "2019-08-07T18:30:05.861Z"
    }
   },
   "outputs": [],
   "source": [
    "Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')\n",
    "Ps_t.index('⋊.k.ɑ.n.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.319309Z",
     "start_time": "2019-08-07T18:30:05.873Z"
    }
   },
   "outputs": [],
   "source": [
    "contact_idx = Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')\n",
    "cont_idx = Ps_t.index('⋊.k.ɑ.n.t')\n",
    "cont_l = len(ds2t('⋊.k.ɑ.n.t')) # 5\n",
    "cont_triph_l = cont_l - 2 # 3\n",
    "cont_CM = CMsByLengthByWordformIndex_torch[cont_l][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')]\n",
    "assert cont_triph_l == cont_CM.shape[1] # 3\n",
    "acoupleof_idx = Cs_t.index('a couple of')\n",
    "acouple_of_pW = get_pW_t(acoupleof_idx)\n",
    "cont_Q_l = CMsByLengthByWordformIndex_torch[cont_triph_l+2]#.float() #:: (|W|, |Y1|, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.319948Z",
     "start_time": "2019-08-07T18:30:05.883Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit -r 10 -n 10 depthSampler2a_t(cont_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.320584Z",
     "start_time": "2019-08-07T18:30:05.893Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit -r 10 -n 10 depthSamplerC_t(cont_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.321206Z",
     "start_time": "2019-08-07T18:30:05.903Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_Y_prime = depthSamplerC_t(cont_CM, m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.321851Z",
     "start_time": "2019-08-07T18:30:05.914Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "\n",
    "#denominator profiling\n",
    "\n",
    "cont_Q_l = CMsByLengthByWordformIndex_torch[cont_triph_l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "cont_V_prime = torch.einsum('mli,kil->mkl', cont_Y_prime.type(my_tt), cont_Q_l)  # :: (m,n,l)\n",
    "cont_M_prime = torch.prod(cont_V_prime, 2) # :: (m,n)\n",
    "cont_N_prime = torch.matmul(cont_M_prime, acouple_of_pW) # :: (m, )\n",
    "# cont_N_prime = torch.einsum('mn,n->m', cont_M_prime, acouple_of_pW) #pretty much the same as two cells below\n",
    "cont_Z_prime = torch.ones(cont_N_prime.shape, dtype=my_tt)\n",
    "torch.div(cont_Z_prime, cont_N_prime, out=cont_Z_prime)\n",
    "# cont_Z_prime = 1.0 / cont_N_prime # :: (m, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.322475Z",
     "start_time": "2019-08-07T18:30:05.924Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit -r 10 -n 10\n",
    "\n",
    "# cont_Q_l = CMsByLengthByWordformIndex_torch[cont_triph_l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "# cont_V_prime = torch.einsum('mli,kil->mkl', cont_Y_prime.type(my_tt), cont_Q_l)  # :: (m,n,l)\n",
    "# cont_M_prime = torch.prod(cont_V_prime, 2) # :: (m,n)\n",
    "# cont_N_prime = torch.ones((50,))\n",
    "# torch.matmul(cont_M_prime, acouple_of_pW, out=cont_N_prime) # :: (m, ) #pretty much the same as the cell above and below\n",
    "# # cont_N_prime = torch.matmul(cont_M_prime, acouple_of_pW) # :: (m, )\n",
    "# # cont_N_prime = torch.einsum('mn,n->m', cont_M_prime, acouple_of_pW) \n",
    "# cont_Z_prime = torch.ones(cont_N_prime.shape, dtype=my_tt)\n",
    "# torch.div(cont_Z_prime, cont_N_prime, out=cont_Z_prime)\n",
    "# # cont_Z_prime = 1.0 / cont_N_prime # :: (m, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.323112Z",
     "start_time": "2019-08-07T18:30:05.937Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "\n",
    "#denominator profiling - including time for lexicon prior lookup\n",
    "\n",
    "cont_Q_l = CMsByLengthByWordformIndex_torch[cont_triph_l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "cont_V_prime = torch.einsum('mli,kil->mkl', cont_Y_prime.type(my_tt), cont_Q_l)  # :: (m,n,l)\n",
    "cont_M_prime = torch.prod(cont_V_prime, 2) # :: (m,n)\n",
    "cont_N_prime = torch.matmul(cont_M_prime, get_pW_t(acoupleof_idx)) # <<<< includes time for lexicon prior lookup\n",
    "cont_Z_prime = torch.ones(cont_N_prime.shape, dtype=my_tt)\n",
    "torch.div(cont_Z_prime, cont_N_prime, out=cont_Z_prime)\n",
    "# cont_Z_prime = 1.0 / cont_N_prime # :: (m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.323751Z",
     "start_time": "2019-08-07T18:30:05.949Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "\n",
    "#numerator profiling\n",
    "\n",
    "contact_L_w = cont_Q_l[contact_idx]#.float()\n",
    "contact_cont_V_prime_w = torch.einsum('mij,ji->mi',cont_Y_prime.type(my_tt), contact_L_w)\n",
    "contact_cont_O_w = torch.prod(contact_cont_V_prime_w, 1) # :: (m,1) likelihoods of each of the m sampled channel prefixes\n",
    " # U_w = pX0f_torch[xhat0f_idx] * O_w ## :: (m,1) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "contact_cont_U_w = acouple_of_pW[contact_idx] * contact_cont_O_w ## :: (m,1) joint probabilities of xhat0f with each of the m sampled channel prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Old* Takeaway**: Sampling and the normalization term (denominator) each take up about half the total time.\n",
    "\n",
    "***Current* takeaway**: the normalization term takes up nearly all of the calculation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization over samples and contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.324447Z",
     "start_time": "2019-08-07T18:30:05.966Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def memAvailable(units='GB'):\n",
    "    if units == 'GB':\n",
    "        scale = 1e9\n",
    "    elif units == 'MB':\n",
    "        scale = 1e6\n",
    "    return psutil.virtual_memory().available / scale\n",
    "\n",
    "def memUsed(units='GB'):\n",
    "    if units == 'GB':\n",
    "        scale = 1e9\n",
    "    elif units == 'MB':\n",
    "        scale = 1e6\n",
    "    return psutil.virtual_memory().used / scale\n",
    "\n",
    "if g:\n",
    "    total_mem_MB = torch.cuda.get_device_properties(0).total_memory / 1e6\n",
    "\n",
    "def gpuMem():\n",
    "#     if g and torch.cuda.is_available():\n",
    "    info = {'total':total_mem_MB,\n",
    "            'allocated':torch.cuda.memory_allocated(0) / (10^6),\n",
    "            'cached':torch.cuda.memory_cached(0) / (10^6)}\n",
    "    return info\n",
    "\n",
    "    print('Total Memory: {0}'.format(total_mem_MB) )\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "        \n",
    "def stampedMemNote(msg='', units='GB', includeGPU=False):\n",
    "    mem_usage = 'VM used vs. available: {0:.2f}{2} vs. {1:.2f}{2}'.format(memUsed(units), memAvailable(units), units)\n",
    "    if g and includeGPU:\n",
    "        g_info = gpuMem()\n",
    "        total, alloc, cached = g_info['total'], g_info['allocated'], g_info['cached']\n",
    "        gpu_usage = 'GPU mem allocated, cached, total: {0:.2f}MB vs. {1:.2f}MB vs. {1:.2f}MB'.format(alloc, cached, total)\n",
    "    if msg == '':\n",
    "        stampedNote(mem_usage)\n",
    "        if g and includeGPU:\n",
    "            print(gpu_usage)\n",
    "        \n",
    "    stampedNote(msg)\n",
    "    print('\\t'+mem_usage)\n",
    "    if g and includeGPU:\n",
    "        print('\\t'+gpu_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.325137Z",
     "start_time": "2019-08-07T18:30:05.977Z"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from https://stackoverflow.com/a/6909532\n",
    "import math\n",
    "def factors(n):\n",
    "    step = 2 if n%2 else 1\n",
    "    fs =    set(reduce(list.__add__,\n",
    "                ([i, n//i] for i in range(1, int(math.sqrt(n))+1, step) if n % i == 0)))\n",
    "    return np.array(sorted(list(fs)))\n",
    "\n",
    "def closest_factor(target, factors):\n",
    "#     fs = np.array(sorted(list(factors)))\n",
    "    fs = factors\n",
    "    distances = np.abs( target - fs )\n",
    "    min_dist = np.min(distances)\n",
    "    min_dist_idx = np.where( distances == min_dist )[0][0]\n",
    "    return int(fs[min_dist_idx])\n",
    "\n",
    "def pXhat0fX0i_pxtc(xhat0f_idx, x0k_CM, m = 50, target_num_batches=30):\n",
    "    '''\n",
    "    This function is exactly like \n",
    "        pXhat0fX0i_pxt\n",
    "    except it uses einsum (and batched computation) to calculate \n",
    "        p(xhat0f|x0k, c) \n",
    "    for *all* contexts c, in target_num_batches batches of contexts\n",
    "    at a time.\n",
    "    \n",
    "    \n",
    "    If ⋊ x_0 x_1 ... x_i x_k denotes\n",
    "    the produced prefix r of the speaker's intended wordform w,\n",
    "     where\n",
    "       x_i = the last fully produced segment\n",
    "       k = i+1\n",
    "       x_k = the next segment\n",
    "       |⋊ x_0 x_1 ... x_i x_k| = l + 2\n",
    "       l = i+1 = the length of the produced prefix (not incl. \n",
    "                 the left word edge or the next segment)\n",
    "\n",
    "    then\n",
    "\n",
    "    x0k_CM :: (|Y1|, l) is the associated l-length stack of size |Y1| vectors\n",
    "     where x0k_CM[:,j] is the distribution p(Y_j|X0^k has been produced)\n",
    "\n",
    "    xhat0f_idx is the index of a wordform w' s.t. \n",
    "\n",
    "    pXhat0fX0i_pxt(xhat0f_idx, x0k_CM) ≈ [p(w'|r)]_c for each context c\n",
    "    '''\n",
    "    stampedNote('> Begin calculation.')\n",
    "    l = x0k_CM.shape[1]\n",
    "    \n",
    "    m_C = len(Cs_t) * m\n",
    "    stampedMemNote('Sampling...')\n",
    "#     stampedNote('Sampling...')\n",
    "    # :: (c*m, l, s)\n",
    "#     Y_prime_C = depthSamplerC_t(x0k_CM, m=m_C)\n",
    "    Y_prime_C = depthSamplerD_t(x0k_CM, m=m_C, numBatches = 10)\n",
    "    \n",
    "    # :: (c, m, l, s)\n",
    "    stampedMemNote('Splitting by context...')\n",
    "#     stampedNote('Splitting by context...')\n",
    "    Y_prime_C_split = torch.stack(torch.chunk(Y_prime_C, len(Cs_t), 0))\n",
    "#     stampedMemNote('Contexts split.')\n",
    "#     del Y_prime_C\n",
    "#     stampedMemNote('Deleted Y_prime_C.')\n",
    "    Q_l = CMsByLengthByWordformIndex_torch[l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "    \n",
    "    \n",
    "    num_contexts = len(Cs_t); num_contexts # c\n",
    "    target_num_batches = target_num_batches\n",
    "    num_batches = closest_factor(target_num_batches, factors(num_contexts))\n",
    "    print(f'Target num batches = {target_num_batches}')\n",
    "    print(f'Actual num batches = {num_batches}')\n",
    "    stampedNote('Batching contexts...')\n",
    "    # (b, c/b, m, l, s)\n",
    "    Y_prime_C_split_batched = torch.stack(torch.chunk(Y_prime_C_split, \n",
    "                                                      num_batches,\n",
    "                                                      0))\n",
    "#     stampedMemNote('Contexts batched.')\n",
    "    \n",
    "    num_batches = Y_prime_C_split_batched.shape[0] # b\n",
    "    num_contexts_per_batch = Y_prime_C_split_batched.shape[1] # c/b\n",
    "    print(f'Contexts per batch = {num_contexts_per_batch}')\n",
    "    \n",
    "    stampedMemNote('Calculating denominator batches...')\n",
    "    Z_prime_C = torch.ones((num_contexts, m), dtype=my_tt)\n",
    "#     stampedMemNote('Z_prime_C created.')\n",
    "#     denominator_batches = []\n",
    "    for b in tqdm(range(num_batches), total=num_batches):\n",
    "        stampedNote('\\t>> Begin batch.')\n",
    "        stampedMemNote('\\t>> V_prime_C_b...')\n",
    "        # :: (c/b, m, n, l)\n",
    "        V_prime_C_batch = torch.einsum('cmli,kil->cmkl',\n",
    "                                       Y_prime_C_split_batched.type(my_tt)[b],\n",
    "                                       Q_l)\n",
    "#         stampedMemNote('V_prime_C_b created.')\n",
    "        \n",
    "        stampedMemNote('\\t>> M_prime_C_b...')\n",
    "        # :: (c/b, m, n)\n",
    "        M_prime_C_batch = torch.prod(V_prime_C_batch, 3)\n",
    "#         stampedMemNote('M_prime_C_b created.')\n",
    "        del V_prime_C_batch\n",
    "#         stampedMemNote('V_prime_C_b deleted.')\n",
    "        \n",
    "        stampedMemNote('\\t>> Creating pW_context_b...')\n",
    "        context_indices = torch.arange(num_contexts_per_batch * b, num_contexts_per_batch * b + num_contexts_per_batch)\n",
    "        # :: (n, c/b)\n",
    "        pW_context_batch = pW_C_torch[:,context_indices]\n",
    "#         del context_indices\n",
    "#         stampedMemNote('pW_context_b created.')\n",
    "        \n",
    "        stampedMemNote('\\t>> N_prime_C_b...')\n",
    "        # :: (c/b, m)\n",
    "        N_prime_C_batch = torch.einsum('cmn,nc->cm', M_prime_C_batch, pW_context_batch) # :: (c/b,m)\n",
    "#         stampedMemNote('N_prime_C_b created.')\n",
    "        del M_prime_C_batch\n",
    "#         del pW_context_batch\n",
    "#         stampedMemNote('M_prime_C_b and pW_context_b deleted.')\n",
    "#         stampedMemNote('M_prime_C_b deleted.')\n",
    "        stampedMemNote('\\t>> Z_prime_C_b...')\n",
    "#         Z_prime_C_batch_old = 1.0 / N_prime_C_batch # :: (c/b,m)\n",
    "#         Z_prime_C_batch_local = torch.ones(N_prime_C_batch.shape, dtype=my_tt)\n",
    "        my_Z_prime_C_batch_region = Z_prime_C[context_indices]\n",
    "        torch.div(input=my_Z_prime_C_batch_region,\n",
    "                  other=N_prime_C_batch,\n",
    "                  out=my_Z_prime_C_batch_region)\n",
    "        Z_prime_C[context_indices] = my_Z_prime_C_batch_region\n",
    "#         torch.div(input=Z_prime_C_batch_local,\n",
    "#                   other=N_prime_C_batch,\n",
    "#                   out=Z_prime_C_batch_local)\n",
    "#         if (not torch.equal(Z_prime_C_batch_local, my_Z_prime_C_batch_region)) or (not torch.equal(my_Z_prime_C_batch_region, Z_prime_C[context_indices])):\n",
    "#             print(f'Z_prime_C_batch_local.shape = {Z_prime_C_batch_local.shape}')\n",
    "#             print(f'Z_prime_C[context_indices].shape = {my_Z_prime_C_batch_region.shape}')\n",
    "#             print( Z_prime_C_batch_local == Z_prime_C[context_indices])\n",
    "#             print( Z_prime_C_batch_local == my_Z_prime_C_batch_region )\n",
    "#             return Z_prime_C_batch_local, Z_prime_C[context_indices], my_Z_prime_C_batch_region\n",
    "#         assert torch.equal(Z_prime_C_batch_local, Z_prime_C[context_indices])\n",
    "#         del context_indices\n",
    "#         del M_prime_C_batch\n",
    "#         del context_indices\n",
    "#         del pW_context_batch\n",
    "#         del N_prime_C_batch\n",
    "#         stampedMemNote('context_indices and N_prime_C_batch deleted.')\n",
    "        stampedNote('\\t>> End batch.')\n",
    "#         stampedNote('Appending Z_prime_C_b...')\n",
    "#         denominator_batches.append(Z_prime_C_batch)\n",
    "\n",
    "#     stampedNote('Consolidating Z_prime_C')\n",
    "#     Z_prime_C = torch.cat(denominator_batches)\n",
    "#     Z_prime_C.shape # (c,m)\n",
    "#     del denominator_batches\n",
    "    del Y_prime_C_split_batched\n",
    "#     stampedMemNote('Y_prime_C_split_batched deleted.')\n",
    "    \n",
    "    stampedMemNote('Calculating numerator...')\n",
    "    stampedNote('Calculating V_prime_w_C...')\n",
    "    xhat0f_L_w = Q_l[xhat0f_idx]#.float() # :: (s, l)\n",
    "    V_prime_w_C = torch.einsum('cmij,ji->cmi',Y_prime_C_split.type(my_tt), xhat0f_L_w) # :: (c,m,l)\n",
    "#     stampedMemNote('V_prime_w_C created.')\n",
    "#     del xhat0f_L_w\n",
    "#     del Y_prime_C_split\n",
    "#     stampedMemNote('xhat0f_L_w and Y_prime_C_split deleted.')\n",
    "    stampedNote('Calculating O_w_C...')\n",
    "    O_w_C = torch.prod(V_prime_w_C, 2) # :: (c,m) likelihoods of each of the m sampled channel prefixes for each of c contexts\n",
    "#     stampedMemNote('O_w_C created.')\n",
    "#     del V_prime_w_C\n",
    "#     stampedMemNote('V_prime_w_C deleted.')\n",
    "    stampedNote('Calculating U_w_C...')\n",
    "    # contact_cont_U_w_ALL = pW_C_torch[contact_idx] * contact_cont_O_w_ALL ## :: (c,m) joint probabilities of xhat0f with each of the m sampled channel prefixes for each of the c contexts\n",
    "    U_w_C = torch.einsum('c,cm->cm', pW_C_torch[xhat0f_idx], O_w_C) ## :: (c,m) joint probabilities of xhat0f with each of the m sampled channel prefixes for each of the c contexts\n",
    "#     stampedMemNote('U_w_C created.')\n",
    "#     del O_w_C\n",
    "#     stampedMemNote('O_w_C deleted.')\n",
    "    \n",
    "    stampedMemNote('Calculating E_w_C...')\n",
    "#     return torch.dot(Z_prime_C, U_w_C) / m\n",
    "    E_w_C = torch.einsum('cm,cm->c', Z_prime_C, U_w_C) / m\n",
    "#     stampedMemNote('E_w_C created.')\n",
    "#     del Z_prime_C\n",
    "#     del U_w_C\n",
    "#     stampedMemNote('Z_prime_C and U_w_C deleted.')\n",
    "    stampedNote('> End calculation.')\n",
    "    return E_w_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.325760Z",
     "start_time": "2019-08-07T18:30:05.989Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.326383Z",
     "start_time": "2019-08-07T18:30:06.003Z"
    }
   },
   "outputs": [],
   "source": [
    "factors(len(Cs_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.327024Z",
     "start_time": "2019-08-07T18:30:06.016Z"
    }
   },
   "outputs": [],
   "source": [
    "closest_factor(30, factors(len(Cs_t)))\n",
    "# np.min(closest_factor(10, factors(len(Cs_t))))\n",
    "# np.where( closest_factor(10, factors(len(Cs_t))) == np.min(closest_factor(10, factors(len(Cs_t)))) )\n",
    "# np.where( closest_factor(10, factors(len(Cs_t))) == np.min(closest_factor(10, factors(len(Cs_t)))) )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.327616Z",
     "start_time": "2019-08-07T18:30:06.034Z"
    }
   },
   "outputs": [],
   "source": [
    "my_range = torch.arange(10); my_range\n",
    "my_batches = torch.split(my_range, 3, 0); my_batches\n",
    "my_starts = torch.tensor(list(map(lambda b: b[0], my_batches)))\n",
    "my_ends = torch.tensor(list(map(lambda b: b[-1]+1, my_batches)))\n",
    "my_starts\n",
    "my_ends\n",
    "torch.stack([my_starts, my_ends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.328316Z",
     "start_time": "2019-08-07T18:30:06.044Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dimension(size_or_index_range, target_batch_size, startEndRange=False):\n",
    "    '''\n",
    "    This is a helper function for use with torch.split. Given the total size \n",
    "    (or a range of indices) of one dimension of a tensor and a target number\n",
    "    of indices per batch size, this returns a structure describing the indices\n",
    "    in each batch.\n",
    "    \n",
    "    If size_or_index_range is an int it represents the size of a dimension\n",
    "    to split into batches, where each batch (except possibly the last) is of \n",
    "    size target_batch_size.\n",
    "    \n",
    "    If size_or_index_range is a 1D tensor of indices, it represents a tensor\n",
    "    of indices to split into batches, wher each batch (except possibly the last)\n",
    "    is of size target_batch_size.\n",
    "    \n",
    "    If startEndRange is True, this returns a shape (b,2) tensor where b is the\n",
    "    number of batches and each column contains the start index and end index+1\n",
    "    of a batch. Note that size_or_index_range must be interpretable as a\n",
    "    contiguous range of indices.\n",
    "    \n",
    "    If startEndRange is False, this returns a tuple of b 1D tensors, where b is\n",
    "    the number of batches and each tensor contains the indices of a batch.\n",
    "    \n",
    "    TODO: generalize for use with whatever numpy's analogue of torch.split is...\n",
    "    '''\n",
    "    if type(size_or_index_range) == int:\n",
    "        index_range = torch.arange(size_or_index_range)\n",
    "    else:\n",
    "        index_range = size_or_index_range\n",
    "    \n",
    "    index_batches = torch.split(index_range, target_batch_size, 0)\n",
    "    \n",
    "    if not startEndRange:\n",
    "        return index_batches\n",
    "    \n",
    "    starts = torch.tensor(list(map(lambda b: b[0], index_batches)))\n",
    "    ends = torch.tensor(list(map(lambda b: b[-1]+1, index_batches)))\n",
    "    startEndRange = torch.stack([starts, ends])\n",
    "    return startEndRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.329081Z",
     "start_time": "2019-08-07T18:30:06.056Z"
    }
   },
   "outputs": [],
   "source": [
    "def denominator_batch_process_g(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices):\n",
    "# def denominator_batch_process_g(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices, V_prime_ce, N_prime_ce):\n",
    "    if benchmark:\n",
    "        profilingStamps = True\n",
    "    else:\n",
    "        profilingStamps = False\n",
    "        \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\t>> Begin batch.', includeGPU=True)\n",
    "    \n",
    "#     Y_prime_ALL_batch = Y_prime_ALL_split_batched_batch.type(my_tt) # :: (c/b, m, l, s)\n",
    "    Y_prime_ALL_batch = Y_prime_ALL_split_batched_batch # :: (c/b, m, l, s)\n",
    " \n",
    "#     print(f'shape Y_prime_ALL_batch = {Y_prime_ALL_batch.shape}')\n",
    "#     print(f'shape Q_l = {Q_l.shape}')\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('\\tV_prime...', includeGPU=True)\n",
    "    # ::(c/b, m, n, l)\n",
    "#     V_prime_ALL_batch = V_prime_ce(Y_prime_ALL_batch, Q_l, backend='torch')\n",
    "    V_prime_ALL_batch = torch.einsum('cmli,kil->cmkl',\n",
    "                                     Y_prime_ALL_batch,\n",
    "                                     Q_l)\n",
    "    del Y_prime_ALL_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tM_prime...', includeGPU=True)\n",
    "    # :: (c/b, m, n)\n",
    "    M_prime_ALL_batch = torch.prod(V_prime_ALL_batch, 3)\n",
    "    del V_prime_ALL_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tpW_C...', includeGPU=True)\n",
    "    # :: (n, c/b)\n",
    "    pW_context_batch = pW_C_torch[:, batch_context_indices].cuda()\n",
    "    del batch_context_indices\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('\\tN_prime...', includeGPU=True)\n",
    "    # :: (c/b, m)\n",
    "#     N_prime_ALL_batch = N_prime_ce(M_prime_ALL_batch, pW_context_batch, backend='torch') # :: (c/b,m)\n",
    "    N_prime_ALL_batch = torch.einsum('cmn,nc->cm', M_prime_ALL_batch, pW_context_batch) # :: (c/b,m)\n",
    "    del M_prime_ALL_batch\n",
    "    del pW_context_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tZ_prime...', includeGPU=True)\n",
    "#     Z_prime_ALL_batch = 1.0 / N_prime_ALL_batch # :: (c/b,m)\n",
    "    Z_prime_ALL_batch = torch.ones(N_prime_ALL_batch.shape, dtype=my_tt, device=gpu)\n",
    "    torch.div(input=Z_prime_ALL_batch,\n",
    "              other=N_prime_ALL_batch,\n",
    "              out=Z_prime_ALL_batch)\n",
    "#     del M_prime_ALL_batch\n",
    "#     del batch_context_indices\n",
    "#     del pW_context_batch\n",
    "    del N_prime_ALL_batch\n",
    "    if profilingStamps:\n",
    "        stampedNote('\\t>> End batch.', includeGPU=True)\n",
    "    result = Z_prime_ALL_batch.cpu()\n",
    "    del Z_prime_ALL_batch\n",
    "    torch.cuda.empty_cache()\n",
    "    return result\n",
    "\n",
    "# def denominator_batch_process_oe(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices, V_prime_ce, N_prime_ce):\n",
    "#     profilingStamps = False\n",
    "#     if profilingStamps:\n",
    "#         stampedMemNote('\\t>> Begin batch.')\n",
    "    \n",
    "#     Y_prime_ALL_batch = Y_prime_ALL_split_batched_batch.type(my_tt) # :: (c/b, m, l, s)\n",
    " \n",
    "# #     print(f'shape Y_prime_ALL_batch = {Y_prime_ALL_batch.shape}')\n",
    "# #     print(f'shape Q_l = {Q_l.shape}')\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedNote('\\tV_prime...')\n",
    "#     # ::(c/b, m, n, l)\n",
    "#     V_prime_ALL_batch = V_prime_ce(Y_prime_ALL_batch, Q_l, backend='torch')\n",
    "# #     V_prime_ALL_batch = torch.einsum('cmli,kil->cmkl',\n",
    "# #                                      Y_prime_ALL_batch,\n",
    "# #                                      Q_l)\n",
    "# #     del Y_prime_ALL_batch\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedMemNote('\\tM_prime...')\n",
    "#     # :: (c/b, m, n)\n",
    "#     M_prime_ALL_batch = torch.prod(V_prime_ALL_batch, 3)\n",
    "#     del V_prime_ALL_batch\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedMemNote('\\tpW_C...')\n",
    "#     # :: (n, c/b)\n",
    "#     pW_context_batch = pW_C_torch[:, batch_context_indices]\n",
    "# #     del batch_context_indices\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedNote('\\tN_prime...')\n",
    "#     # :: (c/b, m)\n",
    "#     N_prime_ALL_batch = N_prime_ce(M_prime_ALL_batch, pW_context_batch, backend='torch') # :: (c/b,m)\n",
    "# #     N_prime_ALL_batch = torch.einsum('cmn,nc->cm', M_prime_ALL_batch, pW_context_batch) # :: (c/b,m)\n",
    "#     del M_prime_ALL_batch\n",
    "# #     del pW_context_batch\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedMemNote('\\tZ_prime...')\n",
    "# #     Z_prime_ALL_batch = 1.0 / N_prime_ALL_batch # :: (c/b,m)\n",
    "#     Z_prime_ALL_batch = torch.ones(N_prime_ALL_batch.shape, dtype=my_tt)\n",
    "#     torch.div(input=Z_prime_ALL_batch,\n",
    "#               other=N_prime_ALL_batch,\n",
    "#               out=Z_prime_ALL_batch)\n",
    "# #     del M_prime_ALL_batch\n",
    "# #     del batch_context_indices\n",
    "# #     del pW_context_batch\n",
    "# #     del N_prime_ALL_batch\n",
    "#     if profilingStamps:\n",
    "#         stampedNote('\\t>> End batch.')\n",
    "#     return Z_prime_ALL_batch\n",
    "\n",
    "def denominator_batch_process(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices):\n",
    "    profilingStamps = False\n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\t>> Begin batch.')\n",
    "    \n",
    "    Y_prime_ALL_batch = Y_prime_ALL_split_batched_batch.type(my_tt) # :: (c/b, m, l, s)\n",
    " \n",
    "#     print(f'shape Y_prime_ALL_batch = {Y_prime_ALL_batch.shape}')\n",
    "#     print(f'shape Q_l = {Q_l.shape}')\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('\\tV_prime...')\n",
    "    # ::(c/b, m, n, l)\n",
    "    V_prime_ALL_batch = torch.einsum('cmli,kil->cmkl',\n",
    "                                     Y_prime_ALL_batch,\n",
    "                                     Q_l)\n",
    "#     del Y_prime_ALL_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tM_prime...')\n",
    "    # :: (c/b, m, n)\n",
    "    M_prime_ALL_batch = torch.prod(V_prime_ALL_batch, 3)\n",
    "    del V_prime_ALL_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tpW_C...')\n",
    "    # :: (n, c/b)\n",
    "    pW_context_batch = pW_C_torch[:, batch_context_indices]\n",
    "#     del batch_context_indices\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('\\tN_prime...')\n",
    "    # :: (c/b, m)\n",
    "    N_prime_ALL_batch = torch.einsum('cmn,nc->cm', M_prime_ALL_batch, pW_context_batch) # :: (c/b,m)\n",
    "    del M_prime_ALL_batch\n",
    "#     del pW_context_batch\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('\\tZ_prime...')\n",
    "#     Z_prime_ALL_batch = 1.0 / N_prime_ALL_batch # :: (c/b,m)\n",
    "    Z_prime_ALL_batch = torch.ones(N_prime_ALL_batch.shape, dtype=my_tt)\n",
    "    torch.div(input=Z_prime_ALL_batch,\n",
    "              other=N_prime_ALL_batch,\n",
    "              out=Z_prime_ALL_batch)\n",
    "#     del M_prime_ALL_batch\n",
    "#     del batch_context_indices\n",
    "#     del pW_context_batch\n",
    "#     del N_prime_ALL_batch\n",
    "    if profilingStamps:\n",
    "        stampedNote('\\t>> End batch.')\n",
    "    return Z_prime_ALL_batch\n",
    "\n",
    "def denominator_calc(CM, m, Q_l, target_contexts_per_batch=500, parallel=False):\n",
    "    profilingStamps = True\n",
    "    if profilingStamps:\n",
    "        stampedMemNote('> Begin calculation.')\n",
    "    num_contexts = len(Cs_t) #c\n",
    "#     num_batches = len(Y_prime_ALL_split_batched) #b\n",
    "#     num_contexts_per_batch = torch.tensor(tuple(map(len, Y_prime_ALL_split_batched_indices))) #c/b\n",
    "    l = CM.shape[1]\n",
    "#     print(f'l = {l}')\n",
    "    \n",
    "    num_samples = m * num_contexts\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('Generating samples...')\n",
    "    #collect enough sampled channel sequences for all c contexts \n",
    "    Y_prime_ALL = depthSamplerD_t(CM, m=num_samples, numBatches = 10)\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('Splitting samples by context...')\n",
    "    #split the samples into stacks of m samples per context\n",
    "    Y_prime_ALL_split = torch.stack(torch.chunk(Y_prime_ALL, num_contexts, 0))\n",
    "#     print(f'shape Y_prime_ALL_split = {Y_prime_ALL_split.shape}')\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('Splitting contexts into batches ...')\n",
    "    #split the contexts into batches\n",
    "#     target_contexts_per_batch = 1000\n",
    "    Y_prime_ALL_split_batched = torch.split(Y_prime_ALL_split,\n",
    "                                            target_contexts_per_batch,\n",
    "                                            0)\n",
    "    Y_prime_ALL_split_batched_indices = split_dimension(num_contexts, target_contexts_per_batch)\n",
    "    num_batches = len(Y_prime_ALL_split_batched)\n",
    "    if profilingStamps:\n",
    "        print(f'Num batches = {num_batches}')\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Processing batches...')\n",
    "    #process the batches\n",
    "    if parallel:\n",
    "        Z_prime_ALL = torch.cat(list( par(delayed(denominator_batch_process)(Y_prime_ALL_split_batched_batch, Q_l, indices)\n",
    "                                      for Y_prime_ALL_split_batched_batch, indices in zip(Y_prime_ALL_split_batched, \n",
    "                                                                                          Y_prime_ALL_split_batched_indices)) ))\n",
    "    else:\n",
    "        Z_prime_ALL = torch.cat([denominator_batch_process(Y_prime_ALL_split_batched_batch, Q_l, indices)\n",
    "                                 for Y_prime_ALL_split_batched_batch, indices in tqdm(zip(Y_prime_ALL_split_batched,\n",
    "                                                                                          Y_prime_ALL_split_batched_indices),\n",
    "                                                                                      total=num_batches)]) \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('> Batches processed.')\n",
    "    return Z_prime_ALL\n",
    "\n",
    "def pXhat0fX0i_ptc(xhat0f_idx, x0k_CM, m = 50, target_contexts_per_batch=600, parallel=True, use_gpu=False):\n",
    "    '''\n",
    "    This function is exactly like \n",
    "        pXhat0fX0i_pxt\n",
    "    except it uses einsum (and batched computation) to calculate \n",
    "        p(xhat0f|x0k, c) \n",
    "    for *all* contexts c, in batches of target_contexts_per_batch\n",
    "    contexts at a time (default 750).\n",
    "    \n",
    "    Currently known optimal target_contexts_per_batch settings:\n",
    "    Quine CPU/Parallel newdic -> 750\n",
    "    Solomonoff CPU/Parallel newdic -> 750\n",
    "    Kotoba CPU/Parallel newdic -> 600-650\n",
    "    Kotoba GPU newdic -> 300-375\n",
    "    \n",
    "    \n",
    "    If ⋊ x_0 x_1 ... x_i x_k denotes\n",
    "    the produced prefix r of the speaker's intended wordform w,\n",
    "     where\n",
    "       x_i = the last fully produced segment\n",
    "       k = i+1\n",
    "       x_k = the next segment\n",
    "       |⋊ x_0 x_1 ... x_i x_k| = l + 2\n",
    "       l = i+1 = the length of the produced prefix (not incl. \n",
    "                 the left word edge or the next segment)\n",
    "\n",
    "    then\n",
    "\n",
    "    x0k_CM :: (|Y1|, l) is the associated l-length stack of size |Y1| vectors\n",
    "     where x0k_CM[:,j] is the distribution p(Y_j|X0^k has been produced)\n",
    "\n",
    "    xhat0f_idx is the index of a wordform w' s.t. \n",
    "\n",
    "    pXhat0fX0i_ptc(xhat0f_idx, x0k_CM) ≈ [p(w'|r)]_c for each context c\n",
    "    '''\n",
    "    profilingStamps = True\n",
    "    if not benchmark:\n",
    "        minterval = 10 #minimum of 10 seconds between tqdm progress updates\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('> Begin calculation.')\n",
    "    l = x0k_CM.shape[1]\n",
    "    num_contexts = len(Cs_t) #c\n",
    "    \n",
    "    if profilingStamps:\n",
    "        print(f'l = {l}')\n",
    "    \n",
    "    m_C = num_contexts * m\n",
    "\n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Sampling...')\n",
    "    # :: (c*m, l, s)\n",
    "#     Y_prime_C = depthSamplerC_t(x0k_CM, m=m_C)\n",
    "    Y_prime_C = depthSamplerD_t(x0k_CM, m=m_C, numBatches = 10)\n",
    "    \n",
    "    # :: (c, m, l, s)\n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Splitting samples by context...')\n",
    "    Y_prime_C_split = torch.stack(torch.chunk(Y_prime_C, len(Cs_t), 0))\n",
    "#     stampedMemNote('Contexts split.')\n",
    "#     del Y_prime_C\n",
    "#     stampedMemNote('Deleted Y_prime_C.')\n",
    "    Q_l = CMsByLengthByWordformIndex_torch[l+2]#.float() #:: (|W|, |Y1|, l)\n",
    "    Q_l_g = Q_l.cuda()\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedNote('Splitting contexts into batches ...')\n",
    "    #split the contexts into batches\n",
    "    Y_prime_C_split_batched = torch.split(Y_prime_C_split,\n",
    "                                          target_contexts_per_batch,\n",
    "                                          0)\n",
    "    Y_prime_C_split_batched_indices = split_dimension(num_contexts, target_contexts_per_batch)\n",
    "    num_batches = len(Y_prime_C_split_batched)\n",
    "    if profilingStamps:t\n",
    "        print(f'Num batches = {num_batches}')\n",
    "    \n",
    "#     if profilingStamps:\n",
    "#         stampedMemNote('Pre-calculating contract expressions for each batch...')\n",
    "#     #calculate contract expressions for each batch\n",
    "#     main_batch_size = len(Y_prime_C_split_batched_indices[0])\n",
    "#     final_batch_size = len(Y_prime_C_split_batched_indices[-1])\n",
    "#     batch_sizes = tuple(map(len, Y_prime_C_split_batched_indices))\n",
    "#     n = pW_C_torch.shape[0]\n",
    "#     s = Y_prime_C.shape[2]\n",
    "    \n",
    "#     V_prime_index_str = 'cmli,kil->cmkl'\n",
    "#     Y_prime_ALL_batch_shape = lambda contexts_per_batch: (contexts_per_batch, m, l, s) # (c/b, m, l, s)\n",
    "#     Q_l_batch_shape = tuple(Q_l.shape) # (n, s, l) = (|W|, |Y1|, l)\n",
    "#     V_prime_ALL_shape_for_batch = lambda contexts_per_batch: (Y_prime_ALL_batch_shape(contexts_per_batch), Q_l_batch_shape)\n",
    "#     V_prime_ALL_shape_main_batch = Y_prime_ALL_batch_shape(main_batch_size), Q_l_batch_shape\n",
    "#     V_prime_ALL_shape_final_batch = Y_prime_ALL_batch_shape(final_batch_size), Q_l_batch_shape\n",
    "#     V_prime_ALL_shapes_by_batch_size = {main_batch_size:V_prime_ALL_shape_main_batch,\n",
    "#                                         final_batch_size:V_prime_ALL_shape_main_batch}\n",
    "# #     V_prime_ALL_shapes_by_batch = tuple(map(V_prime_ALL_shape_for_batch, batch_sizes))\n",
    "#     V_prime_contract_expr_by_batch_size = {main_batch_size:oe.contract_expression(V_prime_index_str, *(V_prime_ALL_shapes_by_batch_size[main_batch_size])),\n",
    "#                                            final_batch_size:oe.contract_expression(V_prime_index_str, *(V_prime_ALL_shapes_by_batch_size[final_batch_size]))}\n",
    "# #     V_prime_contract_exprs_by_batch = tuple(map(lambda shapes: oe.contract_expression(V_prime_index_str, *shapes),\n",
    "# #                                                 V_prime_ALL_shapes_by_batch))    \n",
    "#     N_prime_index_str = 'cmn,nc->cm'\n",
    "#     M_prime_ALL_batch_shape = lambda contexts_per_batch: (contexts_per_batch, m, n) # (c/b, m, n)\n",
    "#     pW_context_batch_shape = lambda contexts_per_batch: (n, contexts_per_batch) # (n, c/b)\n",
    "#     N_prime_shape_for_batch = lambda contexts_per_batch: (M_prime_ALL_batch_shape(contexts_per_batch), pW_context_batch_shape(contexts_per_batch))\n",
    "#     N_prime_shapes_by_batch_size = {main_batch_size:N_prime_shape_for_batch(main_batch_size),\n",
    "#                                     final_batch_size:N_prime_shape_for_batch(final_batch_size)}\n",
    "# #     N_prime_shapes_by_batch = tuple(map(N_prime_shape_for_batch, batch_sizes))\n",
    "#     N_prime_contract_expr_by_batch_size = {main_batch_size:oe.contract_expression(N_prime_index_str, *(N_prime_shapes_by_batch_size[main_batch_size])),\n",
    "#                                            final_batch_size:oe.contract_expression(N_prime_index_str, *(N_prime_shapes_by_batch_size[final_batch_size]))}\n",
    "# #     N_prime_contract_exprs_by_batch = tuple(map(lambda shapes: oe.contract_expression(N_prime_index_str, *shapes),\n",
    "# #                                                 N_prime_shapes_by_batch))\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Processing batches...')\n",
    "    #process the batches\n",
    "    if parallel:\n",
    "        Z_prime_C = torch.cat(list( par(delayed(denominator_batch_process)(Y_prime_C_split_batched_batch, Q_l, indices)\n",
    "                                        for Y_prime_C_split_batched_batch, indices in zip(Y_prime_C_split_batched,\n",
    "                                                                                          Y_prime_C_split_batched_indices)) ))\n",
    "\n",
    "#         Z_prime_C = torch.cat(list( par(delayed(denominator_batch_process_oe)(Y_prime_C_split_batched_batch, Q_l, indices, \n",
    "#                                                                               V_prime_contract_expr_by_batch_size[len(indices)], \n",
    "#                                                                               N_prime_contract_expr_by_batch_size[len(indices)])\n",
    "#                                         for Y_prime_C_split_batched_batch, indices in zip(Y_prime_C_split_batched,\n",
    "#                                                                                           Y_prime_C_split_batched_indices)) ))\n",
    "#         Z_prime_C = torch.cat(list( par(delayed(denominator_batch_process_oe)(Y_prime_C_split_batched_batch, Q_l, indices, V_ce, N_ce)\n",
    "#                                         for Y_prime_C_split_batched_batch, indices, V_ce, N_ce in zip(Y_prime_C_split_batched,\n",
    "#                                                                                                       Y_prime_C_split_batched_indices,\n",
    "#                                                                                                       V_prime_contract_exprs_by_batch,\n",
    "#                                                                                                       N_prime_contract_exprs_by_batch)) ))\n",
    "    elif use_gpu:\n",
    "        Z_prime_C = torch.cat([denominator_batch_process_g(Y_prime_C_split_batched_batch.type(my_tt).cuda(), Q_l_g, indices)\n",
    "                       for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n",
    "                                                                              Y_prime_C_split_batched_indices),\n",
    "                                                                          total=num_batches,\n",
    "                                                                          minterval=minterval)]) \n",
    "#         Z_prime_C = torch.cat([denominator_batch_process_g(Y_prime_C_split_batched_batch.type(my_tt).cuda(), Q_l_g, indices, \n",
    "#                                                             V_prime_contract_expr_by_batch_size[len(indices)], \n",
    "#                                                             N_prime_contract_expr_by_batch_size[len(indices)])\n",
    "#                        for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n",
    "#                                                                               Y_prime_C_split_batched_indices),\n",
    "#                                                                           total=num_batches)]) \n",
    "    else:\n",
    "        Z_prime_C = torch.cat([denominator_batch_process(Y_prime_C_split_batched_batch, Q_l, indices)\n",
    "                               for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n",
    "                                                                                      Y_prime_C_split_batched_indices),\n",
    "                                                                                  total=num_batches,\n",
    "                                                                                  minterval=minterval)]) \n",
    "\n",
    "#         Z_prime_C = torch.cat([denominator_batch_process_oe(Y_prime_C_split_batched_batch, Q_l, indices, \n",
    "#                                                             V_prime_contract_expr_by_batch_size[len(indices)], \n",
    "#                                                             N_prime_contract_expr_by_batch_size[len(indices)])\n",
    "#                                for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n",
    "#                                                                                       Y_prime_C_split_batched_indices),\n",
    "#                                                                                   total=num_batches)]) \n",
    "#         Z_prime_C = torch.cat([denominator_batch_process_oe(Y_prime_C_split_batched_batch, Q_l, indices, V_ce, N_ce)\n",
    "#                                for Y_prime_C_split_batched_batch, indices, V_ce, N_ce in tqdm(zip(Y_prime_C_split_batched,\n",
    "#                                                                                                   Y_prime_C_split_batched_indices,\n",
    "#                                                                                                   V_prime_contract_exprs_by_batch,\n",
    "#                                                                                                   N_prime_contract_exprs_by_batch),\n",
    "#                                                                                                  total=num_batches)]) \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('> Batches processed.')\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Calculating numerator...')\n",
    "        stampedNote('Calculating V_prime_w_C...')\n",
    "    xhat0f_L_w = Q_l[xhat0f_idx]#.float() # :: (s, l)\n",
    "#     V_prime_w_C = oe.contract('cmij,ji->cmi',Y_prime_C_split.type(my_tt), xhat0f_L_w, backend='torch') # :: (c,m,l)\n",
    "    V_prime_w_C = torch.einsum('cmij,ji->cmi',Y_prime_C_split.type(my_tt), xhat0f_L_w) # :: (c,m,l)\n",
    "#     stampedMemNote('V_prime_w_C created.')\n",
    "#     del xhat0f_L_w\n",
    "#     del Y_prime_C_split\n",
    "#     stampedMemNote('xhat0f_L_w and Y_prime_C_split deleted.')\n",
    "    if profilingStamps:\n",
    "        stampedNote('Calculating O_w_C...')\n",
    "    O_w_C = torch.prod(V_prime_w_C, 2) # :: (c,m) likelihoods of each of the m sampled channel prefixes for each of c contexts\n",
    "#     stampedMemNote('O_w_C created.')\n",
    "#     del V_prime_w_C\n",
    "#     stampedMemNote('V_prime_w_C deleted.')\n",
    "    if profilingStamps:\n",
    "        stampedNote('Calculating U_w_C...')\n",
    "    # contact_cont_U_w_ALL = pW_C_torch[contact_idx] * contact_cont_O_w_ALL ## :: (c,m) joint probabilities of xhat0f with each of the m sampled channel prefixes for each of the c contexts\n",
    "    U_w_C = torch.einsum('c,cm->cm', pW_C_torch[xhat0f_idx], O_w_C) ## :: (c,m) joint probabilities of xhat0f with each of the m sampled channel prefixes for each of the c contexts\n",
    "#     stampedMemNote('U_w_C created.')\n",
    "#     del O_w_C\n",
    "#     stampedMemNote('O_w_C deleted.')\n",
    "    \n",
    "    if profilingStamps:\n",
    "        stampedMemNote('Calculating E_w_C...')\n",
    "#     return torch.dot(Z_prime_C, U_w_C) / m\n",
    "    E_w_C = torch.einsum('cm,cm->c', Z_prime_C, U_w_C) / m\n",
    "#     stampedMemNote('E_w_C created.')\n",
    "#     del Z_prime_C\n",
    "#     del U_w_C\n",
    "#     stampedMemNote('Z_prime_C and U_w_C deleted.')\n",
    "    if profilingStamps:\n",
    "        stampedNote('> End calculation.')\n",
    "    return E_w_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.329724Z",
     "start_time": "2019-08-07T18:30:06.067Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.330322Z",
     "start_time": "2019-08-07T18:30:06.079Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Cs_t)\n",
    "len(Cs_t) * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.331015Z",
     "start_time": "2019-08-07T18:30:06.091Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    wlr_n = np.array(sorted(list(wordlengthsInclEdges)))\n",
    "    benchmark_lengths = torch.tensor(list(wlr_n[ np.where(wlr_n % 4 == 0)[0]  ]) + [max(wordlengthsInclEdges)])\n",
    "    benchmark_wordform_idxs = tuple([Ws_t.index(w)\n",
    "                                     for l in benchmark_lengths\n",
    "                                     for w in choice(wordformsOfLength(l, Ws_t, True))])\n",
    "    benchmark_wordforms = tuple([Ws_t[w_idx] for w_idx in benchmark_wordform_idxs])\n",
    "    print(tuple(zip(benchmark_lengths, benchmark_wordforms)))\n",
    "    benchmark_CMs = tuple([CMsByLengthByWordformIndex_torch[len(ds2t(w))][Ws_t.index(w)]\n",
    "                           for w in benchmark_wordforms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.331667Z",
     "start_time": "2019-08-07T18:30:06.105Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # iterate through batch sizes, leaving large batch sizes for last, \n",
    "    # since they're most likely to cause crashes and this way we collect more useful data first...\n",
    "    for batch_size in [50, 100, 200, 300]\n",
    "        for b_l, w_idx, CM in zip(benchmark_lengths, benchmark_wordform_idxs, benchmark_CMs):\n",
    "            stampedMemNote(f'>>> l={b_l}, b={batch_size}', includeGPU=True)\n",
    "            post = pXhat0fX0i_ptc(w_idx, CM, m=n, target_contexts_per_batch=b_l, parallel=l, use_gpu=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.332286Z",
     "start_time": "2019-08-07T18:30:06.117Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 50, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 40.2s = 106295c/40.2 = 2644.15cps\n",
    "    # peak memory usage is about 1.0GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=50, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.332924Z",
     "start_time": "2019-08-07T18:30:06.127Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 200, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 32.1s = 106295c/32.1 = 3311.37cps\n",
    "    # peak GPU memory usage ~2.0 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=200, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.333552Z",
     "start_time": "2019-08-07T18:30:06.141Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 350, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 28.7s = 106295c/28.7s = 3703.66cps\n",
    "    # peak GPU memory usage ~3.1 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=350, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.334178Z",
     "start_time": "2019-08-07T18:30:06.155Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 375, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 28.6s = 106295c/28.6 = 3716.61cps\n",
    "    # peak GPU memory usage ~3.3 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=375, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.334825Z",
     "start_time": "2019-08-07T18:30:06.167Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 375, kotoba, gpu, float32, opt_einsum\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 103s = 106295c/103s = 1032cps\n",
    "    # peak GPU memory usage ~? GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=375, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.335468Z",
     "start_time": "2019-08-07T18:30:06.178Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 400, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 94s = 106295c/94s = 1130.80cps\n",
    "    # peak GPU memory usage ~3.1 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=400, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.336097Z",
     "start_time": "2019-08-07T18:30:06.191Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 500, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 93.0s = 106295c/93.0s = 1142.96cps\n",
    "    # peak GPU memory usage ~4.2 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=500, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.336734Z",
     "start_time": "2019-08-07T18:30:06.202Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # batch size 750, kotoba, gpu, float32\n",
    "    # 5s on sample generation, 1s to split into batches of contexts\n",
    "    # 252s = 106295c/252s = 421.81cps\n",
    "    # peak GPU memory usage ~5.9 GB / 8GB\n",
    "    pContact_cont_C = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=750, parallel=False, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.337367Z",
     "start_time": "2019-08-07T18:30:06.217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #batch size 500, quine, serial\n",
    "    # 40s on sample generation, 1s to split into batches of contexts / float64 / quine\n",
    "    # 6s on sample generation, <1s to split into batches of contexts / float32 / kotoba\n",
    "    # ≈10s per batch, 213 batches = 35m4s = 106295c/2104s = 50.52 cps / float64 / quine\n",
    "    # ≈5s per batch, 213 batches = 17m47s = 106295c/1067s = 99.62 cps / float32 / quine\n",
    "    # 1.16s per batch, 213 batches = 247.08s = 4m7s = 106295c/247s = 430.34 cps / float32 / kotoba\n",
    "    #  - pytorch does make use of all available cpus (for a significant stretch) \n",
    "    #    of each batch step, and the memory overhead is low enough to run on kotoba,\n",
    "    #    but it's clearly possible (serial processing w/ batch size 3000 only takes 90s for all 106295c)\n",
    "    #      - parallelization over (smaller? larger?) batches \n",
    "    #    would waste less time\n",
    "    cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.338007Z",
     "start_time": "2019-08-07T18:30:06.230Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #batch size 500, quine, parallel\n",
    "    # 30s on sample generation, 1s to split into batches of contexts\n",
    "    # 13m39s = 819s = 106295c/819s = 129.79 cps / float64 / quine\n",
    "    # 4-5s per batch, 213 batches ≈10^3s to do ≈10^5c = ≈10^2 cps / float64 / quine\n",
    "    #   = about twice as fast as serial processing with same batch size\n",
    "    # cpu utilization is more consistent per core but far from full utilization\n",
    "    # 11m46s = 706s = 106295c/706s = 150.56 cps / float32 / quine\n",
    "    # 3m59s = 239s = 106295c/239s = 444.75 cps / float32 / kotoba / peak 25GB RAM usage\n",
    "    cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 500, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.338641Z",
     "start_time": "2019-08-07T18:30:06.241Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.339291Z",
     "start_time": "2019-08-07T18:30:06.250Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #batch size 650, quine, parallel\n",
    "    # 24s on sample generation, 1s to split into batches of contexts / float64 / quine\n",
    "    # 12m21s = 741s = 106295c/741s = 143.45 cps / float64 / quine\n",
    "    # 9m14s = 554s = 106295c/554s = 191.87 cps / float32 / quine\n",
    "    # ?m?s = ?s = 106295c/? = ? cps / float32 / kotoba / peak ?GB RAM usage = too high...\n",
    "    cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 650, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T19:37:49.146133Z",
     "start_time": "2019-08-06T19:24:47.590503Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Begin calculation. @ 19:24:47\n",
      "Sampling... @ 19:24:47\n",
      "\tVM used vs. available: 19.74GB vs. 144.22GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    6.5s remaining:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    7.6s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    9.0s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting samples by context... @ 19:25:05\n",
      "\tVM used vs. available: 19.77GB vs. 144.20GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/142 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting contexts into batches ... @ 19:25:06\n",
      "Num batches = 142\n",
      "Pre-calculating contract expressions for each batch... @ 19:25:06\n",
      "\tVM used vs. available: 20.39GB vs. 143.58GB\n",
      "Processing batches... @ 19:25:06\n",
      "\tVM used vs. available: 20.39GB vs. 143.58GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/142 [01:06<2:37:12, 66.90s/it]\u001b[A\n",
      "  1%|▏         | 2/142 [02:17<2:39:01, 68.15s/it]\u001b[A\n",
      "  2%|▏         | 3/142 [03:30<2:40:52, 69.44s/it]\u001b[A\n",
      "  3%|▎         | 4/142 [04:42<2:41:49, 70.36s/it]\u001b[A\n",
      "  4%|▎         | 5/142 [06:22<3:00:59, 79.27s/it]\u001b[A\n",
      "  4%|▍         | 6/142 [07:38<2:56:50, 78.02s/it]\u001b[A\n",
      "  5%|▍         | 7/142 [08:59<2:57:40, 78.97s/it]\u001b[A\n",
      "  6%|▌         | 8/142 [10:12<2:52:15, 77.13s/it]\u001b[A\n",
      "  6%|▋         | 9/142 [11:36<2:55:52, 79.34s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-1dce3e393056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#batch size 750, quine, serial, oe test (reused paths, no storage of constant Q_l, efficient reused path expr calc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcont_Z_prime_ALL_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpXhat0fX0i_ptc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontact_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_CM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_contexts_per_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-287-3e4292f765f6>\u001b[0m in \u001b[0;36mpXhat0fX0i_ptc\u001b[0;34m(xhat0f_idx, x0k_CM, m, target_contexts_per_batch, parallel)\u001b[0m\n\u001b[1;32m    274\u001b[0m                                for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n\u001b[1;32m    275\u001b[0m                                                                                       Y_prime_C_split_batched_indices),\n\u001b[0;32m--> 276\u001b[0;31m                                                                                   total=num_batches)]) \n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;31m#         Z_prime_C = torch.cat([denominator_batch_process_oe(Y_prime_C_split_batched_batch, Q_l, indices, V_ce, N_ce)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;31m#                                for Y_prime_C_split_batched_batch, indices, V_ce, N_ce in tqdm(zip(Y_prime_C_split_batched,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-287-3e4292f765f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                                             \u001b[0mV_prime_contract_expr_by_batch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                             N_prime_contract_expr_by_batch_size[len(indices)])\n\u001b[0;32m--> 274\u001b[0;31m                                for Y_prime_C_split_batched_batch, indices in tqdm(zip(Y_prime_C_split_batched,\n\u001b[0m\u001b[1;32m    275\u001b[0m                                                                                       Y_prime_C_split_batched_indices),\n\u001b[1;32m    276\u001b[0m                                                                                   total=num_batches)]) \n",
      "\u001b[0;32m<ipython-input-287-3e4292f765f6>\u001b[0m in \u001b[0;36mdenominator_batch_process_oe\u001b[0;34m(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices, V_prime_ce, N_prime_ce)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mstampedMemNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tM_prime...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# :: (c/b, m, n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mM_prime_ALL_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_prime_ALL_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mV_prime_ALL_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch size 750, quine, serial, oe test (reused paths, no storage of constant Q_l, efficient reused path expr calc)\n",
    "cont_Z_prime_ALL_p = pXhat0fX0i_ptc(contact_idx, cont_CM, m=50, target_contexts_per_batch=750, parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T09:44:17.431378Z",
     "start_time": "2019-08-06T09:35:39.369095Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Begin calculation. @ 09:35:39\n",
      "\tVM used vs. available: 21.11GB vs. 142.86GB\n",
      "Generating samples... @ 09:35:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    7.0s remaining:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    8.0s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   14.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting samples by context... @ 09:36:03\n",
      "Splitting contexts into batches ... @ 09:36:04\n",
      "Num batches = 142\n",
      "Processing batches... @ 09:36:04\n",
      "\tVM used vs. available: 21.11GB vs. 142.86GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 142 | elapsed:  6.4min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 142 | elapsed:  7.2min remaining:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 142 | elapsed:  8.0min remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 142 out of 142 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Batches processed. @ 09:44:17\n",
      "\tVM used vs. available: 21.13GB vs. 142.85GB\n"
     ]
    }
   ],
   "source": [
    "#batch size 750, quine, parallel\n",
    "# 19s on sample generation, 1s to split into batches of contexts\n",
    "# 10m37s = 637s = 106295c/637s = 166.87 cps / float64\n",
    "# 8m38s = 478s = 106295c/?s = 222.37 cps / float32\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 750, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T09:52:18.010581Z",
     "start_time": "2019-08-06T09:44:17.438110Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Begin calculation. @ 09:44:17\n",
      "\tVM used vs. available: 21.11GB vs. 142.86GB\n",
      "Generating samples... @ 09:44:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    8.1s remaining:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    8.8s remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   10.0s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting samples by context... @ 09:44:41\n",
      "Splitting contexts into batches ... @ 09:44:42\n",
      "Num batches = 126\n",
      "Processing batches... @ 09:44:42\n",
      "\tVM used vs. available: 21.11GB vs. 142.86GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 126 | elapsed:  5.6min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 126 | elapsed:  6.4min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 126 | elapsed:  7.0min remaining:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Batches processed. @ 09:52:17\n",
      "\tVM used vs. available: 21.11GB vs. 142.86GB\n"
     ]
    }
   ],
   "source": [
    "#batch size 850, quine, parallel\n",
    "# 21s on sample generation, 1s to split into batches of contexts\n",
    "# 10m46s = 646s = 106295c/646s = 164.54 cps / float64\n",
    "# 8m1s = 481s = 106295c/481s = 220.99 cps / float32\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 850, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.339893Z",
     "start_time": "2019-08-07T18:30:06.274Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# batch size 1000, quine, parallel\n",
    "# 30s sample generation\n",
    "# peak memory usage is too high\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 1000, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:19:16.759632Z",
     "start_time": "2019-08-06T08:19:16.720786Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'denominator_calc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a007a830483c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   = about ? as fast as ? processing with batch size ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# cpu utilization is ? per core but ? ? full utilization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcont_Z_prime_ALL_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenominator_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_CM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_Q_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'denominator_calc' is not defined"
     ]
    }
   ],
   "source": [
    "#batch size 100, quine, parallel\n",
    "# ?s on sample generation, 1s to split into batches of contexts\n",
    "# ?m?s = ?s = 106295c/?s = ?.? cps\n",
    "# ?s per batch, ? batches ≈?s to do ≈10^5c = ≈? cps\n",
    "#   = about ? as fast as ? processing with batch size ?\n",
    "# cpu utilization is ? per core but ? ? full utilization\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 100, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T21:20:03.223368Z",
     "start_time": "2019-08-04T21:16:00.673038Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples... @ 21:16:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    8.1s remaining:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    9.2s remaining:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    9.8s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   18.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting samples by context... @ 21:16:30\n",
      "Splitting contexts into batches ... @ 21:16:31\n",
      "Num batches = 107\n",
      "Processing batches... @ 21:16:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  2.7min\n",
      "Process ForkPoolWorker-591:\n",
      "Process ForkPoolWorker-595:\n",
      "Process ForkPoolWorker-590:\n",
      "Process ForkPoolWorker-584:\n",
      "Process ForkPoolWorker-580:\n",
      "Process ForkPoolWorker-585:\n",
      "Process ForkPoolWorker-583:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-581:\n",
      "Process ForkPoolWorker-582:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-588:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "Process ForkPoolWorker-586:\n",
      "Process ForkPoolWorker-587:\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-589:\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 147, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-579:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\", line 149, in get\n",
      "    return recv()\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "  File \"/home/AD/emeinhar/anaconda3/envs/anvil/lib/python3.6/site-packages/torch/storage.py\", line 125, in _load_from_bytes\n",
      "    def _load_from_bytes(b):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-441-650d61ac2888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 30s sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcont_Z_prime_ALL_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenominator_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_CM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_Q_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-439-169d631102fc>\u001b[0m in \u001b[0;36mdenominator_calc\u001b[0;34m(CM, m, Q_l, target_contexts_per_batch, parallel)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         Z_prime_ALL = torch.cat(list( par(delayed(denominator_batch_process)(Y_prime_ALL_split_batched_batch, Q_l, indices)\n\u001b[0;32m---> 76\u001b[0;31m                                       for Y_prime_ALL_split_batched_batch, indices in zip(Y_prime_ALL_split_batched, Y_prime_ALL_split_batched_indices)) ))\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         Z_prime_ALL = torch.cat((denominator_batch_process(Y_prime_ALL_split_batched_batch, Q_l, indices)\n",
      "\u001b[0;32m<ipython-input-18-501a20251bb0>\u001b[0m in \u001b[0;36mpar\u001b[0;34m(gen_expr)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBACKEND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPREFER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;34m\"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/site-packages/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmappingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining task handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining result handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anvil/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch size 1000, quine, parallel\n",
    "# 30s sample generation\n",
    "# peak memory usage is too high\n",
    "# cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 1000, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T21:29:39.804338Z",
     "start_time": "2019-08-04T21:23:27.784Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#batch size 50, quine, parallel\n",
    "# 30s sample generation\n",
    "# \n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 50, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T21:29:39.798857Z",
     "start_time": "2019-08-04T21:23:08.957810Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples... @ 21:23:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    6.7s remaining:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    7.4s remaining:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting samples by context... @ 21:23:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting contexts into batches ... @ 21:23:34\n",
      "Num batches = 54\n",
      "Processing batches... @ 21:23:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 10/54 [05:37<24:36, 33.56s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-444-f615f5b16cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 30s sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcont_Z_prime_ALL_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenominator_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_CM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_Q_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-443-640f59e1b8c0>\u001b[0m in \u001b[0;36mdenominator_calc\u001b[0;34m(CM, m, Q_l, target_contexts_per_batch, parallel)\u001b[0m\n\u001b[1;32m     80\u001b[0m                                  for Y_prime_ALL_split_batched_batch, indices in tqdm(zip(Y_prime_ALL_split_batched,\n\u001b[1;32m     81\u001b[0m                                                                                           Y_prime_ALL_split_batched_indices),\n\u001b[0;32m---> 82\u001b[0;31m                                                                                       total=num_batches)]) \n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batches processed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mZ_prime_ALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-443-640f59e1b8c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         Z_prime_ALL = torch.cat([denominator_batch_process(Y_prime_ALL_split_batched_batch, Q_l, indices)\n\u001b[0;32m---> 80\u001b[0;31m                                  for Y_prime_ALL_split_batched_batch, indices in tqdm(zip(Y_prime_ALL_split_batched,\n\u001b[0m\u001b[1;32m     81\u001b[0m                                                                                           Y_prime_ALL_split_batched_indices),\n\u001b[1;32m     82\u001b[0m                                                                                       total=num_batches)]) \n",
      "\u001b[0;32m<ipython-input-443-640f59e1b8c0>\u001b[0m in \u001b[0;36mdenominator_batch_process\u001b[0;34m(Y_prime_ALL_split_batched_batch, Q_l, batch_context_indices)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tM_prime...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# :: (c/b, m, n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mM_prime_ALL_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_prime_ALL_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mV_prime_ALL_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch size 2000, quine, serial\n",
    "# 30s sample generation\n",
    "# 34s/batch -> 54 batches = 1836s = 106295c/1836s = 57.90 cps\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 2000, parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:19:16.765164Z",
     "start_time": "2019-08-06T08:19:16.628Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#batch size 5000, quine, serial\n",
    "# 17s sample generation (?????)\n",
    "# mem usage beforehand ≈16GB\n",
    "# each batch initially requires ≈+70GB \n",
    "# 53m30s = 3810s -> 106295c/3810s = 27.90 cps\n",
    "cont_Z_prime_ALL_p = denominator_calc(cont_CM, 50, cont_Q_l, 5000, parallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact/full wordform-length variant functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variant versions of the functions from section 5.2 are used for Case 3 calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.340634Z",
     "start_time": "2019-08-07T18:30:06.306Z"
    }
   },
   "outputs": [],
   "source": [
    "def pXhat0fX0f_pxt(xhat0f_idx, x0f_CM, c_idx, m = 50):\n",
    "    '''\n",
    "    If ⋊ x_0 x_1 ... x_i x_k denotes\n",
    "    the produced prefix r of the speaker's intended wordform w,\n",
    "     where\n",
    "       x_i = the last fully produced segment\n",
    "       k = i+1\n",
    "       x_k = the next segment\n",
    "       |⋊ x_0 x_1 ... x_i x_k| = l + 2\n",
    "       l = i+1 = the length of the produced prefix (not incl. \n",
    "                 the left word edge or the next segment)\n",
    "\n",
    "    then\n",
    "\n",
    "    x0f_CM :: (|Y1|, l) is the associated l-length stack of size |Y1| vectors\n",
    "     where x0f_CM[:,j] is the distribution p(Y_j|X0^f has been produced)\n",
    "\n",
    "    xhat0f_idx is the index of a wordform w' s.t. \n",
    "        pXhat0fX0f_pxt(xhat0f_idx, x0f_CM) ≈ p(w'|w)\n",
    "    '''\n",
    "    shape_info = False\n",
    "    pW = get_pW_t(c_idx)\n",
    "#     if x0k_CM is None and x0k is None:\n",
    "#         raise Exception('Must specify one of x0k_CM or x0k.')\n",
    "\n",
    "    # Computation proceeds in two steps:\n",
    "    # calculate the denominator p(y0i) for m = 50 sampled y0i's\n",
    "    # calculate the numerator p(y0i|w')p(w') for m = 50 sampled y0i's\n",
    "\n",
    "\n",
    "#     xhat0f_idx = Wmap[xhat0f]\n",
    "\n",
    "#     l = len(ds2t(x0i))\n",
    "#     x0k_CM = CMsByPrefixIndex[prefixMap[x0i]]\n",
    "\n",
    "#     my_Q_l = CMsByLengthByWordformIndex[l - 2]\n",
    "\n",
    "    #Collect m samples from p(Y0i|X0k) = p(Y0i|r), \n",
    "    # where each sample is an l-length stack of one-hot vectors\n",
    "    # where each one-hot vector corresponds to a channel symbol:\n",
    "    Y_prime = depthSamplerC_t(x0f_CM, m)#.float() #:: (m, l, |Y1|)\n",
    "#         if x0k_CM is not None:\n",
    "#             Y_prime = depthSampler2a_t(x0k_CM, m)#.float() #:: (m, l, |Y1|)\n",
    "#             if x0k is not None:\n",
    "# #                 print('foo')\n",
    "#                 print(type(x0k_CM))\n",
    "#                 print(x0k_CM.shape)\n",
    "#                 generated_CM = sample_pY0iX0k(x0k, debug=True)[1]\n",
    "#                 print(type(generated_CM))\n",
    "#                 print(generated_CM.shape)\n",
    "#                 generated_CM_t = torch.tensor(generated_CM)\n",
    "#                 assert torch.allclose(x0k_CM, torch.tensor(sample_pY0iX0k(x0k, debug=True)[1]))\n",
    "# #                 print('bar')\n",
    "#         else:\n",
    "#             Y_prime = torch.tensor(list(map(y0kOHmap, [sample_pY0iX0k(x0k) for each in range(m)])))\n",
    "#         print('baz')\n",
    "    l = Y_prime.shape[1]# + 1\n",
    "    if shape_info:\n",
    "        print(f'|Y_prime| = {Y_prime.shape} = (m, l, |Y1|)')\n",
    "\n",
    "    #Grab ALL the relevant-prefix-length channel matrices \n",
    "    # for every segmental wordform in the lexicon:\n",
    "#         my_Q_l = CMsByLengthByWordformIndex_torch[l - 3]#.float()\n",
    "#         my_Q_l = CMsByLengthByWordformIndex_torch[l - 1]#.float() #:: (|W|, |Y1|, l)\n",
    "    my_Q_l = exactCMsByLengthByWordformIndex_torch[l+2]#.float() #:: (|W|, |Y1|, l) # <<<< this is the only non-cosmetic (variable-renaming) difference from the main version of this function\n",
    "\n",
    "    if shape_info:\n",
    "        print(f'|Q_l| = {my_Q_l.shape} = (|W|=n, |Y1|, l)')\n",
    "\n",
    "#         print(f'{Y_prime.dtype}, {my_Q_l.dtype}')\n",
    "\n",
    "    # NORMALIZATION\n",
    "    V_prime = torch.einsum('mli,kil->mkl', Y_prime.type(my_tt), my_Q_l)  # :: (m,n,l)\n",
    "#         print(f\"|V'| = {V_prime.shape} = (m, n, l)\")\n",
    "    M_prime = torch.prod(V_prime, 2) # :: (m,n)\n",
    "#         print(f\"|M'| = {M_prime.shape} = (m, n)\")\n",
    "#         N_prime = torch.matmul(M_prime, pX0f_torch) # :: (m, ) <- prior probabilities of each of the m sampled channel prefixes\n",
    "    N_prime = torch.matmul(M_prime, pW)\n",
    "#         print(f\"|N'| = {N_prime.shape} = (m, )\")\n",
    "#     Z_prime = 1.0 / N_prime # :: (m, )\n",
    "    Z_prime = torch.ones(N_prime.shape, dtype=my_tt)\n",
    "    torch.div(Z_prime,\n",
    "              N_prime,\n",
    "              out=Z_prime)\n",
    "#         print(f\"|Z'| = {Z_prime.shape} = (m, )\")\n",
    "    if shape_info:\n",
    "        print(f\"|V'| = {V_prime.shape} = (m, n, l)\")\n",
    "        print(f\"|M'| = {M_prime.shape} = (m, n)\")\n",
    "        print(f\"|N'| = {N_prime.shape} = (m, )\")\n",
    "        print(f\"|Z'| = {Z_prime.shape} = (m, )\")\n",
    "\n",
    "    # NUMERATOR\n",
    "    L_w = my_Q_l[xhat0f_idx]#.float()\n",
    "    V_prime_w = torch.einsum('mij,ji->mi',Y_prime.type(my_tt), L_w)\n",
    "#         print(f\"|V'_w| = {V_prime_w.shape} = (m, l)\")\n",
    "    O_w = torch.prod(V_prime_w, 1) # :: (m,) likelihoods of each of the m sampled channel prefixes\n",
    "#         print(f\"|O_w| = {O_w.shape} = (m, )\")\n",
    "#         U_w = pX0f_torch[xhat0f_idx] * O_w ## :: (m,1) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "    U_w = pW[xhat0f_idx] * O_w ## :: (m,) joint probabilities of xhat0f with each of the m sampled channel prefixes\n",
    "#         print(f\"|U_w| = {U_w.shape} = (m, )\")\n",
    "    if shape_info:\n",
    "        print(f\"|V'_w| = {V_prime_w.shape} = (m, l)\")\n",
    "        print(f\"|O_w| = {O_w.shape} = (m, )\")\n",
    "        print(f\"|U_w| = {U_w.shape} = (m, )\")\n",
    "\n",
    "\n",
    "    if shape_info:\n",
    "        E = torch.dot(Z_prime, U_w) / m\n",
    "        print(f\"|E| = scalar: type(E) = {type(E)}\")\n",
    "    return torch.dot(Z_prime, U_w) / m\n",
    "\n",
    "def pXhat0fX0f_pxtn(xhat0f_idx, x0f_CM, c_idx, m = 50):\n",
    "    return pXhat0fX0f_pxt(xhat0f_idx, x0f_CM, c_idx, m = 50).numpy()\n",
    "\n",
    "def wordformTox0fCM(w=None, w_idx=None):\n",
    "#     if w is None and w_idx is None:\n",
    "#         raise Exception('At least one argument must be specified.')\n",
    "    if w is None:\n",
    "        w = Ws_t[w_idx]\n",
    "        w_t = ds2t(w)\n",
    "    else:\n",
    "        w_t = ds2t(w)\n",
    "        w_idx = Ws_t.index(w)\n",
    "    \n",
    "    total_x0f_length = len(w_t) #includes both edge symbols\n",
    "#     word_length_noLE = total_x0f_length - 1 #still includes right edge symbol = \"upcoming segment\"\n",
    "#     word_length_noRE = word_length_noLE - 1\n",
    "#     offset = 2\n",
    "#     assert word_length_produced_so_far - offset >= 0 , f\"{word_length_produced_so_far} - {offset} < 0; w = {w}\"\n",
    "#     assert word_length_produced_so_far - offset < len(CMsByLengthByWordformIndex_torch), f\"{word_length_produced_so_far} - {offset} >= {len(CMsByLengthByWordformIndex_torch)}; w = {w}\"\n",
    "#     assert w_idx in range(CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset].shape[0]), f\"{w_idx}; w = {w}\"\n",
    "#     x0f_cm = CMsByLengthByWordformIndex_torch[word_length_produced_so_far - offset][w_idx]\n",
    "    x0f_cm = exactCMsByLengthByWordformIndex_torch[total_x0f_length][w_idx]\n",
    "    return x0f_cm\n",
    "\n",
    "# for convenience\n",
    "def pXhat0fX0f_pxtn_conv(w_hat, w, c, m = 50):\n",
    "    w_idx = Ws_t.index(w_hat)\n",
    "\n",
    "#     r_idx = Ps_t.index(r)\n",
    "    source_wf = list(wordsWithPrefix(w, Ws))[0]\n",
    "    source_wf_idx = Ws_t.index(source_wf)\n",
    "    \n",
    "    total_x0k_length = len(ds2t(w))# includes left edge symbol + upcoming segment\n",
    "#     prefix_length_noWE = total_x0k_length - 1 #includes upcoming segment\n",
    "#     prefix_length_produced_so_far = prefix_length_noWE - 1\n",
    "#     offset = 1\n",
    "#     x0k_cm = CMsByLengthByWordformIndex_torch[prefix_length_produced_so_far - offset][source_wf_idx]\n",
    "    x0f_cm = exactCMsByLengthByWordformIndex_torch[total_x0k_length][source_wf_idx]\n",
    "    \n",
    "    c_idx = Cs_t.index(c)\n",
    "    \n",
    "    return pXhat0fX0f_pxtn(w_idx, x0f_cm, c_idx, m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.341274Z",
     "start_time": "2019-08-07T18:30:06.318Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    phatXhat0fX0k_baseline('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉',\n",
    "                           '⋊.k.ɑ.n.t.æ.k.t.⋉.⋉',\n",
    "                           'a couple of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.341916Z",
     "start_time": "2019-08-07T18:30:06.328Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'),\n",
    "                   CMsByLengthByWordformIndex_torch[5][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')],\n",
    "                   Cs_t.index('a couple of'),\n",
    "                   m = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.342513Z",
     "start_time": "2019-08-07T18:30:06.341Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'),\n",
    "                   CMsByLengthByWordformIndex_torch[9][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')],\n",
    "                   Cs_t.index('a couple of'),\n",
    "                   m = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.343219Z",
     "start_time": "2019-08-07T18:30:06.355Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.343809Z",
     "start_time": "2019-08-07T18:30:06.365Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'),\n",
    "                   CMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')],\n",
    "                   Cs_t.index('a couple of'),\n",
    "                   m = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.344499Z",
     "start_time": "2019-08-07T18:30:06.377Z"
    }
   },
   "outputs": [],
   "source": [
    "wordlengthsInclEdges\n",
    "len(wordlengthsInclEdges)\n",
    "len([None for each in range(min(wordlengthsInclEdges))])\n",
    "len(exactCMsByLengthByWordformIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.345105Z",
     "start_time": "2019-08-07T18:30:06.393Z"
    }
   },
   "outputs": [],
   "source": [
    "exactCMsByLengthByWordformIndex_torch[10].shape\n",
    "exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')].shape\n",
    "exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.345800Z",
     "start_time": "2019-08-07T18:30:06.406Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ds2t('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.346446Z",
     "start_time": "2019-08-07T18:30:06.416Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit pXhat0fX0i_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], Cs_t.index('a couple of'), m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.347086Z",
     "start_time": "2019-08-07T18:30:06.430Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    %timeit pXhat0fX0f_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'), exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], Cs_t.index('a couple of'), m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.347732Z",
     "start_time": "2019-08-07T18:30:06.441Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    len(ds2t('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')) - 2\n",
    "    pXhat0fX0f_pxt(Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉'),\n",
    "                   exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')],\n",
    "                   Cs_t.index('a couple of'),\n",
    "                   m = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.348381Z",
     "start_time": "2019-08-07T18:30:06.452Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.equal(exactCMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')], \n",
    "            CMsByLengthByWordformIndex_torch[10][Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.349013Z",
     "start_time": "2019-08-07T18:30:06.462Z"
    }
   },
   "outputs": [],
   "source": [
    "n_test_cases = 1000\n",
    "random_wordform_idxs = choices(list(range(len(Ws_t))), k=n_test_cases)\n",
    "random_context_idxs = choices(list(range(len(Cs_t))), k=n_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.349655Z",
     "start_time": "2019-08-07T18:30:06.476Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 10\n",
    "\n",
    "rand_w_idx = choice(random_wordform_idxs)\n",
    "rand_c_idx = choice(random_context_idxs)\n",
    "\n",
    "# 10.5ms on kotoba\n",
    "pXhat0fX0i_pxt(rand_w_idx, wordformTox0fCM(w_idx=rand_w_idx), rand_c_idx, m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.350299Z",
     "start_time": "2019-08-07T18:30:06.490Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 10\n",
    "\n",
    "rand_w_idx = choice(random_wordform_idxs)\n",
    "rand_c_idx = choice(random_context_idxs)\n",
    "\n",
    "# ≈130-150ms on new sidious (pre-sampling+Z_prime change)\n",
    "# ≈132ms on quine\n",
    "# 10.9ms on kotoba\n",
    "pXhat0fX0f_pxt(rand_w_idx, wordformTox0fCM(w_idx=rand_w_idx), rand_c_idx, m = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.350938Z",
     "start_time": "2019-08-07T18:30:06.503Z"
    }
   },
   "outputs": [],
   "source": [
    "del random_wordform_idxs\n",
    "del random_context_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.351577Z",
     "start_time": "2019-08-07T18:30:06.520Z"
    }
   },
   "outputs": [],
   "source": [
    "# Volume organization 3\n",
    "est_time_per_w_c = 10.5 #ms\n",
    "len(Ws_t)\n",
    "len(Cs_t)\n",
    "n_calcs = len(Ws_t) * len(Cs_t)\n",
    "\"{0:,} = {0:.2E} calculations\".format(n_calcs)\n",
    "\n",
    "serial_computation_time_est = est_time_per_w_c * len(Ws_t) * len(Cs_t) #ms\n",
    "est_s = serial_computation_time_est / 1e3\n",
    "est_m = est_s / 60\n",
    "est_h = est_m / 60\n",
    "est_d = est_h / 24\n",
    "\n",
    "est_at_scales = (est_s, est_m, est_h, est_d)\n",
    "\n",
    "toSN = lambda n: \"{0:.2E}\".format(n)\n",
    "\n",
    "tuple(map(toSN,\n",
    "          est_at_scales))\n",
    "\n",
    "est_cps = n_calcs / est_s\n",
    "\"est. {0:,} = {0:.2E} cps\".format(est_cps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation volume organization for Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Section 6.2 for explanation of the \"cases\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slices: Calculate results for groups of reconstructed wordforms by (input prefix, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.352174Z",
     "start_time": "2019-08-07T18:30:06.538Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Ps_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.352864Z",
     "start_time": "2019-08-07T18:30:06.551Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieveKcousins(r, maxK):\n",
    "    word_cousin_indices = union([set(list(cousin_mats[k][Ps_t.index(r),:].nonzero()[0])) for k in range(maxK)])\n",
    "    word_cousins = list(map(lambda w_idx: Ws_t[w_idx], word_cousin_indices))\n",
    "    return word_cousins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.353506Z",
     "start_time": "2019-08-07T18:30:06.560Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    retrieveKcousins('⋊.k.ɑ.n.t', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.354146Z",
     "start_time": "2019-08-07T18:30:06.571Z"
    }
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    cousin_mats[0].shape\n",
    "    cousin_mats[0][Ps_t.index('⋊.k.ɑ.n.t'), Ws_t.index('⋊.k.ɑ.n.t.æ.k.t.⋉.⋉')] #should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.354798Z",
     "start_time": "2019-08-07T18:30:06.587Z"
    }
   },
   "outputs": [],
   "source": [
    "wordsWithPrefix('⋊.k.ɑ.n.t', Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.355418Z",
     "start_time": "2019-08-07T18:30:06.597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nnz_word(my_k, j):\n",
    "    return cousin_mats[my_k][:,j].nnz\n",
    "\n",
    "if r or not e:\n",
    "    cousin_mats[1].shape\n",
    "    word_nnz_counts_by_k = {my_k:par(delayed(get_nnz_word)(my_k, j) for j in range(cousin_mats[my_k].shape[1]))\n",
    "                            for my_k in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.356053Z",
     "start_time": "2019-08-07T18:30:06.608Z"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mean, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.356691Z",
     "start_time": "2019-08-07T18:30:06.620Z"
    }
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    for my_k in range(5):\n",
    "        print(f\"max number of {my_k}-cousins = {max(word_nnz_counts_by_k[my_k])}\")\n",
    "        print(f\"median number of {my_k}-cousins = {median(word_nnz_counts_by_k[my_k])}\")\n",
    "        print(f\"mean number of {my_k}-cousins = {mean(word_nnz_counts_by_k[my_k])}\")\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.357316Z",
     "start_time": "2019-08-07T18:30:06.633Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nnz_prefix(my_k, i):\n",
    "    return cousin_mats[my_k][i,:].nnz\n",
    "\n",
    "if r or not e:\n",
    "    cousin_mats[1].shape\n",
    "    prefix_nnz_counts_by_k = {my_k:par(delayed(get_nnz_prefix)(my_k, i) for i in range(cousin_mats[my_k].shape[0]))\n",
    "                              for my_k in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.357948Z",
     "start_time": "2019-08-07T18:30:06.644Z"
    }
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    for my_k in range(5):\n",
    "        print(f\"max number of {my_k}-cousins = {max(prefix_nnz_counts_by_k[my_k])}\")\n",
    "        print(f\"median number of {my_k}-cousins = {median(prefix_nnz_counts_by_k[my_k])}\")\n",
    "        print(f\"mean number of {my_k}-cousins = {mean(prefix_nnz_counts_by_k[my_k])}\")\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.358580Z",
     "start_time": "2019-08-07T18:30:06.655Z"
    }
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    cousin_mats[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.359206Z",
     "start_time": "2019-08-07T18:30:06.666Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_prefix_idx(w):\n",
    "    return Ps_t.index(w)\n",
    "\n",
    "prefix_indices_of_full_wordforms = par(delayed(to_prefix_idx)(w) for w in Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.359827Z",
     "start_time": "2019-08-07T18:30:06.676Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_word_idx(p):\n",
    "    return Ws_t.index(p)\n",
    "\n",
    "def get_wordform_idx(prefix_idx):\n",
    "    return to_word_idx(Ps_t[prefix_idx])\n",
    "\n",
    "# get_wordform_idx = {prefix_idx:to_word_idx(Ps_t[prefix_idx])\n",
    "#                     for prefix_idx in prefix_indices_of_full_wordforms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.360462Z",
     "start_time": "2019-08-07T18:30:06.689Z"
    }
   },
   "outputs": [],
   "source": [
    "Ps_t[8370]\n",
    "get_wordform_idx(8370)\n",
    "Ws_t[get_wordform_idx(8370)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.361084Z",
     "start_time": "2019-08-07T18:30:06.700Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and not e:\n",
    "    cousin_mats[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.361724Z",
     "start_time": "2019-08-07T18:30:06.712Z"
    }
   },
   "outputs": [],
   "source": [
    "def getKcousin_idxs_of_wordform(w_idx, maxK, asType=None, onlyWordCousins = False, getPrefixIndices=True):\n",
    "    kcousin_indices = union([set(list(cousin_mats[k][:,w_idx].nonzero()[0])) for k in range(maxK)])\n",
    "    if onlyWordCousins:\n",
    "    # top line is ≈100x slower\n",
    "#         kcousin_indices = {prefix_idx for prefix_idx in kcousin_indices if prefix_idx in prefix_indices_of_full_wordforms}\n",
    "        kcousin_indices = set.intersection(kcousin_indices, prefix_indices_of_full_wordforms)\n",
    "    if not getPrefixIndices:\n",
    "        kcousin_indices = set(map(get_wordform_idx, kcousin_indices))\n",
    "    \n",
    "    if asType == 'ndarray':\n",
    "        kcousin_indices = np.array(sorted(list(kcousin_indices)), dtype='int32')\n",
    "        return kcousin_indices\n",
    "    elif asType == 'torch':\n",
    "        kcousin_indices = torch.tensor(sorted(list(kcousin_indices)), dtype=torch.int32)\n",
    "        return kcousin_indices\n",
    "    else:\n",
    "        return kcousin_indices\n",
    "    \n",
    "def getKcousin_idxs_of_prefix(p_idx, maxK, asType=None):\n",
    "    kcousin_indices = union([set(list(cousin_mats[k][p_idx].nonzero()[0])) for k in range(maxK)])\n",
    "    \n",
    "    if asType == 'ndarray':\n",
    "        kcousin_indices = np.array(sorted(list(kcousin_indices)), dtype='int32')\n",
    "        return kcousin_indices\n",
    "    elif asType == 'torch':\n",
    "        kcousin_indices = torch.tensor(sorted(list(kcousin_indices)), dtype=torch.int32)\n",
    "        return kcousin_indices\n",
    "    else:\n",
    "        return kcousin_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.362320Z",
     "start_time": "2019-08-07T18:30:06.722Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and not e:\n",
    "    random_source_wordform\n",
    "    getKcousin_idxs_of_wordform(Ws_t.index(random_source_wordform), 2, 'ndarray', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.362992Z",
     "start_time": "2019-08-07T18:30:06.733Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and not e:\n",
    "    random_source_prefix\n",
    "    getKcousin_idxs_of_prefix(Ps_t.index(random_source_prefix), 2, 'ndarray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.363650Z",
     "start_time": "2019-08-07T18:30:06.744Z"
    }
   },
   "outputs": [],
   "source": [
    "def pXhat0fX0i_pxtn_by_rc(produced_prefix_idx, x0k_cm, c_idx, m = 50, asType='ndarray', parallel=False):\n",
    "    kCousin_idxs = getKcousin_idxs_of_prefix(produced_prefix_idx, k, 'ndarray')\n",
    "    \n",
    "    if x0k_cm is None:\n",
    "        x0k_cm = prefixTox0kCM(Ps_t[produced_prefix_idx])\n",
    "    \n",
    "    if parallel:\n",
    "        estimates = par(delayed(pXhat0fX0i_pxtn)(w_idx, x0k_cm, c_idx, m=m) for w_idx in kCousin_idxs)\n",
    "        estimates = np.array(list(estimates))\n",
    "    else:\n",
    "        estimates = np.array([pXhat0fX0i_pxtn(w_idx, x0k_cm, c_idx, m=m) for w_idx in kCousin_idxs])\n",
    "\n",
    "    if asType == 'ndarray':\n",
    "        return kCousin_idxs, estimates\n",
    "    elif asType == 'torch':\n",
    "#         return torch.tensor(kCousin_idxs), torch.tensor(estimates)\n",
    "        return torch.from_numpy(kCousin_idxs), torch.from_numpy(estimates)\n",
    "    else:\n",
    "        return tuple(kCousin_idxs), tuple(estimates)\n",
    "\n",
    "def pXhat0fX0i_pxtn_by_wc(produced_wordform_idx, x0k_cm, c_idx, m = 50, asType='ndarray', parallel=False):\n",
    "    equivalent_prefix_idx = Ps_t.index(Ws_t[produced_wordform_idx])\n",
    "    return pXhat0fX0i_pxtn_by_rc(equivalent_prefix_idx, x0k_cm, c_idx, m, asType, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.364339Z",
     "start_time": "2019-08-07T18:30:06.760Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefix\n",
    "random_source_prefix_idx = Ps_t.index(random_source_prefix)\n",
    "random_source_prefix_x0kcm = prefixTox0kCM(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.364987Z",
     "start_time": "2019-08-07T18:30:06.773Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and not e:\n",
    "    print(f'x0k = {random_source_prefix}')\n",
    "    print(f'c = {random_context}')\n",
    "    cousin_idxs, post_probs = pXhat0fX0i_pxtn_by_rc(random_source_prefix_idx, random_source_prefix_x0kcm, random_context_idx)\n",
    "    print(f'({k}-cousin W-hat, p(W-hat|X0k = {random_source_prefix}, C = {random_context}))')\n",
    "    [(Ws_t[cousin_idx], post_probs[i]) for i, cousin_idx in enumerate(cousin_idxs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks: Calculate results for groups of input prefixes by context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.365626Z",
     "start_time": "2019-08-07T18:30:06.786Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation volume organization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Section 6.2 for explanation of the \"cases\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.366257Z",
     "start_time": "2019-08-07T18:30:06.805Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation volume organization 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Section 6.2 for explanation of the \"cases\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slices: Calculate $\\{p(\\widehat{W} = w^* | W = w^*, C = c)\\}$ for all $W$ and for a single given $c$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.366895Z",
     "start_time": "2019-08-07T18:30:06.825Z"
    }
   },
   "outputs": [],
   "source": [
    "{w for w in Ws_t if ds2t(w)[0] != leftEdge or ds2t(w)[-1] != rightEdge}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.367536Z",
     "start_time": "2019-08-07T18:30:06.836Z"
    }
   },
   "outputs": [],
   "source": [
    "Ws_t.index('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉')\n",
    "len(ds2t('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉')) #19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.368215Z",
     "start_time": "2019-08-07T18:30:06.849Z"
    }
   },
   "outputs": [],
   "source": [
    "len(CMsByLengthByWordformIndex_torch)\n",
    "CMsByLengthByWordformIndex_torch[19][6304].shape\n",
    "CMsByLengthByWordformIndex_torch[19][6304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.368867Z",
     "start_time": "2019-08-07T18:30:06.879Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ds2t('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ'))\n",
    "len(dsToKfactorSequence(3, '⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ'))\n",
    "prefixTox0kCM('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ').shape\n",
    "\n",
    "len(ds2t('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n'))\n",
    "len(dsToKfactorSequence(3, '⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n'))\n",
    "prefixTox0kCM('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n').shape\n",
    "\n",
    "len(ds2t('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉'))\n",
    "len(dsToKfactorSequence(3, '⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉'))\n",
    "prefixTox0kCM('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉').shape\n",
    "\n",
    "len(ds2t('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉'))\n",
    "len(dsToKfactorSequence(3, '⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉'))\n",
    "prefixTox0kCM('⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.369467Z",
     "start_time": "2019-08-07T18:30:06.893Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ds2t(\"⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n\"))\n",
    "prefixTox0kCM(\"⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n\").shape\n",
    "prefixTox0kCM(\"⋊.t.ɛ.l.ɪ.k.ə.m.j.u.n.ə.k.eɪ.ʃ.ɪ.n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.370159Z",
     "start_time": "2019-08-07T18:30:06.904Z"
    }
   },
   "outputs": [],
   "source": [
    "# CMsByLengthByWordformIndex_torch\n",
    "# X0f_CMs_by_wordform_index = [wordformTox0kCM(w, w_idx) for w_idx, w in enumerate(Ws_t)]\n",
    "# X0f_CMs_by_wordform_index_torch = torch.tensor([wordformTox0kCM(w, w_idx) for w_idx, w in enumerate(Ws_t)])\n",
    "# X0f_CMs_by_wordform_index_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.370768Z",
     "start_time": "2019-08-07T18:30:06.916Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_wstar_wstar_pxtn_by_wc(produced_wordform_idx, x0f_cm, c_idx, m = 50):\n",
    "    return pXhat0fX0f_pxtn(produced_wordform_idx, x0f_cm, c_idx, m = m)\n",
    "#     return pXhat0fX0i_pxtn(produced_wordform_idx, x0f_cm, c_idx, m = m)\n",
    "\n",
    "def p_wstar_wstar_pxtn_by_c(c_idx, m, asType='ndarray', parallel=True):\n",
    "    if parallel:\n",
    "        estimates = np.array(list(par(delayed(p_wstar_wstar_pxtn_by_wc)(w_idx, \n",
    "                                                                        wordformTox0fCM(w_idx=w_idx),\n",
    "                                                                        c_idx,\n",
    "                                                                        m)\n",
    "                                      for w_idx in range(len(Ws_t)))))\n",
    "    else:\n",
    "        estimates = np.array([p_wstar_wstar_pxtn_by_wc(w_idx, \n",
    "                                                       wordformTox0fCM(w_idx=w_idx),\n",
    "                                                       c_idx,\n",
    "                                                       m)\n",
    "                             for w_idx in np.arange(len(Ws_t))])\n",
    "    \n",
    "    if asType =='ndarray':\n",
    "        return estimates\n",
    "    elif asType == 'torch':\n",
    "        return torch.tensor(estimates)\n",
    "    else:\n",
    "        return tuple(estimates)\n",
    "    \n",
    "def p_wstar_wstar_pxt_by_wc(produced_wordform_idx, x0f_cm, c_idx, m = 50):\n",
    "    return pXhat0fX0f_pxt(produced_wordform_idx, x0f_cm, c_idx, m = m)\n",
    "#     return pXhat0fX0i_pxtn(produced_wordform_idx, x0f_cm, c_idx, m = m)\n",
    "\n",
    "def p_wstar_wstar_pxt_by_c(c_idx, m, asType='torch', parallel=True):\n",
    "    if parallel:\n",
    "        estimates = torch.tensor(list(par(delayed(p_wstar_wstar_pxt_by_wc)(w_idx, \n",
    "                                                                           wordformTox0fCM(w_idx=w_idx),\n",
    "                                                                           c_idx,\n",
    "                                                                           m)\n",
    "                                          for w_idx in range(len(Ws_t)))))\n",
    "    else:\n",
    "        estimates = torch.tensor([p_wstar_wstar_pxt_by_wc(w_idx,\n",
    "                                                          wordformTox0fCM(w_idx=w_idx),\n",
    "                                                          c_idx,\n",
    "                                                          m)\n",
    "                                  for w_idx in np.arange(len(Ws_t))])\n",
    "    \n",
    "    if asType =='ndarray':\n",
    "        return estimates.numpy()\n",
    "    elif asType == 'torch':\n",
    "        return estimates\n",
    "    else:\n",
    "        return tuple(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.371462Z",
     "start_time": "2019-08-07T18:30:06.931Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_wordform\n",
    "random_source_wordform_idx = Ws_t.index(random_source_wordform)\n",
    "random_source_wordform_x0f_cm = wordformTox0fCM(random_source_wordform, random_source_wordform_idx)\n",
    "random_source_wordform_x0f_cm.shape\n",
    "random_source_wordform_x0f_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.372096Z",
     "start_time": "2019-08-07T18:30:06.942Z"
    }
   },
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.372733Z",
     "start_time": "2019-08-07T18:30:06.955Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.373364Z",
     "start_time": "2019-08-07T18:30:06.966Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_context_idxs = set(map(lambda c: Cs_t.index(c),\n",
    "                                  choices(Cs_t, k=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.373989Z",
     "start_time": "2019-08-07T18:30:06.977Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # random_c_idx1 = choice(random_context_idxs); Cs_t[random_c_idx1]\n",
    "    random_c_idx1 = Cs_t.index('yeah philadelphia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.374622Z",
     "start_time": "2019-08-07T18:30:06.989Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # 1m56s = 116s = 9172c/116s = 79.069 cps / kotoba\n",
    "    random_slice1 = p_wstar_wstar_pxtn_by_c(random_c_idx1, m=50, asType='ndarray', parallel=True)\n",
    "    random_slice1.shape\n",
    "    random_slice1.nbytes / 1e6\n",
    "    random_slice1.nbytes / 1e9\n",
    "    del random_slice1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.375228Z",
     "start_time": "2019-08-07T18:30:07.001Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # random_c_idx2 = choice(random_context_idxs); Cs_t[random_c_idx2]\n",
    "    random_c_idx2 = Cs_t.index('<rem> little')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.375920Z",
     "start_time": "2019-08-07T18:30:07.014Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    # 1m49s = 109s = 9172c/109s = 84.15 cps / kotoba\n",
    "    random_slice2 = p_wstar_wstar_pxtn_by_c(random_c_idx2, m=50, asType='ndarray', parallel=True)\n",
    "    random_slice2.shape\n",
    "    random_slice2.nbytes / 1e6\n",
    "    random_slice2.nbytes / 1e9\n",
    "    del random_slice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.376523Z",
     "start_time": "2019-08-07T18:30:07.026Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.377218Z",
     "start_time": "2019-08-07T18:30:07.037Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    J = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.377857Z",
     "start_time": "2019-08-07T18:30:07.048Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_c_idx3 = choice(random_context_idxs); Cs_t[random_c_idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.378492Z",
     "start_time": "2019-08-07T18:30:07.059Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_slice3 = p_wstar_wstar_pxtn_by_c(random_c_idx3, m=50, asType='ndarray', parallel=True)\n",
    "    random_slice3.shape\n",
    "    random_slice3.nbytes / 1e6\n",
    "    random_slice3.nbytes / 1e9\n",
    "    del random_slice3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.379126Z",
     "start_time": "2019-08-07T18:30:07.070Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    J = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.379747Z",
     "start_time": "2019-08-07T18:30:07.081Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_c_idx4 = choice(random_context_idxs); Cs_t[random_c_idx4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.380381Z",
     "start_time": "2019-08-07T18:30:07.094Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_slice4 = p_wstar_wstar_pxtn_by_c(random_c_idx4, m=50, asType='ndarray', parallel=True)\n",
    "    random_slice4.shape\n",
    "    random_slice4.nbytes / 1e6\n",
    "    random_slice4.nbytes / 1e9\n",
    "    del random_slice4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.381010Z",
     "start_time": "2019-08-07T18:30:07.105Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    J = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.381640Z",
     "start_time": "2019-08-07T18:30:07.116Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_c_idx5 = choice(random_context_idxs); Cs_t[random_c_idx5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.382276Z",
     "start_time": "2019-08-07T18:30:07.127Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_slice5 = p_wstar_wstar_pxtn_by_c(random_c_idx5, m=50, asType='ndarray', parallel=True)\n",
    "    random_slice5.shape\n",
    "    random_slice5.nbytes / 1e6\n",
    "    random_slice5.nbytes / 1e9\n",
    "    del random_slice5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.382924Z",
     "start_time": "2019-08-07T18:30:07.137Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_c_idx6 = random_c_idx5\n",
    "\n",
    "    random_slice6 = p_wstar_wstar_pxt_by_c(random_c_idx6, m=50, asType='torch', parallel=True)\n",
    "    random_slice6.shape\n",
    "    torch_nbytes(random_slice6) / 1e6\n",
    "    torch_nbytes(random_slice6) / 1e9\n",
    "    del random_slice6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.383560Z",
     "start_time": "2019-08-07T18:30:07.148Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_c_idx6 = random_c_idx5\n",
    "\n",
    "    random_slice6 = p_wstar_wstar_pxt_by_c(random_c_idx6, m=50, asType='torch', parallel=True)\n",
    "    random_slice6.shape\n",
    "    torch_nbytes(random_slice6) / 1e6\n",
    "    torch_nbytes(random_slice6) / 1e9\n",
    "    del random_slice6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.384196Z",
     "start_time": "2019-08-07T18:30:07.159Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    J = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slices: Calculate $\\{p(\\widehat{W} = w^* | W = w^*, C = c)\\}$ for all $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.384820Z",
     "start_time": "2019-08-07T18:30:07.172Z"
    }
   },
   "outputs": [],
   "source": [
    "# pXhat0fX0i_ptc(xhat0f_idx, x0k_CM, m = 50, target_contexts_per_batch=750, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of calculations necessary under different calculation criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the full distribution that could be calculated is $$p(\\widehat{X}_0^f | X_0^k, C) = p(\\widehat{W} | R, C)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.385419Z",
     "start_time": "2019-08-07T18:30:07.198Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Ws_t)\n",
    "len(Ps_t)\n",
    "len(Cs_t)\n",
    "\"{0:,}\".format( len(Ws_t) * len(Ps_t) * len(Cs_t) )\n",
    "\"{0:.2E}\".format( len(Ws_t) * len(Ps_t) * len(Cs_t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chosen value of $k$, there are this many $(w,r)$ pairs to be calculated (for every $c$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.386109Z",
     "start_time": "2019-08-07T18:30:07.212Z"
    }
   },
   "outputs": [],
   "source": [
    "# cousin_mats[k].shape\n",
    "# sum([cousin_mats[k][i,:].nnz for i in range(len(Ps))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.386758Z",
     "start_time": "2019-08-07T18:30:07.223Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and not e:\n",
    "    print(f'k = {k}')\n",
    "\n",
    "    cousin_mats[k].shape\n",
    "\n",
    "    def calc_count(i):\n",
    "        return cousin_mats[k][i,:].nnz\n",
    "\n",
    "    num_seq_pairs_case1 = sum(list(par(delayed(calc_count)(i) for i in range(len(Ps)))))\n",
    "    num_calcs_case1 = len(Cs_t) * num_seq_pairs_case1\n",
    "\n",
    "    print('|(w,r)| pairs to do calculations for every c = {0:,}'.format( num_seq_pairs_case1 ))\n",
    "    print(\"|(w,r)| pairs there would be without the k-edit-distance-restriction = {0:,}\".format( len(Ws) * len(Ps) ))\n",
    "    print(f'|Cs| = {len(Cs_t)}')\n",
    "    print('Est. of total number of calculations to do = {0:,} = {0:.2E}'.format(num_calcs_case1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $r$ is `False`, then the dimensions and number of calculations reduce to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.387395Z",
     "start_time": "2019-08-07T18:30:07.242Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Ws_t)\n",
    "len(Ws_t)\n",
    "len(Cs_t)\n",
    "\"{0:,}\".format( len(Ws_t) * len(Ws_t) * len(Cs_t) )\n",
    "\"{0:.2E}\".format( len(Ws_t) * len(Ws_t) * len(Cs_t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $r$ is `False`, then for the chosen value of $k$, there are this many $(w',w)$ pairs to be calculated (for every $c$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.388034Z",
     "start_time": "2019-08-07T18:30:07.255Z"
    }
   },
   "outputs": [],
   "source": [
    "if r or not e:\n",
    "    print(f'k = {k}')\n",
    "\n",
    "    cousin_mats[k].shape\n",
    "\n",
    "    def calc_count(i):\n",
    "        if Ps_t[i] in Ws:\n",
    "            return cousin_mats[k][i,:].nnz\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    num_seq_pairs_case2 = sum(list(par(delayed(calc_count)(i) for i in range(len(Ps)))))\n",
    "    num_calcs_case2 = len(Cs_t) * num_seq_pairs_case2\n",
    "\n",
    "    print(\"|(w',w)| pairs to do calculations for every c = {0:,}\".format( num_seq_pairs_case2 ))\n",
    "    print(\"|(w',w)| pairs there would be without the k-edit-distance-restriction = {0:,}\".format( len(Ws) * len(Ws) ))\n",
    "    print(f'|Cs| = {len(Cs_t)}')\n",
    "    print('Est. of total number of calculations to do = {0:,} = {0:.2E}'.format(num_calcs_case2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T01:43:02.693343Z",
     "start_time": "2019-07-23T01:43:02.435505Z"
    }
   },
   "source": [
    "If $r$ is `False` and $e$ is `True`, then the dimensions and number of calculations reduce to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.388629Z",
     "start_time": "2019-08-07T18:30:07.272Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Ws)\n",
    "len(Cs_t)\n",
    "num_calcs_case3 = len(Ws_t) * len(Cs_t)\n",
    "\"{0:,}\".format( num_calcs_case3 )\n",
    "\"{0:.2E}\".format( num_calcs_case3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine dimensions and cells to be calculated vs. ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.389322Z",
     "start_time": "2019-08-07T18:30:07.285Z"
    }
   },
   "outputs": [],
   "source": [
    "assert (k != 0 and r) or (k != 0 and (not r) and (not e)) or (k == 0 and (not r) and e) or (k > 0 and (not r) and e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Only calculate full source wordforms rather than all source prefixes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.389960Z",
     "start_time": "2019-08-07T18:30:07.300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"r = {r}\")\n",
    "\n",
    "if r:\n",
    "    source_sequences = Ps_t\n",
    "    print(\"Performing calculations for all source prefixes.\")\n",
    "else:\n",
    "    source_sequences = Ws_t\n",
    "    print(\"Performing calculations only for all complete source wordforms (and *not* all prefixes thereof).\")\n",
    "\n",
    "Ss_t = source_sequences    \n",
    "\n",
    "print(f\"|source_sequences| = {len(source_sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suboption: Given $\\neg r$ and a full source wordform $w^*$, only calculate posterior probability $p(\\widehat{W} = w^* | W = w^*, C = c)$, $\\forall w^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.390562Z",
     "start_time": "2019-08-07T18:30:07.313Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"¬r ∧ e = {not r} ∧ {e} = {(not r) and e}\")\n",
    "\n",
    "if (not r) and e:\n",
    "    print(\"Performing calculations only for those cases where the reconstructed wordform exactly matches the source wordform.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Calculations not restricted only to exact reconstructed wordform matches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suboption: Restrict calculations only to (source sequence, reconstruction sequence) pairs within edit distance $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.391264Z",
     "start_time": "2019-08-07T18:30:07.327Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"k = {k}\")\n",
    "\n",
    "if r or ((not r) and (not e)):\n",
    "    print(f\"Restricting calculations only to (source sequence, reconstructed sequence) pairs within edit distance {k}.\")\n",
    "elif (not r) and e:\n",
    "    print(f\"¬r ∧ e is equivalent to restricting calculations only to (source sequence, reconstructed sequence) pairs within edit distance 0.\")\n",
    "else:\n",
    "    print(\"No restrictions on (source sequence, reconstructed sequence) pairs to be calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altogether..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.391866Z",
     "start_time": "2019-08-07T18:30:07.342Z"
    }
   },
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.392573Z",
     "start_time": "2019-08-07T18:30:07.352Z"
    }
   },
   "outputs": [],
   "source": [
    "if r and (0 < k <= 4):\n",
    "    case = 1    \n",
    "    myShape = (len(Ws_t), len(Ps-t), len(Cs_t))\n",
    "    my_num_calcs = num_calcs_case1\n",
    "    \n",
    "    my_type = f'pW_RC_k{k}'\n",
    "    status = '_unnormalized'\n",
    "\n",
    "if (not r) and (0 < k <= 4) and (not e):\n",
    "    case = 2\n",
    "    myShape = (len(Ws_t), len(Ws_t), len(Cs_t))\n",
    "    my_num_calcs = num_calcs_case2\n",
    "    \n",
    "    my_type = f'pW_WC_k{k}'\n",
    "    status = '_unnormalized'\n",
    "    \n",
    "if (not r) and e:\n",
    "    case = 3\n",
    "#     myShape = (len(Ws), len(Ws), len(Cs))\n",
    "    myShape = (len(Ws_t), len(Cs_t))\n",
    "    my_num_calcs = num_calcs_case3\n",
    "\n",
    "    my_type = f'pW_WC_e'\n",
    "    status = ''\n",
    "\n",
    "print(f'Case:\\n\\t{case}')\n",
    "print(f'Shape of matrix:\\n\\t{myShape}')\n",
    "print('Num elements:')\n",
    "print(\"\\t{0:,} = {0:.2E}\".format(prod(myShape)))\n",
    "print('Num calculations:')\n",
    "print(\"\\t{0:,} = {0:.2E}\".format(my_num_calcs))\n",
    "print('Est. size in GB of result:')\n",
    "print(\"\\t{0:,} = {0:.2E}\".format(my_num_calcs * 8 / 1e9))\n",
    "print(\"Density:\")\n",
    "print(\"\\t{0:,}\".format(my_num_calcs / prod(myShape)))    \n",
    "\n",
    "print('Result suffix:')\n",
    "my_fn_suffix = my_type + status\n",
    "print(f\"\\t{my_fn_suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.393207Z",
     "start_time": "2019-08-07T18:30:07.364Z"
    }
   },
   "outputs": [],
   "source": [
    "abort_threshold_GB = 250\n",
    "size_est_GB = (my_num_calcs * 8 / 1e9)\n",
    "assert size_est_GB <= abort_threshold_GB, \"Estimated size of resulting matrix is ≥ {1} GB:\\n\\tEst. size = {0:,}GB = {0:.2E}GB\\n\\tAborting calculation.\".format(size_est_GB, abort_threshold_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.393833Z",
     "start_time": "2019-08-07T18:30:07.375Z"
    }
   },
   "outputs": [],
   "source": [
    "# if exists(o + my_fn_suffix):\n",
    "#     unnormalized_posterior = np.load(o + my_fn_suffix + '.npy',\n",
    "#                                      allow_pickle = False)\n",
    "# assert not exists(o + my_fn_suffix + '.npy'), 'Unnormalized distribution already found at filepath \\n\\t{0}'.format( o + my_fn_suffix + '.npy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.394462Z",
     "start_time": "2019-08-07T18:30:07.385Z"
    }
   },
   "outputs": [],
   "source": [
    "# unnormalized_posterior_torch = np.zeros(myShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: $r \\land (0 < k \\leq 4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: $\\neg r \\land (0 < k \\leq 4) \\land \\neg e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: $\\neg r \\land e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.395096Z",
     "start_time": "2019-08-07T18:30:07.417Z"
    }
   },
   "outputs": [],
   "source": [
    "myShape\n",
    "# blockShape = (myShape[0], 1) #every context is a slice\n",
    "blockShape = (1, myShape[1]) #every source wordform is a slice\n",
    "blockShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.395724Z",
     "start_time": "2019-08-07T18:30:07.432Z"
    }
   },
   "outputs": [],
   "source": [
    "myShape #dim of eventual tensor representing desired portion of distribution\n",
    "\n",
    "# segWordDomain = (wStart, wEnd); segWordDomain\n",
    "segWordDomain = (0, myShape[0]-1); segWordDomain\n",
    "contextDomain = (0, myShape[1]-1); contextDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.396344Z",
     "start_time": "2019-08-07T18:30:07.449Z"
    }
   },
   "outputs": [],
   "source": [
    "segWordDomainStr = '_' + 'w' + '.' + str(wStart) + '-' + str(wEnd); segWordDomainStr\n",
    "domainRegionStr = segWordDomainStr; domainRegionStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.396977Z",
     "start_time": "2019-08-07T18:30:07.460Z"
    }
   },
   "outputs": [],
   "source": [
    "# dbName = my_fn_suffix + domainRegionStr; dbName\n",
    "dbName = my_fn_suffix# + domainRegionStr; dbName\n",
    "dbName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.397572Z",
     "start_time": "2019-08-07T18:30:07.473Z"
    }
   },
   "outputs": [],
   "source": [
    "dom = tiledb.Domain(tiledb.Dim(name=\"segWord\", domain=segWordDomain, tile=blockShape[0], dtype=np.uint32),\n",
    "                    tiledb.Dim(name=\"context\", domain=contextDomain, tile=blockShape[1], dtype=np.uint32))\n",
    "\n",
    "schema = tiledb.ArraySchema(domain=dom, sparse=False,\n",
    "                            attrs=[tiledb.Attr(name=dbName, dtype=np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.398259Z",
     "start_time": "2019-08-07T18:30:07.488Z"
    }
   },
   "outputs": [],
   "source": [
    "o\n",
    "my_fn_suffix\n",
    "# domainRegionStr\n",
    "dbName\n",
    "o + '.' + dbName\n",
    "array_name = o + '.' + dbName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.398912Z",
     "start_time": "2019-08-07T18:30:07.501Z"
    }
   },
   "outputs": [],
   "source": [
    "if tiledb.object_type(array_name) == \"array\":\n",
    "        print(f\"Array with name\\n\\t{array_name}\\nalready exists.\")\n",
    "else:\n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "    print(f'Array with name\\n\\t{array_name}\\ncreated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.399539Z",
     "start_time": "2019-08-07T18:30:07.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# rm -r 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_WC_e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.400139Z",
     "start_time": "2019-08-07T18:30:07.527Z"
    }
   },
   "outputs": [],
   "source": [
    "# will also work if if given a range of indices and a range of slices...\n",
    "# def slice_writer(slice_context_index, context_slice, A):\n",
    "# # #     with tiledb.SparseArray(arr_name, mode='w') as A:\n",
    "#         A[:,slice_context_index] = context_slice\n",
    "\n",
    "def slice_writer(slice_word_index, word_slice, A):\n",
    "# #     with tiledb.SparseArray(arr_name, mode='w') as A:\n",
    "        A[slice_word_index,:] = word_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.400835Z",
     "start_time": "2019-08-07T18:30:07.540Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tiledb.Config()\n",
    "# config[\"sm.consolidation.steps\"] = 10\n",
    "config[\"sm.consolidation.steps\"] = 600\n",
    "config[\"sm.consolidation.step_min_frags\"] = 2\n",
    "config[\"sm.consolidation.step_max_frags\"] = 20\n",
    "# tiledb.consolidate(array_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.401473Z",
     "start_time": "2019-08-07T18:30:07.550Z"
    }
   },
   "outputs": [],
   "source": [
    "# if cStart != '' and cEnd != '':\n",
    "#     print(f'Context block start and end given: ({cStart}, {cEnd})')\n",
    "#     blockRange = np.arange(cStart, cEnd)\n",
    "#     assert 0 <= cStart <= len(Cs_t), f'0<= cStart <= len(Cs_t) does not hold: {cStart} vs. {len(Cs_t)}'\n",
    "#     assert 0 <= cEnd <= len(Cs_t), f'0<= cEnd <= len(Cs_t) does not hold: {cEnd} vs. {len(Cs_t)}'\n",
    "# else:\n",
    "#     print(f'Calculating all indices from 0 to len(Cs_t) = {len(Cs_t)}')\n",
    "#     blockRange = np.arange(len(Cs_t))\n",
    "\n",
    "\n",
    "# for c_idx in tqdm(blockRange):\n",
    "#     context_slice = p_wstar_wstar_pxtn_by_c(c_idx, m=50, asType='ndarray', parallel=True)\n",
    "\n",
    "#     with tiledb.SparseArray(array_name, mode='w') as A:\n",
    "#         slice_writer(c_idx, context_slice, A)\n",
    "        \n",
    "#     if c_idx % 100 == 0:\n",
    "#         tiledb.consolidate(config, uri=array_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.402108Z",
     "start_time": "2019-08-07T18:30:07.562Z"
    }
   },
   "outputs": [],
   "source": [
    "if wStart != '' and wEnd != '':\n",
    "    print(f'Word block start and end given: ({wStart}, {wEnd})')\n",
    "    blockRange = np.arange(wStart, wEnd)\n",
    "    assert 0 <= wStart < len(Ws_t), f'0<= wStart < len(Ws_t) does not hold: {wStart} vs. {len(Ws_t)}'\n",
    "    assert 0 <= wEnd < len(Ws_t), f'0<= wEnd < len(Ws_t) does not hold: {wEnd} vs. {len(Ws_t)}'\n",
    "else:\n",
    "    print(f'Calculating all indices from 0 to len(Ws_t)-1 = {len(Ws_t)}-1 = {len(Ws_t)-1}')\n",
    "    blockRange = np.arange(len(Ws_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.402740Z",
     "start_time": "2019-08-07T18:30:07.574Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.403393Z",
     "start_time": "2019-08-07T18:30:08.717Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logCondition(w_idx):\n",
    "    if len(blockRange) < 50:\n",
    "        return True\n",
    "    elif len(blockRange) < 100:\n",
    "        return w_idx % 10 == 0\n",
    "#     elif len(blockRange) < 250:\n",
    "    else:\n",
    "        return w_idx % 25\n",
    "\n",
    "with tiledb.DenseArray(array_name, mode='w') as A:\n",
    "    for w_idx in tqdm(blockRange, total=len(blockRange)):\n",
    "#         word_slice = p_wstar_wstar_pxtn_by_c(c_idx, m=50, asType='ndarray', parallel=True)\n",
    "        \n",
    "        if logCondition(w_idx):\n",
    "            print('')\n",
    "            stampedMemNote(f'w_idx = {w_idx}')\n",
    "            stampedNote('Calculating word slice...')\n",
    "\n",
    "        word_slice = pXhat0fX0i_ptc(w_idx, wordformTox0kCM(w_idx=w_idx), \n",
    "                                    m = n, target_contexts_per_batch=b, \n",
    "                                    parallel=l, use_gpu=g).numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if logCondition(w_idx):\n",
    "            stampedMemNote('\\tWriting to disk...')\n",
    "        slice_writer(w_idx, word_slice, A)\n",
    "        if logCondition(w_idx):\n",
    "            stampedMemNote('\\tWrote to disk.')\n",
    "        \n",
    "        if logCondition(w_idx) and consolidation_enabled:\n",
    "            stampedMemNote('\\tConsolidating...')\n",
    "            tiledb.consolidate(uri=array_name, config = config)\n",
    "            stampedMemNote('\\tConsolidated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.404021Z",
     "start_time": "2019-08-07T18:30:08.726Z"
    }
   },
   "outputs": [],
   "source": [
    "if g:\n",
    "    torch.cuda.max_memory_allocated(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:02.404625Z",
     "start_time": "2019-08-07T18:30:08.738Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_md = {'W':{'from fp':f\"from W dimension of {p}\",\n",
    "               'changes':'sorted full wordforms',\n",
    "               'size':len(Ws_t)},\n",
    "          'C':{'from fp':s,\n",
    "               'changes':'sorted contexts',\n",
    "               'size':len(Cs_t)}}\n",
    "exportMatrixMetadata(md_fp=array_name + '_metadata.json', \n",
    "                     matrix_fp=array_name, \n",
    "                     matrix=None, \n",
    "                     dim_md=dim_md, \n",
    "                     step_name='Step 5b', \n",
    "                     nb_name='Calculate segmental posterior given segmental wordform + context', \n",
    "                     other_md = {})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
