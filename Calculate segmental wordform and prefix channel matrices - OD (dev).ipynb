{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.066145Z",
     "start_time": "2019-09-12T01:27:50.060984Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-/-pytorch-representations\" data-toc-modified-id=\"numpy-/-pytorch-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> / <code>pytorch</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Functions-for-a-single-prefix-or-wordform\" data-toc-modified-id=\"Functions-for-a-single-prefix-or-wordform-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Functions for a single prefix or wordform</a></span></li><li><span><a href=\"#Functions-for-calculating-CMs-for-all-prefixes-of-a-given-length-(in-batches)\" data-toc-modified-id=\"Functions-for-calculating-CMs-for-all-prefixes-of-a-given-length-(in-batches)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Functions for calculating CMs for all prefixes of a given length (in batches)</a></span></li><li><span><a href=\"#Functions-for-calculating-CMs-for-all-wordforms-of-a-given-length-(in-batches)\" data-toc-modified-id=\"Functions-for-calculating-CMs-for-all-wordforms-of-a-given-length-(in-batches)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Functions for calculating CMs for all wordforms of a given length (in batches)</a></span></li><li><span><a href=\"#Functions-for-creating-desired-CM-stacks\" data-toc-modified-id=\"Functions-for-creating-desired-CM-stacks-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Functions for creating desired CM stacks</a></span></li><li><span><a href=\"#trash\" data-toc-modified-id=\"trash-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span><em>trash</em></a></span></li></ul></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices\" data-toc-modified-id=\"Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Segment sequence (all prefixes or just wordforms) channel matrices</a></span></li><li><span><a href=\"#Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)\" data-toc-modified-id=\"Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$).\n",
    "\n",
    "#FIXME update to reflect other exports (including the channel matrix stacks acccctually used in subsequent notebooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.112627Z",
     "start_time": "2019-09-12T01:27:50.075231Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.674995Z",
     "start_time": "2019-09-12T01:27:50.115137Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.682987Z",
     "start_time": "2019-09-12T01:27:50.678009Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "# c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "# c = \"CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json\"\n",
    "c = \"CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json\"\n",
    "# c = \"CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json\"\n",
    "\n",
    "b = ''\n",
    "# b = \"CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy\"\n",
    "b = 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy'\n",
    "# b = 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy'\n",
    "\n",
    "w = ''\n",
    "# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "# w = 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "w = 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "# w = 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_OD'\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_OD'\n",
    "o = 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_OD'\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_OD'\n",
    "\n",
    "p = ''\n",
    "# p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.698794Z",
     "start_time": "2019-09-12T01:27:50.685032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_md = b + \"_metadata.json\"\n",
    "b_o = b.split(\".npy\")[0] + \"Y012s.txt\"\n",
    "\n",
    "b_md\n",
    "b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.708187Z",
     "start_time": "2019-09-12T01:27:50.701698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'matrix shape': [57798, 10540],\n",
       " 'Produced in step': 'Step 4d',\n",
       " 'Produced in notebook': 'Calculate observation distribution given channel models',\n",
       " 'X012s': {'from fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       "  'changes': 'sorted',\n",
       "  'size': 10540},\n",
       " 'Y012s': {'from fp': {'preview': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       "   'postview': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       "   'center': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       "  'changes': 'sorted',\n",
       "  'exported fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       "  'size': 57798}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importDict(b_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.713420Z",
     "start_time": "2019-09-12T01:27:50.710713Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.718214Z",
     "start_time": "2019-09-12T01:27:50.715082Z"
    }
   },
   "outputs": [],
   "source": [
    "# if p == '':\n",
    "#     r = False\n",
    "# else:\n",
    "#     r = True\n",
    "#     print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.726757Z",
     "start_time": "2019-09-12T01:27:50.720607Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:50.731434Z",
     "start_time": "2019-09-12T01:27:50.728939Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:51.773167Z",
     "start_time": "2019-09-12T01:27:50.733036Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:51.778860Z",
     "start_time": "2019-09-12T01:27:51.775365Z"
    }
   },
   "outputs": [],
   "source": [
    "from funcy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:51.783712Z",
     "start_time": "2019-09-12T01:27:51.780831Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:51.790784Z",
     "start_time": "2019-09-12T01:27:51.785648Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:52.119156Z",
     "start_time": "2019-09-12T01:27:51.792724Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:52.123362Z",
     "start_time": "2019-09-12T01:27:52.121011Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.128074Z",
     "start_time": "2019-09-12T01:27:52.125083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "TITAN RTX\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "TITAN RTX\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.get_device_name(1))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.134896Z",
     "start_time": "2019-09-12T01:27:53.130045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu\n",
    "# my_device = gpu\n",
    "torch.cuda.set_device(1)\n",
    "my_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.140614Z",
     "start_time": "2019-09-12T01:27:53.136406Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "# my_ft = cuda_ft\n",
    "# my_dt = cuda_dt\n",
    "\n",
    "my_tt = torch.float64\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.145725Z",
     "start_time": "2019-09-12T01:27:53.142300Z"
    }
   },
   "outputs": [],
   "source": [
    "testing = False\n",
    "benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.270320Z",
     "start_time": "2019-09-12T01:27:53.147446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G        992M         63G        1.8M        123G        185G\r\n",
      "Swap:           18G         15M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.738437Z",
     "start_time": "2019-09-12T01:27:53.273603Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.748831Z",
     "start_time": "2019-09-12T01:27:53.739915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleRightEdgeTriphs = {x012 for x012 in set(conditions(p3Y1X012)) if rightEdge + '.' + rightEdge in x012}\n",
    "doubleRightEdgeTriphs\n",
    "# assert len(doubleRightEdgeTriphs) == 0\n",
    "\n",
    "if len(doubleRightEdgeTriphs) > 0:\n",
    "    print('Restoring p3Y1X012...')\n",
    "    p3Y1X012 = condProbDistAsDicts(p3Y1X012)\n",
    "    \n",
    "    for x012 in doubleRightEdgeTriphs:\n",
    "        del p3Y1X012[x012]\n",
    "    \n",
    "    for x012 in p3Y1X012:\n",
    "        del p3Y1X012[x012][rightEdge]\n",
    "    \n",
    "    assert areNormalized(p3Y1X012)\n",
    "    assert uniformOutcomes(p3Y1X012)\n",
    "    \n",
    "    p3Y1X012 = condDistsAsProbDists(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.756092Z",
     "start_time": "2019-09-12T01:27:53.754011Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "#     assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:53.884563Z",
     "start_time": "2019-09-12T01:27:53.758595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G        1.0G         63G        1.8M        123G        184G\r\n",
      "Swap:           18G         15M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:27:54.022283Z",
     "start_time": "2019-09-12T01:27:53.887920Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:06.624363Z",
     "start_time": "2019-09-12T01:27:54.023607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 30833\n",
      "Restoring lexicon...\n",
      "|Prefixes| = 98570\n",
      "|triphones| in lexicon = 10540\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "if rightEdge + '.' + rightEdge in lexiconTo2factors(Ws):\n",
    "    print('Restoring lexicon...')\n",
    "    \n",
    "    def restoreWordform(w):\n",
    "        w_t = ds2t(w)\n",
    "        assert w_t[-1] == rightEdge and w_t[-2] == rightEdge\n",
    "        w_t = w_t[:-1]\n",
    "        return t2ds(w_t)\n",
    "    \n",
    "    Ws = set(map(restoreWordform, Ws))\n",
    "    Ws_t = tuple(sorted(list(Ws)))\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ps)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:06.752299Z",
     "start_time": "2019-09-12T01:29:06.625881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G        1.0G         63G        1.8M        123G        184G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "del pW_V\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:06.775686Z",
     "start_time": "2019-09-12T01:29:06.755555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 10540\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:06.783980Z",
     "start_time": "2019-09-12T01:29:06.777435Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:11.726455Z",
     "start_time": "2019-09-12T01:29:06.786768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57798, 10540)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.87352736"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "57798"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pC1X012_np = np.load(b)\n",
    "pC1X012_np.shape\n",
    "pC1X012_np.nbytes / 1e9\n",
    "\n",
    "pC1X012_torch = torch.tensor(pC1X012_np)\n",
    "\n",
    "Y012s_t = importSeqs(b_o, tuple)\n",
    "Y012s = set(Y012s_t)\n",
    "len(Y012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:11.917469Z",
     "start_time": "2019-09-12T01:29:11.727887Z"
    }
   },
   "outputs": [],
   "source": [
    "assert pC1X012_np.shape[0] == len(Y012s)\n",
    "assert pC1X012_np.shape[1] == len(X012s)\n",
    "assert lexiconToInventory(Y012s) - edgeSymbols == Y1s\n",
    "assert tuple(sorted(list(Y012s))) == Y012s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:11.926266Z",
     "start_time": "2019-09-12T01:29:11.919295Z"
    }
   },
   "outputs": [],
   "source": [
    "def pC1X012_calc(y012=None, x012=None, asType = 'ndarray'):\n",
    "    if y012 is None:\n",
    "        if asType == 'ndarray':\n",
    "            return pC1X012_np[:, X012s_t.index(x012)]\n",
    "        elif asType == 'torch':\n",
    "            return torch.from_numpy(pC1X012_np[:, X012s_t.index(x012)])\n",
    "        elif asType == 'dict':\n",
    "            return {y012:pC1X012_np[Y012s_t.index(y012), X012s_t.index(x012)]\n",
    "                    for y012 in y012s_t}\n",
    "        else:\n",
    "            raise Exception(\"Acceptable asType arguments = {'dict', 'ndarray', 'torch'}\")\n",
    "    if x012 is None:\n",
    "        if asType == 'ndarray':\n",
    "            return pC1X012_np[Y012s_t.index(y012),:]\n",
    "        elif asType == 'torch':\n",
    "            return torch.from_numpy(pC1X012_np[Y012s_t.index(y012),:])\n",
    "        else:\n",
    "            raise Exception(\"Acceptable asType arguments = {'ndarray', 'torch'}\")\n",
    "    return pC1X012_np[Y012s_t.index(y012), X012s_t.index(x012)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:11.930836Z",
     "start_time": "2019-09-12T01:29:11.928084Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     channelDiphones = set(p3Y1X01.keys())\n",
    "#     print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "#     lexiconDiphones = lexiconTo2factors(Ws)\n",
    "#     unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "#     print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "#     assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "#     print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "#     X01s = lexiconDiphones\n",
    "#     assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:11.935688Z",
     "start_time": "2019-09-12T01:29:11.932410Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "#     # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "#     for x01 in p3Y1X01:\n",
    "#         p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "#     # create new conditioning events\n",
    "#     wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "#     list(wordEndDiphones)[:5]\n",
    "\n",
    "#     # create their distribution over outcomes\n",
    "#     deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "#     deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "#     # add the new wordend conditioning events to the preview distribution\n",
    "#     p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "#     p3Y1X01['aʊ.s']['s']\n",
    "#     p3Y1X01['ɑ.⋉']\n",
    "\n",
    "#     # check that everything worked\n",
    "#     for x01 in p3Y1X01:\n",
    "#         assert rightEdge in p3Y1X01[x01]\n",
    "#     #     if rightEdge not in p3Y1X01[x01]:\n",
    "#     #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "#     assert areNormalized(p3Y1X01)\n",
    "#     assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "#     channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "#     unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "#     print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "#     assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "#     #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "#     # let's trim the preview model's conditioning events\n",
    "#     p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "#     p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "#     X01s_RE = set(p3Y1X01.keys())\n",
    "#     len(X01s_RE)\n",
    "    \n",
    "# #     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` / `pytorch` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.263905Z",
     "start_time": "2019-09-12T01:29:11.937299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.272498Z",
     "start_time": "2019-09-12T01:29:12.267184Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)\n",
    "# XOHmap_torch = {k:torch.tensor(XOHmap[k])\n",
    "#                 for k in XOHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.331175Z",
     "start_time": "2019-09-12T01:29:12.274738Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)\n",
    "# X012OHmap_torch = {k:torch.tensor(X012OHmap[k])\n",
    "#                    for k in X012OHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.336099Z",
     "start_time": "2019-09-12T01:29:12.333116Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.418365Z",
     "start_time": "2019-09-12T01:29:12.338078Z"
    }
   },
   "outputs": [],
   "source": [
    "Y012map = seqsToIndexMap(Y012s)\n",
    "# Y012OHmap = seqsToOneHotMap(Y012s)\n",
    "# Y012OHmap_torch = {k:torch.tensor(Y012OHmap[k])\n",
    "#                    for k in Y012OHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.422899Z",
     "start_time": "2019-09-12T01:29:12.420372Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     X01REmap = seqsToIndexMap(X01s_RE)\n",
    "#     X01REOHs = seqMapToOneHots(X01REmap)\n",
    "#     X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "#     Y1s_RE = outcomes(p3Y1X01)\n",
    "#     len(Y1s_RE)\n",
    "#     Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "#     print(Y1s_RE - Y1s)\n",
    "\n",
    "#     Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "#     Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "#     Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "#     OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.429447Z",
     "start_time": "2019-09-12T01:29:12.424732Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     for x012 in p3Y1X012:\n",
    "#         p3Y1X012[x012].update({rightEdge:0.0})\n",
    "#         assert rightEdge in p3Y1X012[x012]\n",
    "#         assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "#     outcomes(p3Y1X012) == Y1s\n",
    "#     outcomes(p3Y1X012) == Y1s_RE\n",
    "#     areNormalized(p3Y1X012)\n",
    "#     uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.516609Z",
     "start_time": "2019-09-12T01:29:12.431404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([5459, 2190])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 10540)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10540,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap, asTorch=False):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap, asTorch=False):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    if asTorch:\n",
    "#         return torch.tensor([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "        return torch.tensor(np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq]))\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap, asTorch=False):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor(np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq]))\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap, asTorch=False):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor(np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq]))\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap, True)\n",
    "del XOHmap\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap, True)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:12.904559Z",
     "start_time": "2019-09-12T01:29:12.518994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.295078Z",
     "start_time": "2019-09-12T01:29:12.908409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 10540)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "# if r:\n",
    "#     testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "# else:\n",
    "#     testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "p3Y1X012_np.shape\n",
    "\n",
    "p3Y1X012_torch = torch.tensor(p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.301406Z",
     "start_time": "2019-09-12T01:29:13.296496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57798, 10540)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([57798, 10540])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pC1X012_np.shape\n",
    "pC1X012_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.306668Z",
     "start_time": "2019-09-12T01:29:13.303174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.87352736"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pC1X012_np.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.322070Z",
     "start_time": "2019-09-12T01:29:13.308077Z"
    }
   },
   "outputs": [],
   "source": [
    "del p3Y1X012\n",
    "# del p3Y1X012_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.673996Z",
     "start_time": "2019-09-12T01:29:13.323374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.681702Z",
     "start_time": "2019-09-12T01:29:13.677595Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "#     testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "#     p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.687588Z",
     "start_time": "2019-09-12T01:29:13.684185Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.697329Z",
     "start_time": "2019-09-12T01:29:13.689663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.s.l.ɪ.ð.ɚ.⋉'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.713046Z",
     "start_time": "2019-09-12T01:29:13.699258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.d.ɛ.s.t.ɪ.n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.718517Z",
     "start_time": "2019-09-12T01:29:13.715129Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.725922Z",
     "start_time": "2019-09-12T01:29:13.720517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.s.v.ɑ.dʒ.u.b'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:13.772213Z",
     "start_time": "2019-09-12T01:29:13.727780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.t.ɪ'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.æ.ɔɪ.ɑ.z'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.120752Z",
     "start_time": "2019-09-12T01:29:13.773632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.127794Z",
     "start_time": "2019-09-12T01:29:14.124031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ps_t_OH = [dsToTriphoneOHs(x0k, X012OHmap)\n",
    "#            for x0k in Ps_t]\n",
    "\n",
    "# BAD. strangely slow?\n",
    "# Ps_t_np = np.array(list(par(delayed(dsToTriphoneOHs)(x0k, X012OHmap) \n",
    "#                             for x0k in Ps_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.132799Z",
     "start_time": "2019-09-12T01:29:14.130294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ps_t_OH_torch = [torch.tensor(x0k_OH)\n",
    "#                  for x0k_OH in Ps_t_OH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.136976Z",
     "start_time": "2019-09-12T01:29:14.134703Z"
    }
   },
   "outputs": [],
   "source": [
    "#not currently using these...\n",
    "# del Ps_t_OH\n",
    "# del Ps_t_OH_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.550101Z",
     "start_time": "2019-09-12T01:29:14.138797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for a single prefix or wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.564151Z",
     "start_time": "2019-09-12T01:29:14.553557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.n.ɪ', 'n.ɪ.t', 'ɪ.t.ɪ')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.t.ɪ'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.571090Z",
     "start_time": "2019-09-12T01:29:14.565884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10220, 4278, 8521)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.578710Z",
     "start_time": "2019-09-12T01:29:14.572961Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.582937Z",
     "start_time": "2019-09-12T01:29:14.580375Z"
    }
   },
   "outputs": [],
   "source": [
    "# from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.587637Z",
     "start_time": "2019-09-12T01:29:14.584611Z"
    }
   },
   "outputs": [],
   "source": [
    "# # @njit(parallel=True)\n",
    "# @njit\n",
    "def sourcePrefixStackToChannelMatrix(x0k_OHs):\n",
    "    return pC1X012_np @ x0k_OHs.T\n",
    "#     return np.matmul(pC1X012_np, x0k_OHs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.601511Z",
     "start_time": "2019-09-12T01:29:14.589473Z"
    }
   },
   "outputs": [],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(ds2t(x0k))))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('pC1X012_np.shape = {0}'.format(pC1X012_np.shape))\n",
    "#         print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "        print('result = pC1X012_np * triphoneOHs.T')\n",
    "#     result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    result = np.matmul(pC1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "\n",
    "def sourcePrefixToChannelMatrix(x0k):\n",
    "    triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#     C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                    for x012_idx in triphoneIndices] \n",
    "#                   for y1 in Y1s_t])\n",
    "    C = np.array([[pC1X012_np[Y012map[y012], x012_idx] \n",
    "               for x012_idx in triphoneIndices] \n",
    "              for y012 in Y012s_t])\n",
    "    if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "        raise Exception('This is not well-defined.')\n",
    "#         C = np.zeros((len(Y1s_RE), 1))\n",
    "#         C = np.zeros((len(Y012s), 1))\n",
    "#         C[-1] = 1.0\n",
    "#         return C.reshape(len(Y1s_RE),1)\n",
    "#         return C.reshape(len(Y1s),1)\n",
    "    assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "    assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "    return C\n",
    "\n",
    "\n",
    "# if r:\n",
    "#     def sourcePrefixToChannelMatrix(x0k):\n",
    "#         triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#     #                    for x012_idx in triphoneIndices] \n",
    "#     #                   for y1 in Y1s_t])\n",
    "#         if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "#     #         C = np.zeros((len(Y1s), 1))\n",
    "#             C[-1] = 1.0\n",
    "#             return C.reshape(len(Y1s_RE),1)\n",
    "#     #         return C.reshape(len(Y1s),1)\n",
    "#         return C\n",
    "# else:\n",
    "#     def sourcePrefixToChannelMatrix(x0k):\n",
    "#         triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "# #         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#         C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                        for x012_idx in triphoneIndices] \n",
    "#                       for y1 in Y1s_t])\n",
    "#         if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "# #             C = np.zeros((len(Y1s_RE), 1))\n",
    "#             C = np.zeros((len(Y1s), 1))\n",
    "#             C[-1] = 1.0\n",
    "# #             return C.reshape(len(Y1s_RE),1)\n",
    "#             return C.reshape(len(Y1s),1)\n",
    "#         assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "#         assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "#         return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:14.615536Z",
     "start_time": "2019-09-12T01:29:14.603268Z"
    }
   },
   "outputs": [],
   "source": [
    "def sourcePrefixStackToChannelMatrix_t_c(x0k_OHs_tr):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "    return torch.matmul(pC1X012_torch_c, x0k_OHs_tr)\n",
    "\n",
    "def sourcePrefixStackToChannelMatrix_t(x0k_OHs_tr):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "    return torch.matmul(pC1X012_torch, x0k_OHs_tr)\n",
    "\n",
    "def sourcePrefixStackToChannelMatrix_t_tr(x0k_OHs):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "    return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs)\n",
    "\n",
    "def sourcePrefixToChannelMatrix_l_t(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap, True)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(ds2t(x0k))))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('pC1X012_np.shape = {0}'.format(pC1X012_np.shape))\n",
    "#         print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "        print('result = pC1X012_torch * triphoneOHs.t()')\n",
    "#     result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    result = torch.matmul(pC1X012_torch, triphoneOHs.t())\n",
    "    return result\n",
    "\n",
    "def sourcePrefixToChannelMatrix_t(x0k):\n",
    "    triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#     C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                    for x012_idx in triphoneIndices] \n",
    "#                   for y1 in Y1s_t])\n",
    "    C = torch.tensor([[pC1X012_torch[Y012map[y012], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y012 in Y012s_t])\n",
    "    if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "        raise Exception('This is not well-defined.')\n",
    "#         C = np.zeros((len(Y1s_RE), 1))\n",
    "#         C = np.zeros((len(Y012s), 1))\n",
    "#         C[-1] = 1.0\n",
    "#         return C.reshape(len(Y1s_RE),1)\n",
    "#         return C.reshape(len(Y1s),1)\n",
    "    assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "    assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:15.060094Z",
     "start_time": "2019-09-12T01:29:14.622294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋊.n.ɪ.t.ɪ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.60742382e-07, 3.48583936e-07],\n",
       "       [0.00000000e+00, 3.60742382e-07, 3.48583936e-07],\n",
       "       [0.00000000e+00, 2.25938650e-06, 3.48583936e-07],\n",
       "       ...,\n",
       "       [4.78438925e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.78438925e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(random_source_prefix)\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:15.221113Z",
     "start_time": "2019-09-12T01:29:15.062457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋊.n.ɪ.t.ɪ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 3.6074e-07, 3.4858e-07],\n",
       "        [0.0000e+00, 3.6074e-07, 3.4858e-07],\n",
       "        [0.0000e+00, 2.2594e-06, 3.4858e-07],\n",
       "        ...,\n",
       "        [4.7844e-05, 0.0000e+00, 0.0000e+00],\n",
       "        [4.7844e-05, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(random_source_prefix)\n",
    "sourcePrefixToChannelMatrix_l_t(random_source_prefix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:15.228485Z",
     "start_time": "2019-09-12T01:29:15.222574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(p3Y1X012_np, dsToTriphoneOHs(random_source_prefix, X012OHmap).T) == p3Y1X012_np @ dsToTriphoneOHs(random_source_prefix, X012OHmap).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:16.090943Z",
     "start_time": "2019-09-12T01:29:15.229943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixStackToChannelMatrix(dsToTriphoneOHs(random_source_prefix, X012OHmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:17.126206Z",
     "start_time": "2019-09-12T01:29:16.092338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.t.ɪ'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " ...\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.048260Z",
     "start_time": "2019-09-12T01:29:17.127462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.t.ɪ'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " ...\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.051703Z",
     "start_time": "2019-09-12T01:29:18.049691Z"
    }
   },
   "outputs": [],
   "source": [
    "# from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.202953Z",
     "start_time": "2019-09-12T01:29:18.053041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊',\n",
       " '⋊.aɪ',\n",
       " '⋊.aɪ.b',\n",
       " '⋊.aɪ.b.j',\n",
       " '⋊.aɪ.b.j.u',\n",
       " '⋊.aɪ.b.j.u.p',\n",
       " '⋊.aɪ.b.j.u.p.ɹ',\n",
       " '⋊.aɪ.b.j.u.p.ɹ.oʊ',\n",
       " '⋊.aɪ.b.j.u.p.ɹ.oʊ.f',\n",
       " '⋊.aɪ.b.j.u.p.ɹ.oʊ.f.ʌ')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "98570"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'⋊',\n",
       " '⋊.aɪ',\n",
       " '⋊.aʊ',\n",
       " '⋊.b',\n",
       " '⋊.d',\n",
       " '⋊.dʒ',\n",
       " '⋊.eɪ',\n",
       " '⋊.f',\n",
       " '⋊.g',\n",
       " '⋊.h',\n",
       " '⋊.i',\n",
       " '⋊.j',\n",
       " '⋊.k',\n",
       " '⋊.l',\n",
       " '⋊.m',\n",
       " '⋊.n',\n",
       " '⋊.oʊ',\n",
       " '⋊.p',\n",
       " '⋊.s',\n",
       " '⋊.t',\n",
       " '⋊.tʃ',\n",
       " '⋊.u',\n",
       " '⋊.v',\n",
       " '⋊.w',\n",
       " '⋊.z',\n",
       " '⋊.æ',\n",
       " '⋊.ð',\n",
       " '⋊.ɑ',\n",
       " '⋊.ɔɪ',\n",
       " '⋊.ɚ',\n",
       " '⋊.ɛ',\n",
       " '⋊.ɪ',\n",
       " '⋊.ɹ',\n",
       " '⋊.ʃ',\n",
       " '⋊.ʌ',\n",
       " '⋊.ʒ',\n",
       " '⋊.θ'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps_t[:10]\n",
    "len(Ps_t)\n",
    "len({p for p in Ps if len(ds2t(p)) < 3})\n",
    "{p for p in Ps if len(ds2t(p)) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.207455Z",
     "start_time": "2019-09-12T01:29:18.204027Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    random_source_prefixes = choices(tuple({p for p in Ps if len(ds2t(p)) > 2}), k=5000)\n",
    "    random_source_prefixes_OHs = [dsToTriphoneOHs(p, X012OHmap)\n",
    "                                  for p in random_source_prefixes]\n",
    "    random_source_prefixes_OHs_t = [torch.tensor(each) for each in random_source_prefixes_OHs]\n",
    "    random_source_prefixes_OHs_tr_t = [torch.tensor(each).t() for each in random_source_prefixes_OHs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.213121Z",
     "start_time": "2019-09-12T01:29:18.208519Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    sum([torch_nbytes(each)\n",
    "         for each in random_source_prefixes_OHs_tr_t]) / 1e9\n",
    "    # random_source_prefixes_OHs_tr_t\n",
    "    random_source_prefixes_OHs_tr_t_c = [each.cuda() for each in random_source_prefixes_OHs_tr_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.217413Z",
     "start_time": "2019-09-12T01:29:18.214135Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    torch_nbytes(pC1X012_torch) / 1e9\n",
    "    pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.222567Z",
     "start_time": "2019-09-12T01:29:18.218368Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #544ms / cmu+tarski before driver update\n",
    "    #288ms / cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixToChannelMatrix_l(choice(random_source_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.227118Z",
     "start_time": "2019-09-12T01:29:18.223604Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    sourcePrefixToChannelMatrix_l(choice(random_source_prefixes)).nbytes / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.231144Z",
     "start_time": "2019-09-12T01:29:18.228160Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #649ms / cmu+tarski before driver update\n",
    "    #286ms / cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixStackToChannelMatrix(choice(random_source_prefixes_OHs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.236463Z",
     "start_time": "2019-09-12T01:29:18.232157Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #1.11s cmu+tarski before driver update\n",
    "    #116ms cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixStackToChannelMatrix_t_tr(choice(random_source_prefixes_OHs_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.241692Z",
     "start_time": "2019-09-12T01:29:18.238207Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #1.05s cmu+tarski before driver update\n",
    "    #116ms cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixStackToChannelMatrix_t(choice(random_source_prefixes_OHs_tr_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.247282Z",
     "start_time": "2019-09-12T01:29:18.243330Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #run this cell more than once\n",
    "\n",
    "    #57.4𝛍s cmu+tarski before driver update\n",
    "    #37.4𝛍s cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixStackToChannelMatrix_t_c(choice(random_source_prefixes_OHs_tr_t_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.271194Z",
     "start_time": "2019-09-12T01:29:18.248666Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    #run this cell more than once\n",
    "    \n",
    "    #203ms cmu+tarski before driver update\n",
    "    #168ms cmu+tarski\n",
    "    %timeit -n 10 sourcePrefixStackToChannelMatrix_t_c(choice(random_source_prefixes_OHs_t).t().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.276411Z",
     "start_time": "2019-09-12T01:29:18.273069Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    pC1X012_torch_c.shape\n",
    "    len(Y012s_t), len(X012s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.281667Z",
     "start_time": "2019-09-12T01:29:18.278318Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    rsp_OH_trc = random_source_prefixes_OHs_tr_t_c[0]\n",
    "    rsp_OH_trc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.285742Z",
     "start_time": "2019-09-12T01:29:18.282960Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    rsp_OH_CM = sourcePrefixStackToChannelMatrix_t_c(rsp_OH_trc)\n",
    "    rsp_OH_CM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.620938Z",
     "start_time": "2019-09-12T01:29:18.287021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.628625Z",
     "start_time": "2019-09-12T01:29:18.623832Z"
    }
   },
   "outputs": [],
   "source": [
    "if benchmark:\n",
    "    del random_source_prefixes_OHs_tr_t_c\n",
    "    del random_source_prefixes\n",
    "    del random_source_prefixes_OHs\n",
    "    del random_source_prefixes_OHs_t\n",
    "    del random_source_prefixes_OHs_tr_t\n",
    "    del pC1X012_torch_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.634641Z",
     "start_time": "2019-09-12T01:29:18.630260Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.952141Z",
     "start_time": "2019-09-12T01:29:18.635901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson learned:** \n",
    " - Load as much onto the cuda device ahead of time as you can afford. Use `torch_nbytes` to determine size ahead of time.\n",
    " - Make functions parameterizable so that they don't depend on some object that isn't on the device.\n",
    " - Break functions down into things cuda can do and stuff on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.959565Z",
     "start_time": "2019-09-12T01:29:18.955364Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     def sourcePrefixToPreviewVector(x0k):\n",
    "#         xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "#         if len(xp_t) < 2:\n",
    "#             raise Exception('|x0k| must be > 1.')\n",
    "#         if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "#     #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "#             uniformProb = 1.0 / len(Y1s_RE)\n",
    "#             preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "#             return preview_dist.reshape(len(Y1s_RE),1)\n",
    "\n",
    "#         xi = xp_t[-2] #just-completed segment\n",
    "#         xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "#         xik_ds = t2ds((xi, xk))\n",
    "#         preview_dist = p3Y1X01[xik_ds]\n",
    "#     #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "#         return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "#     sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.964270Z",
     "start_time": "2019-09-12T01:29:18.961645Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     # returns p(Y0K|x0k)\n",
    "#     def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "#         # NB:\n",
    "#         # if len(prefix) == n (including leftEdge), \n",
    "#         # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "#         p = prefix\n",
    "#         if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "#     #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "#             return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "#         else: #the extended channel matrix is garbage that should never be asked for\n",
    "#             l = len(ds2t(p))\n",
    "#             return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.968503Z",
     "start_time": "2019-09-12T01:29:18.965970Z"
    }
   },
   "outputs": [],
   "source": [
    "# if f:\n",
    "#     print('Source sequences = wordforms and prefixes')\n",
    "#     source_seqs = prefixes_t #prefixes include full wordforms\n",
    "# else:\n",
    "#     print('Source sequences = just full wordforms')\n",
    "#     source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.973509Z",
     "start_time": "2019-09-12T01:29:18.970090Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "#                          for s in source_seqs]\n",
    "#     xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "#     xCMsByPrefixIndex[3].shape\n",
    "# if r:\n",
    "#     xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(p)\n",
    "#                          for p in prefixes_t]\n",
    "#     xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "#     xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.977530Z",
     "start_time": "2019-09-12T01:29:18.975171Z"
    }
   },
   "outputs": [],
   "source": [
    "#original idea: process prefixes in blocks of 5000, \n",
    "# with order corresponding to ordering in Ps_t\n",
    "#\n",
    "# won't work because some prefixes are too short for pC1X012 to be well-defined\n",
    "\n",
    "# #split the prefixes_t into blocks of 5000\n",
    "# len(Ps_t)\n",
    "# chunk_size = 5000\n",
    "# Ps_t_chunked = [Ps_t[i * chunk_size:(i + 1) * chunk_size] for i in range((len(Ps_t) + chunk_size - 1) // chunk_size )]\n",
    "# [len(chunk) for chunk in Ps_t_chunked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for calculating CMs for all prefixes of a given length (in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:18.982164Z",
     "start_time": "2019-09-12T01:29:18.979213Z"
    }
   },
   "outputs": [],
   "source": [
    "# def trimBoundariesFromSequence(seq):\n",
    "#     temp = list(dottedStringToTuple(seq))\n",
    "#     if len(temp) < 1:\n",
    "#         return seq\n",
    "#     if temp[0] == leftEdge:\n",
    "#         temp = temp[1:]\n",
    "#     if len(temp) < 1:\n",
    "#         return tupleToDottedString(tuple(temp))\n",
    "#     if temp[-1] == rightEdge:\n",
    "#         temp = temp[:-1]\n",
    "#     return tupleToDottedString(tuple(temp))\n",
    "\n",
    "# def stringLengths(L, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         return {len(ds2t(s)) for s in L}\n",
    "#     L_no_edges = {trimBoundariesFromSequence(s) for s in L}\n",
    "#     return stringLengths(L_no_edges, includingEdges = True)\n",
    "\n",
    "# def LbyLength(L, includingEdges = False):\n",
    "#     lengths = stringLengths(L, includingEdges=includingEdges)\n",
    "# #     if includingEdges:\n",
    "# #         my_L = L\n",
    "# #     else:\n",
    "# #         my_L = {trimBoundariesFromSequence(s) for s in L}\n",
    "#     l_to_s = {l:set() for l in lengths}\n",
    "#     for s in L:\n",
    "#         if includingEdges:\n",
    "#             my_s = s\n",
    "#         else:\n",
    "#             my_s = trimBoundariesFromSequence(s)\n",
    "#         my_l = len(ds2t(my_s))\n",
    "#         l_to_s[my_l].add(s)\n",
    "#     return l_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:19.091580Z",
     "start_time": "2019-09-12T01:29:18.983829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = stringLengths(Ws, True)\n",
    "wordlengthsInclEdges\n",
    "prefixlengthsInclEdges = stringLengths(Ps, True)\n",
    "prefixlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:19.337351Z",
     "start_time": "2019-09-12T01:29:19.092959Z"
    }
   },
   "outputs": [],
   "source": [
    "prefixesByLength = walk_values(lambda Ss: tuple(sorted(Ss)), LbyLength(Ps, True))\n",
    "wordformsByLength = walk_values(lambda Ss: tuple(sorted(Ss)), LbyLength(Ws, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:29:50.957920Z",
     "start_time": "2019-09-12T01:29:19.339200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0859s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  20 | elapsed:    0.2s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  20 | elapsed:    0.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.9s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    2.4s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  20 | elapsed:    6.3s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   13.0s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   16.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0471s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  18 | elapsed:    0.3s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  18 | elapsed:    0.3s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  18 | elapsed:    0.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  18 | elapsed:    0.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:    0.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:    1.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "# whole cell now takes 30s / cmu+tarski\n",
    "\n",
    "def mapStringsToIDs(Ss, S_t):\n",
    "    return [S_t.index(s) for s in Ss]\n",
    "\n",
    "def mapSsToID_helper(l, Ss, S_t):\n",
    "    return (l, mapStringsToIDs(Ss, S_t))\n",
    "\n",
    "length_to_prefix_IDs = dict(par(delayed(mapSsToID_helper)(l, prefixesByLength[l], Ps_t)\n",
    "                                for l in prefixesByLength))\n",
    "\n",
    "length_to_wordform_IDs = dict(par(delayed(mapSsToID_helper)(l, wordformsByLength[l], Ws_t)\n",
    "                                  for l in wordformsByLength))\n",
    "\n",
    "#both calculations below together take 2m / cmu+tarski\n",
    "# length_to_prefix_IDs = walk_values(partial(lmap, lambda p: Ps_t.index(p)), \n",
    "#                                           prefixesByLength)\n",
    "\n",
    "length_to_wordform_IDs = walk_values(partial(lmap, lambda w: Ws_t.index(w)), \n",
    "                                          wordformsByLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:39.260915Z",
     "start_time": "2019-09-12T01:29:50.959767Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0390s.) Setting batch_size=10.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 860 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1240 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1660 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1890 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2120 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2370 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2620 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2890 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3160 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3450 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3740 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4050 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 4360 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4690 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 5020 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 5370 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 5720 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 6090 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 6460 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6850 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 7240 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 7650 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 8060 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8490 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 8920 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 9370 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 9820 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 10290 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 10760 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 11250 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11740 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 12250 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 12760 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 13290 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done 13820 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 14370 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 14920 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 15490 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 16060 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 16650 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 17240 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 17850 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 18460 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 19090 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 19720 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 20370 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 21020 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 21690 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 22360 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 23050 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 23740 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 24450 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 25160 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 25890 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 26620 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 27370 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 28120 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 28890 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done 29660 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 30450 tasks      | elapsed:   43.1s\n",
      "[Parallel(n_jobs=-1)]: Done 30833 out of 30833 | elapsed:   43.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0041s.) Setting batch_size=96.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0264s.) Setting batch_size=1452.\n",
      "[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 30833 out of 30833 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# whole cell now takes 1m / cmu+tarski\n",
    "\n",
    "def wordform_id_to_prefix_id_helper(w):\n",
    "    return (Ws_t.index(w), [Ps_t.index(p) for p in Ps if p == w][0])\n",
    "\n",
    "wordform_id_to_prefix_id = dict(par(delayed(wordform_id_to_prefix_id_helper)(w)\n",
    "                                    for w in Ws_t))\n",
    "\n",
    "def prefix_id_to_wordform_id_helper(w_idx):\n",
    "    return (wordform_id_to_prefix_id[w_idx], w_idx)\n",
    "\n",
    "prefix_id_to_wordform_id = dict(par(delayed(prefix_id_to_wordform_id_helper)(w_idx)\n",
    "                                    for w_idx in wordform_id_to_prefix_id))\n",
    "\n",
    "#code below takes 4m / cmu+tarski\n",
    "# wordform_id_to_prefix_id = {Ws_t.index(w):[Ps_t.index(p) for p in Ps if p == w][0]\n",
    "#                             for w in Ws_t}\n",
    "\n",
    "# prefix_id_to_wordform_id = flip(wordform_id_to_prefix_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.353354Z",
     "start_time": "2019-09-12T01:30:39.263732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i.p.u'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'i.p'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'i.p.u.?'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_to_wordform_to_prefix = {l:{w:t2ds(ds2t(w)[:l])\n",
    "                                   for w in Ws}\n",
    "                                for l in wordlengthsInclEdges}\n",
    "\n",
    "coerceDStoLength('i.p.u', 3)\n",
    "coerceDStoLength('i.p.u', 2)\n",
    "coerceDStoLength('i.p.u', 4)\n",
    "    \n",
    "length_to_wordform_to_padded_prefix = {l:{w:coerceDStoLength(w, l)\n",
    "                                          for w in Ws}\n",
    "                                       for l in prefixlengthsInclEdges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.358374Z",
     "start_time": "2019-09-12T01:30:41.354544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ʌ.g.ɹ.ɛ.s.ɚ.z.⋉'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.ʌ.g.ɹ.ɛ.s.ɚ.z.⋉.?.?.?'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw = choice(Ws_t); rw\n",
    "length_to_wordform_to_padded_prefix[12][rw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.363515Z",
     "start_time": "2019-09-12T01:30:41.359531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7, 10540)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds2t(rw))\n",
    "dsToTriphoneOHs(rw, X012OHmap).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.370755Z",
     "start_time": "2019-09-12T01:30:41.365065Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.d.i.k.ɹ.ɪ.m.ʌ.n.ʌ.l.ʌ.z.eɪ.ʃ.ʌ',\n",
       " '⋊.d.i.k.ʌ.n.t.æ.m.ʌ.n.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.d.ɪ.s.g.ɹ.ʌ.n.t.ʌ.l.m.ʌ.n.t.⋉',\n",
       " '⋊.d.ɪ.s.k.ɹ.ɪ.m.ʌ.n.ʌ.t.ɑ.ɹ.i.⋉',\n",
       " '⋊.d.ɪ.s.k.ʌ.m.b.ɑ.b.j.u.l.eɪ.t.ʌ',\n",
       " '⋊.d.ɪ.s.p.ɹ.ʌ.p.ɑ.ɹ.ʃ.ʌ.n.ɪ.t.⋉',\n",
       " '⋊.d.ɪ.s.p.ɹ.ʌ.p.ɑ.ɹ.ʃ.ʌ.n.ʌ.t.l',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ɚ.ʃ.ɪ.p.⋉',\n",
       " '⋊.d.ɪ.s.ɑ.ɹ.g.ʌ.n.ʌ.z.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.f.ʌ.n.d.ʌ.m.ɛ.n.t.ʌ.l.ɪ.s.t.s',\n",
       " '⋊.f.ʌ.n.d.ʌ.m.ɛ.n.t.ʌ.l.ɪ.s.t.⋉',\n",
       " '⋊.f.ʌ.n.d.ʌ.m.ɛ.n.t.ʌ.l.ɪ.z.ʌ.m',\n",
       " '⋊.h.oʊ.m.oʊ.s.ɛ.k.ʃ.u.æ.l.ʌ.t.i.⋉',\n",
       " '⋊.h.ɑ.s.p.ɪ.t.ʌ.l.ʌ.z.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.h.ɛ.t.ɚ.oʊ.s.ɛ.k.ʃ.u.æ.l.ɪ.t.i',\n",
       " '⋊.j.u.n.ɪ.l.æ.t.ɚ.ʌ.l.ɪ.z.ʌ.m.⋉',\n",
       " '⋊.k.aʊ.n.t.ɚ.p.ɹ.ʌ.d.ʌ.k.t.ɪ.v.⋉',\n",
       " '⋊.k.ɑ.n.f.ɹ.ʌ.n.t.eɪ.ʃ.ʌ.n.ʌ.l.⋉',\n",
       " '⋊.k.ɑ.n.f.ʌ.d.ɛ.n.ʃ.i.æ.l.ʌ.t.i',\n",
       " '⋊.k.ɑ.n.s.t.ɪ.t.u.ʃ.ʌ.n.æ.l.ɪ.t',\n",
       " '⋊.k.ɑ.n.s.t.ʌ.t.u.ʃ.ʌ.n.ʌ.l.i.⋉',\n",
       " '⋊.k.ɑ.n.v.ɚ.s.eɪ.ʃ.ʌ.n.ʌ.l.ʌ.s.t',\n",
       " '⋊.k.ɑ.ɹ.d.i.oʊ.v.æ.s.k.j.ʌ.l.ɚ.⋉',\n",
       " '⋊.k.ɹ.ɪ.m.ʌ.n.ʌ.l.ɪ.s.t.ɪ.k.s.⋉',\n",
       " '⋊.k.ʌ.n.g.ɹ.æ.tʃ.ʌ.l.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.k.ʌ.n.t.ɛ.m.p.ɚ.eɪ.n.i.ʌ.s.l.i',\n",
       " '⋊.m.aɪ.k.ɹ.oʊ.b.i.ɑ.l.ʌ.dʒ.ɪ.s.t.⋉',\n",
       " '⋊.m.aɪ.k.ɹ.oʊ.ɑ.ɹ.g.ʌ.n.ɪ.z.ʌ.m.z',\n",
       " '⋊.m.j.u.n.ɪ.s.ʌ.p.æ.l.ɪ.t.i.z.⋉',\n",
       " '⋊.m.æ.k.ɹ.oʊ.ɛ.k.ʌ.n.ɑ.m.ɪ.k.s.⋉',\n",
       " '⋊.m.ɪ.s.k.æ.l.k.j.ʌ.l.eɪ.t.ɪ.d.⋉',\n",
       " '⋊.m.ɪ.s.k.ʌ.m.j.u.n.ʌ.k.eɪ.ʃ.ʌ.n',\n",
       " '⋊.m.ɪ.s.ɹ.ɛ.p.ɹ.ɪ.z.ɛ.n.t.eɪ.ʃ.ʌ',\n",
       " '⋊.m.ɪ.s.ɹ.ɛ.p.ɹ.ɪ.z.ɛ.n.t.ɪ.d.⋉',\n",
       " '⋊.m.ɪ.s.ɹ.ɛ.p.ɹ.ʌ.z.ɛ.n.t.ɪ.ŋ.⋉',\n",
       " '⋊.m.ɪ.s.ʌ.n.d.ɚ.s.t.æ.n.d.ɪ.ŋ.z',\n",
       " '⋊.m.ɪ.s.ʌ.n.d.ɚ.s.t.æ.n.d.ɪ.ŋ.⋉',\n",
       " '⋊.m.ɪ.s.ʌ.p.ɹ.oʊ.p.ɹ.i.eɪ.t.ɪ.d.⋉',\n",
       " '⋊.n.ɑ.n.g.ʌ.v.ɚ.n.m.ɛ.n.t.ʌ.l.⋉',\n",
       " '⋊.p.ɹ.ɑ.p.ʌ.g.ʌ.n.d.ɪ.s.t.ɪ.k.⋉',\n",
       " '⋊.p.ɹ.ɑ.t.ʌ.s.t.ʌ.n.t.ɪ.z.ʌ.m.⋉',\n",
       " '⋊.p.ɹ.ɪ.d.ɪ.k.t.ʌ.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.p.ɹ.ʌ.f.ɛ.ʃ.ʌ.n.ʌ.l.ɪ.z.ʌ.m.⋉',\n",
       " '⋊.p.ɹ.ʌ.k.ɹ.æ.s.t.ʌ.n.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.p.ɹ.ʌ.k.ɹ.æ.s.t.ʌ.n.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.p.ɹ.ʌ.k.ɹ.æ.s.t.ʌ.n.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.s.u.p.ɚ.ʌ.n.t.ɛ.n.d.ʌ.n.t.s.⋉',\n",
       " '⋊.s.ɛ.n.s.eɪ.ʃ.ʌ.n.ʌ.l.aɪ.z.ɪ.ŋ.⋉',\n",
       " '⋊.s.ɛ.n.s.eɪ.ʃ.ʌ.n.ʌ.l.ɪ.s.t.ɪ.k',\n",
       " '⋊.s.ɛ.n.s.eɪ.ʃ.ʌ.n.ʌ.l.ɪ.z.ʌ.m.⋉',\n",
       " '⋊.s.ɛ.n.t.ʌ.m.ɛ.n.t.æ.l.ɪ.t.i.⋉',\n",
       " '⋊.t.ɛ.l.ʌ.k.ʌ.m.j.u.n.ɪ.k.eɪ.ʃ.ʌ',\n",
       " '⋊.t.ɛ.l.ʌ.k.ʌ.m.j.u.n.ʌ.k.eɪ.ʃ.ʌ',\n",
       " '⋊.t.ɹ.æ.n.z.k.ɑ.n.t.ɪ.n.ɛ.n.t.ʌ',\n",
       " '⋊.æ.d.m.ɪ.n.ɪ.s.t.ɹ.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.æ.n.t.aɪ.d.ɪ.p.ɹ.ɛ.s.ʌ.n.t.s.⋉',\n",
       " '⋊.æ.n.θ.ɹ.ʌ.p.ɑ.l.ʌ.dʒ.ɪ.s.t.s.⋉',\n",
       " '⋊.ɑ.t.ʌ.b.aɪ.ʌ.g.ɹ.æ.f.ɪ.k.ʌ.l.⋉',\n",
       " '⋊.ɛ.k.s.k.ɹ.u.s.i.eɪ.t.ɪ.ŋ.l.i.⋉',\n",
       " '⋊.ɛ.k.s.t.ɹ.æ.v.ʌ.g.ʌ.n.t.l.i.⋉',\n",
       " '⋊.ɛ.k.s.t.ɹ.ʌ.k.ɚ.ɪ.k.j.ʌ.l.ɚ.⋉',\n",
       " '⋊.ɛ.k.s.t.ɹ.ʌ.t.ɚ.ɛ.s.t.ɹ.i.ʌ.l',\n",
       " '⋊.ɛ.n.d.oʊ.k.ɹ.ʌ.n.ɑ.l.ʌ.dʒ.ʌ.s.t',\n",
       " '⋊.ɛ.n.v.aɪ.ɹ.ʌ.n.m.ɛ.n.t.ʌ.l.ɪ.s',\n",
       " '⋊.ɛ.p.ɪ.d.i.m.i.ʌ.l.ɑ.dʒ.ɪ.k.ʌ.l',\n",
       " '⋊.ɪ.l.ɛ.k.t.ɹ.oʊ.m.æ.g.n.ɛ.t.ɪ.k',\n",
       " '⋊.ɪ.m.p.ɹ.ɑ.v.ɪ.z.eɪ.ʃ.ʌ.n.ʌ.l.⋉',\n",
       " '⋊.ɪ.n.d.ɪ.s.k.ɹ.ɪ.m.ʌ.n.ʌ.t.l.i',\n",
       " '⋊.ɪ.n.d.ɪ.s.t.ɪ.ŋ.g.w.ɪ.ʃ.ʌ.b.ʌ',\n",
       " '⋊.ɪ.n.d.ɪ.v.ɪ.dʒ.u.ʌ.l.ɪ.s.t.ɪ.k',\n",
       " '⋊.ɪ.n.d.ʌ.s.t.ɹ.i.ʌ.l.ɪ.z.eɪ.ʃ.ʌ',\n",
       " '⋊.ɪ.n.d.ʌ.s.t.ɹ.ʌ.k.t.ɪ.b.ʌ.l.⋉',\n",
       " '⋊.ɪ.n.k.ʌ.n.v.i.n.j.ʌ.n.s.ɪ.z.⋉',\n",
       " '⋊.ɪ.n.s.t.ɪ.t.u.ʃ.ʌ.n.ʌ.l.aɪ.z.d',\n",
       " '⋊.ɪ.n.s.t.ɪ.t.u.ʃ.ʌ.n.ʌ.l.aɪ.z.⋉',\n",
       " '⋊.ɪ.n.s.t.ɪ.t.u.ʃ.ʌ.n.ʌ.l.ɪ.z.eɪ',\n",
       " '⋊.ɪ.n.s.t.ɹ.ʌ.m.ɛ.n.t.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.ɪ.n.s.t.ʌ.n.t.æ.n.i.ʌ.s.l.i.⋉',\n",
       " '⋊.ɪ.n.s.ɪ.g.n.j.ɪ.f.ɪ.k.ʌ.n.t.⋉',\n",
       " '⋊.ɪ.n.t.ɚ.k.ɑ.n.t.ʌ.n.ɛ.n.t.ʌ.l',\n",
       " '⋊.ɪ.n.v.aɪ.ɹ.ʌ.n.m.ɛ.n.t.ʌ.l.i.⋉',\n",
       " '⋊.ɪ.n.v.aɪ.ɹ.ʌ.n.m.ɛ.n.t.ʌ.l.ɪ.s',\n",
       " '⋊.ɪ.ŋ.k.ɑ.n.s.ɪ.s.t.ɛ.n.s.i.z.⋉',\n",
       " '⋊.ɪ.ŋ.k.ɑ.n.s.ʌ.k.w.ɛ.n.tʃ.ʌ.l.⋉',\n",
       " '⋊.ɪ.ɹ.ʌ.s.p.ɑ.n.s.ʌ.b.ɪ.l.ʌ.t.i',\n",
       " '⋊.ɹ.i.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ɪ.d.⋉',\n",
       " '⋊.ɹ.i.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ɪ.ŋ.⋉',\n",
       " '⋊.ɹ.i.s.p.ɑ.n.s.ʌ.b.ɪ.l.ʌ.t.i.z',\n",
       " '⋊.ɹ.i.s.p.ɑ.n.s.ʌ.b.ɪ.l.ʌ.t.i.⋉',\n",
       " '⋊.ɹ.ɛ.p.ɹ.ʌ.z.ɛ.n.t.ʌ.t.ɪ.v.z.⋉',\n",
       " '⋊.ɹ.ɛ.p.ɹ.ʌ.z.ʌ.n.t.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.ʌ.k.j.u.m.j.ʌ.l.eɪ.t.ɪ.v.l.i.⋉',\n",
       " '⋊.ʌ.n.d.ɚ.ɹ.ɛ.p.ɹ.ɪ.z.ɛ.n.t.ɪ.d',\n",
       " '⋊.ʌ.n.k.ɑ.n.s.t.ʌ.t.u.ʃ.ʌ.n.ʌ.l',\n",
       " '⋊.ʌ.n.k.ʌ.n.t.æ.m.ʌ.n.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.ʌ.n.ɹ.ɛ.k.ʌ.g.n.aɪ.z.ʌ.b.ʌ.l.⋉')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixesByLength[16]\n",
    "len(prefixesByLength[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.374246Z",
     "start_time": "2019-09-12T01:30:41.372321Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_sixteen_OH_stack = torch.stack([torch.tensor(dsToTriphoneOHs(p, X012OHmap), dtype=torch.short)\n",
    "#                                   for p in prefixesByLength[16]])\n",
    "# p_sixteen_OH_stack.shape\n",
    "# p_sixteen_OH_stack.dtype\n",
    "\n",
    "# p_sixteen_OH_stack_tr = p_sixteen_OH_stack.transpose(1, 2)\n",
    "# p_sixteen_OH_stack_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.378230Z",
     "start_time": "2019-09-12T01:30:41.375805Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_sixteen_OH_stack_tr_c = p_sixteen_OH_stack_tr.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.382023Z",
     "start_time": "2019-09-12T01:30:41.379750Z"
    }
   },
   "outputs": [],
   "source": [
    "# pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.385891Z",
     "start_time": "2019-09-12T01:30:41.383522Z"
    }
   },
   "outputs": [],
   "source": [
    "# CM_zero = torch.matmul(pC1X012_torch_c, p_sixteen_OH_stack_tr_c[0].type(torch.double))\n",
    "# CM_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.389986Z",
     "start_time": "2019-09-12T01:30:41.387468Z"
    }
   },
   "outputs": [],
   "source": [
    "# pC1X012_torch_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.394084Z",
     "start_time": "2019-09-12T01:30:41.391530Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_sixteen_OH_stack_CMs_c = torch.matmul(pC1X012_torch_c, p_sixteen_OH_stack_tr_c.type(torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.397740Z",
     "start_time": "2019-09-12T01:30:41.395662Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_sixteen_OH_stack_CMs_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.401512Z",
     "start_time": "2019-09-12T01:30:41.399295Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.equal(p_sixteen_OH_stack_CMs_c, \n",
    "#             torch.einsum('ik,pkj->pij',\n",
    "#                          pC1X012_torch_c,\n",
    "#                          p_sixteen_OH_stack_tr_c.type(torch.double)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.405087Z",
     "start_time": "2019-09-12T01:30:41.403056Z"
    }
   },
   "outputs": [],
   "source": [
    "# del p_sixteen_OH_stack\n",
    "# del p_sixteen_OH_stack_tr\n",
    "# del p_sixteen_OH_stack_tr_c\n",
    "# del p_sixteen_OH_stack_CMs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:41.749423Z",
     "start_time": "2019-09-12T01:30:41.406671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         10G         54G        1.8M        123G        175G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:54.329787Z",
     "start_time": "2019-09-12T01:30:41.752220Z"
    }
   },
   "outputs": [],
   "source": [
    "#map each length to a stack of n_l OHs, each of shape (l-2, s), where s = # triphones\n",
    "\n",
    "length_to_prefix_OH_matrices = {l:torch.stack([torch.tensor(dsToTriphoneOHs(p, X012OHmap), dtype=torch.short)\n",
    "                                               for p in prefixesByLength[l]])\n",
    "                                for l in prefixlengthsInclEdges}\n",
    "\n",
    "# length_to_wordform_OH_matrices = {l:torch.stack([torch.tensor(dsToTriphoneOHs(w, X012OHmap), dtype=torch.short)\n",
    "#                                                for w in wordformsByLength[l]])\n",
    "#                                   for l in wordlengthsInclEdges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:54.883379Z",
     "start_time": "2019-09-12T01:30:54.331329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         21G         42G        1.8M        123G        164G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:54.893719Z",
     "start_time": "2019-09-12T01:30:54.886779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_to_prefix_OH_matrices[1].size()[1] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:58.259528Z",
     "start_time": "2019-09-12T01:30:54.895613Z"
    }
   },
   "outputs": [],
   "source": [
    "pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:58.263506Z",
     "start_time": "2019-09-12T01:30:58.261082Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T01:30:58.270233Z",
     "start_time": "2019-09-12T01:30:58.264919Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkList(chunk_size, l):\n",
    "    return [l[i * chunk_size:(i + 1) * chunk_size] for i in range((len(l) + chunk_size - 1) // chunk_size )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:04:03.374979Z",
     "start_time": "2019-09-12T01:30:58.271577Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"1\": shape None, type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"2\": shape None, type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([554, 1, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([554, 57798, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"3\": shape (554, 57798, 1), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:01<00:11,  1.55it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([4402, 2, 10540])\n",
      "\t2 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:14<00:14, 14.49s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:21<00:00, 12.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([4402, 57798, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"4\": shape (4402, 57798, 2), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:26<02:06,  7.93s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([12988, 3, 10540])\n",
      "\t5 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:21<01:27, 21.90s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:43<01:05, 21.94s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:06<00:44, 22.00s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:28<00:22, 22.05s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:35<00:00, 17.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([12988, 57798, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"5\": shape (12988, 57798, 3), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [02:19<09:50, 39.34s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([18100, 4, 10540])\n",
      "\t7 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 1/7 [00:29<02:57, 29.60s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:59<02:28, 29.62s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [01:28<01:58, 29.64s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [01:58<01:28, 29.66s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [02:28<00:59, 29.67s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [02:58<00:29, 29.68s/it]\u001b[A\n",
      "100%|██████████| 7/7 [02:59<00:00, 21.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([18100, 57798, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"6\": shape (18100, 57798, 4), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [05:53<21:23, 91.67s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([18069, 5, 10540])\n",
      "\t7 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 1/7 [00:37<03:42, 37.12s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [01:14<03:05, 37.12s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [01:51<02:28, 37.11s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [02:28<01:51, 37.10s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [03:05<01:14, 37.09s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [03:42<00:37, 37.10s/it]\u001b[A\n",
      "100%|██████████| 7/7 [03:43<00:00, 26.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([18069, 57798, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"7\": shape (18069, 57798, 5), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [10:27<31:44, 146.50s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([15080, 6, 10540])\n",
      "\t6 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 1/6 [00:44<03:42, 44.43s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [01:28<02:57, 44.43s/it]\u001b[A\n",
      " 50%|█████     | 3/6 [02:13<02:13, 44.43s/it]\u001b[A\n",
      " 67%|██████▋   | 4/6 [02:57<01:28, 44.43s/it]\u001b[A\n",
      " 83%|████████▎ | 5/6 [03:42<00:44, 44.43s/it]\u001b[A\n",
      "100%|██████████| 6/6 [03:43<00:00, 31.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([15080, 57798, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"8\": shape (15080, 57798, 6), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [15:02<36:59, 184.92s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([11274, 7, 10540])\n",
      "\t4 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t6 chunks of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 1/6 [00:34<02:52, 34.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 2/6 [01:09<02:18, 34.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 3/6 [01:43<01:43, 34.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 4/6 [02:18<01:09, 34.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 5/6 [02:53<00:34, 34.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 6/6 [03:15<00:00, 30.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([11274, 57798, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"9\": shape (11274, 57798, 7), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [19:45<39:19, 214.48s/it]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([7767, 8, 10540])\n",
      "\t3 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t4 chunks of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t8 chunks of size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [00:55<06:25, 55.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [01:14<04:26, 44.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [01:34<03:05, 37.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [01:54<02:07, 31.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:14<01:24, 28.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [02:33<00:51, 25.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [02:53<00:23, 23.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [03:08<00:00, 21.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([7767, 57798, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"10\": shape (7767, 57798, 8), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [24:17<38:38, 231.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([4828, 9, 10540])\n",
      "\t2 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3 chunks of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t5 chunks of size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1/5 [01:02<04:08, 62.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [01:24<02:30, 50.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [01:46<01:23, 41.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [02:09<00:35, 35.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [02:27<00:00, 30.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([4828, 57798, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"11\": shape (4828, 57798, 9), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [28:05<34:34, 230.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([2814, 10, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 chunks of size 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3 chunks of size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [01:09<02:18, 69.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [01:34<00:55, 55.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [01:54<00:00, 45.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([2814, 57798, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"12\": shape (2814, 57798, 10), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [31:15<29:08, 218.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([1490, 11, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([1490, 57798, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"13\": shape (1490, 57798, 11), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [32:03<19:30, 167.15s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([735, 12, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([735, 57798, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"14\": shape (735, 57798, 12), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [32:32<12:35, 125.93s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([283, 13, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([283, 57798, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"15\": shape (283, 57798, 13), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [32:43<07:37, 91.41s/it] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([96, 14, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([96, 57798, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"16\": shape (96, 57798, 14), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [32:47<04:20, 65.17s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([35, 15, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([35, 57798, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"17\": shape (35, 57798, 15), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [32:49<02:18, 46.12s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([14, 16, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([14, 57798, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"18\": shape (14, 57798, 16), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [32:50<01:05, 32.51s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([3, 17, 10540])\n",
      "\t1 chunks of size 3000\n",
      "\tCM stack shape = torch.Size([3, 57798, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"19\": shape (3, 57798, 17), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [32:50<00:22, 22.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([1, 18, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([1, 57798, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"20\": shape (1, 57798, 18), type \"<f8\">"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [32:50<00:00, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#before driver update:\n",
    "#chunk size 100 -> 5.75m on tarski w/ peak mem usage ~130GB and peak GPU mem usage of ~5GB?\n",
    "#chunk size 300 -> 5.5m on tarski w/ peak mem usage ~130GB and peak GPU mem usage of ~8-9GB\n",
    "#chunk size 1000 -> 5.5m on tarski w/ peak mem usage ~130GB and peak GPU mem usage of ~13GB\n",
    "#chunk size 3000 -> 5.5m on tarski w/ peak mem usage ~130GB and peak GPU mem usage of ~?GB\n",
    "\n",
    "#after driver update:\n",
    "#chunk size 100 -> ?m on tarski w/ peak mem usage = high enough to trigger memguard (>=186GB)\n",
    "# \n",
    "# FIXME need to write the CMs *for each l* to disk / memory-map them.\n",
    "#    *Options include*:\n",
    "#      making a directory and making\n",
    "#         1. tiledb arrays for each l\n",
    "#         2. numpy memory mapped arrays for each l\n",
    "#      3. making a hdf5 <archive?> for the whole thing\n",
    "# \n",
    "#     - Option #1 (tiledb) will have the best concurrent read support (useful for parallelizing segmental posterior calcs)\n",
    "#     - *BUT* if you organize segmental posterior calcs by length, then you can just load the entire tensor into memory for just that length\n",
    "#       -> this is most comparable to what you're already doing and most desirable in terms of performance anyway\n",
    "#     -> Pursue option 3\n",
    "\n",
    "\n",
    "#chunk size 100 -> ≈60m on tarski, peak GPU usage around 6GB\n",
    "#chunk size 2000 -> \n",
    "#chunk size 3000 -> ≈33m on tarski, peak GPU usage around 21GB, but GPU runs out of memory on l=9\n",
    "\n",
    "my_ext = '.hdf5'\n",
    "OD_by_length_prefix_fp = o + '_by_length_by_prefix' + my_ext\n",
    "with h5py.File(OD_by_length_prefix_fp, 'w') as f:\n",
    "#     length_to_prefix_CMs = []\n",
    "    # length_to_prefix_CMs = dict()\n",
    "    \n",
    "    chunk_size = 3000\n",
    "#     chunk_size = 2000\n",
    "#     chunk_size = 1000\n",
    "#     chunk_size = 100\n",
    "#     chunk_size = 300\n",
    "\n",
    "    for l in tqdm(length_to_prefix_OH_matrices):\n",
    "        if length_to_prefix_OH_matrices[l].size()[1] == 0:\n",
    "            \n",
    "            #see http://docs.h5py.org/en/stable/high/dataset.html#creating-and-reading-empty-or-null-datasets-and-attributes\n",
    "            f.create_dataset(str(l), dtype='float64') #h5py empty \n",
    "#             length_to_prefix_CMs.append(None)\n",
    "    #         length_to_prefix_CMs[l] = None\n",
    "        else:\n",
    "            memTrigger()\n",
    "\n",
    "            print('-' * 40)\n",
    "            print(f\"\\tOH stack size = {length_to_prefix_OH_matrices[l].size()}\")\n",
    "\n",
    "            chunks = chunkList(chunk_size, length_to_prefix_OH_matrices[l])\n",
    "            print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "            \n",
    "            try:\n",
    "                chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "            except:\n",
    "                print(f'Chunk size {chunk_size} too large')\n",
    "                torch.cuda.empty_cache()\n",
    "                if chunk_size > 1000:\n",
    "                    chunk_size = chunk_size - 1000\n",
    "                    print(f'Reducing chunk size by 1000 to {chunk_size}')\n",
    "                    chunks = chunkList(chunk_size, length_to_prefix_OH_matrices[l])\n",
    "                    print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "                    try:\n",
    "                        chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "                    except:\n",
    "                        print(f'Chunk size {chunk_size} *still * too large')\n",
    "                        torch.cuda.empty_cache()\n",
    "                        if chunk_size > 1000:\n",
    "                            chunk_size = chunk_size - 1000\n",
    "                            print(f'Reducing chunk size by another 1000 to {chunk_size}')\n",
    "                        else:\n",
    "                            chunk_size = int(chunk_size / 2)\n",
    "                            print(f'Reducing chunk size by half to {chunk_size}')\n",
    "                        chunks = chunkList(chunk_size, length_to_prefix_OH_matrices[l])\n",
    "                        print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "                        chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "                else:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    raise Exception('Out of memory.')\n",
    "            del chunks\n",
    "            \n",
    "            result = torch.cat(chunked_result)\n",
    "            del chunked_result\n",
    "            print(f\"\\tCM stack shape = {result.shape}\")\n",
    "            result_np = result.numpy()\n",
    "            del result\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "#             length_to_prefix_CMs.append(result)\n",
    "    #         length_to_prefix_CMs[l] = result\n",
    "            f.create_dataset(str(l), result_np.shape, dtype='float64', data=result_np)\n",
    "            del result_np\n",
    "\n",
    "#             del chunks\n",
    "#             del chunked_result\n",
    "#             del result\n",
    "#             del result_np\n",
    "            print('')\n",
    "    \n",
    "    \n",
    "# length_to_prefix_CMs = []\n",
    "# # length_to_prefix_CMs = dict()\n",
    "# for l in tqdm(length_to_prefix_OH_matrices):\n",
    "#     if length_to_prefix_OH_matrices[l].size()[1] == 0:\n",
    "#         length_to_prefix_CMs.append(None)\n",
    "# #         length_to_prefix_CMs[l] = None\n",
    "#     else:\n",
    "#         memTrigger()\n",
    "\n",
    "#         print('-' * 40)\n",
    "#         print(f\"\\tOH stack size = {length_to_prefix_OH_matrices[l].size()}\")\n",
    "# #         chunk_size = 3000\n",
    "# #         chunk_size = 1000\n",
    "#         chunk_size = 100\n",
    "# #         chunk_size = 300\n",
    "#         chunks = chunkList(chunk_size, length_to_prefix_OH_matrices[l])\n",
    "#         print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "#         chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "#                                        chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "#                           for chunk in tqdm(chunks, total=len(chunks))]\n",
    "#         result = torch.cat(chunked_result)\n",
    "#         print(f\"\\tCM stack shape = {result.shape}\")\n",
    "#         print(f\"\\tCM stack dtype = {result.dtype}\")\n",
    "#         length_to_prefix_CMs.append(result)\n",
    "# #         length_to_prefix_CMs[l] = result\n",
    "#         torch.cuda.empty_cache()\n",
    "#         del chunks\n",
    "#         del chunked_result\n",
    "#         del result\n",
    "#         print('')\n",
    "# #         result = []\n",
    "# #         for chunk in chunks:\n",
    "# #             result.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:04:04.016192Z",
     "start_time": "2019-09-12T02:04:03.379211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         23G        2.6G         11M        161G        162G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:11:59.101254Z",
     "start_time": "2019-09-12T02:11:59.090641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '3', '4', '5', '6', '7', '8', '9']>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OD_by_length_prefix_f = h5py.File(OD_by_length_prefix_fp, 'r')\n",
    "OD_by_length_prefix_f.keys()\n",
    "group10 = OD_by_length_prefix_f['10']\n",
    "group10.shape\n",
    "group10.dtype\n",
    "OD_by_length_prefix_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T02:16:35.948Z"
    }
   },
   "outputs": [],
   "source": [
    "CMs_by_length_by_prefix_md = {\n",
    "#     'r':r,\n",
    "    'length':len(length_to_prefix_OH_matrices),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W; final ⋉ stripped',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W after final ⋉ stripped',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + '_by_length_by_prefix.hdf5'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_prefix_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {'organization':'Each l maps to a pytorch tensor of dim (n_{l-2}, |Y012s|, l-2), where the ordering of the first dimension corresponds to alphabetical order of prefixes of length l.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T23:30:08.214597Z",
     "start_time": "2019-09-09T23:30:08.211087Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# length_to_prefix_CMs = {l:torch.matmul(pC1X012_torch_c, \n",
    "#                                        length_to_prefix_OH_matrices[l].transpose(1,2).cuda().type(torch.double)).cpu()\n",
    "#                           if length_to_prefix_OH_matrices[l].size()[1] != 0 else None\n",
    "#                         for l in tqdm(length_to_prefix_OH_matrices)}\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for calculating CMs for all wordforms of a given length (in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:15:22.214761Z",
     "start_time": "2019-09-12T02:15:21.142762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         50G        105G         11M         31G        135G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:15:28.341559Z",
     "start_time": "2019-09-12T02:15:23.034422Z"
    }
   },
   "outputs": [],
   "source": [
    "length_to_wordform_OH_matrices = {l:torch.stack([torch.tensor(dsToTriphoneOHs(w, X012OHmap), dtype=torch.short)\n",
    "                                               for w in wordformsByLength[l]])\n",
    "                                  for l in wordlengthsInclEdges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T02:15:29.858519Z",
     "start_time": "2019-09-12T02:15:29.237726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           187G         27G        127G         11M         31G        158G\r\n",
      "Swap:           18G         14M         18G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T02:15:41.570Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([10, 1, 10540])\n",
      "\t1 chunks of size 3000\n",
      "\tCM stack shape = torch.Size([10, 57798, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"3\": shape (10, 57798, 1), type \"<f8\">"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 1/18 [00:00<00:01,  9.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([253, 2, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([253, 57798, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"4\": shape (253, 57798, 2), type \"<f8\">"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 2/18 [00:01<00:08,  1.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([1843, 3, 10540])\n",
      "\t1 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([1843, 57798, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"5\": shape (1843, 57798, 3), type \"<f8\">"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 3/18 [00:17<01:16,  5.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([4166, 4, 10540])\n",
      "\t2 chunks of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:28<00:28, 28.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:40<00:00, 23.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCM stack shape = torch.Size([4166, 57798, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"6\": shape (4166, 57798, 4), type \"<f8\">"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 4/18 [01:04<04:08, 17.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\tOH stack size = torch.Size([5599, 5, 10540])\n",
      "\t2 chunks of size 3000\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#after driver update:\n",
    "\n",
    "#chunk size 100 -> ≈?m on tarski, peak GPU usage around 6GB\n",
    "#chunk size 2000 -> \n",
    "#chunk size 3000 -> ≈est ?m on tarski, peak GPU usage around 21GB, but GPU runs out of memory on l=9\n",
    "\n",
    "my_ext = '.hdf5'\n",
    "OD_by_length_wordform_fp = o + '_by_length_by_wordform' + my_ext\n",
    "with h5py.File(OD_by_length_wordform_fp, 'w') as f:\n",
    "#     length_to_prefix_CMs = []\n",
    "    # length_to_prefix_CMs = dict()\n",
    "    \n",
    "    chunk_size = 3000\n",
    "#     chunk_size = 2000\n",
    "#     chunk_size = 1000\n",
    "#     chunk_size = 100\n",
    "#     chunk_size = 300\n",
    "    \n",
    "    for l in tqdm(length_to_wordform_OH_matrices):\n",
    "        if length_to_wordform_OH_matrices[l].size()[1] == 0:\n",
    "            \n",
    "            #see http://docs.h5py.org/en/stable/high/dataset.html#creating-and-reading-empty-or-null-datasets-and-attributes\n",
    "            f.create_dataset(str(l), dtype='float64') #h5py empty \n",
    "#             length_to_prefix_CMs.append(None)\n",
    "    #         length_to_prefix_CMs[l] = None\n",
    "        else:\n",
    "            memTrigger()\n",
    "\n",
    "            print('-' * 40)\n",
    "            print(f\"\\tOH stack size = {length_to_wordform_OH_matrices[l].size()}\")\n",
    "\n",
    "            chunks = chunkList(chunk_size, length_to_wordform_OH_matrices[l])\n",
    "            print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "            \n",
    "            try:\n",
    "                chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "            except:\n",
    "                print(f'Chunk size {chunk_size} too large')\n",
    "                torch.cuda.empty_cache()\n",
    "                if chunk_size > 1000:\n",
    "                    chunk_size = chunk_size - 1000\n",
    "                    print(f'Reducing chunk size by 1000 to {chunk_size}')\n",
    "                    chunks = chunkList(chunk_size, length_to_wordform_OH_matrices[l])\n",
    "                    print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "                    try:\n",
    "                        chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "                    except:\n",
    "                        print(f'Chunk size {chunk_size} *still * too large')\n",
    "                        torch.cuda.empty_cache()\n",
    "                        if chunk_size > 1000:\n",
    "                            chunk_size = chunk_size - 1000\n",
    "                            print(f'Reducing chunk size by another 1000 to {chunk_size}')\n",
    "                        else:\n",
    "                            chunk_size = int(chunk_size / 2)\n",
    "                            print(f'Reducing chunk size by half to {chunk_size}')\n",
    "                        chunks = chunkList(chunk_size, length_to_wordform_OH_matrices[l])\n",
    "                        print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "                        chunked_result = [torch.matmul(pC1X012_torch_c, \n",
    "                                               chunk.transpose(1,2).type(torch.double).cuda()).cpu()\n",
    "                                  for chunk in tqdm(chunks, total=len(chunks))]\n",
    "                else:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    raise Exception('Out of memory.')\n",
    "            del chunks\n",
    "            \n",
    "            result = torch.cat(chunked_result)\n",
    "            del chunked_result\n",
    "            print(f\"\\tCM stack shape = {result.shape}\")\n",
    "            result_np = result.numpy()\n",
    "            del result\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "#             length_to_prefix_CMs.append(result)\n",
    "    #         length_to_prefix_CMs[l] = result\n",
    "            f.create_dataset(str(l), result_np.shape, dtype='float64', data=result_np)\n",
    "            del result_np\n",
    "\n",
    "#             del chunks\n",
    "#             del chunked_result\n",
    "#             del result\n",
    "#             del result_np\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T02:15:54.469Z"
    }
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T02:15:55.433Z"
    }
   },
   "outputs": [],
   "source": [
    "OD_by_length_wordform_f = h5py.File(OD_by_length_wordform_fp, 'r')\n",
    "OD_by_length_wordform_f.keys()\n",
    "# OD_by_length_wordform_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T02:16:46.974Z"
    }
   },
   "outputs": [],
   "source": [
    "CMs_by_length_by_wordform_md = {\n",
    "#     'r':r,\n",
    "    'length':len(length_to_wordform_OH_matrices),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W; final ⋉ stripped',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W after final ⋉ stripped',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + '_by_length_by_wordform.hdf5'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_wordform_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {'organization':'Each l maps to a pytorch tensor of dim (n_{l-2}, |Y012s|, l-2), where the ordering of the first dimension corresponds to alphabetical order of wordforms of length l.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for creating desired CM stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want \n",
    " - `CMsByPrefixIndex`\n",
    " - `CMsByWordformIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " - `exactCMsByLengthByWordformIndex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The just-computed dictionary `length_to_prefix_CMs` takes up $\\approx$130 GB (for Hammond's `newdic`), so it's not reasonable to expect to be able to create and hold all of these in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T00:05:58.059085Z",
     "start_time": "2019-09-12T00:05:58.051086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_OD'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_OD_by_length_by_prefix.hdf5'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o\n",
    "# o + '_by_length_by_prefix.pickle'\n",
    "o + '_by_length_by_prefix.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T03:00:06.350562Z",
     "start_time": "2019-09-10T03:00:05.987337Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T03:01:59.398763Z",
     "start_time": "2019-09-10T03:01:59.396096Z"
    }
   },
   "outputs": [],
   "source": [
    "# won't work because \"OverflowError: cannot serialize a bytes object larger than 4 GiB\"\n",
    "# pickle.dump(length_to_prefix_CMs, open(o + '_by_length_by_prefix.pickle', 'wb'))\n",
    "\n",
    "# won't work because \"error: 'I' format requires 0 <= number <= 4294967295\"\n",
    "# dill.dump(length_to_prefix_CMs, open(o + '_by_length_by_prefix.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options: \n",
    "# - pickle/dill the tensor for each length\n",
    "# - cast to np.ndarray and memory map the ndarray for each length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMs_by_length_by_prefix_md = {\n",
    "    'r':r,\n",
    "    'length':len(length_to_prefix_CMs)\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W; final ⋉ stripped',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W after final ⋉ stripped',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + '_by_length_by_prefix.hdf5'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_prefix_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {'organization':'Each l maps to a pytorch tensor of dim (n_{l-2}, |Y012s|, l-2), where the ordering of the first dimension corresponds to alphabetical order of prefixes of length l.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *trash*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T22:46:48.957609Z",
     "start_time": "2019-09-09T22:46:48.953496Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkList(chunk_size, l):\n",
    "    return [l[i * chunk_size:(i + 1) * chunk_size] for i in range((len(l) + chunk_size - 1) // chunk_size )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:36:57.933685Z",
     "start_time": "2019-09-09T20:36:57.930942Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:36:58.591051Z",
     "start_time": "2019-09-09T20:36:58.584691Z"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from itertools.groupby documentation\n",
    "def persistent_groupby(data, keyfunc, verbose=False):\n",
    "    groups = []\n",
    "    uniquekeys = []\n",
    "    # data = sorted(data, key=keyfunc)\n",
    "    for k, g in groupby(data, keyfunc):\n",
    "        groups.append(list(g))      # Store group iterator as a list\n",
    "        uniquekeys.append(k)\n",
    "    if verbose:\n",
    "        print(len(groups))\n",
    "        print(len(uniquekeys))\n",
    "        print([len(g) for g in groups])\n",
    "        print(uniquekeys)\n",
    "        groups[0]\n",
    "    return groups, uniquekeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:36:59.785633Z",
     "start_time": "2019-09-09T20:36:59.782770Z"
    }
   },
   "outputs": [],
   "source": [
    "# # adapted from itertools.groupby documentation\n",
    "# groups = []\n",
    "# uniquekeys = []\n",
    "# keyfunc = lambda p: len(ds2t(p)) > 2\n",
    "# # data = sorted(data, key=keyfunc)\n",
    "# data = Ps_t\n",
    "# for k, g in groupby(data, keyfunc):\n",
    "#     groups.append(list(g))      # Store group iterator as a list\n",
    "#     uniquekeys.append(k)\n",
    "# print(len(groups))\n",
    "# print(len(uniquekeys))\n",
    "# print([len(g) for g in groups])\n",
    "# print(uniquekeys)\n",
    "# groups[0]\n",
    "# groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:36:59.950601Z",
     "start_time": "2019-09-09T20:36:59.919085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "72\n",
      "[2, 145, 1, 158, 1, 1436, 1, 1892, 1, 452, 1, 109, 1, 1446, 1, 724, 1, 1174, 1, 75, 1, 256, 1, 3109, 1, 1029, 1, 1917, 1, 742, 1, 205, 1, 2815, 1, 3514, 1, 1337, 1, 276, 1, 2, 1, 643, 1, 573, 1, 64, 1, 1084, 1, 38, 1, 774, 1, 13, 1, 1230, 1, 34, 1, 640, 1, 2343, 1, 1828, 1, 268, 1, 488, 1, 189]\n",
      "[False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "Ps_t_grouped, Ps_t_grouped_uniquekeys = persistent_groupby(Ps_t, \n",
    "                                                           keyfunc = lambda p: len(ds2t(p)) > 2, \n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T22:17:19.732565Z",
     "start_time": "2019-09-09T22:17:18.932927Z"
    }
   },
   "outputs": [],
   "source": [
    "pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.463368Z",
     "start_time": "2019-09-09T20:43:53.379363Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 145\n",
      "\t2 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 2/72 [00:29<17:15, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 158\n",
      "\t2 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 4/72 [01:01<17:14, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1436\n",
      "\t15 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 6/72 [06:00<1:00:55, 55.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1892\n",
      "\t19 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 8/72 [12:33<1:44:19, 97.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 452\n",
      "\t5 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 10/72 [14:07<1:25:19, 82.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 109\n",
      "\t2 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 12/72 [14:30<1:01:11, 61.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1446\n",
      "\t15 chunks of size 100:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-43f69620a18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                            for each in prefixes_OHs_tr_t_c)\n\u001b[1;32m     15\u001b[0m             \u001b[0mgroup_CMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_CMs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mCMsByPrefixIndex_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_CMs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Group of prefixes of length < 3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-43f69620a18f>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m             group_CMs_c = (sourcePrefixStackToChannelMatrix_t_c(each)\n\u001b[1;32m     14\u001b[0m                            for each in prefixes_OHs_tr_t_c)\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mgroup_CMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_CMs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mCMsByPrefixIndex_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_CMs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CMsByPrefixIndex_torch = []\n",
    "for group, key in tqdm(zip(Ps_t_grouped, Ps_t_grouped_uniquekeys), total=len(Ps_t_grouped)):\n",
    "    if key:\n",
    "        print(f\"Group size: {len(group)}\")\n",
    "        prefixes_OHs_tr_t = [torch.tensor(dsToTriphoneOHs(p, X012OHmap)).t()\n",
    "                             for p in group]\n",
    "        chunk_size = 100\n",
    "        chunks = chunkList(chunk_size, prefixes_OHs_tr_t)\n",
    "        print(f\"\\t{len(chunks)} chunks of size {chunk_size}:\")\n",
    "        for chunk in chunks:\n",
    "            prefixes_OHs_tr_t_c = (each.cuda() \n",
    "                                   for each in chunk)\n",
    "            group_CMs_c = (sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in prefixes_OHs_tr_t_c)\n",
    "            group_CMs = (each.cpu() for each in group_CMs_c)\n",
    "            CMsByPrefixIndex_torch.extend(group_CMs)\n",
    "    else:\n",
    "        print(f\"Group of prefixes of length < 3\")\n",
    "        group_CMs = [None for p in group]\n",
    "        CMsByPrefixIndex_torch.extend(group_CMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.465151Z",
     "start_time": "2019-09-09T20:45:37.113Z"
    }
   },
   "outputs": [],
   "source": [
    "len(CMsByPrefixIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.465771Z",
     "start_time": "2019-09-09T20:45:38.600Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.466257Z",
     "start_time": "2019-09-09T20:45:39.935Z"
    }
   },
   "outputs": [],
   "source": [
    "Ps_t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.466842Z",
     "start_time": "2019-09-09T20:45:41.985Z"
    }
   },
   "outputs": [],
   "source": [
    "aɪdCM = CMsByPrefixIndex_torch[2]\n",
    "aɪdCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T21:00:08.467795Z",
     "start_time": "2019-09-09T20:45:43.361Z"
    }
   },
   "outputs": [],
   "source": [
    "aɪdCM.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:42:57.726720Z",
     "start_time": "2019-09-09T20:42:57.722593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aɪdCM[6892, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:36.922954Z",
     "start_time": "2019-07-27T22:17:36.918804Z"
    }
   },
   "outputs": [],
   "source": [
    "# CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "#                      for s in source_seqs]\n",
    "# CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "# CMsByPrefixIndex[3].shape\n",
    "# CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(p)\n",
    "#                      for p in prefixes_t]\n",
    "# CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "# CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:41:17.671125Z",
     "start_time": "2019-09-09T19:41:17.657888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{w for w in Ws_t if len(ds2t(w)) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:41:19.543352Z",
     "start_time": "2019-09-09T19:41:19.528982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[9172]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "Ws_t_grouped, Ws_t_grouped_uniquekeys = persistent_groupby(Ws_t, \n",
    "                                                           keyfunc = lambda w: len(ds2t(w)) > 2, \n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:22:16.204603Z",
     "start_time": "2019-09-09T20:19:07.198327Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1835 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1835 chunks of size 5:\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:09\n",
      "\t\tPerforming cuda ops... @ 13:19:09\n",
      "\t\tLoading data onto cpu... @ 13:19:09\n",
      "\t\tAccumulating results...> @ 13:19:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/1835 [00:01<33:15,  1.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:10\n",
      "\t\tPerforming cuda ops... @ 13:19:10\n",
      "\t\tLoading data onto cpu... @ 13:19:10\n",
      "\t\tAccumulating results...> @ 13:19:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/1835 [00:02<32:49,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:11\n",
      "\t\tPerforming cuda ops... @ 13:19:11\n",
      "\t\tLoading data onto cpu... @ 13:19:11\n",
      "\t\tAccumulating results...> @ 13:19:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/1835 [00:03<32:31,  1.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:12\n",
      "\t\tPerforming cuda ops... @ 13:19:12\n",
      "\t\tLoading data onto cpu... @ 13:19:12\n",
      "\t\tAccumulating results...> @ 13:19:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 4/1835 [00:04<32:16,  1.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:13\n",
      "\t\tPerforming cuda ops... @ 13:19:13\n",
      "\t\tLoading data onto cpu... @ 13:19:13\n",
      "\t\tAccumulating results...> @ 13:19:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 5/1835 [00:05<32:06,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:14\n",
      "\t\tPerforming cuda ops... @ 13:19:14\n",
      "\t\tLoading data onto cpu... @ 13:19:14\n",
      "\t\tAccumulating results...> @ 13:19:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 6/1835 [00:06<31:58,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:15\n",
      "\t\tPerforming cuda ops... @ 13:19:15\n",
      "\t\tLoading data onto cpu... @ 13:19:15\n",
      "\t\tAccumulating results...> @ 13:19:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/1835 [00:07<31:53,  1.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:16\n",
      "\t\tPerforming cuda ops... @ 13:19:16\n",
      "\t\tLoading data onto cpu... @ 13:19:16\n",
      "\t\tAccumulating results...> @ 13:19:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 8/1835 [00:08<31:48,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:17\n",
      "\t\tPerforming cuda ops... @ 13:19:17\n",
      "\t\tLoading data onto cpu... @ 13:19:17\n",
      "\t\tAccumulating results...> @ 13:19:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 9/1835 [00:09<31:46,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:18\n",
      "\t\tPerforming cuda ops... @ 13:19:18\n",
      "\t\tLoading data onto cpu... @ 13:19:18\n",
      "\t\tAccumulating results...> @ 13:19:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 10/1835 [00:10<31:44,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:19\n",
      "\t\tPerforming cuda ops... @ 13:19:19\n",
      "\t\tLoading data onto cpu... @ 13:19:19\n",
      "\t\tAccumulating results...> @ 13:19:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 11/1835 [00:11<31:43,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:20\n",
      "\t\tPerforming cuda ops... @ 13:19:20\n",
      "\t\tLoading data onto cpu... @ 13:19:20\n",
      "\t\tAccumulating results...> @ 13:19:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 12/1835 [00:12<31:41,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:21\n",
      "\t\tPerforming cuda ops... @ 13:19:21\n",
      "\t\tLoading data onto cpu... @ 13:19:21\n",
      "\t\tAccumulating results...> @ 13:19:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 13/1835 [00:13<31:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:23\n",
      "\t\tPerforming cuda ops... @ 13:19:23\n",
      "\t\tLoading data onto cpu... @ 13:19:23\n",
      "\t\tAccumulating results...> @ 13:19:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 14/1835 [00:14<31:38,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:24\n",
      "\t\tPerforming cuda ops... @ 13:19:24\n",
      "\t\tLoading data onto cpu... @ 13:19:24\n",
      "\t\tAccumulating results...> @ 13:19:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 15/1835 [00:15<31:37,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:25\n",
      "\t\tPerforming cuda ops... @ 13:19:25\n",
      "\t\tLoading data onto cpu... @ 13:19:25\n",
      "\t\tAccumulating results...> @ 13:19:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 16/1835 [00:16<31:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:26\n",
      "\t\tPerforming cuda ops... @ 13:19:26\n",
      "\t\tLoading data onto cpu... @ 13:19:26\n",
      "\t\tAccumulating results...> @ 13:19:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 17/1835 [00:17<31:33,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:27\n",
      "\t\tPerforming cuda ops... @ 13:19:27\n",
      "\t\tLoading data onto cpu... @ 13:19:27\n",
      "\t\tAccumulating results...> @ 13:19:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 18/1835 [00:18<31:33,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:28\n",
      "\t\tPerforming cuda ops... @ 13:19:28\n",
      "\t\tLoading data onto cpu... @ 13:19:28\n",
      "\t\tAccumulating results...> @ 13:19:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 19/1835 [00:19<31:32,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:29\n",
      "\t\tPerforming cuda ops... @ 13:19:29\n",
      "\t\tLoading data onto cpu... @ 13:19:29\n",
      "\t\tAccumulating results...> @ 13:19:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 20/1835 [00:20<31:30,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:30\n",
      "\t\tPerforming cuda ops... @ 13:19:30\n",
      "\t\tLoading data onto cpu... @ 13:19:30\n",
      "\t\tAccumulating results...> @ 13:19:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 21/1835 [00:21<31:29,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:31\n",
      "\t\tPerforming cuda ops... @ 13:19:31\n",
      "\t\tLoading data onto cpu... @ 13:19:31\n",
      "\t\tAccumulating results...> @ 13:19:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 22/1835 [00:22<31:27,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:32\n",
      "\t\tPerforming cuda ops... @ 13:19:32\n",
      "\t\tLoading data onto cpu... @ 13:19:32\n",
      "\t\tAccumulating results...> @ 13:19:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 23/1835 [00:24<31:26,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:33\n",
      "\t\tPerforming cuda ops... @ 13:19:33\n",
      "\t\tLoading data onto cpu... @ 13:19:33\n",
      "\t\tAccumulating results...> @ 13:19:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 24/1835 [00:25<31:25,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:34\n",
      "\t\tPerforming cuda ops... @ 13:19:34\n",
      "\t\tLoading data onto cpu... @ 13:19:34\n",
      "\t\tAccumulating results...> @ 13:19:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 25/1835 [00:26<31:24,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:35\n",
      "\t\tPerforming cuda ops... @ 13:19:35\n",
      "\t\tLoading data onto cpu... @ 13:19:35\n",
      "\t\tAccumulating results...> @ 13:19:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 26/1835 [00:27<31:22,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:36\n",
      "\t\tPerforming cuda ops... @ 13:19:36\n",
      "\t\tLoading data onto cpu... @ 13:19:36\n",
      "\t\tAccumulating results...> @ 13:19:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 27/1835 [00:28<31:21,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:37\n",
      "\t\tPerforming cuda ops... @ 13:19:37\n",
      "\t\tLoading data onto cpu... @ 13:19:37\n",
      "\t\tAccumulating results...> @ 13:19:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 28/1835 [00:29<31:21,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:38\n",
      "\t\tPerforming cuda ops... @ 13:19:38\n",
      "\t\tLoading data onto cpu... @ 13:19:38\n",
      "\t\tAccumulating results...> @ 13:19:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 29/1835 [00:30<31:20,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:39\n",
      "\t\tPerforming cuda ops... @ 13:19:39\n",
      "\t\tLoading data onto cpu... @ 13:19:39\n",
      "\t\tAccumulating results...> @ 13:19:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 30/1835 [00:31<31:19,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:40\n",
      "\t\tPerforming cuda ops... @ 13:19:40\n",
      "\t\tLoading data onto cpu... @ 13:19:40\n",
      "\t\tAccumulating results...> @ 13:19:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 31/1835 [00:32<31:17,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:41\n",
      "\t\tPerforming cuda ops... @ 13:19:41\n",
      "\t\tLoading data onto cpu... @ 13:19:41\n",
      "\t\tAccumulating results...> @ 13:19:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 32/1835 [00:33<31:15,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:42\n",
      "\t\tPerforming cuda ops... @ 13:19:42\n",
      "\t\tLoading data onto cpu... @ 13:19:42\n",
      "\t\tAccumulating results...> @ 13:19:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 33/1835 [00:34<31:14,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:43\n",
      "\t\tPerforming cuda ops... @ 13:19:43\n",
      "\t\tLoading data onto cpu... @ 13:19:43\n",
      "\t\tAccumulating results...> @ 13:19:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 34/1835 [00:35<31:13,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:44\n",
      "\t\tPerforming cuda ops... @ 13:19:44\n",
      "\t\tLoading data onto cpu... @ 13:19:44\n",
      "\t\tAccumulating results...> @ 13:19:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 35/1835 [00:36<31:13,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:45\n",
      "\t\tPerforming cuda ops... @ 13:19:45\n",
      "\t\tLoading data onto cpu... @ 13:19:45\n",
      "\t\tAccumulating results...> @ 13:19:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 36/1835 [00:37<31:12,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:46\n",
      "\t\tPerforming cuda ops... @ 13:19:46\n",
      "\t\tLoading data onto cpu... @ 13:19:46\n",
      "\t\tAccumulating results...> @ 13:19:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 37/1835 [00:38<31:11,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:48\n",
      "\t\tPerforming cuda ops... @ 13:19:48\n",
      "\t\tLoading data onto cpu... @ 13:19:48\n",
      "\t\tAccumulating results...> @ 13:19:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 38/1835 [00:39<31:10,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:49\n",
      "\t\tPerforming cuda ops... @ 13:19:49\n",
      "\t\tLoading data onto cpu... @ 13:19:49\n",
      "\t\tAccumulating results...> @ 13:19:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 39/1835 [00:40<31:09,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:50\n",
      "\t\tPerforming cuda ops... @ 13:19:50\n",
      "\t\tLoading data onto cpu... @ 13:19:50\n",
      "\t\tAccumulating results...> @ 13:19:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 40/1835 [00:41<31:08,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:51\n",
      "\t\tPerforming cuda ops... @ 13:19:51\n",
      "\t\tLoading data onto cpu... @ 13:19:51\n",
      "\t\tAccumulating results...> @ 13:19:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 41/1835 [00:42<31:07,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:52\n",
      "\t\tPerforming cuda ops... @ 13:19:52\n",
      "\t\tLoading data onto cpu... @ 13:19:52\n",
      "\t\tAccumulating results...> @ 13:19:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 42/1835 [00:43<31:06,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:53\n",
      "\t\tPerforming cuda ops... @ 13:19:53\n",
      "\t\tLoading data onto cpu... @ 13:19:53\n",
      "\t\tAccumulating results...> @ 13:19:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 43/1835 [00:44<31:05,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:54\n",
      "\t\tPerforming cuda ops... @ 13:19:54\n",
      "\t\tLoading data onto cpu... @ 13:19:54\n",
      "\t\tAccumulating results...> @ 13:19:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 44/1835 [00:45<31:03,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:55\n",
      "\t\tPerforming cuda ops... @ 13:19:55\n",
      "\t\tLoading data onto cpu... @ 13:19:55\n",
      "\t\tAccumulating results...> @ 13:19:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 45/1835 [00:46<31:02,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:56\n",
      "\t\tPerforming cuda ops... @ 13:19:56\n",
      "\t\tLoading data onto cpu... @ 13:19:56\n",
      "\t\tAccumulating results...> @ 13:19:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 46/1835 [00:47<31:01,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:57\n",
      "\t\tPerforming cuda ops... @ 13:19:57\n",
      "\t\tLoading data onto cpu... @ 13:19:57\n",
      "\t\tAccumulating results...> @ 13:19:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 47/1835 [00:48<31:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:58\n",
      "\t\tPerforming cuda ops... @ 13:19:58\n",
      "\t\tLoading data onto cpu... @ 13:19:58\n",
      "\t\tAccumulating results...> @ 13:19:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 48/1835 [00:50<30:59,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:19:59\n",
      "\t\tPerforming cuda ops... @ 13:19:59\n",
      "\t\tLoading data onto cpu... @ 13:19:59\n",
      "\t\tAccumulating results...> @ 13:19:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 49/1835 [00:51<30:58,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:00\n",
      "\t\tPerforming cuda ops... @ 13:20:00\n",
      "\t\tLoading data onto cpu... @ 13:20:00\n",
      "\t\tAccumulating results...> @ 13:20:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 50/1835 [00:52<30:57,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:01\n",
      "\t\tPerforming cuda ops... @ 13:20:01\n",
      "\t\tLoading data onto cpu... @ 13:20:01\n",
      "\t\tAccumulating results...> @ 13:20:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 51/1835 [00:53<30:56,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:02\n",
      "\t\tPerforming cuda ops... @ 13:20:02\n",
      "\t\tLoading data onto cpu... @ 13:20:02\n",
      "\t\tAccumulating results...> @ 13:20:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 52/1835 [00:54<30:55,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:03\n",
      "\t\tPerforming cuda ops... @ 13:20:03\n",
      "\t\tLoading data onto cpu... @ 13:20:03\n",
      "\t\tAccumulating results...> @ 13:20:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 53/1835 [00:55<30:54,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:04\n",
      "\t\tPerforming cuda ops... @ 13:20:04\n",
      "\t\tLoading data onto cpu... @ 13:20:04\n",
      "\t\tAccumulating results...> @ 13:20:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 54/1835 [00:56<30:53,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:05\n",
      "\t\tPerforming cuda ops... @ 13:20:05\n",
      "\t\tLoading data onto cpu... @ 13:20:05\n",
      "\t\tAccumulating results...> @ 13:20:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 55/1835 [00:57<30:52,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:06\n",
      "\t\tPerforming cuda ops... @ 13:20:06\n",
      "\t\tLoading data onto cpu... @ 13:20:06\n",
      "\t\tAccumulating results...> @ 13:20:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 56/1835 [00:58<30:51,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:07\n",
      "\t\tPerforming cuda ops... @ 13:20:07\n",
      "\t\tLoading data onto cpu... @ 13:20:07\n",
      "\t\tAccumulating results...> @ 13:20:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 57/1835 [00:59<30:50,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:08\n",
      "\t\tPerforming cuda ops... @ 13:20:08\n",
      "\t\tLoading data onto cpu... @ 13:20:08\n",
      "\t\tAccumulating results...> @ 13:20:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 58/1835 [01:00<30:49,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:09\n",
      "\t\tPerforming cuda ops... @ 13:20:09\n",
      "\t\tLoading data onto cpu... @ 13:20:09\n",
      "\t\tAccumulating results...> @ 13:20:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 59/1835 [01:01<30:47,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:10\n",
      "\t\tPerforming cuda ops... @ 13:20:10\n",
      "\t\tLoading data onto cpu... @ 13:20:10\n",
      "\t\tAccumulating results...> @ 13:20:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 60/1835 [01:02<30:46,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:11\n",
      "\t\tPerforming cuda ops... @ 13:20:11\n",
      "\t\tLoading data onto cpu... @ 13:20:11\n",
      "\t\tAccumulating results...> @ 13:20:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 61/1835 [01:03<30:45,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:12\n",
      "\t\tPerforming cuda ops... @ 13:20:12\n",
      "\t\tLoading data onto cpu... @ 13:20:12\n",
      "\t\tAccumulating results...> @ 13:20:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 62/1835 [01:04<30:44,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:14\n",
      "\t\tPerforming cuda ops... @ 13:20:14\n",
      "\t\tLoading data onto cpu... @ 13:20:14\n",
      "\t\tAccumulating results...> @ 13:20:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 63/1835 [01:05<30:43,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:15\n",
      "\t\tPerforming cuda ops... @ 13:20:15\n",
      "\t\tLoading data onto cpu... @ 13:20:15\n",
      "\t\tAccumulating results...> @ 13:20:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 64/1835 [01:06<30:42,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:16\n",
      "\t\tPerforming cuda ops... @ 13:20:16\n",
      "\t\tLoading data onto cpu... @ 13:20:16\n",
      "\t\tAccumulating results...> @ 13:20:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 65/1835 [01:07<30:41,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:17\n",
      "\t\tPerforming cuda ops... @ 13:20:17\n",
      "\t\tLoading data onto cpu... @ 13:20:17\n",
      "\t\tAccumulating results...> @ 13:20:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 66/1835 [01:08<30:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:18\n",
      "\t\tPerforming cuda ops... @ 13:20:18\n",
      "\t\tLoading data onto cpu... @ 13:20:18\n",
      "\t\tAccumulating results...> @ 13:20:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 67/1835 [01:09<30:39,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:19\n",
      "\t\tPerforming cuda ops... @ 13:20:19\n",
      "\t\tLoading data onto cpu... @ 13:20:19\n",
      "\t\tAccumulating results...> @ 13:20:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 68/1835 [01:10<30:39,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:20\n",
      "\t\tPerforming cuda ops... @ 13:20:20\n",
      "\t\tLoading data onto cpu... @ 13:20:20\n",
      "\t\tAccumulating results...> @ 13:20:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 69/1835 [01:11<30:37,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:21\n",
      "\t\tPerforming cuda ops... @ 13:20:21\n",
      "\t\tLoading data onto cpu... @ 13:20:21\n",
      "\t\tAccumulating results...> @ 13:20:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 70/1835 [01:12<30:36,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:22\n",
      "\t\tPerforming cuda ops... @ 13:20:22\n",
      "\t\tLoading data onto cpu... @ 13:20:22\n",
      "\t\tAccumulating results...> @ 13:20:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 71/1835 [01:13<30:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:23\n",
      "\t\tPerforming cuda ops... @ 13:20:23\n",
      "\t\tLoading data onto cpu... @ 13:20:23\n",
      "\t\tAccumulating results...> @ 13:20:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 72/1835 [01:14<30:34,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:24\n",
      "\t\tPerforming cuda ops... @ 13:20:24\n",
      "\t\tLoading data onto cpu... @ 13:20:24\n",
      "\t\tAccumulating results...> @ 13:20:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 73/1835 [01:16<30:33,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:25\n",
      "\t\tPerforming cuda ops... @ 13:20:25\n",
      "\t\tLoading data onto cpu... @ 13:20:25\n",
      "\t\tAccumulating results...> @ 13:20:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 74/1835 [01:17<30:32,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:26\n",
      "\t\tPerforming cuda ops... @ 13:20:26\n",
      "\t\tLoading data onto cpu... @ 13:20:26\n",
      "\t\tAccumulating results...> @ 13:20:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 75/1835 [01:18<30:31,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:27\n",
      "\t\tPerforming cuda ops... @ 13:20:27\n",
      "\t\tLoading data onto cpu... @ 13:20:27\n",
      "\t\tAccumulating results...> @ 13:20:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 76/1835 [01:19<30:30,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:28\n",
      "\t\tPerforming cuda ops... @ 13:20:28\n",
      "\t\tLoading data onto cpu... @ 13:20:28\n",
      "\t\tAccumulating results...> @ 13:20:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 77/1835 [01:20<30:29,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:29\n",
      "\t\tPerforming cuda ops... @ 13:20:29\n",
      "\t\tLoading data onto cpu... @ 13:20:29\n",
      "\t\tAccumulating results...> @ 13:20:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 78/1835 [01:21<30:29,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:30\n",
      "\t\tPerforming cuda ops... @ 13:20:30\n",
      "\t\tLoading data onto cpu... @ 13:20:30\n",
      "\t\tAccumulating results...> @ 13:20:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 79/1835 [01:22<30:28,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:31\n",
      "\t\tPerforming cuda ops... @ 13:20:31\n",
      "\t\tLoading data onto cpu... @ 13:20:31\n",
      "\t\tAccumulating results...> @ 13:20:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 80/1835 [01:23<30:27,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:32\n",
      "\t\tPerforming cuda ops... @ 13:20:32\n",
      "\t\tLoading data onto cpu... @ 13:20:32\n",
      "\t\tAccumulating results...> @ 13:20:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 81/1835 [01:24<30:25,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:33\n",
      "\t\tPerforming cuda ops... @ 13:20:33\n",
      "\t\tLoading data onto cpu... @ 13:20:33\n",
      "\t\tAccumulating results...> @ 13:20:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 82/1835 [01:25<30:24,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:34\n",
      "\t\tPerforming cuda ops... @ 13:20:34\n",
      "\t\tLoading data onto cpu... @ 13:20:34\n",
      "\t\tAccumulating results...> @ 13:20:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 83/1835 [01:26<30:23,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:35\n",
      "\t\tPerforming cuda ops... @ 13:20:35\n",
      "\t\tLoading data onto cpu... @ 13:20:35\n",
      "\t\tAccumulating results...> @ 13:20:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 84/1835 [01:27<30:22,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:36\n",
      "\t\tPerforming cuda ops... @ 13:20:36\n",
      "\t\tLoading data onto cpu... @ 13:20:36\n",
      "\t\tAccumulating results...> @ 13:20:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 85/1835 [01:28<30:21,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:37\n",
      "\t\tPerforming cuda ops... @ 13:20:37\n",
      "\t\tLoading data onto cpu... @ 13:20:37\n",
      "\t\tAccumulating results...> @ 13:20:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 86/1835 [01:29<30:19,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:39\n",
      "\t\tPerforming cuda ops... @ 13:20:39\n",
      "\t\tLoading data onto cpu... @ 13:20:39\n",
      "\t\tAccumulating results...> @ 13:20:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 87/1835 [01:30<30:19,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:40\n",
      "\t\tPerforming cuda ops... @ 13:20:40\n",
      "\t\tLoading data onto cpu... @ 13:20:40\n",
      "\t\tAccumulating results...> @ 13:20:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 88/1835 [01:31<30:18,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:41\n",
      "\t\tPerforming cuda ops... @ 13:20:41\n",
      "\t\tLoading data onto cpu... @ 13:20:41\n",
      "\t\tAccumulating results...> @ 13:20:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 89/1835 [01:32<30:17,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:42\n",
      "\t\tPerforming cuda ops... @ 13:20:42\n",
      "\t\tLoading data onto cpu... @ 13:20:42\n",
      "\t\tAccumulating results...> @ 13:20:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 90/1835 [01:33<30:16,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:43\n",
      "\t\tPerforming cuda ops... @ 13:20:43\n",
      "\t\tLoading data onto cpu... @ 13:20:43\n",
      "\t\tAccumulating results...> @ 13:20:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 91/1835 [01:34<30:15,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:44\n",
      "\t\tPerforming cuda ops... @ 13:20:44\n",
      "\t\tLoading data onto cpu... @ 13:20:44\n",
      "\t\tAccumulating results...> @ 13:20:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 92/1835 [01:35<30:13,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:45\n",
      "\t\tPerforming cuda ops... @ 13:20:45\n",
      "\t\tLoading data onto cpu... @ 13:20:45\n",
      "\t\tAccumulating results...> @ 13:20:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 93/1835 [01:36<30:12,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:46\n",
      "\t\tPerforming cuda ops... @ 13:20:46\n",
      "\t\tLoading data onto cpu... @ 13:20:46\n",
      "\t\tAccumulating results...> @ 13:20:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 94/1835 [01:37<30:11,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:47\n",
      "\t\tPerforming cuda ops... @ 13:20:47\n",
      "\t\tLoading data onto cpu... @ 13:20:47\n",
      "\t\tAccumulating results...> @ 13:20:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 95/1835 [01:38<30:10,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:48\n",
      "\t\tPerforming cuda ops... @ 13:20:48\n",
      "\t\tLoading data onto cpu... @ 13:20:48\n",
      "\t\tAccumulating results...> @ 13:20:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 96/1835 [01:39<30:09,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:49\n",
      "\t\tPerforming cuda ops... @ 13:20:49\n",
      "\t\tLoading data onto cpu... @ 13:20:49\n",
      "\t\tAccumulating results...> @ 13:20:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 97/1835 [01:41<30:09,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:50\n",
      "\t\tPerforming cuda ops... @ 13:20:50\n",
      "\t\tLoading data onto cpu... @ 13:20:50\n",
      "\t\tAccumulating results...> @ 13:20:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 98/1835 [01:42<30:07,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:51\n",
      "\t\tPerforming cuda ops... @ 13:20:51\n",
      "\t\tLoading data onto cpu... @ 13:20:51\n",
      "\t\tAccumulating results...> @ 13:20:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 99/1835 [01:43<30:06,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:52\n",
      "\t\tPerforming cuda ops... @ 13:20:52\n",
      "\t\tLoading data onto cpu... @ 13:20:52\n",
      "\t\tAccumulating results...> @ 13:20:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 100/1835 [01:44<30:06,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:53\n",
      "\t\tPerforming cuda ops... @ 13:20:53\n",
      "\t\tLoading data onto cpu... @ 13:20:53\n",
      "\t\tAccumulating results...> @ 13:20:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 101/1835 [01:45<30:05,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:54\n",
      "\t\tPerforming cuda ops... @ 13:20:54\n",
      "\t\tLoading data onto cpu... @ 13:20:54\n",
      "\t\tAccumulating results...> @ 13:20:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 102/1835 [01:46<30:04,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:55\n",
      "\t\tPerforming cuda ops... @ 13:20:55\n",
      "\t\tLoading data onto cpu... @ 13:20:55\n",
      "\t\tAccumulating results...> @ 13:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 103/1835 [01:47<30:03,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:56\n",
      "\t\tPerforming cuda ops... @ 13:20:56\n",
      "\t\tLoading data onto cpu... @ 13:20:56\n",
      "\t\tAccumulating results...> @ 13:20:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 104/1835 [01:48<30:02,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:57\n",
      "\t\tPerforming cuda ops... @ 13:20:57\n",
      "\t\tLoading data onto cpu... @ 13:20:57\n",
      "\t\tAccumulating results...> @ 13:20:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 105/1835 [01:49<30:01,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:58\n",
      "\t\tPerforming cuda ops... @ 13:20:58\n",
      "\t\tLoading data onto cpu... @ 13:20:58\n",
      "\t\tAccumulating results...> @ 13:20:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 106/1835 [01:50<30:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:20:59\n",
      "\t\tPerforming cuda ops... @ 13:20:59\n",
      "\t\tLoading data onto cpu... @ 13:20:59\n",
      "\t\tAccumulating results...> @ 13:20:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 107/1835 [01:51<29:58,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:00\n",
      "\t\tPerforming cuda ops... @ 13:21:00\n",
      "\t\tLoading data onto cpu... @ 13:21:00\n",
      "\t\tAccumulating results...> @ 13:21:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 108/1835 [01:52<29:57,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:01\n",
      "\t\tPerforming cuda ops... @ 13:21:01\n",
      "\t\tLoading data onto cpu... @ 13:21:01\n",
      "\t\tAccumulating results...> @ 13:21:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 109/1835 [01:53<29:56,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:02\n",
      "\t\tPerforming cuda ops... @ 13:21:02\n",
      "\t\tLoading data onto cpu... @ 13:21:02\n",
      "\t\tAccumulating results...> @ 13:21:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 110/1835 [01:54<29:55,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:03\n",
      "\t\tPerforming cuda ops... @ 13:21:03\n",
      "\t\tLoading data onto cpu... @ 13:21:03\n",
      "\t\tAccumulating results...> @ 13:21:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 111/1835 [01:55<29:54,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:05\n",
      "\t\tPerforming cuda ops... @ 13:21:05\n",
      "\t\tLoading data onto cpu... @ 13:21:05\n",
      "\t\tAccumulating results...> @ 13:21:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 112/1835 [01:56<29:53,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:06\n",
      "\t\tPerforming cuda ops... @ 13:21:06\n",
      "\t\tLoading data onto cpu... @ 13:21:06\n",
      "\t\tAccumulating results...> @ 13:21:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 113/1835 [01:57<29:52,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:07\n",
      "\t\tPerforming cuda ops... @ 13:21:07\n",
      "\t\tLoading data onto cpu... @ 13:21:07\n",
      "\t\tAccumulating results...> @ 13:21:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 114/1835 [01:58<29:51,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:08\n",
      "\t\tPerforming cuda ops... @ 13:21:08\n",
      "\t\tLoading data onto cpu... @ 13:21:08\n",
      "\t\tAccumulating results...> @ 13:21:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 115/1835 [01:59<29:50,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:09\n",
      "\t\tPerforming cuda ops... @ 13:21:09\n",
      "\t\tLoading data onto cpu... @ 13:21:09\n",
      "\t\tAccumulating results...> @ 13:21:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 116/1835 [02:00<29:49,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:10\n",
      "\t\tPerforming cuda ops... @ 13:21:10\n",
      "\t\tLoading data onto cpu... @ 13:21:10\n",
      "\t\tAccumulating results...> @ 13:21:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 117/1835 [02:01<29:48,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:11\n",
      "\t\tPerforming cuda ops... @ 13:21:11\n",
      "\t\tLoading data onto cpu... @ 13:21:11\n",
      "\t\tAccumulating results...> @ 13:21:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 118/1835 [02:02<29:47,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:12\n",
      "\t\tPerforming cuda ops... @ 13:21:12\n",
      "\t\tLoading data onto cpu... @ 13:21:12\n",
      "\t\tAccumulating results...> @ 13:21:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 119/1835 [02:03<29:46,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:13\n",
      "\t\tPerforming cuda ops... @ 13:21:13\n",
      "\t\tLoading data onto cpu... @ 13:21:13\n",
      "\t\tAccumulating results...> @ 13:21:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 120/1835 [02:04<29:45,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:14\n",
      "\t\tPerforming cuda ops... @ 13:21:14\n",
      "\t\tLoading data onto cpu... @ 13:21:14\n",
      "\t\tAccumulating results...> @ 13:21:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 121/1835 [02:06<29:43,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:15\n",
      "\t\tPerforming cuda ops... @ 13:21:15\n",
      "\t\tLoading data onto cpu... @ 13:21:15\n",
      "\t\tAccumulating results...> @ 13:21:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 122/1835 [02:07<29:42,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:16\n",
      "\t\tPerforming cuda ops... @ 13:21:16\n",
      "\t\tLoading data onto cpu... @ 13:21:16\n",
      "\t\tAccumulating results...> @ 13:21:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 123/1835 [02:08<29:41,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:17\n",
      "\t\tPerforming cuda ops... @ 13:21:17\n",
      "\t\tLoading data onto cpu... @ 13:21:17\n",
      "\t\tAccumulating results...> @ 13:21:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 124/1835 [02:09<29:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:18\n",
      "\t\tPerforming cuda ops... @ 13:21:18\n",
      "\t\tLoading data onto cpu... @ 13:21:18\n",
      "\t\tAccumulating results...> @ 13:21:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 125/1835 [02:10<29:40,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:19\n",
      "\t\tPerforming cuda ops... @ 13:21:19\n",
      "\t\tLoading data onto cpu... @ 13:21:19\n",
      "\t\tAccumulating results...> @ 13:21:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 126/1835 [02:11<29:38,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:20\n",
      "\t\tPerforming cuda ops... @ 13:21:20\n",
      "\t\tLoading data onto cpu... @ 13:21:20\n",
      "\t\tAccumulating results...> @ 13:21:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 127/1835 [02:12<29:37,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:21\n",
      "\t\tPerforming cuda ops... @ 13:21:21\n",
      "\t\tLoading data onto cpu... @ 13:21:21\n",
      "\t\tAccumulating results...> @ 13:21:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 128/1835 [02:13<29:37,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:22\n",
      "\t\tPerforming cuda ops... @ 13:21:22\n",
      "\t\tLoading data onto cpu... @ 13:21:22\n",
      "\t\tAccumulating results...> @ 13:21:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 129/1835 [02:14<29:36,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:23\n",
      "\t\tPerforming cuda ops... @ 13:21:23\n",
      "\t\tLoading data onto cpu... @ 13:21:23\n",
      "\t\tAccumulating results...> @ 13:21:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 130/1835 [02:15<29:35,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:24\n",
      "\t\tPerforming cuda ops... @ 13:21:24\n",
      "\t\tLoading data onto cpu... @ 13:21:24\n",
      "\t\tAccumulating results...> @ 13:21:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 131/1835 [02:16<29:33,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:25\n",
      "\t\tPerforming cuda ops... @ 13:21:25\n",
      "\t\tLoading data onto cpu... @ 13:21:25\n",
      "\t\tAccumulating results...> @ 13:21:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 132/1835 [02:17<29:32,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:26\n",
      "\t\tPerforming cuda ops... @ 13:21:26\n",
      "\t\tLoading data onto cpu... @ 13:21:26\n",
      "\t\tAccumulating results...> @ 13:21:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 133/1835 [02:18<29:31,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:27\n",
      "\t\tPerforming cuda ops... @ 13:21:27\n",
      "\t\tLoading data onto cpu... @ 13:21:27\n",
      "\t\tAccumulating results...> @ 13:21:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 134/1835 [02:19<29:30,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:28\n",
      "\t\tPerforming cuda ops... @ 13:21:28\n",
      "\t\tLoading data onto cpu... @ 13:21:28\n",
      "\t\tAccumulating results...> @ 13:21:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 135/1835 [02:20<29:29,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:30\n",
      "\t\tPerforming cuda ops... @ 13:21:30\n",
      "\t\tLoading data onto cpu... @ 13:21:30\n",
      "\t\tAccumulating results...> @ 13:21:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 136/1835 [02:21<29:28,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:31\n",
      "\t\tPerforming cuda ops... @ 13:21:31\n",
      "\t\tLoading data onto cpu... @ 13:21:31\n",
      "\t\tAccumulating results...> @ 13:21:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 137/1835 [02:22<29:27,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:32\n",
      "\t\tPerforming cuda ops... @ 13:21:32\n",
      "\t\tLoading data onto cpu... @ 13:21:32\n",
      "\t\tAccumulating results...> @ 13:21:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 138/1835 [02:23<29:26,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:33\n",
      "\t\tPerforming cuda ops... @ 13:21:33\n",
      "\t\tLoading data onto cpu... @ 13:21:33\n",
      "\t\tAccumulating results...> @ 13:21:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 139/1835 [02:24<29:25,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:34\n",
      "\t\tPerforming cuda ops... @ 13:21:34\n",
      "\t\tLoading data onto cpu... @ 13:21:34\n",
      "\t\tAccumulating results...> @ 13:21:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 140/1835 [02:25<29:25,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:35\n",
      "\t\tPerforming cuda ops... @ 13:21:35\n",
      "\t\tLoading data onto cpu... @ 13:21:35\n",
      "\t\tAccumulating results...> @ 13:21:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 141/1835 [02:26<29:24,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:36\n",
      "\t\tPerforming cuda ops... @ 13:21:36\n",
      "\t\tLoading data onto cpu... @ 13:21:36\n",
      "\t\tAccumulating results...> @ 13:21:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 142/1835 [02:27<29:24,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:37\n",
      "\t\tPerforming cuda ops... @ 13:21:37\n",
      "\t\tLoading data onto cpu... @ 13:21:37\n",
      "\t\tAccumulating results...> @ 13:21:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 143/1835 [02:28<29:23,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:38\n",
      "\t\tPerforming cuda ops... @ 13:21:38\n",
      "\t\tLoading data onto cpu... @ 13:21:38\n",
      "\t\tAccumulating results...> @ 13:21:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 144/1835 [02:29<29:21,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:39\n",
      "\t\tPerforming cuda ops... @ 13:21:39\n",
      "\t\tLoading data onto cpu... @ 13:21:39\n",
      "\t\tAccumulating results...> @ 13:21:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 145/1835 [02:30<29:20,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:40\n",
      "\t\tPerforming cuda ops... @ 13:21:40\n",
      "\t\tLoading data onto cpu... @ 13:21:40\n",
      "\t\tAccumulating results...> @ 13:21:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 146/1835 [02:32<29:19,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:41\n",
      "\t\tPerforming cuda ops... @ 13:21:41\n",
      "\t\tLoading data onto cpu... @ 13:21:41\n",
      "\t\tAccumulating results...> @ 13:21:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 147/1835 [02:33<29:17,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:42\n",
      "\t\tPerforming cuda ops... @ 13:21:42\n",
      "\t\tLoading data onto cpu... @ 13:21:42\n",
      "\t\tAccumulating results...> @ 13:21:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 148/1835 [02:34<29:16,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:43\n",
      "\t\tPerforming cuda ops... @ 13:21:43\n",
      "\t\tLoading data onto cpu... @ 13:21:43\n",
      "\t\tAccumulating results...> @ 13:21:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 149/1835 [02:35<29:15,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:44\n",
      "\t\tPerforming cuda ops... @ 13:21:44\n",
      "\t\tLoading data onto cpu... @ 13:21:44\n",
      "\t\tAccumulating results...> @ 13:21:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 150/1835 [02:36<29:15,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:45\n",
      "\t\tPerforming cuda ops... @ 13:21:45\n",
      "\t\tLoading data onto cpu... @ 13:21:45\n",
      "\t\tAccumulating results...> @ 13:21:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 151/1835 [02:37<29:14,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:46\n",
      "\t\tPerforming cuda ops... @ 13:21:46\n",
      "\t\tLoading data onto cpu... @ 13:21:46\n",
      "\t\tAccumulating results...> @ 13:21:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 152/1835 [02:38<29:13,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:47\n",
      "\t\tPerforming cuda ops... @ 13:21:47\n",
      "\t\tLoading data onto cpu... @ 13:21:47\n",
      "\t\tAccumulating results...> @ 13:21:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 153/1835 [02:39<29:12,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:48\n",
      "\t\tPerforming cuda ops... @ 13:21:48\n",
      "\t\tLoading data onto cpu... @ 13:21:48\n",
      "\t\tAccumulating results...> @ 13:21:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 154/1835 [02:40<29:11,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:49\n",
      "\t\tPerforming cuda ops... @ 13:21:49\n",
      "\t\tLoading data onto cpu... @ 13:21:49\n",
      "\t\tAccumulating results...> @ 13:21:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 155/1835 [02:41<29:10,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:50\n",
      "\t\tPerforming cuda ops... @ 13:21:50\n",
      "\t\tLoading data onto cpu... @ 13:21:50\n",
      "\t\tAccumulating results...> @ 13:21:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 156/1835 [02:42<29:09,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:51\n",
      "\t\tPerforming cuda ops... @ 13:21:51\n",
      "\t\tLoading data onto cpu... @ 13:21:51\n",
      "\t\tAccumulating results...> @ 13:21:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 157/1835 [02:43<29:07,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:52\n",
      "\t\tPerforming cuda ops... @ 13:21:52\n",
      "\t\tLoading data onto cpu... @ 13:21:52\n",
      "\t\tAccumulating results...> @ 13:21:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 158/1835 [02:44<29:06,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:53\n",
      "\t\tPerforming cuda ops... @ 13:21:53\n",
      "\t\tLoading data onto cpu... @ 13:21:53\n",
      "\t\tAccumulating results...> @ 13:21:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 159/1835 [02:45<29:04,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:55\n",
      "\t\tPerforming cuda ops... @ 13:21:55\n",
      "\t\tLoading data onto cpu... @ 13:21:55\n",
      "\t\tAccumulating results...> @ 13:21:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 160/1835 [02:46<29:04,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:56\n",
      "\t\tPerforming cuda ops... @ 13:21:56\n",
      "\t\tLoading data onto cpu... @ 13:21:56\n",
      "\t\tAccumulating results...> @ 13:21:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 161/1835 [02:47<29:03,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:57\n",
      "\t\tPerforming cuda ops... @ 13:21:57\n",
      "\t\tLoading data onto cpu... @ 13:21:57\n",
      "\t\tAccumulating results...> @ 13:21:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 162/1835 [02:48<29:02,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:58\n",
      "\t\tPerforming cuda ops... @ 13:21:58\n",
      "\t\tLoading data onto cpu... @ 13:21:58\n",
      "\t\tAccumulating results...> @ 13:21:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 163/1835 [02:49<29:01,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:21:59\n",
      "\t\tPerforming cuda ops... @ 13:21:59\n",
      "\t\tLoading data onto cpu... @ 13:21:59\n",
      "\t\tAccumulating results...> @ 13:21:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 164/1835 [02:50<29:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:00\n",
      "\t\tPerforming cuda ops... @ 13:22:00\n",
      "\t\tLoading data onto cpu... @ 13:22:00\n",
      "\t\tAccumulating results...> @ 13:22:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 165/1835 [02:51<29:00,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:01\n",
      "\t\tPerforming cuda ops... @ 13:22:01\n",
      "\t\tLoading data onto cpu... @ 13:22:01\n",
      "\t\tAccumulating results...> @ 13:22:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 166/1835 [02:52<28:58,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:02\n",
      "\t\tPerforming cuda ops... @ 13:22:02\n",
      "\t\tLoading data onto cpu... @ 13:22:02\n",
      "\t\tAccumulating results...> @ 13:22:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 167/1835 [02:53<28:57,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:03\n",
      "\t\tPerforming cuda ops... @ 13:22:03\n",
      "\t\tLoading data onto cpu... @ 13:22:03\n",
      "\t\tAccumulating results...> @ 13:22:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 168/1835 [02:54<28:56,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:04\n",
      "\t\tPerforming cuda ops... @ 13:22:04\n",
      "\t\tLoading data onto cpu... @ 13:22:04\n",
      "\t\tAccumulating results...> @ 13:22:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 169/1835 [02:55<28:56,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:05\n",
      "\t\tPerforming cuda ops... @ 13:22:05\n",
      "\t\tLoading data onto cpu... @ 13:22:05\n",
      "\t\tAccumulating results...> @ 13:22:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 170/1835 [02:57<28:54,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:06\n",
      "\t\tPerforming cuda ops... @ 13:22:06\n",
      "\t\tLoading data onto cpu... @ 13:22:06\n",
      "\t\tAccumulating results...> @ 13:22:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 171/1835 [02:58<28:53,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:07\n",
      "\t\tPerforming cuda ops... @ 13:22:07\n",
      "\t\tLoading data onto cpu... @ 13:22:07\n",
      "\t\tAccumulating results...> @ 13:22:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 172/1835 [02:59<28:52,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:08\n",
      "\t\tPerforming cuda ops... @ 13:22:08\n",
      "\t\tLoading data onto cpu... @ 13:22:08\n",
      "\t\tAccumulating results...> @ 13:22:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 173/1835 [03:00<28:51,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:09\n",
      "\t\tPerforming cuda ops... @ 13:22:09\n",
      "\t\tLoading data onto cpu... @ 13:22:09\n",
      "\t\tAccumulating results...> @ 13:22:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 174/1835 [03:01<28:50,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:10\n",
      "\t\tPerforming cuda ops... @ 13:22:10\n",
      "\t\tLoading data onto cpu... @ 13:22:10\n",
      "\t\tAccumulating results...> @ 13:22:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 175/1835 [03:02<28:49,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:11\n",
      "\t\tPerforming cuda ops... @ 13:22:11\n",
      "\t\tLoading data onto cpu... @ 13:22:11\n",
      "\t\tAccumulating results...> @ 13:22:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 176/1835 [03:03<28:47,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:12\n",
      "\t\tPerforming cuda ops... @ 13:22:12\n",
      "\t\tLoading data onto cpu... @ 13:22:12\n",
      "\t\tAccumulating results...> @ 13:22:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 177/1835 [03:04<28:47,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:13\n",
      "\t\tPerforming cuda ops... @ 13:22:13\n",
      "\t\tLoading data onto cpu... @ 13:22:13\n",
      "\t\tAccumulating results...> @ 13:22:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 178/1835 [03:05<28:45,  1.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache.\n",
      "\t\t<Loading chunk onto cuda device... @ 13:22:14\n",
      "\t\tPerforming cuda ops... @ 13:22:14\n",
      "\t\tLoading data onto cpu... @ 13:22:14\n",
      "\t\tAccumulating results...> @ 13:22:14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8ac89a668ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\\tAccumulating results...>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mCMsByWordformIndex_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_CMs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clearing GPU cache.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-8ac89a668ac6>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\\tLoading data onto cpu...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mgroup_CMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_CMs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#             group_CMs = [each.cpu() for each in group_CMs_c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#             group_CMs = (torch.empty(each.shape).copy_(each, True) for each in group_CMs_c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "CMsByWordformIndex_torch = []\n",
    "for group, key in tqdm(zip(Ws_t_grouped, Ws_t_grouped_uniquekeys), total=len(Ws_t_grouped)):\n",
    "    if key:\n",
    "        print(f\"Group size: {len(group)}\")\n",
    "        wordforms_OHs_tr_t = [torch.tensor(dsToTriphoneOHs(w, X012OHmap)).t()\n",
    "                             for w in group]\n",
    "        \n",
    "        #50 -> 10s to transfer from GPU to CPU\n",
    "        #5 -> 1s to transfer from GPU to CPU\n",
    "        chunk_size = 5\n",
    "        \n",
    "        chunks = chunkList(chunk_size, wordforms_OHs_tr_t)\n",
    "        print(f\"\\t{len(chunks)} chunks of size {chunk_size}:\")\n",
    "        for chunk in tqdm(chunks):\n",
    "            stampedNote(\"\\t\\t<Loading chunk onto cuda device...\")\n",
    "            wordforms_OHs_tr_t_c = [each.cuda() \n",
    "                                    for each in chunk]\n",
    "            \n",
    "            stampedNote(\"\\t\\tPerforming cuda ops...\")\n",
    "            group_CMs_c = (sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in wordforms_OHs_tr_t_c)\n",
    "            \n",
    "            stampedNote(\"\\t\\tLoading data onto cpu...\")\n",
    "            group_CMs = (each.cpu() for each in group_CMs_c)\n",
    "#             group_CMs = [each.cpu() for each in group_CMs_c]\n",
    "#             group_CMs = (torch.empty(each.shape).copy_(each, True) for each in group_CMs_c)\n",
    "            \n",
    "            stampedNote(\"\\t\\tAccumulating results...>\")\n",
    "            CMsByWordformIndex_torch.extend(group_CMs)\n",
    "            \n",
    "            print(\"Clearing GPU cache.\")\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(f\"Group of wordforms of length < 3\")\n",
    "        group_CMs = [None for w in group]\n",
    "        CMsByWordformIndex_torch.extend(group_CMs)\n",
    "#     print(\"Clearing GPU cache.\")\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:39.884050Z",
     "start_time": "2019-07-27T22:17:39.881473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByWordformIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:39.891087Z",
     "start_time": "2019-07-27T22:17:39.885004Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:39.896584Z",
     "start_time": "2019-07-27T22:17:39.892231Z"
    }
   },
   "outputs": [],
   "source": [
    "# CMsByWordformIndex = [sourcePrefixToChannelMatrix_l(w)\n",
    "#                      for w in Ws_t]\n",
    "# CMsByWordformIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByWordformIndex[1:]]\n",
    "\n",
    "# CMsByWordformIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:39.901190Z",
     "start_time": "2019-07-27T22:17:39.897563Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsOfLength(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         return {w for w in Ws if len(ds2t(w)) == l}\n",
    "#     return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:39.913296Z",
     "start_time": "2019-07-27T22:17:39.902143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.d.ɛ.m.ə.n.s.t.ɹ.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.ʃ.ɪ.n.⋉',\n",
       " '⋊.d.ɹ.ɑ.m.ə.t.ə.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.dʒ.ʌ.s.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.f.l.ɛ.k.s.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.f.ə.l.æ.n.θ.ɹ.ə.p.ɪ.s.t.⋉',\n",
       " '⋊.g.l.oʊ.ɹ.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.g.ɹ.æ.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.j.u.n.j.ɪ.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.l.æ.s.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.l.ɑ.s.t.ɹ.ə.f.oʊ.b.i.ə.⋉',\n",
       " '⋊.k.w.ɑ.l.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.æ.p.ə.t.ə.l.ɪ.s.t.ɪ.k.⋉',\n",
       " '⋊.k.ɑ.m.p.l.ə.m.ɛ.n.t.ɚ.i.⋉',\n",
       " '⋊.k.ɑ.n.s.ɪ.n.t.ɹ.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.ɑ.n.t.ɹ.ə.b.j.u.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.ə.n.s.t.ɪ.tʃ.u.ə.n.s.i.⋉',\n",
       " '⋊.k.ə.n.t.ɪ.n.j.u.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.æ.g.n.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.æ.n.ə.f.ɪ.s.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ə.n.ɪ.p.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ɛ.l.oʊ.d.ɹ.ə.m.æ.t.ɪ.k.⋉',\n",
       " '⋊.m.ɛ.t.ə.m.ɑ.ɹ.f.ə.s.ɪ.s.⋉',\n",
       " '⋊.m.ɪ.s.ɪ.n.f.ɚ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ɪ.s.ʌ.n.d.ɚ.s.t.æ.n.d.⋉',\n",
       " '⋊.n.aɪ.t.ɹ.oʊ.g.l.ɪ.s.ɚ.ə.n.⋉',\n",
       " '⋊.p.eɪ.l.i.ɑ.n.t.ɑ.l.ə.dʒ.i.⋉',\n",
       " '⋊.p.j.ʊ.ɹ.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.p.ɹ.æ.k.t.ɪ.k.æ.l.ɪ.t.i.⋉',\n",
       " '⋊.p.ɹ.ɑ.d.ʌ.k.t.ɪ.v.ə.t.i.⋉',\n",
       " '⋊.p.ɹ.ə.k.ɹ.æ.s.t.ɪ.n.eɪ.t.⋉',\n",
       " '⋊.p.ɹ.ɪ.s.ɪ.p.ə.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.s.aɪ.k.oʊ.ə.n.æ.l.ɪ.s.ɪ.s.⋉',\n",
       " '⋊.s.k.ɪ.t.s.ə.f.ɹ.i.n.i.ə.⋉',\n",
       " '⋊.s.k.ɪ.t.s.ə.f.ɹ.ɛ.n.ɪ.k.⋉',\n",
       " '⋊.s.p.ɑ.n.t.eɪ.n.i.ɪ.s.l.i.⋉',\n",
       " '⋊.s.p.ɛ.s.ɪ.f.ɪ.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.s.p.ɪ.ɹ.ɪ.tʃ.u.ə.l.ɪ.z.m.⋉',\n",
       " '⋊.s.p.ɪ.ɹ.ɪ.tʃ.ʊ.æ.l.ə.t.i.⋉',\n",
       " '⋊.s.t.ɹ.æ.t.ɪ.s.f.ɪ.ɹ.ɪ.k.⋉',\n",
       " '⋊.s.ə.f.ɪ.s.t.ɪ.k.eɪ.t.ɪ.d.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.f.ɚ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.k.ɹ.ɪ.p.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.p.ɚ.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.ɪ.g.ɪ.n.ɑ.m.ə.t.ɹ.i.⋉',\n",
       " '⋊.t.ʊ.b.ɚ.k.j.ʊ.l.oʊ.s.ɪ.s.⋉',\n",
       " '⋊.v.ʌ.l.n.ɚ.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.æ.n.t.ɪ.h.ɪ.s.t.ə.m.i.n.⋉',\n",
       " '⋊.ɑ.ɹ.t.ɪ.k.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ə.d.m.ɪ.n.ɪ.s.t.ɹ.eɪ.t.ɚ.⋉',\n",
       " '⋊.ə.d.æ.p.t.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.ə.k.ʌ.m.p.ə.n.i.m.ə.n.t.⋉',\n",
       " '⋊.ə.p.ɹ.ɑ.k.s.ɪ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɛ.k.s.ə.b.ɪ.ʃ.ɪ.n.ɪ.z.m.⋉',\n",
       " '⋊.ɪ.k.s.k.ɹ.u.ʃ.i.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.ɪ.k.s.p.l.æ.n.ɪ.t.oʊ.ɹ.i.⋉',\n",
       " '⋊.ɪ.k.s.t.ɹ.æ.v.ɪ.g.ɪ.n.t.⋉',\n",
       " '⋊.ɪ.m.p.ɑ.s.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.ɪ.m.p.ɹ.ɑ.v.ə.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.k.ɪ.n.v.i.n.j.ə.n.s.⋉',\n",
       " '⋊.ɪ.n.s.aɪ.k.l.ə.p.i.d.i.ə.⋉',\n",
       " '⋊.ɪ.n.s.t.ə.n.t.eɪ.n.i.ə.s.⋉',\n",
       " '⋊.ɪ.n.t.ɚ.p.ɹ.ə.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.t.ɹ.ə.s.p.ɛ.k.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.ɑ.ɹ.t.ɪ.k.j.ʊ.l.ɪ.t.⋉',\n",
       " '⋊.ɹ.i.ɪ.n.k.ɑ.ɹ.n.eɪ.ʃ.ɪ.n.⋉'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16, Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.000670Z",
     "start_time": "2019-07-27T22:17:39.914274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{3: 5,\n",
       " 4: 173,\n",
       " 5: 1266,\n",
       " 6: 1790,\n",
       " 7: 1646,\n",
       " 8: 1328,\n",
       " 9: 1052,\n",
       " 10: 828,\n",
       " 11: 508,\n",
       " 12: 329,\n",
       " 13: 150,\n",
       " 14: 66,\n",
       " 15: 24,\n",
       " 16: 6,\n",
       " 18: 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.004910Z",
     "start_time": "2019-07-27T22:17:40.001868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.009714Z",
     "start_time": "2019-07-27T22:17:40.005848Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         maxL = max(wordlengthsInclEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "#     else:\n",
    "#         maxL = max(wordlengthsNotIncludingEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.092020Z",
     "start_time": "2019-07-27T22:17:40.010662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 9172,\n",
       " 4: 9167,\n",
       " 5: 8994,\n",
       " 6: 7728,\n",
       " 7: 5938,\n",
       " 8: 4292,\n",
       " 9: 2964,\n",
       " 10: 1912,\n",
       " 11: 1084,\n",
       " 12: 576,\n",
       " 13: 247,\n",
       " 14: 97,\n",
       " 15: 31,\n",
       " 16: 7,\n",
       " 18: 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.099591Z",
     "start_time": "2019-07-27T22:17:40.093293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3, 4))\n",
    "torch.zeros((3,4))\n",
    "torch.zeros((3,4)) == torch.tensor(np.zeros((3, 4)), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.262793Z",
     "start_time": "2019-07-27T22:17:40.100620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ɹ.ʌ.p.tʃ.ɚ.⋉'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform\n",
    "len(ds2t(random_source_wordform))\n",
    "random_source_wordform_CM = sourcePrefixToChannelMatrix(random_source_wordform)\n",
    "random_source_wordform_CM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.287443Z",
     "start_time": "2019-07-27T22:17:40.263951Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif exact_length_only:\n",
    "        cm = np.zeros(shape=(len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        cm = sourcePrefixToChannelMatrix(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == cm.shape[1], f\"{cm.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        cm = np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)\n",
    "        assert key_length - 2 == cm.shape[1], f\"{cm.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    \n",
    "\n",
    "# def makeCMbyWordformAndLength_t_c\n",
    "\n",
    "def dsToOHstack_tr_t(ds):\n",
    "    '''\n",
    "    Takes a dotted string (representing a source wordform or prefix),\n",
    "    converts it to a stack of one hot (numpy) vectors, \n",
    "    takes the transpose,\n",
    "    and returns a pytorch tensor version of that transpose.\n",
    "    \n",
    "    This is an intermediate step of processing for the function below.\n",
    "    '''\n",
    "    ds_OHs = dsToTriphoneOHs(ds, X012OHmap)\n",
    "    ds_OHs_tr_t = torch.tensor(ds_OHs.T)\n",
    "    return ds_OHs_tr_t\n",
    "\n",
    "# correct but slow translation of the original function to use pytorch and cuda \n",
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength_t_c(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    \n",
    "    if x0f_length == key_length:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "        return w_CM\n",
    "#         return sourcePrefixStackToChannelMatrix(x0f)\n",
    "    elif exact_length_only:\n",
    "        cm = torch.zeros((len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0k)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "#         cm = sourcePrefixStackToChannelMatrix_t_c(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == w_CM.shape[1], f\"{w_CM.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM\n",
    "    else:\n",
    "        #grab the source \n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        w_CM_padded = torch.nn.functional.pad(w_CM,\n",
    "                                              (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "                                              'constant',\n",
    "                                              value=0.0)\n",
    "        assert key_length - 2 == w_CM_padded.shape[1], f\"{w_CM_padded.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM_padded\n",
    "    \n",
    "def makeCMbyWordformAndLength_t_c_pre(wordform, key_length, exact_length_only = False):\n",
    "    '''\n",
    "    Takes in a wordform and parameters and returns \n",
    "    a preprocessed stack of OHs as a torch tensor, \n",
    "    ready to be loaded onto a cuda device for the main \n",
    "    computation that yields a channel matrix for the \n",
    "    source sequence (wordform).\n",
    "    \n",
    "    Preprocessing steps include trimming wordforms (or rather\n",
    "    their OH stacks) that are too long relative to the key \n",
    "    length and returning a stack of all zeros (representing\n",
    "    the channel matrix) when exact_length_only is True and\n",
    "    |wordform| != key_length. (The resulting stack is left\n",
    "    untouched by subsequent steps.)\n",
    "    '''\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    \n",
    "    if x0f_length == key_length:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        return w_OHs_tr_t\n",
    "    elif exact_length_only:\n",
    "        cm = torch.zeros((len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0k)\n",
    "        return w_OHs_tr_t\n",
    "    else:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        return w_OHs_tr_t\n",
    "\n",
    "def makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result, key_length, exact_length_only = False):\n",
    "    '''\n",
    "    Takes in a processed stack of OHs (a torch tensor loaded on a cuda device), \n",
    "    and turns it into a tensor representing the corresponding channel matrix (still on the cuda device).\n",
    "    '''\n",
    "    x0f_length = loaded_prep_result.shape[1] + 2\n",
    "    if exact_length_only and x0f_length != key_length:\n",
    "#     if exact_length_only and torch.sum(loaded_prep_result).item() == 0.0:\n",
    "        return loaded_prep_result\n",
    "    w_OHs_tr_c = loaded_prep_result\n",
    "    w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "    return w_CM_c\n",
    "\n",
    "def makeCMbyWordformAndLength_t_c_post(w_CM, key_length, exact_length_only = False, asType='torch'):\n",
    "    '''\n",
    "    Takes in a wordform channel matrix (a torch tensor or np.ndarray on the CPU), \n",
    "    and performs any necessary post-processing. \n",
    "    \n",
    "    The only condition under which post-processing will occur is if the\n",
    "    channel matrix needs to be padded with zero vectors to represent the\n",
    "    lack of insertion errors and the fact that a source sequence with l\n",
    "    consecutive triphones will never generate a channel sequence of anything\n",
    "    other than exactly l channel triphones.\n",
    "    '''\n",
    "    x0f_length = w_CM.shape[1] + 2\n",
    "    if exact_length_only and x0f_length != key_length:\n",
    "#     if exact_length_only and torch.sum(w_CM).item() == 0.0:\n",
    "        return w_CM\n",
    "    if x0f_length < key_length:\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        if asType == 'torch':\n",
    "            w_CM_padded = torch.nn.functional.pad(w_CM,\n",
    "                                                  (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "                                                  'constant',\n",
    "                                                  value=0.0)\n",
    "        if asType == 'ndarray':\n",
    "            w_CM_padded = np.pad(w_CM, ((0,0), (0, goal_l - w_CM.shape[1] - 2)), \n",
    "                                 'constant', constant_values=0.0)\n",
    "#         assert key_length - 2 == w_CM_padded.shape[1], f\"{w_CM_padded.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM_padded\n",
    "    \n",
    "#     assert key_length - 2 == w_CM.shape[1], f\"{w_CM.shape[1]} != {key_length - 2}\\n\\t key_length = {key_length}\"\n",
    "    return w_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.296144Z",
     "start_time": "2019-07-27T22:17:40.288583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,3))\n",
    "np.pad(np.zeros((2,3)), ((0,0), (0, 6 - np.zeros((2,3)).shape[1] - 2)),\n",
    "       'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.305957Z",
     "start_time": "2019-07-27T22:17:40.297068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3))\n",
    "#note the reverse ordering effect of the second argument relative to np.pad!\n",
    "torch.nn.functional.pad(torch.zeros((2,3)), (0, 0, 0, 6 - torch.zeros((2,3)).shape[1] - 2), 'constant', value=0.0)\n",
    "torch.nn.functional.pad(torch.zeros((2,3)), (0, 6 - torch.zeros((2,3)).shape[1] - 2, 0, 0), 'constant', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.309760Z",
     "start_time": "2019-07-27T22:17:40.307441Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     # returns p(Y0K|x0f)\n",
    "#     def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "#         x0f = wordform\n",
    "#         x0f_t = ds2t(x0f)\n",
    "#         x0f_length = len(x0f_t)\n",
    "#         if x0f_length == key_length:\n",
    "#             return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "#         elif x0f_length > key_length:\n",
    "#     #         print('middle case')\n",
    "#             #trim the wordform to be a prefix of length = key_length\n",
    "#             x0k_t = x0f_t[:key_length]\n",
    "#             x0k = t2ds(x0k_t)\n",
    "#     #         print('x0k: {0}'.format(x0k))\n",
    "#             return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "#         else:\n",
    "#             #grab the source \n",
    "#             my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "#             goal_l = key_length\n",
    "#             return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "#                           'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.316559Z",
     "start_time": "2019-07-27T22:17:40.311188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.322629Z",
     "start_time": "2019-07-27T22:17:40.317863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word lengths:\n",
      "{17}\n"
     ]
    }
   ],
   "source": [
    "if sorted(list(wordlengthsInclEdges)) != sorted(list(range(min(wordlengthsInclEdges), \\\n",
    "                                                           max(wordlengthsInclEdges)+1))):\n",
    "    print(\"Missing word lengths:\")\n",
    "    print({l for l in range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1) if l not in wordlengthsInclEdges})\n",
    "    wordlengthsInclEdges_range = list(range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1))\n",
    "else:\n",
    "    wordlengthsInclEdges_range = sorted(list(wordlengthsInclEdges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.327107Z",
     "start_time": "2019-07-27T22:17:40.323832Z"
    }
   },
   "outputs": [],
   "source": [
    "# ?tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.331571Z",
     "start_time": "2019-07-27T22:17:40.327919Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # demonstrates that makeChannelMatrixByWordformAndLength_t_c will run without errors, but slow\n",
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "# cmsByLengthByWordformIndex_torch = offset + [torch.tensor([makeChannelMatrixByWordformAndLength_t_c(w, l)\n",
    "#                                                            for w in tqdm(Ws_t, \n",
    "#                                                                          total=len(Ws_t))])\n",
    "#                                              for l in tqdm(wordlengthsInclEdges_range, \n",
    "#                                                            total=len(wordlengthsInclEdges_range))]\n",
    "# # cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.341916Z",
     "start_time": "2019-07-27T22:17:40.332387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l.aɪ.z.⋉'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7381, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7381, 6])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = Ws_t[0]; w0\n",
    "len(ds2t(w0))\n",
    "w0_pre = makeCMbyWordformAndLength_t_c_pre(wordform=w0, key_length=3, exact_length_only = False)\n",
    "w0_pre.shape\n",
    "dsToOHstack_tr_t(w0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:17:40.345418Z",
     "start_time": "2019-07-27T22:17:40.343244Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote, startNote, endNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T23:54:40.927272Z",
     "start_time": "2019-07-27T23:54:40.754487Z"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 10000000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T00:04:32.771432Z",
     "start_time": "2019-07-27T23:54:49.496888Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 16:54:49\n",
      "11\n",
      "11\n",
      "[35, 1, 2247, 1, 2014, 1, 3119, 1, 305, 1, 1447]\n",
      "[3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3]\n",
      "Group key and size = (3,35)\n",
      "Size of Ws_OHs_tr_t = 0.00206668GB\n",
      "\t1 chunks of size 100\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:54:49\n",
      "\tPerforming cuda ops... @ 16:54:49\n",
      "\tLoading CMs back onto the CPU... @ 16:54:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 1/11 [00:07<01:12,  7.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:54:56\n",
      "Clearing GPU cache... @ 16:54:56\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 100\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 16:54:56\n",
      "\tPerforming cuda ops... @ 16:54:56\n",
      "\tLoading CMs back onto the CPU... @ 16:54:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 2/11 [00:07<00:46,  5.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:54:57\n",
      "Clearing GPU cache... @ 16:54:57\n",
      "Group key and size = (3,2247)\n",
      "Size of Ws_OHs_tr_t = 0.132680856GB\n",
      "\t23 chunks of size 100\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:54:57\n",
      "\tPerforming cuda ops... @ 16:54:57\n",
      "\tLoading CMs back onto the CPU... @ 16:54:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 1/23 [00:20<07:35, 20.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:55:17\n",
      "Clearing GPU cache... @ 16:55:17\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:55:17\n",
      "\tPerforming cuda ops... @ 16:55:17\n",
      "\tLoading CMs back onto the CPU... @ 16:55:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 2/23 [00:41<07:15, 20.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:55:38\n",
      "Clearing GPU cache... @ 16:55:38\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:55:38\n",
      "\tPerforming cuda ops... @ 16:55:38\n",
      "\tLoading CMs back onto the CPU... @ 16:55:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 3/23 [01:02<06:54, 20.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:55:59\n",
      "Clearing GPU cache... @ 16:55:59\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:55:59\n",
      "\tPerforming cuda ops... @ 16:55:59\n",
      "\tLoading CMs back onto the CPU... @ 16:55:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 4/23 [01:22<06:33, 20.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:56:20\n",
      "Clearing GPU cache... @ 16:56:20\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:56:20\n",
      "\tPerforming cuda ops... @ 16:56:20\n",
      "\tLoading CMs back onto the CPU... @ 16:56:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 5/23 [01:43<06:13, 20.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:56:40\n",
      "Clearing GPU cache... @ 16:56:40\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:56:40\n",
      "\tPerforming cuda ops... @ 16:56:40\n",
      "\tLoading CMs back onto the CPU... @ 16:56:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 6/23 [02:04<05:53, 20.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:57:01\n",
      "Clearing GPU cache... @ 16:57:01\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:57:01\n",
      "\tPerforming cuda ops... @ 16:57:01\n",
      "\tLoading CMs back onto the CPU... @ 16:57:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 7/23 [02:25<05:32, 20.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:57:22\n",
      "Clearing GPU cache... @ 16:57:22\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:57:22\n",
      "\tPerforming cuda ops... @ 16:57:22\n",
      "\tLoading CMs back onto the CPU... @ 16:57:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 8/23 [02:46<05:11, 20.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:57:43\n",
      "Clearing GPU cache... @ 16:57:43\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:57:43\n",
      "\tPerforming cuda ops... @ 16:57:43\n",
      "\tLoading CMs back onto the CPU... @ 16:57:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 9/23 [03:07<04:51, 20.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:58:04\n",
      "Clearing GPU cache... @ 16:58:04\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:58:04\n",
      "\tPerforming cuda ops... @ 16:58:04\n",
      "\tLoading CMs back onto the CPU... @ 16:58:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 10/23 [03:27<04:30, 20.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:58:24\n",
      "Clearing GPU cache... @ 16:58:25\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:58:25\n",
      "\tPerforming cuda ops... @ 16:58:25\n",
      "\tLoading CMs back onto the CPU... @ 16:58:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 11/23 [03:48<04:10, 20.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 16:58:45\n",
      "Clearing GPU cache... @ 16:58:45\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:58:45\n",
      "\tPerforming cuda ops... @ 16:58:45\n",
      "\tLoading CMs back onto the CPU... @ 16:58:45\n",
      "Adding processed CMs to list for l = 3. @ 16:59:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 12/23 [04:09<03:49, 20.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 16:59:06\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:59:06\n",
      "\tPerforming cuda ops... @ 16:59:06\n",
      "\tLoading CMs back onto the CPU... @ 16:59:06\n",
      "Adding processed CMs to list for l = 3. @ 16:59:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 13/23 [04:30<03:28, 20.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 16:59:27\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:59:27\n",
      "\tPerforming cuda ops... @ 16:59:27\n",
      "\tLoading CMs back onto the CPU... @ 16:59:27\n",
      "Adding processed CMs to list for l = 3. @ 16:59:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 14/23 [04:51<03:08, 20.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 16:59:48\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 16:59:48\n",
      "\tPerforming cuda ops... @ 16:59:48\n",
      "\tLoading CMs back onto the CPU... @ 16:59:48\n",
      "Adding processed CMs to list for l = 3. @ 17:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 15/23 [05:12<02:47, 20.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:00:09\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:00:09\n",
      "\tPerforming cuda ops... @ 17:00:09\n",
      "\tLoading CMs back onto the CPU... @ 17:00:09\n",
      "Adding processed CMs to list for l = 3. @ 17:00:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 16/23 [05:33<02:26, 20.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:00:30\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:00:30\n",
      "\tPerforming cuda ops... @ 17:00:30\n",
      "\tLoading CMs back onto the CPU... @ 17:00:30\n",
      "Adding processed CMs to list for l = 3. @ 17:00:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 17/23 [05:54<02:05, 20.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:00:51\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:00:51\n",
      "\tPerforming cuda ops... @ 17:00:51\n",
      "\tLoading CMs back onto the CPU... @ 17:00:51\n",
      "Adding processed CMs to list for l = 3. @ 17:01:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 18/23 [06:15<01:44, 20.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:01:12\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:01:12\n",
      "\tPerforming cuda ops... @ 17:01:12\n",
      "\tLoading CMs back onto the CPU... @ 17:01:12\n",
      "Adding processed CMs to list for l = 3. @ 17:01:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 19/23 [06:36<01:23, 20.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:01:33\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:01:33\n",
      "\tPerforming cuda ops... @ 17:01:33\n",
      "\tLoading CMs back onto the CPU... @ 17:01:33\n",
      "Adding processed CMs to list for l = 3. @ 17:01:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 20/23 [06:57<01:03, 21.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:01:54\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:01:54\n",
      "\tPerforming cuda ops... @ 17:01:54\n",
      "\tLoading CMs back onto the CPU... @ 17:01:54\n",
      "Adding processed CMs to list for l = 3. @ 17:02:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████▏| 21/23 [07:18<00:42, 21.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:02:15\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:02:15\n",
      "\tPerforming cuda ops... @ 17:02:15\n",
      "\tLoading CMs back onto the CPU... @ 17:02:15\n",
      "Adding processed CMs to list for l = 3. @ 17:02:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 22/23 [07:39<00:21, 21.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:02:36\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:02:36\n",
      "\tPerforming cuda ops... @ 17:02:36\n",
      "\tLoading CMs back onto the CPU... @ 17:02:36\n",
      "Adding processed CMs to list for l = 3. @ 17:02:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 23/23 [07:49<00:00, 17.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 3/11 [07:57<19:16, 144.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:02:46\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 100\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 17:02:46\n",
      "\tPerforming cuda ops... @ 17:02:46\n",
      "\tLoading CMs back onto the CPU... @ 17:02:46\n",
      "Adding processed CMs to list for l = 3. @ 17:02:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 4/11 [07:57<11:49, 101.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:02:47\n",
      "Group key and size = (3,2014)\n",
      "Size of Ws_OHs_tr_t = 0.118922672GB\n",
      "\t21 chunks of size 100\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:02:47\n",
      "\tPerforming cuda ops... @ 17:02:47\n",
      "\tLoading CMs back onto the CPU... @ 17:02:47\n",
      "Adding processed CMs to list for l = 3. @ 17:03:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 1/21 [00:21<07:01, 21.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:03:08\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:03:08\n",
      "\tPerforming cuda ops... @ 17:03:08\n",
      "\tLoading CMs back onto the CPU... @ 17:03:08\n",
      "Adding processed CMs to list for l = 3. @ 17:03:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 2/21 [00:42<06:40, 21.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:03:29\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:03:29\n",
      "\tPerforming cuda ops... @ 17:03:29\n",
      "\tLoading CMs back onto the CPU... @ 17:03:29\n",
      "Adding processed CMs to list for l = 3. @ 17:03:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 3/21 [01:03<06:19, 21.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:03:50\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:03:50\n",
      "\tPerforming cuda ops... @ 17:03:50\n",
      "\tLoading CMs back onto the CPU... @ 17:03:50\n",
      "Adding processed CMs to list for l = 3. @ 17:04:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 4/21 [01:24<05:59, 21.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache... @ 17:04:12\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 17:04:12\n",
      "\tPerforming cuda ops... @ 17:04:12\n",
      "\tLoading CMs back onto the CPU... @ 17:04:12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-5843b83c4df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tLoading CMs back onto the CPU...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mw_CMs_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_CMs_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# w_CMs_cpu = [each.cpu() for each in w_CMs_c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#x0f_length != key_length && exact_length_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset\n",
    "# awaiting_postProcessingByLength = []\n",
    "\n",
    "def case_classifier(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return 1\n",
    "    elif exact_length_only:\n",
    "        return 2\n",
    "    elif x0f_length > key_length:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "    startNote(f\"l = {l}\")\n",
    "    \n",
    "    Ws_t_grouped, Ws_t_grouped_uniquekeys = persistent_groupby(Ws_t,\n",
    "                                                               keyfunc = lambda w: case_classifier(w, l),\n",
    "                                                               verbose = True)\n",
    "    constant_length_CMs = torch.tensor([], dtype=torch.float64)\n",
    "    # constant_length_CMs = []\n",
    "    for group, key in tqdm(zip(Ws_t_grouped, Ws_t_grouped_uniquekeys), total=len(Ws_t_grouped_uniquekeys)):\n",
    "        print(f\"Group key and size = ({key},{len(group)})\")\n",
    "        \n",
    "        Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "                                                         key_length=l,\n",
    "                                                         exact_length_only=False)\n",
    "                       for w in group]\n",
    "        print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "        \n",
    "        \n",
    "        # constant_length_chunks = []\n",
    "        # constant_length_chunks_before_post_processing = []\n",
    "        chunk_size = 100\n",
    "        chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "#         chunks = chunkList(chunk_size, group)\n",
    "        print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "        for chunk in tqdm(chunks, total=len(chunks)):\n",
    "            \n",
    "            if key == 1 or key == 3: #x0f_length == key_length\n",
    "                print(f\"\\tKey = {key}\")\n",
    "                \n",
    "                stampedNote(\"\\tLoading OH stacks onto cuda device...\")\n",
    "                wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "                \n",
    "                stampedNote(\"\\tPerforming cuda ops...\")\n",
    "                w_CMs_c = torch.stack([sourcePrefixStackToChannelMatrix_t_c(each) for each in wordforms_OHs_tr_t_c])\n",
    "                # w_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each) for each in wordforms_OHs_tr_t_c]\n",
    "                \n",
    "                stampedNote(\"\\tLoading CMs back onto the CPU...\")\n",
    "                w_CMs_cpu = w_CMs_c.cpu()\n",
    "                # w_CMs_cpu = [each.cpu() for each in w_CMs_c]\n",
    "            elif key == 2: #x0f_length != key_length && exact_length_only\n",
    "                print(f\"\\tKey = {key}\")\n",
    "                w_CMs_cpu = torch.stack(chunk)\n",
    "            # elif key == 3: #x0f_length > key_length\n",
    "            #     print(f\"\\tKey = {key}\")\n",
    "            else: #x0f_length < key_length\n",
    "                print(f\"\\tKey = {key}\")\n",
    "                \n",
    "                stampedNote(\"\\tLoading OH stacks onto cuda device...\")\n",
    "                wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "                \n",
    "                stampedNote(\"\\tPerforming cuda ops...\")\n",
    "                w_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each) for each in wordforms_OHs_tr_t_c]\n",
    "                \n",
    "                stampedNote(\"\\tLoading CMs back onto the CPU...\")\n",
    "                w_CMs_cpu = [each.cpu() for each in w_CMs_c]\n",
    "                \n",
    "                stampedNote(\"\\tPadding CMs...\")\n",
    "                goal_l = l\n",
    "                #extend channel matrices with padding\n",
    "                w_CMs_cpu = torch.stack([torch.nn.functional.pad(w_CM,\n",
    "                                                                 (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "                                                                 'constant',\n",
    "                                                                 value=0.0)\n",
    "                                         for w_CM in w_CMs_cpu])\n",
    "            \n",
    "            # stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "            # constant_length_chunks.append(w_CMs_cpu)\n",
    "            stampedNote(f\"Adding processed CMs to list for l = {l}.\")\n",
    "            constant_length_CMs = torch.cat((constant_length_CMs, w_CMs_cpu))\n",
    "            # constant_length_CMs.extend(w_CMs_cpu)\n",
    "            \n",
    "            stampedNote(\"Clearing GPU cache...\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    stampedNote('Consolidating CMs into a constant-length block (tensor).')\n",
    "    constant_length_block = constant_length_CMs\n",
    "    # constant_length_block = torch.stack(constant_length_CMs)\n",
    "\n",
    "    stampedNote('Adding block to list of blocks.\\n')\n",
    "    cmsByLengthByWordformIndex_torch.append(constant_length_block)\n",
    "    \n",
    "    endNote(f\"l = {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T22:53:12.262279Z",
     "start_time": "2019-07-27T22:21:26.658308Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 15:21:26\n",
      "11\n",
      "11\n",
      "[35, 1, 2247, 1, 2014, 1, 3119, 1, 305, 1, 1447]\n",
      "[3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3]\n",
      "Group key and size = (3,35)\n",
      "Size of Ws_OHs_tr_t = 0.00206668GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:21:26\n",
      "\tPerforming cuda ops... @ 15:21:26\n",
      "\tLoading CMs back onto the CPU... @ 15:21:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.15s/it]\u001b[A\u001b[A\n",
      "  9%|▉         | 1/11 [00:07<01:11,  7.15s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:21:33\n",
      "Clearing GPU cache... @ 15:21:33\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:21:33\n",
      "\tPerforming cuda ops... @ 15:21:33\n",
      "\tLoading CMs back onto the CPU... @ 15:21:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\u001b[A\u001b[A\n",
      " 18%|█▊        | 2/11 [00:07<00:45,  5.07s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:21:34\n",
      "Clearing GPU cache... @ 15:21:34\n",
      "Group key and size = (3,2247)\n",
      "Size of Ws_OHs_tr_t = 0.132680856GB\n",
      "\t3 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:21:34\n",
      "\tPerforming cuda ops... @ 15:21:34\n",
      "\tLoading CMs back onto the CPU... @ 15:21:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [03:24<06:49, 204.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:24:58\n",
      "Clearing GPU cache... @ 15:24:58\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:24:58\n",
      "\tPerforming cuda ops... @ 15:24:58\n",
      "\tLoading CMs back onto the CPU... @ 15:24:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [06:51<03:25, 205.40s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:28:25\n",
      "Clearing GPU cache... @ 15:28:25\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:28:25\n",
      "\tPerforming cuda ops... @ 15:28:25\n",
      "\tLoading CMs back onto the CPU... @ 15:28:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 3/3 [07:42<00:00, 159.12s/it]\u001b[A\u001b[A\n",
      " 27%|██▋       | 3/11 [07:50<18:59, 142.41s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:29:16\n",
      "Clearing GPU cache... @ 15:29:16\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:29:16\n",
      "\tPerforming cuda ops... @ 15:29:16\n",
      "\tLoading CMs back onto the CPU... @ 15:29:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\u001b[A\u001b[A\n",
      " 36%|███▋      | 4/11 [07:50<11:38, 99.76s/it] \u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:29:17\n",
      "Clearing GPU cache... @ 15:29:17\n",
      "Group key and size = (3,2014)\n",
      "Size of Ws_OHs_tr_t = 0.118922672GB\n",
      "\t3 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:29:17\n",
      "\tPerforming cuda ops... @ 15:29:17\n",
      "\tLoading CMs back onto the CPU... @ 15:29:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [03:26<06:53, 206.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:32:44\n",
      "Clearing GPU cache... @ 15:32:44\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:32:44\n",
      "\tPerforming cuda ops... @ 15:32:44\n",
      "\tLoading CMs back onto the CPU... @ 15:32:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [06:53<03:26, 206.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:36:11\n",
      "Clearing GPU cache... @ 15:36:11\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:36:11\n",
      "\tPerforming cuda ops... @ 15:36:11\n",
      "\tLoading CMs back onto the CPU... @ 15:36:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 3/3 [06:56<00:00, 145.71s/it]\u001b[A\u001b[A\n",
      " 45%|████▌     | 5/11 [14:47<19:29, 194.88s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:36:13\n",
      "Clearing GPU cache... @ 15:36:13\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:36:13\n",
      "\tPerforming cuda ops... @ 15:36:13\n",
      "\tLoading CMs back onto the CPU... @ 15:36:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\u001b[A\u001b[A\n",
      " 55%|█████▍    | 6/11 [14:47<11:22, 136.48s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:36:14\n",
      "Clearing GPU cache... @ 15:36:14\n",
      "Group key and size = (3,3119)\n",
      "Size of Ws_OHs_tr_t = 0.184170712GB\n",
      "\t4 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:36:14\n",
      "\tPerforming cuda ops... @ 15:36:14\n",
      "\tLoading CMs back onto the CPU... @ 15:36:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|██▌       | 1/4 [03:26<10:20, 206.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:39:41\n",
      "Clearing GPU cache... @ 15:39:41\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:39:41\n",
      "\tPerforming cuda ops... @ 15:39:41\n",
      "\tLoading CMs back onto the CPU... @ 15:39:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 2/4 [06:53<06:53, 206.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:43:08\n",
      "Clearing GPU cache... @ 15:43:08\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:43:08\n",
      "\tPerforming cuda ops... @ 15:43:08\n",
      "\tLoading CMs back onto the CPU... @ 15:43:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|███████▌  | 3/4 [10:20<03:26, 206.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:46:35\n",
      "Clearing GPU cache... @ 15:46:35\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:46:35\n",
      "\tPerforming cuda ops... @ 15:46:35\n",
      "\tLoading CMs back onto the CPU... @ 15:46:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 4/4 [10:45<00:00, 152.23s/it]\u001b[A\u001b[A\n",
      " 64%|██████▎   | 7/11 [25:33<19:16, 289.19s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:46:59\n",
      "Clearing GPU cache... @ 15:46:59\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:46:59\n",
      "\tPerforming cuda ops... @ 15:46:59\n",
      "\tLoading CMs back onto the CPU... @ 15:46:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\u001b[A\u001b[A\n",
      " 73%|███████▎  | 8/11 [25:33<10:07, 202.50s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:46:59\n",
      "Clearing GPU cache... @ 15:46:59\n",
      "Group key and size = (3,305)\n",
      "Size of Ws_OHs_tr_t = 0.01800964GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:46:59\n",
      "\tPerforming cuda ops... @ 15:46:59\n",
      "\tLoading CMs back onto the CPU... @ 15:46:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:03<00:00, 63.10s/it]\u001b[A\u001b[A\n",
      " 82%|████████▏ | 9/11 [26:36<05:21, 160.68s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:48:03\n",
      "Clearing GPU cache... @ 15:48:03\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:48:03\n",
      "\tPerforming cuda ops... @ 15:48:03\n",
      "\tLoading CMs back onto the CPU... @ 15:48:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\u001b[A\u001b[A\n",
      " 91%|█████████ | 10/11 [26:36<01:52, 112.54s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:48:03\n",
      "Clearing GPU cache... @ 15:48:03\n",
      "Group key and size = (3,1447)\n",
      "Size of Ws_OHs_tr_t = 0.085442456GB\n",
      "\t2 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:48:03\n",
      "\tPerforming cuda ops... @ 15:48:03\n",
      "\tLoading CMs back onto the CPU... @ 15:48:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 1/2 [03:26<03:26, 206.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:51:30\n",
      "Clearing GPU cache... @ 15:51:30\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:51:30\n",
      "\tPerforming cuda ops... @ 15:51:30\n",
      "\tLoading CMs back onto the CPU... @ 15:51:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 2/2 [04:59<00:00, 172.57s/it]\u001b[A\u001b[A\n",
      "100%|██████████| 11/11 [31:36<00:00, 168.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 3. @ 15:53:02\n",
      "Clearing GPU cache... @ 15:53:02\n",
      "Consolidating CMs into a constant-length block (tensor). @ 15:53:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [31:37<7:54:26, 1897.78s/it]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding block to list of blocks.\n",
      " @ 15:53:04\n",
      "End l = 3 @ 15:53:04\n",
      "Start l = 4 @ 15:53:04\n",
      "334\n",
      "334\n",
      "[17, 1, 5, 1, 11, 1, 1, 38, 1, 2, 1, 18, 1, 22, 1, 15, 1, 84, 1, 11, 1, 91, 1, 39, 1, 228, 1, 21, 1, 26, 1, 14, 1, 19, 1, 404, 1, 12, 1, 33, 1, 3, 1, 64, 1, 3, 1, 4, 1, 4, 1, 1, 1, 4, 2, 3, 2, 41, 1, 20, 1, 108, 1, 125, 1, 144, 1, 13, 1, 46, 1, 3, 1, 207, 1, 10, 1, 10, 1, 12, 1, 38, 1, 8, 1, 125, 1, 13, 1, 102, 2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 25, 1, 58, 1, 15, 1, 4, 1, 141, 1, 327, 1, 378, 1, 21, 1, 23, 1, 17, 1, 22, 1, 73, 1, 152, 1, 7, 1, 24, 1, 18, 1, 37, 1, 8, 1, 148, 1, 249, 1, 2, 1, 12, 1, 12, 1, 21, 1, 19, 1, 44, 1, 70, 1, 5, 1, 7, 1, 38, 1, 1, 1, 1, 14, 1, 36, 1, 21, 1, 255, 1, 96, 1, 358, 1, 7, 1, 16, 1, 29, 1, 221, 1, 295, 1, 120, 1, 1, 1, 87, 1, 242, 1, 26, 1, 15, 1, 16, 1, 100, 1, 230, 1, 18, 1, 66, 1, 15, 1, 1, 1, 13, 1, 143, 1, 17, 1, 7, 1, 6, 1, 1, 1, 69, 1, 85, 1, 5, 1, 51, 1, 104, 1, 38, 1, 27, 1, 9, 1, 10, 1, 7, 1, 2, 3, 1, 1, 1, 1, 2, 1, 29, 1, 10, 1, 28, 1, 14, 1, 33, 1, 65, 1, 5, 1, 1, 285, 1, 18, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 4, 2, 62, 1, 12, 1, 29, 1, 17, 1, 2, 1, 14, 1, 3, 1, 83, 1, 382, 1, 9, 2, 8, 1, 19, 1, 17, 1, 44, 1, 80, 1, 22, 1, 57, 1, 294, 1, 15, 1, 7, 1, 1, 1, 165, 2, 6, 1, 16, 1, 37]\n",
      "[3, 1, 3, 1, 3, 4, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 4, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 4, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 4, 1, 3, 1, 3, 4, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3]\n",
      "Group key and size = (3,17)\n",
      "Size of Ws_OHs_tr_t = 0.002007632GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:04\n",
      "\tPerforming cuda ops... @ 15:53:04\n",
      "\tLoading CMs back onto the CPU... @ 15:53:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\u001b[A\u001b[A\n",
      "  0%|          | 1/334 [00:03<20:11,  3.64s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 4. @ 15:53:08\n",
      "Clearing GPU cache... @ 15:53:08\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 0.000118096GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:08\n",
      "\tPerforming cuda ops... @ 15:53:08\n",
      "\tLoading CMs back onto the CPU... @ 15:53:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\u001b[A\u001b[A\n",
      "  1%|          | 2/334 [00:03<14:27,  2.61s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 4. @ 15:53:08\n",
      "Clearing GPU cache... @ 15:53:08\n",
      "Group key and size = (3,5)\n",
      "Size of Ws_OHs_tr_t = 0.00059048GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:08\n",
      "\tPerforming cuda ops... @ 15:53:08\n",
      "\tLoading CMs back onto the CPU... @ 15:53:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\u001b[A\u001b[A\n",
      "  1%|          | 3/334 [00:04<11:48,  2.14s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 4. @ 15:53:09\n",
      "Clearing GPU cache... @ 15:53:09\n",
      "Group key and size = (1,1)\n",
      "Size of Ws_OHs_tr_t = 0.000118096GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 1\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:09\n",
      "\tPerforming cuda ops... @ 15:53:09\n",
      "\tLoading CMs back onto the CPU... @ 15:53:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\u001b[A\u001b[A\n",
      "  1%|          | 4/334 [00:05<08:35,  1.56s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 4. @ 15:53:09\n",
      "Clearing GPU cache... @ 15:53:09\n",
      "Group key and size = (3,11)\n",
      "Size of Ws_OHs_tr_t = 0.001299056GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 3\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:09\n",
      "\tPerforming cuda ops... @ 15:53:09\n",
      "\tLoading CMs back onto the CPU... @ 15:53:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\u001b[A\u001b[A\n",
      "  1%|▏         | 5/334 [00:07<09:45,  1.78s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed CMs to list for l = 4. @ 15:53:11\n",
      "Clearing GPU cache... @ 15:53:11\n",
      "Group key and size = (4,1)\n",
      "Size of Ws_OHs_tr_t = 5.9048e-05GB\n",
      "\t1 chunks of size 1000\n",
      "\tKey = 4\n",
      "\tLoading OH stacks onto cuda device... @ 15:53:11\n",
      "\tPerforming cuda ops... @ 15:53:11\n",
      "\tLoading CMs back onto the CPU... @ 15:53:11\n",
      "\tPadding CMs... @ 15:53:12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'key_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-69b2e4eb6182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tPadding CMs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mgoal_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0;31m#extend channel matrices with padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 w_CMs_cpu = [torch.nn.functional.pad(w_CM,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'key_length' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "# cmsByLengthByWordformIndex_torch = offset\n",
    "# # awaiting_postProcessingByLength = []\n",
    "\n",
    "# def case_classifier(wordform, key_length, exact_length_only = False):\n",
    "#     x0f = wordform\n",
    "#     x0f_t = ds2t(x0f)\n",
    "#     x0f_length = len(x0f_t)\n",
    "#     if x0f_length == key_length:\n",
    "#         return 1\n",
    "#     elif exact_length_only:\n",
    "#         return 2\n",
    "#     elif x0f_length > key_length:\n",
    "#         return 3\n",
    "#     else:\n",
    "#         return 4\n",
    "\n",
    "# for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "#     startNote(f\"l = {l}\")\n",
    "    \n",
    "#     Ws_t_grouped, Ws_t_grouped_uniquekeys = persistent_groupby(Ws_t,\n",
    "#                                                                keyfunc = lambda w: case_classifier(w, l),\n",
    "#                                                                verbose = True)\n",
    "#     constant_length_CMs = []\n",
    "#     for group, key in tqdm(zip(Ws_t_grouped, Ws_t_grouped_uniquekeys), total=len(Ws_t_grouped_uniquekeys)):\n",
    "#         print(f\"Group key and size = ({key},{len(group)})\")\n",
    "        \n",
    "#         Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "#                                                          key_length=l,\n",
    "#                                                          exact_length_only=False)\n",
    "#                        for w in group]\n",
    "#         print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "        \n",
    "        \n",
    "#         # constant_length_chunks = []\n",
    "#         # constant_length_chunks_before_post_processing = []\n",
    "#         chunk_size = 1000\n",
    "#         chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "# #         chunks = chunkList(chunk_size, group)\n",
    "#         print(f\"\\t{len(chunks)} chunks of size {chunk_size}\")\n",
    "#         for chunk in tqdm(chunks, total=len(chunks)):\n",
    "            \n",
    "#             if key == 1 or key == 3: #x0f_length == key_length\n",
    "#                 print(f\"\\tKey = {key}\")\n",
    "                \n",
    "#                 stampedNote(\"\\tLoading OH stacks onto cuda device...\")\n",
    "#                 wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "                \n",
    "#                 stampedNote(\"\\tPerforming cuda ops...\")\n",
    "#                 w_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each) for each in wordforms_OHs_tr_t_c]\n",
    "                \n",
    "#                 stampedNote(\"\\tLoading CMs back onto the CPU...\")\n",
    "#                 w_CMs_cpu = [each.cpu() for each in w_CMs_c]\n",
    "#             elif key == 2: #x0f_length != key_length && exact_length_only\n",
    "#                 print(f\"\\tKey = {key}\")\n",
    "#                 w_CMs_cpu = chunk\n",
    "#             # elif key == 3: #x0f_length > key_length\n",
    "#             #     print(f\"\\tKey = {key}\")\n",
    "#             else: #x0f_length < key_length\n",
    "#                 print(f\"\\tKey = {key}\")\n",
    "                \n",
    "#                 stampedNote(\"\\tLoading OH stacks onto cuda device...\")\n",
    "#                 wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "                \n",
    "#                 stampedNote(\"\\tPerforming cuda ops...\")\n",
    "#                 w_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each) for each in wordforms_OHs_tr_t_c]\n",
    "                \n",
    "#                 stampedNote(\"\\tLoading CMs back onto the CPU...\")\n",
    "#                 w_CMs_cpu = [each.cpu() for each in w_CMs_c]\n",
    "                \n",
    "#                 stampedNote(\"\\tPadding CMs...\")\n",
    "#                 goal_l = l\n",
    "#                 #extend channel matrices with padding\n",
    "#                 w_CMs_cpu = [torch.nn.functional.pad(w_CM,\n",
    "#                                                      (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "#                                                      'constant',\n",
    "#                                                      value=0.0)\n",
    "#                              for w_CM in w_CMs_cpu]\n",
    "            \n",
    "#             # stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "#             # constant_length_chunks.append(w_CMs_cpu)\n",
    "#             stampedNote(f\"Adding processed CMs to list for l = {l}.\")\n",
    "#             constant_length_CMs.extend(w_CMs_cpu)\n",
    "            \n",
    "#             stampedNote(\"Clearing GPU cache...\")\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#     stampedNote('Consolidating CMs into a constant-length block (tensor).')\n",
    "#     constant_length_block = torch.stack(constant_length_CMs)\n",
    "\n",
    "#     stampedNote('Adding block to list of blocks.\\n')\n",
    "#     cmsByLengthByWordformIndex_torch.append(constant_length_block)\n",
    "    \n",
    "#     endNote(f\"l = {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-27T00:36:08.357Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 17:36:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Ws_OHs_tr_t = 0.541588256GB\n",
      "Loading OH stacks onto cuda device... @ 17:36:08\n",
      "Performing cuda op... @ 17:36:08\n",
      "Loading back onto the cpu... @ 17:36:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/92 [00:20<30:56, 20.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:36:29\n",
      "Clearing GPU cache... @ 17:36:29\n",
      "Loading OH stacks onto cuda device... @ 17:36:29\n",
      "Performing cuda op... @ 17:36:29\n",
      "Loading back onto the cpu... @ 17:36:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/92 [00:40<30:39, 20.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:36:49\n",
      "Clearing GPU cache... @ 17:36:49\n",
      "Loading OH stacks onto cuda device... @ 17:36:49\n",
      "Performing cuda op... @ 17:36:49\n",
      "Loading back onto the cpu... @ 17:36:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/92 [01:01<30:21, 20.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:10\n",
      "Clearing GPU cache... @ 17:37:10\n",
      "Loading OH stacks onto cuda device... @ 17:37:10\n",
      "Performing cuda op... @ 17:37:10\n",
      "Loading back onto the cpu... @ 17:37:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/92 [01:21<30:02, 20.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:30\n",
      "Clearing GPU cache... @ 17:37:30\n",
      "Loading OH stacks onto cuda device... @ 17:37:30\n",
      "Performing cuda op... @ 17:37:30\n",
      "Loading back onto the cpu... @ 17:37:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 5/92 [01:42<29:43, 20.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:51\n",
      "Clearing GPU cache... @ 17:37:51\n",
      "Loading OH stacks onto cuda device... @ 17:37:51\n",
      "Performing cuda op... @ 17:37:51\n",
      "Loading back onto the cpu... @ 17:37:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 6/92 [02:03<29:23, 20.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:11\n",
      "Clearing GPU cache... @ 17:38:11\n",
      "Loading OH stacks onto cuda device... @ 17:38:11\n",
      "Performing cuda op... @ 17:38:11\n",
      "Loading back onto the cpu... @ 17:38:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 7/92 [02:23<29:07, 20.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:32\n",
      "Clearing GPU cache... @ 17:38:32\n",
      "Loading OH stacks onto cuda device... @ 17:38:32\n",
      "Performing cuda op... @ 17:38:32\n",
      "Loading back onto the cpu... @ 17:38:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 8/92 [02:44<28:50, 20.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:53\n",
      "Clearing GPU cache... @ 17:38:53\n",
      "Loading OH stacks onto cuda device... @ 17:38:53\n",
      "Performing cuda op... @ 17:38:53\n",
      "Loading back onto the cpu... @ 17:38:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 9/92 [03:05<28:32, 20.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:13\n",
      "Clearing GPU cache... @ 17:39:13\n",
      "Loading OH stacks onto cuda device... @ 17:39:13\n",
      "Performing cuda op... @ 17:39:13\n",
      "Loading back onto the cpu... @ 17:39:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 10/92 [03:25<28:13, 20.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:34\n",
      "Clearing GPU cache... @ 17:39:34\n",
      "Loading OH stacks onto cuda device... @ 17:39:34\n",
      "Performing cuda op... @ 17:39:34\n",
      "Loading back onto the cpu... @ 17:39:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 11/92 [03:46<27:53, 20.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:55\n",
      "Clearing GPU cache... @ 17:39:55\n",
      "Loading OH stacks onto cuda device... @ 17:39:55\n",
      "Performing cuda op... @ 17:39:55\n",
      "Loading back onto the cpu... @ 17:39:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 12/92 [04:07<27:34, 20.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:16\n",
      "Clearing GPU cache... @ 17:40:16\n",
      "Loading OH stacks onto cuda device... @ 17:40:16\n",
      "Performing cuda op... @ 17:40:16\n",
      "Loading back onto the cpu... @ 17:40:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 13/92 [04:27<27:13, 20.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:36\n",
      "Clearing GPU cache... @ 17:40:36\n",
      "Loading OH stacks onto cuda device... @ 17:40:36\n",
      "Performing cuda op... @ 17:40:36\n",
      "Loading back onto the cpu... @ 17:40:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 14/92 [04:48<26:53, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:57\n",
      "Clearing GPU cache... @ 17:40:57\n",
      "Loading OH stacks onto cuda device... @ 17:40:57\n",
      "Performing cuda op... @ 17:40:57\n",
      "Loading back onto the cpu... @ 17:40:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 15/92 [05:09<26:33, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:18\n",
      "Clearing GPU cache... @ 17:41:18\n",
      "Loading OH stacks onto cuda device... @ 17:41:18\n",
      "Performing cuda op... @ 17:41:18\n",
      "Loading back onto the cpu... @ 17:41:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 16/92 [05:30<26:12, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:38\n",
      "Clearing GPU cache... @ 17:41:38\n",
      "Loading OH stacks onto cuda device... @ 17:41:38\n",
      "Performing cuda op... @ 17:41:38\n",
      "Loading back onto the cpu... @ 17:41:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 17/92 [05:50<25:52, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:59\n",
      "Clearing GPU cache... @ 17:41:59\n",
      "Loading OH stacks onto cuda device... @ 17:41:59\n",
      "Performing cuda op... @ 17:41:59\n",
      "Loading back onto the cpu... @ 17:41:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|█▉        | 18/92 [06:11<25:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:42:20\n",
      "Clearing GPU cache... @ 17:42:20\n",
      "Loading OH stacks onto cuda device... @ 17:42:20\n",
      "Performing cuda op... @ 17:42:20\n",
      "Loading back onto the cpu... @ 17:42:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 19/92 [06:32<25:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:42:40\n",
      "Clearing GPU cache... @ 17:42:40\n",
      "Loading OH stacks onto cuda device... @ 17:42:40\n",
      "Performing cuda op... @ 17:42:40\n",
      "Loading back onto the cpu... @ 17:42:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 20/92 [06:52<24:50, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:01\n",
      "Clearing GPU cache... @ 17:43:01\n",
      "Loading OH stacks onto cuda device... @ 17:43:01\n",
      "Performing cuda op... @ 17:43:01\n",
      "Loading back onto the cpu... @ 17:43:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 21/92 [07:13<24:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:22\n",
      "Clearing GPU cache... @ 17:43:22\n",
      "Loading OH stacks onto cuda device... @ 17:43:22\n",
      "Performing cuda op... @ 17:43:22\n",
      "Loading back onto the cpu... @ 17:43:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 22/92 [07:34<24:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:42\n",
      "Clearing GPU cache... @ 17:43:42\n",
      "Loading OH stacks onto cuda device... @ 17:43:43\n",
      "Performing cuda op... @ 17:43:43\n",
      "Loading back onto the cpu... @ 17:43:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 23/92 [07:54<23:48, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:03\n",
      "Clearing GPU cache... @ 17:44:03\n",
      "Loading OH stacks onto cuda device... @ 17:44:03\n",
      "Performing cuda op... @ 17:44:03\n",
      "Loading back onto the cpu... @ 17:44:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 24/92 [08:15<23:27, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:24\n",
      "Clearing GPU cache... @ 17:44:24\n",
      "Loading OH stacks onto cuda device... @ 17:44:24\n",
      "Performing cuda op... @ 17:44:24\n",
      "Loading back onto the cpu... @ 17:44:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 25/92 [08:36<23:06, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:45\n",
      "Clearing GPU cache... @ 17:44:45\n",
      "Loading OH stacks onto cuda device... @ 17:44:45\n",
      "Performing cuda op... @ 17:44:45\n",
      "Loading back onto the cpu... @ 17:44:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 26/92 [08:56<22:45, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:05\n",
      "Clearing GPU cache... @ 17:45:05\n",
      "Loading OH stacks onto cuda device... @ 17:45:05\n",
      "Performing cuda op... @ 17:45:05\n",
      "Loading back onto the cpu... @ 17:45:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 27/92 [09:17<22:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:26\n",
      "Clearing GPU cache... @ 17:45:26\n",
      "Loading OH stacks onto cuda device... @ 17:45:26\n",
      "Performing cuda op... @ 17:45:26\n",
      "Loading back onto the cpu... @ 17:45:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 28/92 [09:38<22:04, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:47\n",
      "Clearing GPU cache... @ 17:45:47\n",
      "Loading OH stacks onto cuda device... @ 17:45:47\n",
      "Performing cuda op... @ 17:45:47\n",
      "Loading back onto the cpu... @ 17:45:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 29/92 [09:59<21:43, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:07\n",
      "Clearing GPU cache... @ 17:46:07\n",
      "Loading OH stacks onto cuda device... @ 17:46:07\n",
      "Performing cuda op... @ 17:46:07\n",
      "Loading back onto the cpu... @ 17:46:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 30/92 [10:19<21:23, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:28\n",
      "Clearing GPU cache... @ 17:46:28\n",
      "Loading OH stacks onto cuda device... @ 17:46:28\n",
      "Performing cuda op... @ 17:46:28\n",
      "Loading back onto the cpu... @ 17:46:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 31/92 [10:40<21:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:49\n",
      "Clearing GPU cache... @ 17:46:49\n",
      "Loading OH stacks onto cuda device... @ 17:46:49\n",
      "Performing cuda op... @ 17:46:49\n",
      "Loading back onto the cpu... @ 17:46:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 32/92 [11:01<20:41, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:09\n",
      "Clearing GPU cache... @ 17:47:09\n",
      "Loading OH stacks onto cuda device... @ 17:47:09\n",
      "Performing cuda op... @ 17:47:09\n",
      "Loading back onto the cpu... @ 17:47:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 33/92 [11:21<20:21, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:30\n",
      "Clearing GPU cache... @ 17:47:30\n",
      "Loading OH stacks onto cuda device... @ 17:47:30\n",
      "Performing cuda op... @ 17:47:30\n",
      "Loading back onto the cpu... @ 17:47:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 34/92 [11:42<20:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:51\n",
      "Clearing GPU cache... @ 17:47:51\n",
      "Loading OH stacks onto cuda device... @ 17:47:51\n",
      "Performing cuda op... @ 17:47:51\n",
      "Loading back onto the cpu... @ 17:47:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 35/92 [12:03<19:39, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:12\n",
      "Clearing GPU cache... @ 17:48:12\n",
      "Loading OH stacks onto cuda device... @ 17:48:12\n",
      "Performing cuda op... @ 17:48:12\n",
      "Loading back onto the cpu... @ 17:48:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 36/92 [12:23<19:18, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:32\n",
      "Clearing GPU cache... @ 17:48:32\n",
      "Loading OH stacks onto cuda device... @ 17:48:32\n",
      "Performing cuda op... @ 17:48:32\n",
      "Loading back onto the cpu... @ 17:48:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 37/92 [12:44<18:58, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:53\n",
      "Clearing GPU cache... @ 17:48:53\n",
      "Loading OH stacks onto cuda device... @ 17:48:53\n",
      "Performing cuda op... @ 17:48:53\n",
      "Loading back onto the cpu... @ 17:48:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 38/92 [13:05<18:37, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:14\n",
      "Clearing GPU cache... @ 17:49:14\n",
      "Loading OH stacks onto cuda device... @ 17:49:14\n",
      "Performing cuda op... @ 17:49:14\n",
      "Loading back onto the cpu... @ 17:49:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 39/92 [13:26<18:16, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:34\n",
      "Clearing GPU cache... @ 17:49:34\n",
      "Loading OH stacks onto cuda device... @ 17:49:34\n",
      "Performing cuda op... @ 17:49:34\n",
      "Loading back onto the cpu... @ 17:49:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 40/92 [13:46<17:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:55\n",
      "Clearing GPU cache... @ 17:49:55\n",
      "Loading OH stacks onto cuda device... @ 17:49:55\n",
      "Performing cuda op... @ 17:49:55\n",
      "Loading back onto the cpu... @ 17:49:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▍     | 41/92 [14:07<17:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:16\n",
      "Clearing GPU cache... @ 17:50:16\n",
      "Loading OH stacks onto cuda device... @ 17:50:16\n",
      "Performing cuda op... @ 17:50:16\n",
      "Loading back onto the cpu... @ 17:50:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 42/92 [14:28<17:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:36\n",
      "Clearing GPU cache... @ 17:50:36\n",
      "Loading OH stacks onto cuda device... @ 17:50:36\n",
      "Performing cuda op... @ 17:50:36\n",
      "Loading back onto the cpu... @ 17:50:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 43/92 [14:48<16:54, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:57\n",
      "Clearing GPU cache... @ 17:50:57\n",
      "Loading OH stacks onto cuda device... @ 17:50:57\n",
      "Performing cuda op... @ 17:50:57\n",
      "Loading back onto the cpu... @ 17:50:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 44/92 [15:09<16:33, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:18\n",
      "Clearing GPU cache... @ 17:51:18\n",
      "Loading OH stacks onto cuda device... @ 17:51:18\n",
      "Performing cuda op... @ 17:51:18\n",
      "Loading back onto the cpu... @ 17:51:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 45/92 [15:30<16:12, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:39\n",
      "Clearing GPU cache... @ 17:51:39\n",
      "Loading OH stacks onto cuda device... @ 17:51:39\n",
      "Performing cuda op... @ 17:51:39\n",
      "Loading back onto the cpu... @ 17:51:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 46/92 [15:50<15:52, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:59\n",
      "Clearing GPU cache... @ 17:51:59\n",
      "Loading OH stacks onto cuda device... @ 17:51:59\n",
      "Performing cuda op... @ 17:51:59\n",
      "Loading back onto the cpu... @ 17:51:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 47/92 [16:11<15:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:52:20\n",
      "Clearing GPU cache... @ 17:52:20\n",
      "Loading OH stacks onto cuda device... @ 17:52:20\n",
      "Performing cuda op... @ 17:52:20\n",
      "Loading back onto the cpu... @ 17:52:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 48/92 [16:32<15:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:52:41\n",
      "Clearing GPU cache... @ 17:52:41\n",
      "Loading OH stacks onto cuda device... @ 17:52:41\n",
      "Performing cuda op... @ 17:52:41\n",
      "Loading back onto the cpu... @ 17:52:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 49/92 [16:53<14:49, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:01\n",
      "Clearing GPU cache... @ 17:53:01\n",
      "Loading OH stacks onto cuda device... @ 17:53:01\n",
      "Performing cuda op... @ 17:53:01\n",
      "Loading back onto the cpu... @ 17:53:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 50/92 [17:13<14:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:22\n",
      "Clearing GPU cache... @ 17:53:22\n",
      "Loading OH stacks onto cuda device... @ 17:53:22\n",
      "Performing cuda op... @ 17:53:22\n",
      "Loading back onto the cpu... @ 17:53:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 51/92 [17:34<14:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:43\n",
      "Clearing GPU cache... @ 17:53:43\n",
      "Loading OH stacks onto cuda device... @ 17:53:43\n",
      "Performing cuda op... @ 17:53:43\n",
      "Loading back onto the cpu... @ 17:53:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 52/92 [17:55<13:47, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:03\n",
      "Clearing GPU cache... @ 17:54:03\n",
      "Loading OH stacks onto cuda device... @ 17:54:03\n",
      "Performing cuda op... @ 17:54:03\n",
      "Loading back onto the cpu... @ 17:54:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 53/92 [18:15<13:27, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:24\n",
      "Clearing GPU cache... @ 17:54:24\n",
      "Loading OH stacks onto cuda device... @ 17:54:24\n",
      "Performing cuda op... @ 17:54:24\n",
      "Loading back onto the cpu... @ 17:54:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|█████▊    | 54/92 [18:36<13:06, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:45\n",
      "Clearing GPU cache... @ 17:54:45\n",
      "Loading OH stacks onto cuda device... @ 17:54:45\n",
      "Performing cuda op... @ 17:54:45\n",
      "Loading back onto the cpu... @ 17:54:45\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "# cmsByLengthByWordformIndex_torch = offset\n",
    "# awaiting_postProcessingByLength = []\n",
    "\n",
    "# for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "#     startNote(f\"l = {l}\")\n",
    "    \n",
    "#     Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "#                                                      key_length=l,\n",
    "#                                                      exact_length_only = False)\n",
    "#                    for w in Ws_t]\n",
    "#     print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "    \n",
    "# #     constant_length_chunks = []\n",
    "#     constant_length_chunks_before_post_processing = []\n",
    "#     chunk_size = 1000\n",
    "#     chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "#     for chunk in tqdm(chunks, total=len(chunks)):\n",
    "#         stampedNote(\"Loading OH stacks onto cuda device...\")\n",
    "#         wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "        \n",
    "#         stampedNote(\"Performing cuda op...\")\n",
    "#         wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "#                                                                   key_length=l, \n",
    "#                                                                   exact_length_only = False)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "# #         wordforms_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "# #                            for each in wordforms_OHs_tr_t_c]\n",
    "        \n",
    "#         stampedNote(\"Loading back onto the cpu...\")\n",
    "#         wordforms_CMs_cpu = [each.cpu() for each in wordforms_CMs_c]\n",
    "        \n",
    "#         stampedNote(\"Adding unpostprocessed chunk to chunk list...\")\n",
    "#         constant_length_chunks_before_post_processing.append(wordforms_CMs_cpu)\n",
    "        \n",
    "#         stampedNote(\"Clearing GPU cache...\")\n",
    "#         torch.cuda.empty_cache()\n",
    "# #         stampedNote(\"Performing post-processing...\")\n",
    "# # #         wordforms_CMs_n = np.array(list(par(delayed(makeCMbyWordformAndLength_t_c_post)(w_CM=each.numpy(), \n",
    "# # #                                                                                           key_length=l, \n",
    "# # #                                                                                           exact_length_only = False,\n",
    "# # #                                                                                           asType = 'ndarray')\n",
    "# # #                                        for each in wordforms_CMs_cpu)))\n",
    "# # #         wordforms_CMs_t = torch.tensor(wordforms_CMs_n)\n",
    "# #         wordforms_CMs_t = torch.stack([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "# #                                                                           key_length=l, \n",
    "# #                                                                           exact_length_only = False)\n",
    "# #                                        for each in wordforms_CMs_cpu])\n",
    "# #         stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "# #         constant_length_chunks.append(wordforms_CMs_t)\n",
    "\n",
    "#     stampedNote('Consolidating unpostprocessed chunks into a single list for this length.')\n",
    "#     unpostprocessed_length_block = [cm for chunk in constant_length_chunks_before_post_processing for cm in chunk]\n",
    "# #     stampedNote('Consolidating chunks into a constant-length block (tensor).')\n",
    "# #     constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "    \n",
    "# #     stampedNote(\"Clearing GPU cache...\")\n",
    "# #     torch.cuda.empty_cache()\n",
    "    \n",
    "#     stampedNote('Adding block to list of blocks.\\n')\n",
    "#     awaiting_postProcessingByLength.append(unpostprocessed_length_block)\n",
    "# #     cmsByLengthByWordformIndex_torch.append(constant_length_block)\n",
    "#     endNote(f\"l = {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do all the post-processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:45:52.434435Z",
     "start_time": "2019-07-26T23:14:05.373374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 16:14:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Ws_OHs_tr_t = 0.541588256GB\n",
      "Loading OH stacks onto cuda device... @ 16:14:05\n",
      "Performing cuda op... @ 16:14:10\n",
      "Loading back onto the cpu... @ 16:14:10\n",
      "Clearing GPU cache... @ 16:14:10\n",
      "Performing post-processing... @ 16:14:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/92 [00:25<38:40, 25.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:14:31\n",
      "Loading OH stacks onto cuda device... @ 16:14:31\n",
      "Performing cuda op... @ 16:14:31\n",
      "Loading back onto the cpu... @ 16:14:31\n",
      "Clearing GPU cache... @ 16:14:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/92 [00:46<36:01, 24.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:14:51\n",
      "Adding processed chunk to chunk list. @ 16:14:51\n",
      "Loading OH stacks onto cuda device... @ 16:14:51\n",
      "Performing cuda op... @ 16:14:51\n",
      "Loading back onto the cpu... @ 16:14:51\n",
      "Clearing GPU cache... @ 16:14:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/92 [01:06<34:09, 23.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:15:12\n",
      "Adding processed chunk to chunk list. @ 16:15:12\n",
      "Loading OH stacks onto cuda device... @ 16:15:12\n",
      "Performing cuda op... @ 16:15:12\n",
      "Loading back onto the cpu... @ 16:15:12\n",
      "Clearing GPU cache... @ 16:15:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/92 [01:27<32:44, 22.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:15:33\n",
      "Adding processed chunk to chunk list. @ 16:15:33\n",
      "Loading OH stacks onto cuda device... @ 16:15:33\n",
      "Performing cuda op... @ 16:15:33\n",
      "Loading back onto the cpu... @ 16:15:33\n",
      "Clearing GPU cache... @ 16:15:33\n",
      "Performing post-processing... @ 16:15:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 5/92 [01:48<31:40, 21.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:15:53\n",
      "Loading OH stacks onto cuda device... @ 16:15:53\n",
      "Performing cuda op... @ 16:15:53\n",
      "Loading back onto the cpu... @ 16:15:53\n",
      "Clearing GPU cache... @ 16:15:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 6/92 [02:08<30:49, 21.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:16:14\n",
      "Adding processed chunk to chunk list. @ 16:16:14\n",
      "Loading OH stacks onto cuda device... @ 16:16:14\n",
      "Performing cuda op... @ 16:16:14\n",
      "Loading back onto the cpu... @ 16:16:14\n",
      "Clearing GPU cache... @ 16:16:14\n",
      "Performing post-processing... @ 16:16:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 7/92 [02:29<30:07, 21.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:16:35\n",
      "Loading OH stacks onto cuda device... @ 16:16:35\n",
      "Performing cuda op... @ 16:16:35\n",
      "Loading back onto the cpu... @ 16:16:35\n",
      "Clearing GPU cache... @ 16:16:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 8/92 [02:50<29:31, 21.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:16:55\n",
      "Adding processed chunk to chunk list. @ 16:16:55\n",
      "Loading OH stacks onto cuda device... @ 16:16:55\n",
      "Performing cuda op... @ 16:16:55\n",
      "Loading back onto the cpu... @ 16:16:55\n",
      "Clearing GPU cache... @ 16:16:55\n",
      "Performing post-processing... @ 16:16:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 9/92 [03:10<29:00, 20.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:17:16\n",
      "Loading OH stacks onto cuda device... @ 16:17:16\n",
      "Performing cuda op... @ 16:17:16\n",
      "Loading back onto the cpu... @ 16:17:16\n",
      "Clearing GPU cache... @ 16:17:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 10/92 [03:31<28:33, 20.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:17:37\n",
      "Adding processed chunk to chunk list. @ 16:17:37\n",
      "Loading OH stacks onto cuda device... @ 16:17:37\n",
      "Performing cuda op... @ 16:17:37\n",
      "Loading back onto the cpu... @ 16:17:37\n",
      "Clearing GPU cache... @ 16:17:37\n",
      "Performing post-processing... @ 16:17:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 11/92 [03:52<28:07, 20.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:17:58\n",
      "Loading OH stacks onto cuda device... @ 16:17:58\n",
      "Performing cuda op... @ 16:17:58\n",
      "Loading back onto the cpu... @ 16:17:58\n",
      "Clearing GPU cache... @ 16:17:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 12/92 [04:13<27:43, 20.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:18:18\n",
      "Adding processed chunk to chunk list. @ 16:18:18\n",
      "Loading OH stacks onto cuda device... @ 16:18:18\n",
      "Performing cuda op... @ 16:18:18\n",
      "Loading back onto the cpu... @ 16:18:18\n",
      "Clearing GPU cache... @ 16:18:18\n",
      "Performing post-processing... @ 16:18:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 13/92 [04:33<27:20, 20.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:18:39\n",
      "Loading OH stacks onto cuda device... @ 16:18:39\n",
      "Performing cuda op... @ 16:18:39\n",
      "Loading back onto the cpu... @ 16:18:39\n",
      "Clearing GPU cache... @ 16:18:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 14/92 [04:54<26:58, 20.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:19:00\n",
      "Adding processed chunk to chunk list. @ 16:19:00\n",
      "Loading OH stacks onto cuda device... @ 16:19:00\n",
      "Performing cuda op... @ 16:19:00\n",
      "Loading back onto the cpu... @ 16:19:00\n",
      "Clearing GPU cache... @ 16:19:00\n",
      "Performing post-processing... @ 16:19:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 15/92 [05:15<26:36, 20.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:19:20\n",
      "Loading OH stacks onto cuda device... @ 16:19:20\n",
      "Performing cuda op... @ 16:19:20\n",
      "Loading back onto the cpu... @ 16:19:20\n",
      "Clearing GPU cache... @ 16:19:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 16/92 [05:35<26:15, 20.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:19:41\n",
      "Adding processed chunk to chunk list. @ 16:19:41\n",
      "Loading OH stacks onto cuda device... @ 16:19:41\n",
      "Performing cuda op... @ 16:19:41\n",
      "Loading back onto the cpu... @ 16:19:41\n",
      "Clearing GPU cache... @ 16:19:41\n",
      "Performing post-processing... @ 16:19:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 17/92 [05:56<25:53, 20.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:20:02\n",
      "Loading OH stacks onto cuda device... @ 16:20:02\n",
      "Performing cuda op... @ 16:20:02\n",
      "Loading back onto the cpu... @ 16:20:02\n",
      "Clearing GPU cache... @ 16:20:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|█▉        | 18/92 [06:17<25:32, 20.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:20:22\n",
      "Adding processed chunk to chunk list. @ 16:20:22\n",
      "Loading OH stacks onto cuda device... @ 16:20:22\n",
      "Performing cuda op... @ 16:20:22\n",
      "Loading back onto the cpu... @ 16:20:22\n",
      "Clearing GPU cache... @ 16:20:22\n",
      "Performing post-processing... @ 16:20:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 19/92 [06:38<25:11, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:20:43\n",
      "Loading OH stacks onto cuda device... @ 16:20:43\n",
      "Performing cuda op... @ 16:20:43\n",
      "Loading back onto the cpu... @ 16:20:43\n",
      "Clearing GPU cache... @ 16:20:43\n",
      "Performing post-processing... @ 16:21:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 20/92 [06:58<24:51, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:21:04\n",
      "Loading OH stacks onto cuda device... @ 16:21:04\n",
      "Performing cuda op... @ 16:21:04\n",
      "Loading back onto the cpu... @ 16:21:04\n",
      "Clearing GPU cache... @ 16:21:04\n",
      "Performing post-processing... @ 16:21:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 21/92 [07:19<24:30, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:21:25\n",
      "Loading OH stacks onto cuda device... @ 16:21:25\n",
      "Performing cuda op... @ 16:21:25\n",
      "Loading back onto the cpu... @ 16:21:25\n",
      "Clearing GPU cache... @ 16:21:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 22/92 [07:40<24:09, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:21:45\n",
      "Adding processed chunk to chunk list. @ 16:21:45\n",
      "Loading OH stacks onto cuda device... @ 16:21:45\n",
      "Performing cuda op... @ 16:21:45\n",
      "Loading back onto the cpu... @ 16:21:45\n",
      "Clearing GPU cache... @ 16:21:45\n",
      "Performing post-processing... @ 16:21:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 23/92 [08:00<23:48, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:22:06\n",
      "Loading OH stacks onto cuda device... @ 16:22:06\n",
      "Performing cuda op... @ 16:22:06\n",
      "Loading back onto the cpu... @ 16:22:06\n",
      "Clearing GPU cache... @ 16:22:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 24/92 [08:21<23:28, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:22:27\n",
      "Adding processed chunk to chunk list. @ 16:22:27\n",
      "Loading OH stacks onto cuda device... @ 16:22:27\n",
      "Performing cuda op... @ 16:22:27\n",
      "Loading back onto the cpu... @ 16:22:27\n",
      "Clearing GPU cache... @ 16:22:27\n",
      "Performing post-processing... @ 16:22:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 25/92 [08:42<23:07, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:22:47\n",
      "Loading OH stacks onto cuda device... @ 16:22:47\n",
      "Performing cuda op... @ 16:22:47\n",
      "Loading back onto the cpu... @ 16:22:47\n",
      "Clearing GPU cache... @ 16:22:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 26/92 [09:02<22:46, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:23:08\n",
      "Adding processed chunk to chunk list. @ 16:23:08\n",
      "Loading OH stacks onto cuda device... @ 16:23:08\n",
      "Performing cuda op... @ 16:23:08\n",
      "Loading back onto the cpu... @ 16:23:08\n",
      "Clearing GPU cache... @ 16:23:08\n",
      "Performing post-processing... @ 16:23:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 27/92 [09:23<22:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:23:29\n",
      "Loading OH stacks onto cuda device... @ 16:23:29\n",
      "Performing cuda op... @ 16:23:29\n",
      "Loading back onto the cpu... @ 16:23:29\n",
      "Clearing GPU cache... @ 16:23:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 28/92 [09:44<22:05, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:23:49\n",
      "Adding processed chunk to chunk list. @ 16:23:50\n",
      "Loading OH stacks onto cuda device... @ 16:23:50\n",
      "Performing cuda op... @ 16:23:50\n",
      "Loading back onto the cpu... @ 16:23:50\n",
      "Clearing GPU cache... @ 16:23:50\n",
      "Performing post-processing... @ 16:23:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 29/92 [10:05<21:44, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:24:10\n",
      "Loading OH stacks onto cuda device... @ 16:24:10\n",
      "Performing cuda op... @ 16:24:10\n",
      "Loading back onto the cpu... @ 16:24:10\n",
      "Clearing GPU cache... @ 16:24:10\n",
      "Performing post-processing... @ 16:24:31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 30/92 [10:25<21:23, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:24:31\n",
      "Loading OH stacks onto cuda device... @ 16:24:31\n",
      "Performing cuda op... @ 16:24:31\n",
      "Loading back onto the cpu... @ 16:24:31\n",
      "Clearing GPU cache... @ 16:24:31\n",
      "Performing post-processing... @ 16:24:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 31/92 [10:46<21:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:24:52\n",
      "Loading OH stacks onto cuda device... @ 16:24:52\n",
      "Performing cuda op... @ 16:24:52\n",
      "Loading back onto the cpu... @ 16:24:52\n",
      "Clearing GPU cache... @ 16:24:52\n",
      "Performing post-processing... @ 16:25:12"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 32/92 [11:07<20:42, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:25:12\n",
      "Loading OH stacks onto cuda device... @ 16:25:12\n",
      "Performing cuda op... @ 16:25:12\n",
      "Loading back onto the cpu... @ 16:25:12\n",
      "Clearing GPU cache... @ 16:25:12\n",
      "Performing post-processing... @ 16:25:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 33/92 [11:27<20:21, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:25:33\n",
      "Loading OH stacks onto cuda device... @ 16:25:33\n",
      "Performing cuda op... @ 16:25:33\n",
      "Loading back onto the cpu... @ 16:25:33\n",
      "Clearing GPU cache... @ 16:25:33\n",
      "Performing post-processing... @ 16:25:54"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 34/92 [11:48<20:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:25:54\n",
      "Loading OH stacks onto cuda device... @ 16:25:54\n",
      "Performing cuda op... @ 16:25:54\n",
      "Loading back onto the cpu... @ 16:25:54\n",
      "Clearing GPU cache... @ 16:25:54\n",
      "Performing post-processing... @ 16:25:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 35/92 [12:09<19:40, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:26:14\n",
      "Loading OH stacks onto cuda device... @ 16:26:14\n",
      "Performing cuda op... @ 16:26:14\n",
      "Loading back onto the cpu... @ 16:26:14\n",
      "Clearing GPU cache... @ 16:26:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 36/92 [12:29<19:19, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:26:35\n",
      "Adding processed chunk to chunk list. @ 16:26:35\n",
      "Loading OH stacks onto cuda device... @ 16:26:35\n",
      "Performing cuda op... @ 16:26:35\n",
      "Loading back onto the cpu... @ 16:26:35\n",
      "Clearing GPU cache... @ 16:26:35\n",
      "Performing post-processing... @ 16:26:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 37/92 [12:50<18:58, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:26:56\n",
      "Loading OH stacks onto cuda device... @ 16:26:56\n",
      "Performing cuda op... @ 16:26:56\n",
      "Loading back onto the cpu... @ 16:26:56\n",
      "Clearing GPU cache... @ 16:26:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 38/92 [13:11<18:38, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:27:17\n",
      "Adding processed chunk to chunk list. @ 16:27:17\n",
      "Loading OH stacks onto cuda device... @ 16:27:17\n",
      "Performing cuda op... @ 16:27:17\n",
      "Loading back onto the cpu... @ 16:27:17\n",
      "Clearing GPU cache... @ 16:27:17\n",
      "Performing post-processing... @ 16:27:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 39/92 [13:32<18:17, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:27:37\n",
      "Loading OH stacks onto cuda device... @ 16:27:37\n",
      "Performing cuda op... @ 16:27:37\n",
      "Loading back onto the cpu... @ 16:27:37\n",
      "Clearing GPU cache... @ 16:27:37\n",
      "Performing post-processing... @ 16:27:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 40/92 [13:52<17:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:27:58\n",
      "Loading OH stacks onto cuda device... @ 16:27:58\n",
      "Performing cuda op... @ 16:27:58\n",
      "Loading back onto the cpu... @ 16:27:58\n",
      "Clearing GPU cache... @ 16:27:58\n",
      "Performing post-processing... @ 16:27:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▍     | 41/92 [14:13<17:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:28:19\n",
      "Loading OH stacks onto cuda device... @ 16:28:19\n",
      "Performing cuda op... @ 16:28:19\n",
      "Loading back onto the cpu... @ 16:28:19\n",
      "Clearing GPU cache... @ 16:28:19\n",
      "Performing post-processing... @ 16:28:39"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 42/92 [14:34<17:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:28:39\n",
      "Loading OH stacks onto cuda device... @ 16:28:39\n",
      "Performing cuda op... @ 16:28:39\n",
      "Loading back onto the cpu... @ 16:28:39\n",
      "Clearing GPU cache... @ 16:28:39\n",
      "Performing post-processing... @ 16:28:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 43/92 [14:54<16:54, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:29:00\n",
      "Loading OH stacks onto cuda device... @ 16:29:00\n",
      "Performing cuda op... @ 16:29:00\n",
      "Loading back onto the cpu... @ 16:29:00\n",
      "Clearing GPU cache... @ 16:29:00\n",
      "Performing post-processing... @ 16:29:21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 44/92 [15:15<16:33, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:29:21\n",
      "Loading OH stacks onto cuda device... @ 16:29:21\n",
      "Performing cuda op... @ 16:29:21\n",
      "Loading back onto the cpu... @ 16:29:21\n",
      "Clearing GPU cache... @ 16:29:21\n",
      "Performing post-processing... @ 16:29:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 45/92 [15:36<16:12, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:29:41\n",
      "Loading OH stacks onto cuda device... @ 16:29:41\n",
      "Performing cuda op... @ 16:29:41\n",
      "Loading back onto the cpu... @ 16:29:41\n",
      "Clearing GPU cache... @ 16:29:41\n",
      "Performing post-processing... @ 16:30:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 46/92 [15:57<15:52, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:30:02\n",
      "Loading OH stacks onto cuda device... @ 16:30:02\n",
      "Performing cuda op... @ 16:30:02\n",
      "Loading back onto the cpu... @ 16:30:02\n",
      "Clearing GPU cache... @ 16:30:02\n",
      "Performing post-processing... @ 16:30:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 47/92 [16:17<15:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:30:23\n",
      "Loading OH stacks onto cuda device... @ 16:30:23\n",
      "Performing cuda op... @ 16:30:23\n",
      "Loading back onto the cpu... @ 16:30:23\n",
      "Clearing GPU cache... @ 16:30:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 48/92 [16:38<15:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:30:44\n",
      "Adding processed chunk to chunk list. @ 16:30:44\n",
      "Loading OH stacks onto cuda device... @ 16:30:44\n",
      "Performing cuda op... @ 16:30:44\n",
      "Loading back onto the cpu... @ 16:30:44\n",
      "Clearing GPU cache... @ 16:30:44\n",
      "Performing post-processing... @ 16:30:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 49/92 [16:59<14:50, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:31:04\n",
      "Loading OH stacks onto cuda device... @ 16:31:04\n",
      "Performing cuda op... @ 16:31:04\n",
      "Loading back onto the cpu... @ 16:31:04\n",
      "Clearing GPU cache... @ 16:31:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 50/92 [17:19<14:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:31:25\n",
      "Adding processed chunk to chunk list. @ 16:31:25\n",
      "Loading OH stacks onto cuda device... @ 16:31:25\n",
      "Performing cuda op... @ 16:31:25\n",
      "Loading back onto the cpu... @ 16:31:25\n",
      "Clearing GPU cache... @ 16:31:25\n",
      "Performing post-processing... @ 16:31:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 51/92 [17:40<14:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:31:46\n",
      "Loading OH stacks onto cuda device... @ 16:31:46\n",
      "Performing cuda op... @ 16:31:46\n",
      "Loading back onto the cpu... @ 16:31:46\n",
      "Clearing GPU cache... @ 16:31:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 52/92 [18:01<13:48, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:32:06\n",
      "Adding processed chunk to chunk list. @ 16:32:06\n",
      "Loading OH stacks onto cuda device... @ 16:32:06\n",
      "Performing cuda op... @ 16:32:06\n",
      "Loading back onto the cpu... @ 16:32:06\n",
      "Clearing GPU cache... @ 16:32:06\n",
      "Performing post-processing... @ 16:32:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 53/92 [18:21<13:27, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:32:27\n",
      "Loading OH stacks onto cuda device... @ 16:32:27\n",
      "Performing cuda op... @ 16:32:27\n",
      "Loading back onto the cpu... @ 16:32:27\n",
      "Clearing GPU cache... @ 16:32:27\n",
      "Performing post-processing... @ 16:32:48"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|█████▊    | 54/92 [18:42<13:06, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:32:48\n",
      "Loading OH stacks onto cuda device... @ 16:32:48\n",
      "Performing cuda op... @ 16:32:48\n",
      "Loading back onto the cpu... @ 16:32:48\n",
      "Clearing GPU cache... @ 16:32:48\n",
      "Performing post-processing... @ 16:32:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████▉    | 55/92 [19:03<12:45, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:33:08\n",
      "Loading OH stacks onto cuda device... @ 16:33:08\n",
      "Performing cuda op... @ 16:33:08\n",
      "Loading back onto the cpu... @ 16:33:08\n",
      "Clearing GPU cache... @ 16:33:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████    | 56/92 [19:24<12:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:33:29\n",
      "Adding processed chunk to chunk list. @ 16:33:29\n",
      "Loading OH stacks onto cuda device... @ 16:33:29\n",
      "Performing cuda op... @ 16:33:29\n",
      "Loading back onto the cpu... @ 16:33:29\n",
      "Clearing GPU cache... @ 16:33:29\n",
      "Performing post-processing... @ 16:33:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 57/92 [19:44<12:04, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:33:50\n",
      "Loading OH stacks onto cuda device... @ 16:33:50\n",
      "Performing cuda op... @ 16:33:50\n",
      "Loading back onto the cpu... @ 16:33:50\n",
      "Clearing GPU cache... @ 16:33:50\n",
      "Performing post-processing... @ 16:33:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 58/92 [20:05<11:43, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:11\n",
      "Loading OH stacks onto cuda device... @ 16:34:11\n",
      "Performing cuda op... @ 16:34:11\n",
      "Loading back onto the cpu... @ 16:34:11\n",
      "Clearing GPU cache... @ 16:34:11\n",
      "Performing post-processing... @ 16:34:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 59/92 [20:26<11:23, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:31\n",
      "Loading OH stacks onto cuda device... @ 16:34:31\n",
      "Performing cuda op... @ 16:34:31\n",
      "Loading back onto the cpu... @ 16:34:31\n",
      "Clearing GPU cache... @ 16:34:31\n",
      "Performing post-processing... @ 16:34:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▌   | 60/92 [20:46<11:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:52\n",
      "Loading OH stacks onto cuda device... @ 16:34:52\n",
      "Performing cuda op... @ 16:34:52\n",
      "Loading back onto the cpu... @ 16:34:52\n",
      "Clearing GPU cache... @ 16:34:52\n",
      "Performing post-processing... @ 16:34:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████▋   | 61/92 [21:07<10:41, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:13\n",
      "Loading OH stacks onto cuda device... @ 16:35:13\n",
      "Performing cuda op... @ 16:35:13\n",
      "Loading back onto the cpu... @ 16:35:13\n",
      "Clearing GPU cache... @ 16:35:13\n",
      "Performing post-processing... @ 16:35:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 62/92 [21:28<10:20, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:33\n",
      "Loading OH stacks onto cuda device... @ 16:35:33\n",
      "Performing cuda op... @ 16:35:33\n",
      "Loading back onto the cpu... @ 16:35:33\n",
      "Clearing GPU cache... @ 16:35:33\n",
      "Performing post-processing... @ 16:35:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 63/92 [21:48<10:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:54\n",
      "Loading OH stacks onto cuda device... @ 16:35:54\n",
      "Performing cuda op... @ 16:35:54\n",
      "Loading back onto the cpu... @ 16:35:54\n",
      "Clearing GPU cache... @ 16:35:54\n",
      "Performing post-processing... @ 16:35:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████▉   | 64/92 [22:09<09:39, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:15\n",
      "Loading OH stacks onto cuda device... @ 16:36:15\n",
      "Performing cuda op... @ 16:36:15\n",
      "Loading back onto the cpu... @ 16:36:15\n",
      "Clearing GPU cache... @ 16:36:15\n",
      "Performing post-processing... @ 16:36:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|███████   | 65/92 [22:30<09:18, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:35\n",
      "Loading OH stacks onto cuda device... @ 16:36:35\n",
      "Performing cuda op... @ 16:36:35\n",
      "Loading back onto the cpu... @ 16:36:35\n",
      "Clearing GPU cache... @ 16:36:35\n",
      "Performing post-processing... @ 16:36:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 66/92 [22:51<08:58, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:56\n",
      "Loading OH stacks onto cuda device... @ 16:36:56\n",
      "Performing cuda op... @ 16:36:56\n",
      "Loading back onto the cpu... @ 16:36:56\n",
      "Clearing GPU cache... @ 16:36:56\n",
      "Performing post-processing... @ 16:36:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 67/92 [23:11<08:37, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:17\n",
      "Loading OH stacks onto cuda device... @ 16:37:17\n",
      "Performing cuda op... @ 16:37:17\n",
      "Loading back onto the cpu... @ 16:37:17\n",
      "Clearing GPU cache... @ 16:37:17\n",
      "Performing post-processing... @ 16:37:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▍  | 68/92 [23:32<08:16, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:38\n",
      "Loading OH stacks onto cuda device... @ 16:37:38\n",
      "Performing cuda op... @ 16:37:38\n",
      "Loading back onto the cpu... @ 16:37:38\n",
      "Clearing GPU cache... @ 16:37:38\n",
      "Performing post-processing... @ 16:37:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 69/92 [23:53<07:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:58\n",
      "Loading OH stacks onto cuda device... @ 16:37:58\n",
      "Performing cuda op... @ 16:37:58\n",
      "Loading back onto the cpu... @ 16:37:58\n",
      "Clearing GPU cache... @ 16:37:58\n",
      "Performing post-processing... @ 16:37:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 70/92 [24:13<07:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:38:19\n",
      "Loading OH stacks onto cuda device... @ 16:38:19\n",
      "Performing cuda op... @ 16:38:19\n",
      "Loading back onto the cpu... @ 16:38:19\n",
      "Clearing GPU cache... @ 16:38:19\n",
      "Performing post-processing... @ 16:38:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|███████▋  | 71/92 [24:34<07:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:38:40\n",
      "Loading OH stacks onto cuda device... @ 16:38:40\n",
      "Performing cuda op... @ 16:38:40\n",
      "Loading back onto the cpu... @ 16:38:40\n",
      "Clearing GPU cache... @ 16:38:40\n",
      "Performing post-processing... @ 16:38:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 72/92 [24:55<06:54, 20.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:00\n",
      "Loading OH stacks onto cuda device... @ 16:39:00\n",
      "Performing cuda op... @ 16:39:00\n",
      "Loading back onto the cpu... @ 16:39:00\n",
      "Clearing GPU cache... @ 16:39:00\n",
      "Performing post-processing... @ 16:39:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████▉  | 73/92 [25:16<06:34, 20.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:21\n",
      "Loading OH stacks onto cuda device... @ 16:39:21\n",
      "Performing cuda op... @ 16:39:21\n",
      "Loading back onto the cpu... @ 16:39:21\n",
      "Clearing GPU cache... @ 16:39:21\n",
      "Performing post-processing... @ 16:39:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 74/92 [25:37<06:14, 20.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:42\n",
      "Loading OH stacks onto cuda device... @ 16:39:42\n",
      "Performing cuda op... @ 16:39:42\n",
      "Loading back onto the cpu... @ 16:39:42\n",
      "Clearing GPU cache... @ 16:39:42\n",
      "Performing post-processing... @ 16:39:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 75/92 [25:57<05:53, 20.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:03\n",
      "Loading OH stacks onto cuda device... @ 16:40:03\n",
      "Performing cuda op... @ 16:40:03\n",
      "Loading back onto the cpu... @ 16:40:03\n",
      "Clearing GPU cache... @ 16:40:03\n",
      "Performing post-processing... @ 16:40:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 76/92 [26:18<05:33, 20.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:24\n",
      "Loading OH stacks onto cuda device... @ 16:40:24\n",
      "Performing cuda op... @ 16:40:24\n",
      "Loading back onto the cpu... @ 16:40:24\n",
      "Clearing GPU cache... @ 16:40:24\n",
      "Performing post-processing... @ 16:40:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▎ | 77/92 [26:39<05:12, 20.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:45\n",
      "Loading OH stacks onto cuda device... @ 16:40:45\n",
      "Performing cuda op... @ 16:40:45\n",
      "Loading back onto the cpu... @ 16:40:45\n",
      "Clearing GPU cache... @ 16:40:45\n",
      "Performing post-processing... @ 16:40:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▍ | 78/92 [27:00<04:51, 20.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:06\n",
      "Loading OH stacks onto cuda device... @ 16:41:06\n",
      "Performing cuda op... @ 16:41:06\n",
      "Loading back onto the cpu... @ 16:41:06\n",
      "Clearing GPU cache... @ 16:41:06\n",
      "Performing post-processing... @ 16:41:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 79/92 [27:21<04:31, 20.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:27\n",
      "Loading OH stacks onto cuda device... @ 16:41:27\n",
      "Performing cuda op... @ 16:41:27\n",
      "Loading back onto the cpu... @ 16:41:27\n",
      "Clearing GPU cache... @ 16:41:27\n",
      "Performing post-processing... @ 16:41:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 80/92 [27:42<04:10, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:47\n",
      "Loading OH stacks onto cuda device... @ 16:41:47\n",
      "Performing cuda op... @ 16:41:47\n",
      "Loading back onto the cpu... @ 16:41:47\n",
      "Clearing GPU cache... @ 16:41:47\n",
      "Performing post-processing... @ 16:41:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 81/92 [28:03<03:49, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:08\n",
      "Loading OH stacks onto cuda device... @ 16:42:08\n",
      "Performing cuda op... @ 16:42:08\n",
      "Loading back onto the cpu... @ 16:42:08\n",
      "Clearing GPU cache... @ 16:42:08\n",
      "Performing post-processing... @ 16:42:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████▉ | 82/92 [28:23<03:28, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:29\n",
      "Loading OH stacks onto cuda device... @ 16:42:29\n",
      "Performing cuda op... @ 16:42:29\n",
      "Loading back onto the cpu... @ 16:42:29\n",
      "Clearing GPU cache... @ 16:42:29\n",
      "Performing post-processing... @ 16:42:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 83/92 [28:44<03:07, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:50\n",
      "Loading OH stacks onto cuda device... @ 16:42:50\n",
      "Performing cuda op... @ 16:42:50\n",
      "Loading back onto the cpu... @ 16:42:50\n",
      "Clearing GPU cache... @ 16:42:50\n",
      "Performing post-processing... @ 16:42:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|█████████▏| 84/92 [29:05<02:46, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:11\n",
      "Loading OH stacks onto cuda device... @ 16:43:11\n",
      "Performing cuda op... @ 16:43:11\n",
      "Loading back onto the cpu... @ 16:43:11\n",
      "Clearing GPU cache... @ 16:43:11\n",
      "Performing post-processing... @ 16:43:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 85/92 [29:26<02:26, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:32\n",
      "Loading OH stacks onto cuda device... @ 16:43:32\n",
      "Performing cuda op... @ 16:43:32\n",
      "Loading back onto the cpu... @ 16:43:32\n",
      "Clearing GPU cache... @ 16:43:32\n",
      "Performing post-processing... @ 16:43:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 86/92 [29:47<02:05, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:53\n",
      "Loading OH stacks onto cuda device... @ 16:43:53\n",
      "Performing cuda op... @ 16:43:53\n",
      "Loading back onto the cpu... @ 16:43:53\n",
      "Clearing GPU cache... @ 16:43:53\n",
      "Performing post-processing... @ 16:43:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▍| 87/92 [30:08<01:44, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:13\n",
      "Loading OH stacks onto cuda device... @ 16:44:13\n",
      "Performing cuda op... @ 16:44:13\n",
      "Loading back onto the cpu... @ 16:44:13\n",
      "Clearing GPU cache... @ 16:44:13\n",
      "Performing post-processing... @ 16:44:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 88/92 [30:29<01:23, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:34\n",
      "Loading OH stacks onto cuda device... @ 16:44:34\n",
      "Performing cuda op... @ 16:44:34\n",
      "Loading back onto the cpu... @ 16:44:34\n",
      "Clearing GPU cache... @ 16:44:34\n",
      "Performing post-processing... @ 16:44:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 89/92 [30:49<01:02, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:55\n",
      "Loading OH stacks onto cuda device... @ 16:44:55\n",
      "Performing cuda op... @ 16:44:55\n",
      "Loading back onto the cpu... @ 16:44:55\n",
      "Clearing GPU cache... @ 16:44:55\n",
      "Performing post-processing... @ 16:44:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 90/92 [31:10<00:41, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:16\n",
      "Loading OH stacks onto cuda device... @ 16:45:16\n",
      "Performing cuda op... @ 16:45:16\n",
      "Loading back onto the cpu... @ 16:45:16\n",
      "Clearing GPU cache... @ 16:45:16\n",
      "Performing post-processing... @ 16:45:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|█████████▉| 91/92 [31:31<00:20, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:37\n",
      "Loading OH stacks onto cuda device... @ 16:45:37\n",
      "Performing cuda op... @ 16:45:37\n",
      "Loading back onto the cpu... @ 16:45:37\n",
      "Clearing GPU cache... @ 16:45:37\n",
      "Performing post-processing... @ 16:45:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 92/92 [31:46<00:00, 19.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:52\n",
      "Consolidating chunks into a constant-length block (tensor). @ 16:45:52\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-59a2d2c4df80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mconstant_length_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordforms_CMs_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Consolidating chunks into a constant-length block (tensor).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mconstant_length_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstant_length_chunks\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding block to list of blocks.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mendNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"l = {l}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset\n",
    "\n",
    "for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "    startNote(f\"l = {l}\")\n",
    "    \n",
    "    Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "                                                     key_length=l,\n",
    "                                                     exact_length_only = False)\n",
    "                   for w in Ws_t]\n",
    "    print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "    \n",
    "    constant_length_chunks = []\n",
    "    chunk_size = 2500\n",
    "    chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "    for chunk in tqdm(chunks, total=len(chunks)):\n",
    "        stampedNote(\"Loading OH stacks onto cuda device...\")\n",
    "        wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "        stampedNote(\"Performing cuda op...\")\n",
    "#         wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "#                                                                   key_length=l, \n",
    "#                                                                   exact_length_only = False)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "        wordforms_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in wordforms_OHs_tr_t_c]\n",
    "        \n",
    "        stampedNote(\"Loading back onto the cpu...\")\n",
    "        wordforms_CMs_cpu = (each.cpu() for each in wordforms_CMs_c)\n",
    "        stampedNote(\"Performing post-processing...\")\n",
    "#         wordforms_CMs_n = np.array(list(par(delayed(makeCMbyWordformAndLength_t_c_post)(w_CM=each.numpy(), \n",
    "#                                                                                           key_length=l, \n",
    "#                                                                                           exact_length_only = False,\n",
    "#                                                                                           asType = 'ndarray')\n",
    "#                                        for each in wordforms_CMs_cpu)))\n",
    "#         wordforms_CMs_t = torch.tensor(wordforms_CMs_n)\n",
    "        wordforms_CMs_t = torch.stack([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "                                                                          key_length=l, \n",
    "                                                                          exact_length_only = False)\n",
    "                                       for each in wordforms_CMs_cpu])\n",
    "        stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "        constant_length_chunks.append(wordforms_CMs_t)\n",
    "        stampedNote(\"Clearing GPU cache...\")\n",
    "        torch.cuda.empty_cache()\n",
    "    stampedNote('Consolidating chunks into a constant-length block (tensor).')\n",
    "    constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "    stampedNote('Adding block to list of blocks.\\n')\n",
    "    endNote(f\"l = {l}\")\n",
    "    cmsByLengthByWordformIndex_torch.append(constant_length_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:04:05.386079Z",
     "start_time": "2019-07-26T23:04:04.163Z"
    }
   },
   "outputs": [],
   "source": [
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "# cmsByLengthByWordformIndex_torch = offset\n",
    "\n",
    "# for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "#     print(f\"l = {l}\")\n",
    "#     constant_length_chunks = []\n",
    "#     chunk_size = 100\n",
    "#     chunks_of_words = chunkList(chunk_size, Ws_t)\n",
    "#     for chunk in tqdm(chunks_of_words, total=len(chunks_of_words)):\n",
    "#         ws = chunk\n",
    "#         ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "#                                                          key_length=l,\n",
    "#                                                          exact_length_only = False)\n",
    "#                        for w in ws]\n",
    "#         wordforms_OHs_tr_t_c = [each.cuda() for each in ws_OHs_tr_t]\n",
    "#         wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "#                                                                   key_length=l, \n",
    "#                                                                   exact_length_only = False)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "#         wordforms_CMs_cpu = [each.cpu() for each in wordforms_CMs_c]\n",
    "#         wordforms_CMs_t = torch.tensor([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "#                                                                            key_length=l, \n",
    "#                                                                            exact_length_only = False)\n",
    "#                                         for each in wordforms_CMs_cpu])\n",
    "#         constant_length_chunks.append(wordforms_CMs_t)\n",
    "#     print('Consolidating chunks into a constant-length block (tensor).')\n",
    "#     constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "#     print(f\"constant_length_block.shape = {constant_length_block.shape}\")\n",
    "#     print('Adding block to list of blocks.\\n')\n",
    "#     cmsByLengthByWordformIndex_torch.append(constant_length_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:04:05.386662Z",
     "start_time": "2019-07-26T23:04:04.591Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrates that makeChannelMatrixByWordformAndLength_t_c will run without errors, but slow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset + [torch.tensor([makeChannelMatrixByWordformAndLength_t_c(w, l)\n",
    "                                                           for w in tqdm(Ws_t, \n",
    "                                                                         total=len(Ws_t))])\n",
    "                                             for l in tqdm(wordlengthsInclEdges_range, \n",
    "                                                           total=len(wordlengthsInclEdges_range))]\n",
    "# cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.867776Z",
     "start_time": "2019-07-24T21:54:07.698848Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "offset = [np.zeros(shape=(0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.902590Z",
     "start_time": "2019-07-24T21:54:22.869287Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in wordlengthsInclEdges_range:\n",
    "    assert all(cm.shape[1] == l - 2 for cm in cmsByLengthByWordformIndex[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.909309Z",
     "start_time": "2019-07-24T21:54:22.904096Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = offset + [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                                      for w in Ws_t])\n",
    "                                            for l in wordlengthsInclEdges_range]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.310885Z",
     "start_time": "2019-07-24T21:54:22.910819Z"
    }
   },
   "outputs": [],
   "source": [
    "exactCMsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l, exact_length_only = True)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "exactCMsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), exactCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment sequence (all prefixes or just wordforms) channel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `CMsByWordformIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " - `exactCMsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.318016Z",
     "start_time": "2019-07-24T21:54:24.312658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByWordformIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.329330Z",
     "start_time": "2019-07-24T21:54:24.319395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.341972Z",
     "start_time": "2019-07-24T21:54:24.330798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 38, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.015572096"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[2].shape\n",
    "cmsByLengthByWordformIndex[3].shape\n",
    "cmsByLengthByWordformIndex[10].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.697748Z",
     "start_time": "2019-07-24T21:54:24.343457Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))\n",
    "    pickle.dump(CMsByWordformIndex, open(o + 'CMs_by_wordform_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.759827Z",
     "start_time": "2019-07-24T21:54:24.699318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.766724Z",
     "start_time": "2019-07-24T21:54:24.761786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.803809Z",
     "start_time": "2019-07-24T21:54:24.768566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_prefix_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByPrefixIndex) if r else len(CMsByPrefixIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_prefix_index.pickle' if r else o + 'CMs_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_prefix_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.811621Z",
     "start_time": "2019-07-24T21:54:24.805592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(CMsByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'CMs_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.819439Z",
     "start_time": "2019-07-24T21:54:24.817389Z"
    }
   },
   "outputs": [],
   "source": [
    "# importDict(o + '.pW_C' + '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:30.653058Z",
     "start_time": "2019-07-24T21:54:24.821320Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    pickle.dump(exactCMsByLengthByWordformIndex, open(o + 'exact_CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:30.722681Z",
     "start_time": "2019-07-24T21:54:30.654567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByLengthByWordformIndex) if r else len(cmsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_length_by_wordform_index.pickle' if r else o + 'CMs_by_length_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:58.016185Z",
     "start_time": "2019-07-24T21:54:57.971893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "exact_CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(exactCMsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'exact_CMs_by_length_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     exact_CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:59.604073Z",
     "start_time": "2019-07-24T21:54:59.598079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:59.893777Z",
     "start_time": "2019-07-24T21:54:59.845739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy',\n",
       " 'Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json',\n",
       " 'p3Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy_metadata.json',\n",
       " 'pYX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle_metadata.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2Y012s.txt',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       " 'Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle_metadata.json']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.389549Z",
     "start_time": "2019-07-24T21:55:00.363581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving p3Y1X012_np to filepath 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy'\n"
     ]
    }
   ],
   "source": [
    "#if not r, export numpy representation of triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012' + '.npy', p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.593216Z",
     "start_time": "2019-07-24T21:55:00.588808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if not r:\n",
    "    CM_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "             'changes':'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         CM_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.840185Z",
     "start_time": "2019-07-24T21:55:00.836400Z"
    }
   },
   "outputs": [],
   "source": [
    "#if r, export .json of modified triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving extended, human-readable version of p3Y1X012 to filepath '{o + 'p3Y1X012_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X012_RE' + '.json', condProbDistAsDicts(p3Y1X012))\n",
    "          \n",
    "    print(f\"Saving extended, human-readable version of p3Y1X01 to filepath '{o + 'p3Y1X01_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X01_RE' + '.json', condProbDistAsDicts(p3Y1X01))\n",
    "\n",
    "#if r, export numpy representation of triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012_RE' + '.npy', p3Y1X012_np)\n",
    "    print(f\"Saving p3Y1X01_np to filepath '{o + 'p3Y1X01_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X01_RE' + '.npy', p3Y1X01_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:01.055288Z",
     "start_time": "2019-07-24T21:55:01.052060Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    CD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X012_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:01.318057Z",
     "start_time": "2019-07-24T21:55:01.314912Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    PD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X01_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X01_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X01_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
