{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:41.653105Z",
     "start_time": "2019-07-27T03:27:41.649666Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-/-pytorch-representations\" data-toc-modified-id=\"numpy-/-pytorch-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> / <code>pytorch</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices\" data-toc-modified-id=\"Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Segment sequence (all prefixes or just wordforms) channel matrices</a></span></li><li><span><a href=\"#Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)\" data-toc-modified-id=\"Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$).\n",
    "\n",
    "#FIXME update to reflect other exports (including the channel matrix stacks acccctually used in subsequent notebooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:41.831649Z",
     "start_time": "2019-07-27T03:27:41.677040Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.725098Z",
     "start_time": "2019-07-27T03:27:41.832886Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.732142Z",
     "start_time": "2019-07-27T03:27:43.727939Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "# c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "c = \"CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json\"\n",
    "\n",
    "b = ''\n",
    "# b = \n",
    "b = \"CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy\"\n",
    "\n",
    "w = ''\n",
    "# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "w = 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'\n",
    "o = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_OD'\n",
    "\n",
    "p = ''\n",
    "# p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.745425Z",
     "start_time": "2019-07-27T03:27:43.734062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_md = b + \"_metadata.json\"\n",
    "b_o = b.split(\".npy\")[0] + \"Y012s.txt\"\n",
    "\n",
    "b_md\n",
    "b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.811355Z",
     "start_time": "2019-07-27T03:27:43.746500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'matrix shape': [57798, 7381],\n",
       " 'Produced in step': 'Step 4d',\n",
       " 'Produced in notebook': 'Calculate observation distribution given channel models',\n",
       " 'X012s': {'from fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       "  'changes': 'sorted',\n",
       "  'size': 7381},\n",
       " 'Y012s': {'from fp': {'preview': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       "   'postview': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       "   'center': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       "  'changes': 'sorted',\n",
       "  'exported fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       "  'size': 57798}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importDict(b_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.815647Z",
     "start_time": "2019-07-27T03:27:43.813181Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.820315Z",
     "start_time": "2019-07-27T03:27:43.817294Z"
    }
   },
   "outputs": [],
   "source": [
    "# if p == '':\n",
    "#     r = False\n",
    "# else:\n",
    "#     r = True\n",
    "#     print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.849235Z",
     "start_time": "2019-07-27T03:27:43.822651Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:43.853591Z",
     "start_time": "2019-07-27T03:27:43.851315Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.326865Z",
     "start_time": "2019-07-27T03:27:43.854994Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.434054Z",
     "start_time": "2019-07-27T03:27:49.328430Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.438821Z",
     "start_time": "2019-07-27T03:27:49.435614Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = 10\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.444418Z",
     "start_time": "2019-07-27T03:27:49.440182Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.641940Z",
     "start_time": "2019-07-27T03:27:49.445782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.get_device_name(1))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.647390Z",
     "start_time": "2019-07-27T03:27:49.643581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu\n",
    "# my_device = gpu\n",
    "torch.cuda.set_device(1)\n",
    "my_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:49.651598Z",
     "start_time": "2019-07-27T03:27:49.648721Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "# my_ft = cuda_ft\n",
    "# my_dt = cuda_dt\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:50.135061Z",
     "start_time": "2019-07-27T03:27:49.652886Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:50.143159Z",
     "start_time": "2019-07-27T03:27:50.136735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleRightEdgeTriphs = {x012 for x012 in set(conditions(p3Y1X012)) if rightEdge + '.' + rightEdge in x012}\n",
    "doubleRightEdgeTriphs\n",
    "# assert len(doubleRightEdgeTriphs) == 0\n",
    "\n",
    "if len(doubleRightEdgeTriphs) > 0:\n",
    "    print('Restoring p3Y1X012...')\n",
    "    p3Y1X012 = condProbDistAsDicts(p3Y1X012)\n",
    "    \n",
    "    for x012 in doubleRightEdgeTriphs:\n",
    "        del p3Y1X012[x012]\n",
    "    \n",
    "    for x012 in p3Y1X012:\n",
    "        del p3Y1X012[x012][rightEdge]\n",
    "    \n",
    "    assert areNormalized(p3Y1X012)\n",
    "    assert uniformOutcomes(p3Y1X012)\n",
    "    \n",
    "    p3Y1X012 = condDistsAsProbDists(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:50.148191Z",
     "start_time": "2019-07-27T03:27:50.144593Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "#     assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:50.310174Z",
     "start_time": "2019-07-27T03:27:50.149826Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:54.711895Z",
     "start_time": "2019-07-27T03:27:50.311666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 9172\n",
      "Restoring lexicon...\n",
      "|Prefixes| = 33059\n",
      "|triphones| in lexicon = 7381\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "if rightEdge + '.' + rightEdge in lexiconTo2factors(Ws):\n",
    "    print('Restoring lexicon...')\n",
    "    \n",
    "    def restoreWordform(w):\n",
    "        w_t = ds2t(w)\n",
    "        assert w_t[-1] == rightEdge and w_t[-2] == rightEdge\n",
    "        w_t = w_t[:-1]\n",
    "        return t2ds(w_t)\n",
    "    \n",
    "    Ws = set(map(restoreWordform, Ws))\n",
    "    Ws_t = tuple(sorted(list(Ws)))\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ps)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:54.723443Z",
     "start_time": "2019-07-27T03:27:54.713958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 7381\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:27:54.730831Z",
     "start_time": "2019-07-27T03:27:54.727620Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.194165Z",
     "start_time": "2019-07-27T03:27:54.732892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57798, 7381)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.412856304"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "57798"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pC1X012_np = np.load(b)\n",
    "pC1X012_np.shape\n",
    "pC1X012_np.nbytes / 1e9\n",
    "\n",
    "pC1X012_torch = torch.tensor(pC1X012_np)\n",
    "\n",
    "Y012s_t = importSeqs(b_o, tuple)\n",
    "Y012s = set(Y012s_t)\n",
    "len(Y012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.341597Z",
     "start_time": "2019-07-27T03:28:28.195676Z"
    }
   },
   "outputs": [],
   "source": [
    "assert pC1X012_np.shape[0] == len(Y012s)\n",
    "assert pC1X012_np.shape[1] == len(X012s)\n",
    "assert lexiconToInventory(Y012s) - edgeSymbols == Y1s\n",
    "assert tuple(sorted(list(Y012s))) == Y012s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.348403Z",
     "start_time": "2019-07-27T03:28:28.343149Z"
    }
   },
   "outputs": [],
   "source": [
    "def pC1X012_calc(y012=None, x012=None, asType = 'ndarray'):\n",
    "    if y012 is None:\n",
    "        if asType == 'ndarray':\n",
    "            return pC1X012_np[:, X012s_t.index(x012)]\n",
    "        elif asType == 'torch':\n",
    "            return torch.from_numpy(pC1X012_np[:, X012s_t.index(x012)])\n",
    "        elif asType == 'dict':\n",
    "            return {y012:pC1X012_np[Y012s_t.index(y012), X012s_t.index(x012)]\n",
    "                    for y012 in y012s_t}\n",
    "        else:\n",
    "            raise Exception(\"Acceptable asType arguments = {'dict', 'ndarray', 'torch'}\")\n",
    "    if x012 is None:\n",
    "        if asType == 'ndarray':\n",
    "            return pC1X012_np[Y012s_t.index(y012),:]\n",
    "        elif asType == 'torch':\n",
    "            return torch.from_numpy(pC1X012_np[Y012s_t.index(y012),:])\n",
    "        else:\n",
    "            raise Exception(\"Acceptable asType arguments = {'ndarray', 'torch'}\")\n",
    "    return pC1X012_np[Y012s_t.index(y012), X012s_t.index(x012)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.354467Z",
     "start_time": "2019-07-27T03:28:28.349775Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     channelDiphones = set(p3Y1X01.keys())\n",
    "#     print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "#     lexiconDiphones = lexiconTo2factors(Ws)\n",
    "#     unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "#     print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "#     assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "#     print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "#     X01s = lexiconDiphones\n",
    "#     assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.360627Z",
     "start_time": "2019-07-27T03:28:28.355790Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "#     # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "#     for x01 in p3Y1X01:\n",
    "#         p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "#     # create new conditioning events\n",
    "#     wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "#     list(wordEndDiphones)[:5]\n",
    "\n",
    "#     # create their distribution over outcomes\n",
    "#     deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "#     deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "#     # add the new wordend conditioning events to the preview distribution\n",
    "#     p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "#     p3Y1X01['aʊ.s']['s']\n",
    "#     p3Y1X01['ɑ.⋉']\n",
    "\n",
    "#     # check that everything worked\n",
    "#     for x01 in p3Y1X01:\n",
    "#         assert rightEdge in p3Y1X01[x01]\n",
    "#     #     if rightEdge not in p3Y1X01[x01]:\n",
    "#     #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "#     assert areNormalized(p3Y1X01)\n",
    "#     assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "#     channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "#     unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "#     print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "#     assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "#     #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "#     # let's trim the preview model's conditioning events\n",
    "#     p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "#     p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "#     X01s_RE = set(p3Y1X01.keys())\n",
    "#     len(X01s_RE)\n",
    "    \n",
    "# #     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` / `pytorch` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.384305Z",
     "start_time": "2019-07-27T03:28:28.361926Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)\n",
    "XOHmap_torch = {k:torch.tensor(XOHmap[k])\n",
    "                for k in XOHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.597833Z",
     "start_time": "2019-07-27T03:28:28.385909Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)\n",
    "X012OHmap_torch = {k:torch.tensor(X012OHmap[k])\n",
    "                   for k in X012OHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:28.601274Z",
     "start_time": "2019-07-27T03:28:28.599295Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:38.912664Z",
     "start_time": "2019-07-27T03:28:28.602736Z"
    }
   },
   "outputs": [],
   "source": [
    "Y012map = seqsToIndexMap(Y012s)\n",
    "Y012OHmap = seqsToOneHotMap(Y012s)\n",
    "Y012OHmap_torch = {k:torch.tensor(Y012OHmap[k])\n",
    "                   for k in Y012OHmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:38.916128Z",
     "start_time": "2019-07-27T03:28:38.914150Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     X01REmap = seqsToIndexMap(X01s_RE)\n",
    "#     X01REOHs = seqMapToOneHots(X01REmap)\n",
    "#     X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "#     Y1s_RE = outcomes(p3Y1X01)\n",
    "#     len(Y1s_RE)\n",
    "#     Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "#     print(Y1s_RE - Y1s)\n",
    "\n",
    "#     Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "#     Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "#     Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "#     OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:38.968699Z",
     "start_time": "2019-07-27T03:28:38.917408Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     for x012 in p3Y1X012:\n",
    "#         p3Y1X012[x012].update({rightEdge:0.0})\n",
    "#         assert rightEdge in p3Y1X012[x012]\n",
    "#         assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "#     outcomes(p3Y1X012) == Y1s\n",
    "#     outcomes(p3Y1X012) == Y1s_RE\n",
    "#     areNormalized(p3Y1X012)\n",
    "#     uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.026391Z",
     "start_time": "2019-07-27T03:28:38.970444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3740, 1441])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 7381)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7381,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap, asTorch=False):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap, asTorch=False):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    if asTorch:\n",
    "#         return torch.tensor([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "        return torch.tensor(np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq]))\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap, asTorch=False):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor(np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq]))\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap, asTorch=False):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    if asTorch:\n",
    "        return torch.tensor(np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq]))\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap, True)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap, True)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.231622Z",
     "start_time": "2019-07-27T03:28:39.027767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 7381)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "# if r:\n",
    "#     testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "# else:\n",
    "#     testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "p3Y1X012_np.shape\n",
    "\n",
    "p3Y1X012_torch = torch.tensor(p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.237663Z",
     "start_time": "2019-07-27T03:28:39.233282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57798, 7381)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([57798, 7381])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pC1X012_np.shape\n",
    "pC1X012_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.240723Z",
     "start_time": "2019-07-27T03:28:39.238943Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "#     testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "#     p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.245980Z",
     "start_time": "2019-07-27T03:28:39.241964Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.252551Z",
     "start_time": "2019-07-27T03:28:39.247215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ɹ.ɪ.l.ɪ.dʒ.ɪ.s.⋉'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.259292Z",
     "start_time": "2019-07-27T03:28:39.253842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.s.ɪ.g.ɚ'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.262924Z",
     "start_time": "2019-07-27T03:28:39.260617Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.270180Z",
     "start_time": "2019-07-27T03:28:39.264205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.f.tʃ.m.ɛ.dʒ.aɪ.θ.aʊ'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:39.278198Z",
     "start_time": "2019-07-27T03:28:39.271562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.k.æ.m'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.ɪ.θ.eɪ'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:41.359347Z",
     "start_time": "2019-07-27T03:28:39.279555Z"
    }
   },
   "outputs": [],
   "source": [
    "Ps_t_OH = [dsToTriphoneOHs(x0k, X012OHmap)\n",
    "           for x0k in Ps_t]\n",
    "\n",
    "# BAD. strangely slow?\n",
    "# Ps_t_np = np.array(list(par(delayed(dsToTriphoneOHs)(x0k, X012OHmap) \n",
    "#                             for x0k in Ps_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.435376Z",
     "start_time": "2019-07-27T03:28:41.360850Z"
    }
   },
   "outputs": [],
   "source": [
    "Ps_t_OH_torch = [torch.tensor(x0k_OH)\n",
    "                 for x0k_OH in Ps_t_OH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.441700Z",
     "start_time": "2019-07-27T03:28:44.436849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.k.æ', 'k.æ.m')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.k.æ.m'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.501450Z",
     "start_time": "2019-07-27T03:28:44.442956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7051, 1837)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.507695Z",
     "start_time": "2019-07-27T03:28:44.503129Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.512105Z",
     "start_time": "2019-07-27T03:28:44.509093Z"
    }
   },
   "outputs": [],
   "source": [
    "# from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.517574Z",
     "start_time": "2019-07-27T03:28:44.513399Z"
    }
   },
   "outputs": [],
   "source": [
    "# # @njit(parallel=True)\n",
    "# @njit\n",
    "def sourcePrefixStackToChannelMatrix(x0k_OHs):\n",
    "    return pC1X012_np @ x0k_OHs.T\n",
    "#     return np.matmul(pC1X012_np, x0k_OHs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.527746Z",
     "start_time": "2019-07-27T03:28:44.518849Z"
    }
   },
   "outputs": [],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(ds2t(x0k))))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('pC1X012_np.shape = {0}'.format(pC1X012_np.shape))\n",
    "#         print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "        print('result = pC1X012_np * triphoneOHs.T')\n",
    "#     result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    result = np.matmul(pC1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "\n",
    "def sourcePrefixToChannelMatrix(x0k):\n",
    "    triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#     C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                    for x012_idx in triphoneIndices] \n",
    "#                   for y1 in Y1s_t])\n",
    "    C = np.array([[pC1X012_np[Y012map[y012], x012_idx] \n",
    "               for x012_idx in triphoneIndices] \n",
    "              for y012 in Y012s_t])\n",
    "    if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "        raise Exception('This is not well-defined.')\n",
    "#         C = np.zeros((len(Y1s_RE), 1))\n",
    "#         C = np.zeros((len(Y012s), 1))\n",
    "#         C[-1] = 1.0\n",
    "#         return C.reshape(len(Y1s_RE),1)\n",
    "#         return C.reshape(len(Y1s),1)\n",
    "    assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "    assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "    return C\n",
    "\n",
    "\n",
    "# if r:\n",
    "#     def sourcePrefixToChannelMatrix(x0k):\n",
    "#         triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#     #                    for x012_idx in triphoneIndices] \n",
    "#     #                   for y1 in Y1s_t])\n",
    "#         if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "#     #         C = np.zeros((len(Y1s), 1))\n",
    "#             C[-1] = 1.0\n",
    "#             return C.reshape(len(Y1s_RE),1)\n",
    "#     #         return C.reshape(len(Y1s),1)\n",
    "#         return C\n",
    "# else:\n",
    "#     def sourcePrefixToChannelMatrix(x0k):\n",
    "#         triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "# #         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#         C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                        for x012_idx in triphoneIndices] \n",
    "#                       for y1 in Y1s_t])\n",
    "#         if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "# #             C = np.zeros((len(Y1s_RE), 1))\n",
    "#             C = np.zeros((len(Y1s), 1))\n",
    "#             C[-1] = 1.0\n",
    "# #             return C.reshape(len(Y1s_RE),1)\n",
    "#             return C.reshape(len(Y1s),1)\n",
    "#         assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "#         assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "#         return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:44.539231Z",
     "start_time": "2019-07-27T03:28:44.529142Z"
    }
   },
   "outputs": [],
   "source": [
    "def sourcePrefixStackToChannelMatrix_t_c(x0k_OHs_tr):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "    return torch.matmul(pC1X012_torch_c, x0k_OHs_tr)\n",
    "\n",
    "def sourcePrefixStackToChannelMatrix_t(x0k_OHs_tr):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "    return torch.matmul(pC1X012_torch, x0k_OHs_tr)\n",
    "\n",
    "def sourcePrefixStackToChannelMatrix_t_tr(x0k_OHs):\n",
    "#     return pC1X012_torch @ x0k_OHs.t()\n",
    "    return torch.matmul(pC1X012_torch, x0k_OHs.t())\n",
    "#     return torch.matmul(pC1X012_torch, x0k_OHs)\n",
    "\n",
    "def sourcePrefixToChannelMatrix_l_t(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap, True)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(ds2t(x0k))))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('pC1X012_np.shape = {0}'.format(pC1X012_np.shape))\n",
    "#         print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "        print('result = pC1X012_torch * triphoneOHs.t()')\n",
    "#     result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    result = torch.matmul(pC1X012_torch, triphoneOHs.t())\n",
    "    return result\n",
    "\n",
    "def sourcePrefixToChannelMatrix_t(x0k):\n",
    "    triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#     C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "#     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "#                    for x012_idx in triphoneIndices] \n",
    "#                   for y1 in Y1s_t])\n",
    "    C = torch.tensor([[pC1X012_torch[Y012map[y012], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y012 in Y012s_t])\n",
    "    if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "        raise Exception('This is not well-defined.')\n",
    "#         C = np.zeros((len(Y1s_RE), 1))\n",
    "#         C = np.zeros((len(Y012s), 1))\n",
    "#         C[-1] = 1.0\n",
    "#         return C.reshape(len(Y1s_RE),1)\n",
    "#         return C.reshape(len(Y1s),1)\n",
    "    assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "    assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:45.897795Z",
     "start_time": "2019-07-27T03:28:44.540655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋊.k.æ.m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   5.51032142e-06],\n",
       "       [  0.00000000e+00,   1.51733489e-06],\n",
       "       [  0.00000000e+00,   2.14822676e-05],\n",
       "       ..., \n",
       "       [  4.78438925e-05,   0.00000000e+00],\n",
       "       [  4.78438925e-05,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(random_source_prefix)\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:46.713196Z",
     "start_time": "2019-07-27T03:28:45.899364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋊.k.æ.m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 5.5103e-06],\n",
       "        [0.0000e+00, 1.5173e-06],\n",
       "        [0.0000e+00, 2.1482e-05],\n",
       "        ...,\n",
       "        [4.7844e-05, 0.0000e+00],\n",
       "        [4.7844e-05, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(random_source_prefix)\n",
    "sourcePrefixToChannelMatrix_l_t(random_source_prefix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:46.719185Z",
     "start_time": "2019-07-27T03:28:46.714811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(p3Y1X012_np, dsToTriphoneOHs(random_source_prefix, X012OHmap).T) == p3Y1X012_np @ dsToTriphoneOHs(random_source_prefix, X012OHmap).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:47.830352Z",
     "start_time": "2019-07-27T03:28:46.720654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       ..., \n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixStackToChannelMatrix(dsToTriphoneOHs(random_source_prefix, X012OHmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:49.239409Z",
     "start_time": "2019-07-27T03:28:47.831491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.k.æ.m'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " ..., \n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.287005Z",
     "start_time": "2019-07-27T03:28:49.240459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.k.æ.m'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " ..., \n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.290579Z",
     "start_time": "2019-07-27T03:28:50.288479Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:16:21.084560Z",
     "start_time": "2019-07-26T22:16:21.036555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊',\n",
       " '⋊.aɪ',\n",
       " '⋊.aɪ.d',\n",
       " '⋊.aɪ.d.i',\n",
       " '⋊.aɪ.d.i.l',\n",
       " '⋊.aɪ.d.i.l.aɪ',\n",
       " '⋊.aɪ.d.i.l.aɪ.z',\n",
       " '⋊.aɪ.d.i.l.aɪ.z.⋉',\n",
       " '⋊.aɪ.d.i.l.i',\n",
       " '⋊.aɪ.d.i.l.i.⋉')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "33059"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'⋊',\n",
       " '⋊.aɪ',\n",
       " '⋊.aʊ',\n",
       " '⋊.b',\n",
       " '⋊.d',\n",
       " '⋊.dʒ',\n",
       " '⋊.eɪ',\n",
       " '⋊.f',\n",
       " '⋊.g',\n",
       " '⋊.h',\n",
       " '⋊.i',\n",
       " '⋊.j',\n",
       " '⋊.k',\n",
       " '⋊.l',\n",
       " '⋊.m',\n",
       " '⋊.n',\n",
       " '⋊.oʊ',\n",
       " '⋊.p',\n",
       " '⋊.s',\n",
       " '⋊.t',\n",
       " '⋊.tʃ',\n",
       " '⋊.u',\n",
       " '⋊.v',\n",
       " '⋊.w',\n",
       " '⋊.z',\n",
       " '⋊.æ',\n",
       " '⋊.ð',\n",
       " '⋊.ɑ',\n",
       " '⋊.ɔɪ',\n",
       " '⋊.ə',\n",
       " '⋊.ɚ',\n",
       " '⋊.ɛ',\n",
       " '⋊.ɪ',\n",
       " '⋊.ɹ',\n",
       " '⋊.ʃ',\n",
       " '⋊.ʌ',\n",
       " '⋊.θ'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps_t[:10]\n",
    "len(Ps_t)\n",
    "len({p for p in Ps if len(ds2t(p)) < 3})\n",
    "{p for p in Ps if len(ds2t(p)) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T17:31:37.162183Z",
     "start_time": "2019-07-26T17:31:35.773291Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefixes = choices(tuple({p for p in Ps if len(ds2t(p)) > 2}), k=5000)\n",
    "random_source_prefixes_OHs = [dsToTriphoneOHs(p, X012OHmap)\n",
    "                              for p in random_source_prefixes]\n",
    "random_source_prefixes_OHs_t = [torch.tensor(each) for each in random_source_prefixes_OHs]\n",
    "random_source_prefixes_OHs_tr_t = [torch.tensor(each).t() for each in random_source_prefixes_OHs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T17:31:40.379092Z",
     "start_time": "2019-07-26T17:31:37.163523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.421403456"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch_nbytes(each)\n",
    "     for each in random_source_prefixes_OHs_tr_t]) / 1e9\n",
    "# random_source_prefixes_OHs_tr_t\n",
    "random_source_prefixes_OHs_tr_t_c = [each.cuda() for each in random_source_prefixes_OHs_tr_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T17:31:40.819943Z",
     "start_time": "2019-07-26T17:31:40.381124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.412856304"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_nbytes(pC1X012_torch) / 1e9\n",
    "pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:48:37.560428Z",
     "start_time": "2019-07-26T01:48:04.389672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 ms ± 36.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixToChannelMatrix_l(choice(random_source_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:48:38.142047Z",
     "start_time": "2019-07-26T01:48:37.562138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.31192"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourcePrefixToChannelMatrix_l(choice(random_source_prefixes)).nbytes / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:49:15.699811Z",
     "start_time": "2019-07-26T01:48:38.143890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536 ms ± 17.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixStackToChannelMatrix(choice(random_source_prefixes_OHs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:51:37.863908Z",
     "start_time": "2019-07-26T01:50:55.466179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606 ms ± 43.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixStackToChannelMatrix_t_tr(choice(random_source_prefixes_OHs_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:52:19.925658Z",
     "start_time": "2019-07-26T01:51:37.865585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 ms ± 29.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixStackToChannelMatrix_t(choice(random_source_prefixes_OHs_tr_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T01:57:27.175258Z",
     "start_time": "2019-07-26T01:57:27.167386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.7 µs ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixStackToChannelMatrix_t_c(choice(random_source_prefixes_OHs_tr_t_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:03:59.153697Z",
     "start_time": "2019-07-26T02:03:45.033657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 ms ± 4.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "\n",
    "sourcePrefixStackToChannelMatrix_t_c(choice(random_source_prefixes_OHs_t).t().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:08:43.658852Z",
     "start_time": "2019-07-26T02:08:43.656315Z"
    }
   },
   "outputs": [],
   "source": [
    "del random_source_prefixes_OHs_tr_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:24:32.679930Z",
     "start_time": "2019-07-26T02:24:32.675654Z"
    }
   },
   "outputs": [],
   "source": [
    "del pC1X012_torch_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:24:33.114364Z",
     "start_time": "2019-07-26T02:24:33.110121Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson learned:** \n",
    " - Load as much onto the cuda device ahead of time as you can afford. Use `torch_nbytes` to determine size ahead of time.\n",
    " - Make functions parameterizable so that they don't depend on some object that isn't on the device.\n",
    " - Break functions down into things cuda can do and stuff on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:28.553306Z",
     "start_time": "2019-07-25T01:09:28.550302Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     def sourcePrefixToPreviewVector(x0k):\n",
    "#         xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "#         if len(xp_t) < 2:\n",
    "#             raise Exception('|x0k| must be > 1.')\n",
    "#         if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "#     #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "#             uniformProb = 1.0 / len(Y1s_RE)\n",
    "#             preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "#             return preview_dist.reshape(len(Y1s_RE),1)\n",
    "\n",
    "#         xi = xp_t[-2] #just-completed segment\n",
    "#         xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "#         xik_ds = t2ds((xi, xk))\n",
    "#         preview_dist = p3Y1X01[xik_ds]\n",
    "#     #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "#         return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "#     sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:34.282782Z",
     "start_time": "2019-07-25T01:09:34.279931Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     # returns p(Y0K|x0k)\n",
    "#     def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "#         # NB:\n",
    "#         # if len(prefix) == n (including leftEdge), \n",
    "#         # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "#         p = prefix\n",
    "#         if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "#     #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "#             return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "#         else: #the extended channel matrix is garbage that should never be asked for\n",
    "#             l = len(ds2t(p))\n",
    "#             return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:35.409263Z",
     "start_time": "2019-07-25T01:09:35.405122Z"
    }
   },
   "outputs": [],
   "source": [
    "# if f:\n",
    "#     print('Source sequences = wordforms and prefixes')\n",
    "#     source_seqs = prefixes_t #prefixes include full wordforms\n",
    "# else:\n",
    "#     print('Source sequences = just full wordforms')\n",
    "#     source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:09:41.501287Z",
     "start_time": "2019-07-25T01:09:41.497832Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "#                          for s in source_seqs]\n",
    "#     xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "#     xCMsByPrefixIndex[3].shape\n",
    "# if r:\n",
    "#     xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(p)\n",
    "#                          for p in prefixes_t]\n",
    "#     xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "#     xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original idea: process prefixes in blocks of 5000, \n",
    "# with order corresponding to ordering in Ps_t\n",
    "#\n",
    "# won't work because some prefixes are too short for pC1X012 to be well-defined\n",
    "\n",
    "# #split the prefixes_t into blocks of 5000\n",
    "# len(Ps_t)\n",
    "# chunk_size = 5000\n",
    "# Ps_t_chunked = [Ps_t[i * chunk_size:(i + 1) * chunk_size] for i in range((len(Ps_t) + chunk_size - 1) // chunk_size )]\n",
    "# [len(chunk) for chunk in Ps_t_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.296362Z",
     "start_time": "2019-07-27T03:28:50.291832Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkList(chunk_size, l):\n",
    "    return [l[i * chunk_size:(i + 1) * chunk_size] for i in range((len(l) + chunk_size - 1) // chunk_size )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.301228Z",
     "start_time": "2019-07-27T03:28:50.297574Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.306253Z",
     "start_time": "2019-07-27T03:28:50.302613Z"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from itertools.groupby documentation\n",
    "def persistent_groupby(data, keyfunc, verbose=False):\n",
    "    groups = []\n",
    "    uniquekeys = []\n",
    "    # data = sorted(data, key=keyfunc)\n",
    "    for k, g in groupby(data, keyfunc):\n",
    "        groups.append(list(g))      # Store group iterator as a list\n",
    "        uniquekeys.append(k)\n",
    "    if verbose:\n",
    "        print(len(groups))\n",
    "        print(len(uniquekeys))\n",
    "        print([len(g) for g in groups])\n",
    "        print(uniquekeys)\n",
    "        groups[0]\n",
    "    return groups, uniquekeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.311009Z",
     "start_time": "2019-07-27T03:28:50.307432Z"
    }
   },
   "outputs": [],
   "source": [
    "# # adapted from itertools.groupby documentation\n",
    "# groups = []\n",
    "# uniquekeys = []\n",
    "# keyfunc = lambda p: len(ds2t(p)) > 2\n",
    "# # data = sorted(data, key=keyfunc)\n",
    "# data = Ps_t\n",
    "# for k, g in groupby(data, keyfunc):\n",
    "#     groups.append(list(g))      # Store group iterator as a list\n",
    "#     uniquekeys.append(k)\n",
    "# print(len(groups))\n",
    "# print(len(uniquekeys))\n",
    "# print([len(g) for g in groups])\n",
    "# print(uniquekeys)\n",
    "# groups[0]\n",
    "# groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:50.338914Z",
     "start_time": "2019-07-27T03:28:50.312068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "72\n",
      "[2, 145, 1, 158, 1, 1436, 1, 1892, 1, 452, 1, 109, 1, 1446, 1, 724, 1, 1174, 1, 75, 1, 256, 1, 3109, 1, 1029, 1, 1917, 1, 742, 1, 205, 1, 2815, 1, 3514, 1, 1337, 1, 276, 1, 2, 1, 643, 1, 573, 1, 64, 1, 1084, 1, 38, 1, 774, 1, 13, 1, 1230, 1, 34, 1, 640, 1, 2343, 1, 1828, 1, 268, 1, 488, 1, 189]\n",
      "[False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "Ps_t_grouped, Ps_t_grouped_uniquekeys = persistent_groupby(Ps_t, \n",
    "                                                           keyfunc = lambda p: len(ds2t(p)) > 2, \n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:28:57.834194Z",
     "start_time": "2019-07-27T03:28:50.340056Z"
    }
   },
   "outputs": [],
   "source": [
    "pC1X012_torch_c = pC1X012_torch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.102065Z",
     "start_time": "2019-07-27T03:28:57.835626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 145\n",
      "\t2 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 158\n",
      "\t2 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:00, 14.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t15 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1892\n",
      "\t19 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 452\n",
      "\t5 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 109\n",
      "\t2 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [00:01,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t15 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [00:01,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t8 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [00:01,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t12 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 75\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 256\n",
      "\t3 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 3109\n",
      "\t32 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [00:02,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1029\n",
      "\t11 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [00:02,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1917\n",
      "\t20 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:03,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 742\n",
      "\t8 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 205\n",
      "\t3 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 2815\n",
      "\t29 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [00:04,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 3514\n",
      "\t36 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [00:04,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1337\n",
      "\t14 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [00:05,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 276\n",
      "\t3 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 2\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 643\n",
      "\t7 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:05,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 573\n",
      "\t6 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 64\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [00:05,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t11 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 38\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [00:06, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t8 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 13\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 1230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "58it [00:06, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t13 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 34\n",
      "\t1 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "62it [00:06, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t7 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 2343\n",
      "\t24 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "64it [00:07,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 1828\n",
      "\t19 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:08,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group of prefixes of length < 3\n",
      "Group size: 268\n",
      "\t3 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 488\n",
      "\t5 chunks of size 100:\n",
      "Group of prefixes of length < 3\n",
      "Group size: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "72it [00:08,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CMsByPrefixIndex_torch = []\n",
    "for group, key in tqdm(zip(Ps_t_grouped, Ps_t_grouped_uniquekeys)):\n",
    "    if key:\n",
    "        print(f\"Group size: {len(group)}\")\n",
    "        prefixes_OHs_tr_t = [torch.tensor(dsToTriphoneOHs(p, X012OHmap)).t()\n",
    "                             for p in group]\n",
    "        chunk_size = 100\n",
    "        chunks = chunkList(chunk_size, prefixes_OHs_tr_t)\n",
    "        print(f\"\\t{len(chunks)} chunks of size {chunk_size}:\")\n",
    "        for chunk in chunks:\n",
    "            prefixes_OHs_tr_t_c = [each.cuda() \n",
    "                                   for each in chunk]\n",
    "            group_CMs_c = (sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in chunk)\n",
    "            group_CMs = [each.cpu() for each in chunk]\n",
    "            CMsByPrefixIndex_torch.extend(group_CMs)\n",
    "    else:\n",
    "        print(f\"Group of prefixes of length < 3\")\n",
    "        group_CMs = [None for p in group]\n",
    "        CMsByPrefixIndex_torch.extend(group_CMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.110316Z",
     "start_time": "2019-07-27T03:29:06.106655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33059"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.118856Z",
     "start_time": "2019-07-27T03:29:06.112776Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.123736Z",
     "start_time": "2019-07-27T03:29:06.120216Z"
    }
   },
   "outputs": [],
   "source": [
    "# CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "#                      for s in source_seqs]\n",
    "# CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "# CMsByPrefixIndex[3].shape\n",
    "# CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(p)\n",
    "#                      for p in prefixes_t]\n",
    "# CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "# CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.139261Z",
     "start_time": "2019-07-27T03:29:06.125343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{w for w in Ws_t if len(ds2t(w)) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:06.154558Z",
     "start_time": "2019-07-27T03:29:06.140571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[9172]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "Ws_t_grouped, Ws_t_grouped_uniquekeys = persistent_groupby(Ws_t, \n",
    "                                                           keyfunc = lambda w: len(ds2t(w)) > 2, \n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.078918Z",
     "start_time": "2019-07-27T03:29:06.156121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 9172\n",
      "\t92 chunks of size 100:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "CMsByWordformIndex_torch = []\n",
    "for group, key in tqdm(zip(Ws_t_grouped, Ws_t_grouped_uniquekeys)):\n",
    "    if key:\n",
    "        print(f\"Group size: {len(group)}\")\n",
    "        wordforms_OHs_tr_t = [torch.tensor(dsToTriphoneOHs(w, X012OHmap)).t()\n",
    "                             for w in group]\n",
    "        chunk_size = 100\n",
    "        chunks = chunkList(chunk_size, wordforms_OHs_tr_t)\n",
    "        print(f\"\\t{len(chunks)} chunks of size {chunk_size}:\")\n",
    "        for chunk in chunks:\n",
    "            wordforms_OHs_tr_t_c = [each.cuda() \n",
    "                                    for each in chunk]\n",
    "            group_CMs_c = (sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in chunk)\n",
    "            group_CMs = [each.cpu() for each in chunk]\n",
    "            CMsByWordformIndex_torch.extend(group_CMs)\n",
    "    else:\n",
    "        print(f\"Group of wordforms of length < 3\")\n",
    "        group_CMs = [None for w in group]\n",
    "        CMsByWordformIndex_torch.extend(group_CMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.084168Z",
     "start_time": "2019-07-27T03:29:09.080417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByWordformIndex_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.102662Z",
     "start_time": "2019-07-27T03:29:09.086421Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.107679Z",
     "start_time": "2019-07-27T03:29:09.104839Z"
    }
   },
   "outputs": [],
   "source": [
    "# CMsByWordformIndex = [sourcePrefixToChannelMatrix_l(w)\n",
    "#                      for w in Ws_t]\n",
    "# CMsByWordformIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByWordformIndex[1:]]\n",
    "\n",
    "# CMsByWordformIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.113832Z",
     "start_time": "2019-07-27T03:29:09.109632Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsOfLength(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         return {w for w in Ws if len(ds2t(w)) == l}\n",
    "#     return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.129712Z",
     "start_time": "2019-07-27T03:29:09.115516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.d.ɛ.m.ə.n.s.t.ɹ.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.ʃ.ɪ.n.⋉',\n",
       " '⋊.d.ɹ.ɑ.m.ə.t.ə.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.dʒ.ʌ.s.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.f.l.ɛ.k.s.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.f.ə.l.æ.n.θ.ɹ.ə.p.ɪ.s.t.⋉',\n",
       " '⋊.g.l.oʊ.ɹ.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.g.ɹ.æ.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.j.u.n.j.ɪ.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.l.æ.s.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.l.ɑ.s.t.ɹ.ə.f.oʊ.b.i.ə.⋉',\n",
       " '⋊.k.w.ɑ.l.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.æ.p.ə.t.ə.l.ɪ.s.t.ɪ.k.⋉',\n",
       " '⋊.k.ɑ.m.p.l.ə.m.ɛ.n.t.ɚ.i.⋉',\n",
       " '⋊.k.ɑ.n.s.ɪ.n.t.ɹ.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.ɑ.n.t.ɹ.ə.b.j.u.ʃ.ɪ.n.⋉',\n",
       " '⋊.k.ə.n.s.t.ɪ.tʃ.u.ə.n.s.i.⋉',\n",
       " '⋊.k.ə.n.t.ɪ.n.j.u.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.æ.g.n.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.æ.n.ə.f.ɪ.s.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ə.n.ɪ.p.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ɛ.l.oʊ.d.ɹ.ə.m.æ.t.ɪ.k.⋉',\n",
       " '⋊.m.ɛ.t.ə.m.ɑ.ɹ.f.ə.s.ɪ.s.⋉',\n",
       " '⋊.m.ɪ.s.ɪ.n.f.ɚ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.m.ɪ.s.ʌ.n.d.ɚ.s.t.æ.n.d.⋉',\n",
       " '⋊.n.aɪ.t.ɹ.oʊ.g.l.ɪ.s.ɚ.ə.n.⋉',\n",
       " '⋊.p.eɪ.l.i.ɑ.n.t.ɑ.l.ə.dʒ.i.⋉',\n",
       " '⋊.p.j.ʊ.ɹ.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.p.ɹ.æ.k.t.ɪ.k.æ.l.ɪ.t.i.⋉',\n",
       " '⋊.p.ɹ.ɑ.d.ʌ.k.t.ɪ.v.ə.t.i.⋉',\n",
       " '⋊.p.ɹ.ə.k.ɹ.æ.s.t.ɪ.n.eɪ.t.⋉',\n",
       " '⋊.p.ɹ.ɪ.s.ɪ.p.ə.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.s.aɪ.k.oʊ.ə.n.æ.l.ɪ.s.ɪ.s.⋉',\n",
       " '⋊.s.k.ɪ.t.s.ə.f.ɹ.i.n.i.ə.⋉',\n",
       " '⋊.s.k.ɪ.t.s.ə.f.ɹ.ɛ.n.ɪ.k.⋉',\n",
       " '⋊.s.p.ɑ.n.t.eɪ.n.i.ɪ.s.l.i.⋉',\n",
       " '⋊.s.p.ɛ.s.ɪ.f.ɪ.k.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.s.p.ɪ.ɹ.ɪ.tʃ.u.ə.l.ɪ.z.m.⋉',\n",
       " '⋊.s.p.ɪ.ɹ.ɪ.tʃ.ʊ.æ.l.ə.t.i.⋉',\n",
       " '⋊.s.t.ɹ.æ.t.ɪ.s.f.ɪ.ɹ.ɪ.k.⋉',\n",
       " '⋊.s.ə.f.ɪ.s.t.ɪ.k.eɪ.t.ɪ.d.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.f.ɚ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.k.ɹ.ɪ.p.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.æ.n.s.p.ɚ.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.t.ɹ.ɪ.g.ɪ.n.ɑ.m.ə.t.ɹ.i.⋉',\n",
       " '⋊.t.ʊ.b.ɚ.k.j.ʊ.l.oʊ.s.ɪ.s.⋉',\n",
       " '⋊.v.ʌ.l.n.ɚ.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.æ.n.t.ɪ.h.ɪ.s.t.ə.m.i.n.⋉',\n",
       " '⋊.ɑ.ɹ.t.ɪ.k.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ə.d.m.ɪ.n.ɪ.s.t.ɹ.eɪ.t.ɚ.⋉',\n",
       " '⋊.ə.d.æ.p.t.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.ə.k.ʌ.m.p.ə.n.i.m.ə.n.t.⋉',\n",
       " '⋊.ə.p.ɹ.ɑ.k.s.ɪ.m.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɛ.k.s.ə.b.ɪ.ʃ.ɪ.n.ɪ.z.m.⋉',\n",
       " '⋊.ɪ.k.s.k.ɹ.u.ʃ.i.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.ɪ.k.s.p.l.æ.n.ɪ.t.oʊ.ɹ.i.⋉',\n",
       " '⋊.ɪ.k.s.t.ɹ.æ.v.ɪ.g.ɪ.n.t.⋉',\n",
       " '⋊.ɪ.m.p.ɑ.s.ə.b.ɪ.l.ɪ.t.i.⋉',\n",
       " '⋊.ɪ.m.p.ɹ.ɑ.v.ə.z.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.k.ɪ.n.v.i.n.j.ə.n.s.⋉',\n",
       " '⋊.ɪ.n.s.aɪ.k.l.ə.p.i.d.i.ə.⋉',\n",
       " '⋊.ɪ.n.s.t.ə.n.t.eɪ.n.i.ə.s.⋉',\n",
       " '⋊.ɪ.n.t.ɚ.p.ɹ.ə.t.eɪ.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.t.ɹ.ə.s.p.ɛ.k.ʃ.ɪ.n.⋉',\n",
       " '⋊.ɪ.n.ɑ.ɹ.t.ɪ.k.j.ʊ.l.ɪ.t.⋉',\n",
       " '⋊.ɹ.i.ɪ.n.k.ɑ.ɹ.n.eɪ.ʃ.ɪ.n.⋉'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16, Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.220151Z",
     "start_time": "2019-07-27T03:29:09.130837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{3: 5,\n",
       " 4: 173,\n",
       " 5: 1266,\n",
       " 6: 1790,\n",
       " 7: 1646,\n",
       " 8: 1328,\n",
       " 9: 1052,\n",
       " 10: 828,\n",
       " 11: 508,\n",
       " 12: 329,\n",
       " 13: 150,\n",
       " 14: 66,\n",
       " 15: 24,\n",
       " 16: 6,\n",
       " 18: 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.224374Z",
     "start_time": "2019-07-27T03:29:09.221555Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.229036Z",
     "start_time": "2019-07-27T03:29:09.225729Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         maxL = max(wordlengthsInclEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "#     else:\n",
    "#         maxL = max(wordlengthsNotIncludingEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.329305Z",
     "start_time": "2019-07-27T03:29:09.230082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 9172,\n",
       " 4: 9167,\n",
       " 5: 8994,\n",
       " 6: 7728,\n",
       " 7: 5938,\n",
       " 8: 4292,\n",
       " 9: 2964,\n",
       " 10: 1912,\n",
       " 11: 1084,\n",
       " 12: 576,\n",
       " 13: 247,\n",
       " 14: 97,\n",
       " 15: 31,\n",
       " 16: 7,\n",
       " 18: 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.336421Z",
     "start_time": "2019-07-27T03:29:09.330361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3, 4))\n",
    "torch.zeros((3,4))\n",
    "torch.zeros((3,4)) == torch.tensor(np.zeros((3, 4)), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.538797Z",
     "start_time": "2019-07-27T03:29:09.337701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ɹ.ɪ.l.ɪ.dʒ.ɪ.s.⋉'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(57798, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform\n",
    "len(ds2t(random_source_wordform))\n",
    "random_source_wordform_CM = sourcePrefixToChannelMatrix(random_source_wordform)\n",
    "random_source_wordform_CM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.558245Z",
     "start_time": "2019-07-27T03:29:09.540192Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif exact_length_only:\n",
    "        cm = np.zeros(shape=(len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        cm = sourcePrefixToChannelMatrix(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == cm.shape[1], f\"{cm.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        cm = np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)\n",
    "        assert key_length - 2 == cm.shape[1], f\"{cm.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    \n",
    "\n",
    "# def makeCMbyWordformAndLength_t_c\n",
    "\n",
    "def dsToOHstack_tr_t(ds):\n",
    "    '''\n",
    "    Takes a dotted string (representing a source wordform or prefix),\n",
    "    converts it to a stack of one hot (numpy) vectors, \n",
    "    takes the transpose,\n",
    "    and returns a pytorch tensor version of that transpose.\n",
    "    \n",
    "    This is an intermediate step of processing for the function below.\n",
    "    '''\n",
    "    ds_OHs = dsToTriphoneOHs(ds, X012OHmap)\n",
    "    ds_OHs_tr_t = torch.tensor(ds_OHs.T)\n",
    "    return ds_OHs_tr_t\n",
    "\n",
    "# correct but slow translation of the original function to use pytorch and cuda \n",
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength_t_c(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    \n",
    "    if x0f_length == key_length:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "        return w_CM\n",
    "#         return sourcePrefixStackToChannelMatrix(x0f)\n",
    "    elif exact_length_only:\n",
    "        cm = torch.zeros((len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0k)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "#         cm = sourcePrefixStackToChannelMatrix_t_c(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == w_CM.shape[1], f\"{w_CM.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM\n",
    "    else:\n",
    "        #grab the source \n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        w_OHs_tr_c = w_OHs_tr_t.cuda()\n",
    "        w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "        w_CM = w_CM_c.cpu()\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        w_CM_padded = torch.nn.functional.pad(w_CM,\n",
    "                                              (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "                                              'constant',\n",
    "                                              value=0.0)\n",
    "        assert key_length - 2 == w_CM_padded.shape[1], f\"{w_CM_padded.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM_padded\n",
    "    \n",
    "def makeCMbyWordformAndLength_t_c_pre(wordform, key_length, exact_length_only = False):\n",
    "    '''\n",
    "    Takes in a wordform and parameters and returns \n",
    "    a preprocessed stack of OHs as a torch tensor, \n",
    "    ready to be loaded onto a cuda device for the main \n",
    "    computation that yields a channel matrix for the \n",
    "    source sequence (wordform).\n",
    "    \n",
    "    Preprocessing steps include trimming wordforms (or rather\n",
    "    their OH stacks) that are too long relative to the key \n",
    "    length and returning a stack of all zeros (representing\n",
    "    the channel matrix) when exact_length_only is True and\n",
    "    |wordform| != key_length. (The resulting stack is left\n",
    "    untouched by subsequent steps.)\n",
    "    '''\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    \n",
    "    if x0f_length == key_length:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        return w_OHs_tr_t\n",
    "    elif exact_length_only:\n",
    "        cm = torch.zeros((len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0k)\n",
    "        return w_OHs_tr_t\n",
    "    else:\n",
    "        w_OHs_tr_t = dsToOHstack_tr_t(x0f)\n",
    "        return w_OHs_tr_t\n",
    "\n",
    "def makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result, key_length, exact_length_only = False):\n",
    "    '''\n",
    "    Takes in a processed stack of OHs (a torch tensor loaded on a cuda device), \n",
    "    and turns it into a tensor representing the corresponding channel matrix (still on the cuda device).\n",
    "    '''\n",
    "    x0f_length = loaded_prep_result.shape[1] + 2\n",
    "    if exact_length_only and x0f_length != key_length:\n",
    "#     if exact_length_only and torch.sum(loaded_prep_result).item() == 0.0:\n",
    "        return loaded_prep_result\n",
    "    w_OHs_tr_c = loaded_prep_result\n",
    "    w_CM_c = sourcePrefixStackToChannelMatrix_t_c(w_OHs_tr_c)\n",
    "    return w_CM_c\n",
    "\n",
    "def makeCMbyWordformAndLength_t_c_post(w_CM, key_length, exact_length_only = False, asType='torch'):\n",
    "    '''\n",
    "    Takes in a wordform channel matrix (a torch tensor or np.ndarray on the CPU), \n",
    "    and performs any necessary post-processing. \n",
    "    \n",
    "    The only condition under which post-processing will occur is if the\n",
    "    channel matrix needs to be padded with zero vectors to represent the\n",
    "    lack of insertion errors and the fact that a source sequence with l\n",
    "    consecutive triphones will never generate a channel sequence of anything\n",
    "    other than exactly l channel triphones.\n",
    "    '''\n",
    "    x0f_length = w_CM.shape[1] + 2\n",
    "    if exact_length_only and x0f_length != key_length:\n",
    "#     if exact_length_only and torch.sum(w_CM).item() == 0.0:\n",
    "        return w_CM\n",
    "    if x0f_length < key_length:\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        if asType == 'torch':\n",
    "            w_CM_padded = torch.nn.functional.pad(w_CM,\n",
    "                                                  (0, goal_l - w_CM.shape[1] - 2, 0, 0),\n",
    "                                                  'constant',\n",
    "                                                  value=0.0)\n",
    "        if asType == 'ndarray':\n",
    "            w_CM_padded = np.pad(w_CM, ((0,0), (0, goal_l - w_CM.shape[1] - 2)), \n",
    "                                 'constant', constant_values=0.0)\n",
    "#         assert key_length - 2 == w_CM_padded.shape[1], f\"{w_CM_padded.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return w_CM_padded\n",
    "    \n",
    "#     assert key_length - 2 == w_CM.shape[1], f\"{w_CM.shape[1]} != {key_length - 2}\\n\\t key_length = {key_length}\"\n",
    "    return w_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.566407Z",
     "start_time": "2019-07-27T03:29:09.559663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,3))\n",
    "np.pad(np.zeros((2,3)), ((0,0), (0, 6 - np.zeros((2,3)).shape[1] - 2)),\n",
    "       'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.574837Z",
     "start_time": "2019-07-27T03:29:09.567492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3))\n",
    "#note the reverse ordering effect of the second argument relative to np.pad!\n",
    "torch.nn.functional.pad(torch.zeros((2,3)), (0, 0, 0, 6 - torch.zeros((2,3)).shape[1] - 2), 'constant', value=0.0)\n",
    "torch.nn.functional.pad(torch.zeros((2,3)), (0, 6 - torch.zeros((2,3)).shape[1] - 2, 0, 0), 'constant', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.577957Z",
     "start_time": "2019-07-27T03:29:09.576129Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     # returns p(Y0K|x0f)\n",
    "#     def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "#         x0f = wordform\n",
    "#         x0f_t = ds2t(x0f)\n",
    "#         x0f_length = len(x0f_t)\n",
    "#         if x0f_length == key_length:\n",
    "#             return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "#         elif x0f_length > key_length:\n",
    "#     #         print('middle case')\n",
    "#             #trim the wordform to be a prefix of length = key_length\n",
    "#             x0k_t = x0f_t[:key_length]\n",
    "#             x0k = t2ds(x0k_t)\n",
    "#     #         print('x0k: {0}'.format(x0k))\n",
    "#             return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "#         else:\n",
    "#             #grab the source \n",
    "#             my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "#             goal_l = key_length\n",
    "#             return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "#                           'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.583731Z",
     "start_time": "2019-07-27T03:29:09.578998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.588754Z",
     "start_time": "2019-07-27T03:29:09.584843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word lengths:\n",
      "{17}\n"
     ]
    }
   ],
   "source": [
    "if sorted(list(wordlengthsInclEdges)) != sorted(list(range(min(wordlengthsInclEdges), \\\n",
    "                                                           max(wordlengthsInclEdges)+1))):\n",
    "    print(\"Missing word lengths:\")\n",
    "    print({l for l in range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1) if l not in wordlengthsInclEdges})\n",
    "    wordlengthsInclEdges_range = list(range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1))\n",
    "else:\n",
    "    wordlengthsInclEdges_range = sorted(list(wordlengthsInclEdges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.592943Z",
     "start_time": "2019-07-27T03:29:09.589799Z"
    }
   },
   "outputs": [],
   "source": [
    "# ?tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.597781Z",
     "start_time": "2019-07-27T03:29:09.593938Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # demonstrates that makeChannelMatrixByWordformAndLength_t_c will run without errors, but slow\n",
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "# cmsByLengthByWordformIndex_torch = offset + [torch.tensor([makeChannelMatrixByWordformAndLength_t_c(w, l)\n",
    "#                                                            for w in tqdm(Ws_t, \n",
    "#                                                                          total=len(Ws_t))])\n",
    "#                                              for l in tqdm(wordlengthsInclEdges_range, \n",
    "#                                                            total=len(wordlengthsInclEdges_range))]\n",
    "# # cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.606662Z",
     "start_time": "2019-07-27T03:29:09.598834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.aɪ.d.i.l.aɪ.z.⋉'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7381, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7381, 6])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = Ws_t[0]; w0\n",
    "len(ds2t(w0))\n",
    "w0_pre = makeCMbyWordformAndLength_t_c_pre(wordform=w0, key_length=3, exact_length_only = False)\n",
    "w0_pre.shape\n",
    "dsToOHstack_tr_t(w0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-27T03:29:09.609670Z",
     "start_time": "2019-07-27T03:29:09.607751Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote, startNote, endNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-27T00:36:08.357Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 17:36:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Ws_OHs_tr_t = 0.541588256GB\n",
      "Loading OH stacks onto cuda device... @ 17:36:08\n",
      "Performing cuda op... @ 17:36:08\n",
      "Loading back onto the cpu... @ 17:36:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/92 [00:20<30:56, 20.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:36:29\n",
      "Clearing GPU cache... @ 17:36:29\n",
      "Loading OH stacks onto cuda device... @ 17:36:29\n",
      "Performing cuda op... @ 17:36:29\n",
      "Loading back onto the cpu... @ 17:36:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/92 [00:40<30:39, 20.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:36:49\n",
      "Clearing GPU cache... @ 17:36:49\n",
      "Loading OH stacks onto cuda device... @ 17:36:49\n",
      "Performing cuda op... @ 17:36:49\n",
      "Loading back onto the cpu... @ 17:36:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/92 [01:01<30:21, 20.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:10\n",
      "Clearing GPU cache... @ 17:37:10\n",
      "Loading OH stacks onto cuda device... @ 17:37:10\n",
      "Performing cuda op... @ 17:37:10\n",
      "Loading back onto the cpu... @ 17:37:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/92 [01:21<30:02, 20.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:30\n",
      "Clearing GPU cache... @ 17:37:30\n",
      "Loading OH stacks onto cuda device... @ 17:37:30\n",
      "Performing cuda op... @ 17:37:30\n",
      "Loading back onto the cpu... @ 17:37:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 5/92 [01:42<29:43, 20.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:37:51\n",
      "Clearing GPU cache... @ 17:37:51\n",
      "Loading OH stacks onto cuda device... @ 17:37:51\n",
      "Performing cuda op... @ 17:37:51\n",
      "Loading back onto the cpu... @ 17:37:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 6/92 [02:03<29:23, 20.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:11\n",
      "Clearing GPU cache... @ 17:38:11\n",
      "Loading OH stacks onto cuda device... @ 17:38:11\n",
      "Performing cuda op... @ 17:38:11\n",
      "Loading back onto the cpu... @ 17:38:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 7/92 [02:23<29:07, 20.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:32\n",
      "Clearing GPU cache... @ 17:38:32\n",
      "Loading OH stacks onto cuda device... @ 17:38:32\n",
      "Performing cuda op... @ 17:38:32\n",
      "Loading back onto the cpu... @ 17:38:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 8/92 [02:44<28:50, 20.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:38:53\n",
      "Clearing GPU cache... @ 17:38:53\n",
      "Loading OH stacks onto cuda device... @ 17:38:53\n",
      "Performing cuda op... @ 17:38:53\n",
      "Loading back onto the cpu... @ 17:38:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 9/92 [03:05<28:32, 20.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:13\n",
      "Clearing GPU cache... @ 17:39:13\n",
      "Loading OH stacks onto cuda device... @ 17:39:13\n",
      "Performing cuda op... @ 17:39:13\n",
      "Loading back onto the cpu... @ 17:39:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 10/92 [03:25<28:13, 20.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:34\n",
      "Clearing GPU cache... @ 17:39:34\n",
      "Loading OH stacks onto cuda device... @ 17:39:34\n",
      "Performing cuda op... @ 17:39:34\n",
      "Loading back onto the cpu... @ 17:39:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 11/92 [03:46<27:53, 20.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:39:55\n",
      "Clearing GPU cache... @ 17:39:55\n",
      "Loading OH stacks onto cuda device... @ 17:39:55\n",
      "Performing cuda op... @ 17:39:55\n",
      "Loading back onto the cpu... @ 17:39:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 12/92 [04:07<27:34, 20.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:16\n",
      "Clearing GPU cache... @ 17:40:16\n",
      "Loading OH stacks onto cuda device... @ 17:40:16\n",
      "Performing cuda op... @ 17:40:16\n",
      "Loading back onto the cpu... @ 17:40:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 13/92 [04:27<27:13, 20.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:36\n",
      "Clearing GPU cache... @ 17:40:36\n",
      "Loading OH stacks onto cuda device... @ 17:40:36\n",
      "Performing cuda op... @ 17:40:36\n",
      "Loading back onto the cpu... @ 17:40:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 14/92 [04:48<26:53, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:40:57\n",
      "Clearing GPU cache... @ 17:40:57\n",
      "Loading OH stacks onto cuda device... @ 17:40:57\n",
      "Performing cuda op... @ 17:40:57\n",
      "Loading back onto the cpu... @ 17:40:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 15/92 [05:09<26:33, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:18\n",
      "Clearing GPU cache... @ 17:41:18\n",
      "Loading OH stacks onto cuda device... @ 17:41:18\n",
      "Performing cuda op... @ 17:41:18\n",
      "Loading back onto the cpu... @ 17:41:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 16/92 [05:30<26:12, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:38\n",
      "Clearing GPU cache... @ 17:41:38\n",
      "Loading OH stacks onto cuda device... @ 17:41:38\n",
      "Performing cuda op... @ 17:41:38\n",
      "Loading back onto the cpu... @ 17:41:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 17/92 [05:50<25:52, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:41:59\n",
      "Clearing GPU cache... @ 17:41:59\n",
      "Loading OH stacks onto cuda device... @ 17:41:59\n",
      "Performing cuda op... @ 17:41:59\n",
      "Loading back onto the cpu... @ 17:41:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|█▉        | 18/92 [06:11<25:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:42:20\n",
      "Clearing GPU cache... @ 17:42:20\n",
      "Loading OH stacks onto cuda device... @ 17:42:20\n",
      "Performing cuda op... @ 17:42:20\n",
      "Loading back onto the cpu... @ 17:42:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 19/92 [06:32<25:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:42:40\n",
      "Clearing GPU cache... @ 17:42:40\n",
      "Loading OH stacks onto cuda device... @ 17:42:40\n",
      "Performing cuda op... @ 17:42:40\n",
      "Loading back onto the cpu... @ 17:42:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 20/92 [06:52<24:50, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:01\n",
      "Clearing GPU cache... @ 17:43:01\n",
      "Loading OH stacks onto cuda device... @ 17:43:01\n",
      "Performing cuda op... @ 17:43:01\n",
      "Loading back onto the cpu... @ 17:43:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 21/92 [07:13<24:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:22\n",
      "Clearing GPU cache... @ 17:43:22\n",
      "Loading OH stacks onto cuda device... @ 17:43:22\n",
      "Performing cuda op... @ 17:43:22\n",
      "Loading back onto the cpu... @ 17:43:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 22/92 [07:34<24:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:43:42\n",
      "Clearing GPU cache... @ 17:43:42\n",
      "Loading OH stacks onto cuda device... @ 17:43:43\n",
      "Performing cuda op... @ 17:43:43\n",
      "Loading back onto the cpu... @ 17:43:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 23/92 [07:54<23:48, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:03\n",
      "Clearing GPU cache... @ 17:44:03\n",
      "Loading OH stacks onto cuda device... @ 17:44:03\n",
      "Performing cuda op... @ 17:44:03\n",
      "Loading back onto the cpu... @ 17:44:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 24/92 [08:15<23:27, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:24\n",
      "Clearing GPU cache... @ 17:44:24\n",
      "Loading OH stacks onto cuda device... @ 17:44:24\n",
      "Performing cuda op... @ 17:44:24\n",
      "Loading back onto the cpu... @ 17:44:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 25/92 [08:36<23:06, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:44:45\n",
      "Clearing GPU cache... @ 17:44:45\n",
      "Loading OH stacks onto cuda device... @ 17:44:45\n",
      "Performing cuda op... @ 17:44:45\n",
      "Loading back onto the cpu... @ 17:44:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 26/92 [08:56<22:45, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:05\n",
      "Clearing GPU cache... @ 17:45:05\n",
      "Loading OH stacks onto cuda device... @ 17:45:05\n",
      "Performing cuda op... @ 17:45:05\n",
      "Loading back onto the cpu... @ 17:45:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 27/92 [09:17<22:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:26\n",
      "Clearing GPU cache... @ 17:45:26\n",
      "Loading OH stacks onto cuda device... @ 17:45:26\n",
      "Performing cuda op... @ 17:45:26\n",
      "Loading back onto the cpu... @ 17:45:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 28/92 [09:38<22:04, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:45:47\n",
      "Clearing GPU cache... @ 17:45:47\n",
      "Loading OH stacks onto cuda device... @ 17:45:47\n",
      "Performing cuda op... @ 17:45:47\n",
      "Loading back onto the cpu... @ 17:45:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 29/92 [09:59<21:43, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:07\n",
      "Clearing GPU cache... @ 17:46:07\n",
      "Loading OH stacks onto cuda device... @ 17:46:07\n",
      "Performing cuda op... @ 17:46:07\n",
      "Loading back onto the cpu... @ 17:46:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 30/92 [10:19<21:23, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:28\n",
      "Clearing GPU cache... @ 17:46:28\n",
      "Loading OH stacks onto cuda device... @ 17:46:28\n",
      "Performing cuda op... @ 17:46:28\n",
      "Loading back onto the cpu... @ 17:46:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 31/92 [10:40<21:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:46:49\n",
      "Clearing GPU cache... @ 17:46:49\n",
      "Loading OH stacks onto cuda device... @ 17:46:49\n",
      "Performing cuda op... @ 17:46:49\n",
      "Loading back onto the cpu... @ 17:46:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 32/92 [11:01<20:41, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:09\n",
      "Clearing GPU cache... @ 17:47:09\n",
      "Loading OH stacks onto cuda device... @ 17:47:09\n",
      "Performing cuda op... @ 17:47:09\n",
      "Loading back onto the cpu... @ 17:47:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 33/92 [11:21<20:21, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:30\n",
      "Clearing GPU cache... @ 17:47:30\n",
      "Loading OH stacks onto cuda device... @ 17:47:30\n",
      "Performing cuda op... @ 17:47:30\n",
      "Loading back onto the cpu... @ 17:47:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 34/92 [11:42<20:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:47:51\n",
      "Clearing GPU cache... @ 17:47:51\n",
      "Loading OH stacks onto cuda device... @ 17:47:51\n",
      "Performing cuda op... @ 17:47:51\n",
      "Loading back onto the cpu... @ 17:47:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 35/92 [12:03<19:39, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:12\n",
      "Clearing GPU cache... @ 17:48:12\n",
      "Loading OH stacks onto cuda device... @ 17:48:12\n",
      "Performing cuda op... @ 17:48:12\n",
      "Loading back onto the cpu... @ 17:48:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 36/92 [12:23<19:18, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:32\n",
      "Clearing GPU cache... @ 17:48:32\n",
      "Loading OH stacks onto cuda device... @ 17:48:32\n",
      "Performing cuda op... @ 17:48:32\n",
      "Loading back onto the cpu... @ 17:48:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 37/92 [12:44<18:58, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:48:53\n",
      "Clearing GPU cache... @ 17:48:53\n",
      "Loading OH stacks onto cuda device... @ 17:48:53\n",
      "Performing cuda op... @ 17:48:53\n",
      "Loading back onto the cpu... @ 17:48:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 38/92 [13:05<18:37, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:14\n",
      "Clearing GPU cache... @ 17:49:14\n",
      "Loading OH stacks onto cuda device... @ 17:49:14\n",
      "Performing cuda op... @ 17:49:14\n",
      "Loading back onto the cpu... @ 17:49:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 39/92 [13:26<18:16, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:34\n",
      "Clearing GPU cache... @ 17:49:34\n",
      "Loading OH stacks onto cuda device... @ 17:49:34\n",
      "Performing cuda op... @ 17:49:34\n",
      "Loading back onto the cpu... @ 17:49:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 40/92 [13:46<17:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:49:55\n",
      "Clearing GPU cache... @ 17:49:55\n",
      "Loading OH stacks onto cuda device... @ 17:49:55\n",
      "Performing cuda op... @ 17:49:55\n",
      "Loading back onto the cpu... @ 17:49:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▍     | 41/92 [14:07<17:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:16\n",
      "Clearing GPU cache... @ 17:50:16\n",
      "Loading OH stacks onto cuda device... @ 17:50:16\n",
      "Performing cuda op... @ 17:50:16\n",
      "Loading back onto the cpu... @ 17:50:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 42/92 [14:28<17:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:36\n",
      "Clearing GPU cache... @ 17:50:36\n",
      "Loading OH stacks onto cuda device... @ 17:50:36\n",
      "Performing cuda op... @ 17:50:36\n",
      "Loading back onto the cpu... @ 17:50:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 43/92 [14:48<16:54, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:50:57\n",
      "Clearing GPU cache... @ 17:50:57\n",
      "Loading OH stacks onto cuda device... @ 17:50:57\n",
      "Performing cuda op... @ 17:50:57\n",
      "Loading back onto the cpu... @ 17:50:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 44/92 [15:09<16:33, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:18\n",
      "Clearing GPU cache... @ 17:51:18\n",
      "Loading OH stacks onto cuda device... @ 17:51:18\n",
      "Performing cuda op... @ 17:51:18\n",
      "Loading back onto the cpu... @ 17:51:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 45/92 [15:30<16:12, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:39\n",
      "Clearing GPU cache... @ 17:51:39\n",
      "Loading OH stacks onto cuda device... @ 17:51:39\n",
      "Performing cuda op... @ 17:51:39\n",
      "Loading back onto the cpu... @ 17:51:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 46/92 [15:50<15:52, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:51:59\n",
      "Clearing GPU cache... @ 17:51:59\n",
      "Loading OH stacks onto cuda device... @ 17:51:59\n",
      "Performing cuda op... @ 17:51:59\n",
      "Loading back onto the cpu... @ 17:51:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 47/92 [16:11<15:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:52:20\n",
      "Clearing GPU cache... @ 17:52:20\n",
      "Loading OH stacks onto cuda device... @ 17:52:20\n",
      "Performing cuda op... @ 17:52:20\n",
      "Loading back onto the cpu... @ 17:52:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 48/92 [16:32<15:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:52:41\n",
      "Clearing GPU cache... @ 17:52:41\n",
      "Loading OH stacks onto cuda device... @ 17:52:41\n",
      "Performing cuda op... @ 17:52:41\n",
      "Loading back onto the cpu... @ 17:52:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 49/92 [16:53<14:49, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:01\n",
      "Clearing GPU cache... @ 17:53:01\n",
      "Loading OH stacks onto cuda device... @ 17:53:01\n",
      "Performing cuda op... @ 17:53:01\n",
      "Loading back onto the cpu... @ 17:53:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 50/92 [17:13<14:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:22\n",
      "Clearing GPU cache... @ 17:53:22\n",
      "Loading OH stacks onto cuda device... @ 17:53:22\n",
      "Performing cuda op... @ 17:53:22\n",
      "Loading back onto the cpu... @ 17:53:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 51/92 [17:34<14:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:53:43\n",
      "Clearing GPU cache... @ 17:53:43\n",
      "Loading OH stacks onto cuda device... @ 17:53:43\n",
      "Performing cuda op... @ 17:53:43\n",
      "Loading back onto the cpu... @ 17:53:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 52/92 [17:55<13:47, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:03\n",
      "Clearing GPU cache... @ 17:54:03\n",
      "Loading OH stacks onto cuda device... @ 17:54:03\n",
      "Performing cuda op... @ 17:54:03\n",
      "Loading back onto the cpu... @ 17:54:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 53/92 [18:15<13:27, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:24\n",
      "Clearing GPU cache... @ 17:54:24\n",
      "Loading OH stacks onto cuda device... @ 17:54:24\n",
      "Performing cuda op... @ 17:54:24\n",
      "Loading back onto the cpu... @ 17:54:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|█████▊    | 54/92 [18:36<13:06, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding unpostprocessed chunk to chunk list @ 17:54:45\n",
      "Clearing GPU cache... @ 17:54:45\n",
      "Loading OH stacks onto cuda device... @ 17:54:45\n",
      "Performing cuda op... @ 17:54:45\n",
      "Loading back onto the cpu... @ 17:54:45\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset\n",
    "awaiting_postProcessingByLength = []\n",
    "\n",
    "for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "    startNote(f\"l = {l}\")\n",
    "    \n",
    "    Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "                                                     key_length=l,\n",
    "                                                     exact_length_only = False)\n",
    "                   for w in Ws_t]\n",
    "    print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "    \n",
    "#     constant_length_chunks = []\n",
    "    constant_length_chunks_before_post_processing = []\n",
    "    chunk_size = 1000\n",
    "    chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "    for chunk in tqdm(chunks, total=len(chunks)):\n",
    "        stampedNote(\"Loading OH stacks onto cuda device...\")\n",
    "        wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "        \n",
    "        stampedNote(\"Performing cuda op...\")\n",
    "        wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "                                                                  key_length=l, \n",
    "                                                                  exact_length_only = False)\n",
    "                           for each in wordforms_OHs_tr_t_c]\n",
    "#         wordforms_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "        \n",
    "        stampedNote(\"Loading back onto the cpu...\")\n",
    "        wordforms_CMs_cpu = [each.cpu() for each in wordforms_CMs_c]\n",
    "        \n",
    "        stampedNote(\"Adding unpostprocessed chunk to chunk list...\")\n",
    "        constant_length_chunks_before_post_processing.append(wordforms_CMs_cpu)\n",
    "        \n",
    "        stampedNote(\"Clearing GPU cache...\")\n",
    "        torch.cuda.empty_cache()\n",
    "#         stampedNote(\"Performing post-processing...\")\n",
    "# #         wordforms_CMs_n = np.array(list(par(delayed(makeCMbyWordformAndLength_t_c_post)(w_CM=each.numpy(), \n",
    "# #                                                                                           key_length=l, \n",
    "# #                                                                                           exact_length_only = False,\n",
    "# #                                                                                           asType = 'ndarray')\n",
    "# #                                        for each in wordforms_CMs_cpu)))\n",
    "# #         wordforms_CMs_t = torch.tensor(wordforms_CMs_n)\n",
    "#         wordforms_CMs_t = torch.stack([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "#                                                                           key_length=l, \n",
    "#                                                                           exact_length_only = False)\n",
    "#                                        for each in wordforms_CMs_cpu])\n",
    "#         stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "#         constant_length_chunks.append(wordforms_CMs_t)\n",
    "\n",
    "    stampedNote('Consolidating unpostprocessed chunks into a single list for this length.')\n",
    "    unpostprocessed_length_block = [cm for chunk in constant_length_chunks_before_post_processing for cm in chunk]\n",
    "#     stampedNote('Consolidating chunks into a constant-length block (tensor).')\n",
    "#     constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "    \n",
    "#     stampedNote(\"Clearing GPU cache...\")\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "    stampedNote('Adding block to list of blocks.\\n')\n",
    "    endNote(f\"l = {l}\")\n",
    "    awaiting_postProcessingByLength.append(unpostprocessed_length_block)\n",
    "#     cmsByLengthByWordformIndex_torch.append(constant_length_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do all the post-processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:45:52.434435Z",
     "start_time": "2019-07-26T23:14:05.373374Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start l = 3 @ 16:14:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Ws_OHs_tr_t = 0.541588256GB\n",
      "Loading OH stacks onto cuda device... @ 16:14:05\n",
      "Performing cuda op... @ 16:14:10\n",
      "Loading back onto the cpu... @ 16:14:10\n",
      "Clearing GPU cache... @ 16:14:10\n",
      "Performing post-processing... @ 16:14:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/92 [00:25<38:40, 25.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:14:31\n",
      "Loading OH stacks onto cuda device... @ 16:14:31\n",
      "Performing cuda op... @ 16:14:31\n",
      "Loading back onto the cpu... @ 16:14:31\n",
      "Clearing GPU cache... @ 16:14:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/92 [00:46<36:01, 24.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:14:51\n",
      "Adding processed chunk to chunk list. @ 16:14:51\n",
      "Loading OH stacks onto cuda device... @ 16:14:51\n",
      "Performing cuda op... @ 16:14:51\n",
      "Loading back onto the cpu... @ 16:14:51\n",
      "Clearing GPU cache... @ 16:14:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/92 [01:06<34:09, 23.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:15:12\n",
      "Adding processed chunk to chunk list. @ 16:15:12\n",
      "Loading OH stacks onto cuda device... @ 16:15:12\n",
      "Performing cuda op... @ 16:15:12\n",
      "Loading back onto the cpu... @ 16:15:12\n",
      "Clearing GPU cache... @ 16:15:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/92 [01:27<32:44, 22.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:15:33\n",
      "Adding processed chunk to chunk list. @ 16:15:33\n",
      "Loading OH stacks onto cuda device... @ 16:15:33\n",
      "Performing cuda op... @ 16:15:33\n",
      "Loading back onto the cpu... @ 16:15:33\n",
      "Clearing GPU cache... @ 16:15:33\n",
      "Performing post-processing... @ 16:15:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 5/92 [01:48<31:40, 21.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:15:53\n",
      "Loading OH stacks onto cuda device... @ 16:15:53\n",
      "Performing cuda op... @ 16:15:53\n",
      "Loading back onto the cpu... @ 16:15:53\n",
      "Clearing GPU cache... @ 16:15:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 6/92 [02:08<30:49, 21.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:16:14\n",
      "Adding processed chunk to chunk list. @ 16:16:14\n",
      "Loading OH stacks onto cuda device... @ 16:16:14\n",
      "Performing cuda op... @ 16:16:14\n",
      "Loading back onto the cpu... @ 16:16:14\n",
      "Clearing GPU cache... @ 16:16:14\n",
      "Performing post-processing... @ 16:16:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 7/92 [02:29<30:07, 21.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:16:35\n",
      "Loading OH stacks onto cuda device... @ 16:16:35\n",
      "Performing cuda op... @ 16:16:35\n",
      "Loading back onto the cpu... @ 16:16:35\n",
      "Clearing GPU cache... @ 16:16:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 8/92 [02:50<29:31, 21.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:16:55\n",
      "Adding processed chunk to chunk list. @ 16:16:55\n",
      "Loading OH stacks onto cuda device... @ 16:16:55\n",
      "Performing cuda op... @ 16:16:55\n",
      "Loading back onto the cpu... @ 16:16:55\n",
      "Clearing GPU cache... @ 16:16:55\n",
      "Performing post-processing... @ 16:16:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|▉         | 9/92 [03:10<29:00, 20.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:17:16\n",
      "Loading OH stacks onto cuda device... @ 16:17:16\n",
      "Performing cuda op... @ 16:17:16\n",
      "Loading back onto the cpu... @ 16:17:16\n",
      "Clearing GPU cache... @ 16:17:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 10/92 [03:31<28:33, 20.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:17:37\n",
      "Adding processed chunk to chunk list. @ 16:17:37\n",
      "Loading OH stacks onto cuda device... @ 16:17:37\n",
      "Performing cuda op... @ 16:17:37\n",
      "Loading back onto the cpu... @ 16:17:37\n",
      "Clearing GPU cache... @ 16:17:37\n",
      "Performing post-processing... @ 16:17:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 11/92 [03:52<28:07, 20.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:17:58\n",
      "Loading OH stacks onto cuda device... @ 16:17:58\n",
      "Performing cuda op... @ 16:17:58\n",
      "Loading back onto the cpu... @ 16:17:58\n",
      "Clearing GPU cache... @ 16:17:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 12/92 [04:13<27:43, 20.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:18:18\n",
      "Adding processed chunk to chunk list. @ 16:18:18\n",
      "Loading OH stacks onto cuda device... @ 16:18:18\n",
      "Performing cuda op... @ 16:18:18\n",
      "Loading back onto the cpu... @ 16:18:18\n",
      "Clearing GPU cache... @ 16:18:18\n",
      "Performing post-processing... @ 16:18:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 13/92 [04:33<27:20, 20.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:18:39\n",
      "Loading OH stacks onto cuda device... @ 16:18:39\n",
      "Performing cuda op... @ 16:18:39\n",
      "Loading back onto the cpu... @ 16:18:39\n",
      "Clearing GPU cache... @ 16:18:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 14/92 [04:54<26:58, 20.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:19:00\n",
      "Adding processed chunk to chunk list. @ 16:19:00\n",
      "Loading OH stacks onto cuda device... @ 16:19:00\n",
      "Performing cuda op... @ 16:19:00\n",
      "Loading back onto the cpu... @ 16:19:00\n",
      "Clearing GPU cache... @ 16:19:00\n",
      "Performing post-processing... @ 16:19:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 15/92 [05:15<26:36, 20.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:19:20\n",
      "Loading OH stacks onto cuda device... @ 16:19:20\n",
      "Performing cuda op... @ 16:19:20\n",
      "Loading back onto the cpu... @ 16:19:20\n",
      "Clearing GPU cache... @ 16:19:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 16/92 [05:35<26:15, 20.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:19:41\n",
      "Adding processed chunk to chunk list. @ 16:19:41\n",
      "Loading OH stacks onto cuda device... @ 16:19:41\n",
      "Performing cuda op... @ 16:19:41\n",
      "Loading back onto the cpu... @ 16:19:41\n",
      "Clearing GPU cache... @ 16:19:41\n",
      "Performing post-processing... @ 16:19:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 17/92 [05:56<25:53, 20.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:20:02\n",
      "Loading OH stacks onto cuda device... @ 16:20:02\n",
      "Performing cuda op... @ 16:20:02\n",
      "Loading back onto the cpu... @ 16:20:02\n",
      "Clearing GPU cache... @ 16:20:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|█▉        | 18/92 [06:17<25:32, 20.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:20:22\n",
      "Adding processed chunk to chunk list. @ 16:20:22\n",
      "Loading OH stacks onto cuda device... @ 16:20:22\n",
      "Performing cuda op... @ 16:20:22\n",
      "Loading back onto the cpu... @ 16:20:22\n",
      "Clearing GPU cache... @ 16:20:22\n",
      "Performing post-processing... @ 16:20:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 19/92 [06:38<25:11, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:20:43\n",
      "Loading OH stacks onto cuda device... @ 16:20:43\n",
      "Performing cuda op... @ 16:20:43\n",
      "Loading back onto the cpu... @ 16:20:43\n",
      "Clearing GPU cache... @ 16:20:43\n",
      "Performing post-processing... @ 16:21:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 20/92 [06:58<24:51, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:21:04\n",
      "Loading OH stacks onto cuda device... @ 16:21:04\n",
      "Performing cuda op... @ 16:21:04\n",
      "Loading back onto the cpu... @ 16:21:04\n",
      "Clearing GPU cache... @ 16:21:04\n",
      "Performing post-processing... @ 16:21:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 21/92 [07:19<24:30, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:21:25\n",
      "Loading OH stacks onto cuda device... @ 16:21:25\n",
      "Performing cuda op... @ 16:21:25\n",
      "Loading back onto the cpu... @ 16:21:25\n",
      "Clearing GPU cache... @ 16:21:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 22/92 [07:40<24:09, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:21:45\n",
      "Adding processed chunk to chunk list. @ 16:21:45\n",
      "Loading OH stacks onto cuda device... @ 16:21:45\n",
      "Performing cuda op... @ 16:21:45\n",
      "Loading back onto the cpu... @ 16:21:45\n",
      "Clearing GPU cache... @ 16:21:45\n",
      "Performing post-processing... @ 16:21:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 23/92 [08:00<23:48, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:22:06\n",
      "Loading OH stacks onto cuda device... @ 16:22:06\n",
      "Performing cuda op... @ 16:22:06\n",
      "Loading back onto the cpu... @ 16:22:06\n",
      "Clearing GPU cache... @ 16:22:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 24/92 [08:21<23:28, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:22:27\n",
      "Adding processed chunk to chunk list. @ 16:22:27\n",
      "Loading OH stacks onto cuda device... @ 16:22:27\n",
      "Performing cuda op... @ 16:22:27\n",
      "Loading back onto the cpu... @ 16:22:27\n",
      "Clearing GPU cache... @ 16:22:27\n",
      "Performing post-processing... @ 16:22:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 25/92 [08:42<23:07, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:22:47\n",
      "Loading OH stacks onto cuda device... @ 16:22:47\n",
      "Performing cuda op... @ 16:22:47\n",
      "Loading back onto the cpu... @ 16:22:47\n",
      "Clearing GPU cache... @ 16:22:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 26/92 [09:02<22:46, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:23:08\n",
      "Adding processed chunk to chunk list. @ 16:23:08\n",
      "Loading OH stacks onto cuda device... @ 16:23:08\n",
      "Performing cuda op... @ 16:23:08\n",
      "Loading back onto the cpu... @ 16:23:08\n",
      "Clearing GPU cache... @ 16:23:08\n",
      "Performing post-processing... @ 16:23:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 27/92 [09:23<22:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:23:29\n",
      "Loading OH stacks onto cuda device... @ 16:23:29\n",
      "Performing cuda op... @ 16:23:29\n",
      "Loading back onto the cpu... @ 16:23:29\n",
      "Clearing GPU cache... @ 16:23:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 28/92 [09:44<22:05, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:23:49\n",
      "Adding processed chunk to chunk list. @ 16:23:50\n",
      "Loading OH stacks onto cuda device... @ 16:23:50\n",
      "Performing cuda op... @ 16:23:50\n",
      "Loading back onto the cpu... @ 16:23:50\n",
      "Clearing GPU cache... @ 16:23:50\n",
      "Performing post-processing... @ 16:23:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 29/92 [10:05<21:44, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:24:10\n",
      "Loading OH stacks onto cuda device... @ 16:24:10\n",
      "Performing cuda op... @ 16:24:10\n",
      "Loading back onto the cpu... @ 16:24:10\n",
      "Clearing GPU cache... @ 16:24:10\n",
      "Performing post-processing... @ 16:24:31"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 30/92 [10:25<21:23, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:24:31\n",
      "Loading OH stacks onto cuda device... @ 16:24:31\n",
      "Performing cuda op... @ 16:24:31\n",
      "Loading back onto the cpu... @ 16:24:31\n",
      "Clearing GPU cache... @ 16:24:31\n",
      "Performing post-processing... @ 16:24:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 31/92 [10:46<21:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:24:52\n",
      "Loading OH stacks onto cuda device... @ 16:24:52\n",
      "Performing cuda op... @ 16:24:52\n",
      "Loading back onto the cpu... @ 16:24:52\n",
      "Clearing GPU cache... @ 16:24:52\n",
      "Performing post-processing... @ 16:25:12"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 32/92 [11:07<20:42, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:25:12\n",
      "Loading OH stacks onto cuda device... @ 16:25:12\n",
      "Performing cuda op... @ 16:25:12\n",
      "Loading back onto the cpu... @ 16:25:12\n",
      "Clearing GPU cache... @ 16:25:12\n",
      "Performing post-processing... @ 16:25:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 33/92 [11:27<20:21, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:25:33\n",
      "Loading OH stacks onto cuda device... @ 16:25:33\n",
      "Performing cuda op... @ 16:25:33\n",
      "Loading back onto the cpu... @ 16:25:33\n",
      "Clearing GPU cache... @ 16:25:33\n",
      "Performing post-processing... @ 16:25:54"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 34/92 [11:48<20:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:25:54\n",
      "Loading OH stacks onto cuda device... @ 16:25:54\n",
      "Performing cuda op... @ 16:25:54\n",
      "Loading back onto the cpu... @ 16:25:54\n",
      "Clearing GPU cache... @ 16:25:54\n",
      "Performing post-processing... @ 16:25:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 35/92 [12:09<19:40, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:26:14\n",
      "Loading OH stacks onto cuda device... @ 16:26:14\n",
      "Performing cuda op... @ 16:26:14\n",
      "Loading back onto the cpu... @ 16:26:14\n",
      "Clearing GPU cache... @ 16:26:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 36/92 [12:29<19:19, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:26:35\n",
      "Adding processed chunk to chunk list. @ 16:26:35\n",
      "Loading OH stacks onto cuda device... @ 16:26:35\n",
      "Performing cuda op... @ 16:26:35\n",
      "Loading back onto the cpu... @ 16:26:35\n",
      "Clearing GPU cache... @ 16:26:35\n",
      "Performing post-processing... @ 16:26:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 37/92 [12:50<18:58, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:26:56\n",
      "Loading OH stacks onto cuda device... @ 16:26:56\n",
      "Performing cuda op... @ 16:26:56\n",
      "Loading back onto the cpu... @ 16:26:56\n",
      "Clearing GPU cache... @ 16:26:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 38/92 [13:11<18:38, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:27:17\n",
      "Adding processed chunk to chunk list. @ 16:27:17\n",
      "Loading OH stacks onto cuda device... @ 16:27:17\n",
      "Performing cuda op... @ 16:27:17\n",
      "Loading back onto the cpu... @ 16:27:17\n",
      "Clearing GPU cache... @ 16:27:17\n",
      "Performing post-processing... @ 16:27:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 39/92 [13:32<18:17, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:27:37\n",
      "Loading OH stacks onto cuda device... @ 16:27:37\n",
      "Performing cuda op... @ 16:27:37\n",
      "Loading back onto the cpu... @ 16:27:37\n",
      "Clearing GPU cache... @ 16:27:37\n",
      "Performing post-processing... @ 16:27:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 40/92 [13:52<17:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:27:58\n",
      "Loading OH stacks onto cuda device... @ 16:27:58\n",
      "Performing cuda op... @ 16:27:58\n",
      "Loading back onto the cpu... @ 16:27:58\n",
      "Clearing GPU cache... @ 16:27:58\n",
      "Performing post-processing... @ 16:27:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▍     | 41/92 [14:13<17:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:28:19\n",
      "Loading OH stacks onto cuda device... @ 16:28:19\n",
      "Performing cuda op... @ 16:28:19\n",
      "Loading back onto the cpu... @ 16:28:19\n",
      "Clearing GPU cache... @ 16:28:19\n",
      "Performing post-processing... @ 16:28:39"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 42/92 [14:34<17:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:28:39\n",
      "Loading OH stacks onto cuda device... @ 16:28:39\n",
      "Performing cuda op... @ 16:28:39\n",
      "Loading back onto the cpu... @ 16:28:39\n",
      "Clearing GPU cache... @ 16:28:39\n",
      "Performing post-processing... @ 16:28:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 43/92 [14:54<16:54, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:29:00\n",
      "Loading OH stacks onto cuda device... @ 16:29:00\n",
      "Performing cuda op... @ 16:29:00\n",
      "Loading back onto the cpu... @ 16:29:00\n",
      "Clearing GPU cache... @ 16:29:00\n",
      "Performing post-processing... @ 16:29:21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 44/92 [15:15<16:33, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:29:21\n",
      "Loading OH stacks onto cuda device... @ 16:29:21\n",
      "Performing cuda op... @ 16:29:21\n",
      "Loading back onto the cpu... @ 16:29:21\n",
      "Clearing GPU cache... @ 16:29:21\n",
      "Performing post-processing... @ 16:29:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 45/92 [15:36<16:12, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:29:41\n",
      "Loading OH stacks onto cuda device... @ 16:29:41\n",
      "Performing cuda op... @ 16:29:41\n",
      "Loading back onto the cpu... @ 16:29:41\n",
      "Clearing GPU cache... @ 16:29:41\n",
      "Performing post-processing... @ 16:30:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 46/92 [15:57<15:52, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:30:02\n",
      "Loading OH stacks onto cuda device... @ 16:30:02\n",
      "Performing cuda op... @ 16:30:02\n",
      "Loading back onto the cpu... @ 16:30:02\n",
      "Clearing GPU cache... @ 16:30:02\n",
      "Performing post-processing... @ 16:30:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 47/92 [16:17<15:31, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:30:23\n",
      "Loading OH stacks onto cuda device... @ 16:30:23\n",
      "Performing cuda op... @ 16:30:23\n",
      "Loading back onto the cpu... @ 16:30:23\n",
      "Clearing GPU cache... @ 16:30:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 48/92 [16:38<15:10, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:30:44\n",
      "Adding processed chunk to chunk list. @ 16:30:44\n",
      "Loading OH stacks onto cuda device... @ 16:30:44\n",
      "Performing cuda op... @ 16:30:44\n",
      "Loading back onto the cpu... @ 16:30:44\n",
      "Clearing GPU cache... @ 16:30:44\n",
      "Performing post-processing... @ 16:30:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 49/92 [16:59<14:50, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:31:04\n",
      "Loading OH stacks onto cuda device... @ 16:31:04\n",
      "Performing cuda op... @ 16:31:04\n",
      "Loading back onto the cpu... @ 16:31:04\n",
      "Clearing GPU cache... @ 16:31:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 50/92 [17:19<14:29, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:31:25\n",
      "Adding processed chunk to chunk list. @ 16:31:25\n",
      "Loading OH stacks onto cuda device... @ 16:31:25\n",
      "Performing cuda op... @ 16:31:25\n",
      "Loading back onto the cpu... @ 16:31:25\n",
      "Clearing GPU cache... @ 16:31:25\n",
      "Performing post-processing... @ 16:31:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 51/92 [17:40<14:08, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:31:46\n",
      "Loading OH stacks onto cuda device... @ 16:31:46\n",
      "Performing cuda op... @ 16:31:46\n",
      "Loading back onto the cpu... @ 16:31:46\n",
      "Clearing GPU cache... @ 16:31:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 52/92 [18:01<13:48, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:32:06\n",
      "Adding processed chunk to chunk list. @ 16:32:06\n",
      "Loading OH stacks onto cuda device... @ 16:32:06\n",
      "Performing cuda op... @ 16:32:06\n",
      "Loading back onto the cpu... @ 16:32:06\n",
      "Clearing GPU cache... @ 16:32:06\n",
      "Performing post-processing... @ 16:32:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 53/92 [18:21<13:27, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:32:27\n",
      "Loading OH stacks onto cuda device... @ 16:32:27\n",
      "Performing cuda op... @ 16:32:27\n",
      "Loading back onto the cpu... @ 16:32:27\n",
      "Clearing GPU cache... @ 16:32:27\n",
      "Performing post-processing... @ 16:32:48"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 59%|█████▊    | 54/92 [18:42<13:06, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding processed chunk to chunk list. @ 16:32:48\n",
      "Loading OH stacks onto cuda device... @ 16:32:48\n",
      "Performing cuda op... @ 16:32:48\n",
      "Loading back onto the cpu... @ 16:32:48\n",
      "Clearing GPU cache... @ 16:32:48\n",
      "Performing post-processing... @ 16:32:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████▉    | 55/92 [19:03<12:45, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:33:08\n",
      "Loading OH stacks onto cuda device... @ 16:33:08\n",
      "Performing cuda op... @ 16:33:08\n",
      "Loading back onto the cpu... @ 16:33:08\n",
      "Clearing GPU cache... @ 16:33:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████    | 56/92 [19:24<12:25, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing post-processing... @ 16:33:29\n",
      "Adding processed chunk to chunk list. @ 16:33:29\n",
      "Loading OH stacks onto cuda device... @ 16:33:29\n",
      "Performing cuda op... @ 16:33:29\n",
      "Loading back onto the cpu... @ 16:33:29\n",
      "Clearing GPU cache... @ 16:33:29\n",
      "Performing post-processing... @ 16:33:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 57/92 [19:44<12:04, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:33:50\n",
      "Loading OH stacks onto cuda device... @ 16:33:50\n",
      "Performing cuda op... @ 16:33:50\n",
      "Loading back onto the cpu... @ 16:33:50\n",
      "Clearing GPU cache... @ 16:33:50\n",
      "Performing post-processing... @ 16:33:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 58/92 [20:05<11:43, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:11\n",
      "Loading OH stacks onto cuda device... @ 16:34:11\n",
      "Performing cuda op... @ 16:34:11\n",
      "Loading back onto the cpu... @ 16:34:11\n",
      "Clearing GPU cache... @ 16:34:11\n",
      "Performing post-processing... @ 16:34:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 59/92 [20:26<11:23, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:31\n",
      "Loading OH stacks onto cuda device... @ 16:34:31\n",
      "Performing cuda op... @ 16:34:31\n",
      "Loading back onto the cpu... @ 16:34:31\n",
      "Clearing GPU cache... @ 16:34:31\n",
      "Performing post-processing... @ 16:34:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▌   | 60/92 [20:46<11:02, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:34:52\n",
      "Loading OH stacks onto cuda device... @ 16:34:52\n",
      "Performing cuda op... @ 16:34:52\n",
      "Loading back onto the cpu... @ 16:34:52\n",
      "Clearing GPU cache... @ 16:34:52\n",
      "Performing post-processing... @ 16:34:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████▋   | 61/92 [21:07<10:41, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:13\n",
      "Loading OH stacks onto cuda device... @ 16:35:13\n",
      "Performing cuda op... @ 16:35:13\n",
      "Loading back onto the cpu... @ 16:35:13\n",
      "Clearing GPU cache... @ 16:35:13\n",
      "Performing post-processing... @ 16:35:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 62/92 [21:28<10:20, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:33\n",
      "Loading OH stacks onto cuda device... @ 16:35:33\n",
      "Performing cuda op... @ 16:35:33\n",
      "Loading back onto the cpu... @ 16:35:33\n",
      "Clearing GPU cache... @ 16:35:33\n",
      "Performing post-processing... @ 16:35:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 63/92 [21:48<10:00, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:35:54\n",
      "Loading OH stacks onto cuda device... @ 16:35:54\n",
      "Performing cuda op... @ 16:35:54\n",
      "Loading back onto the cpu... @ 16:35:54\n",
      "Clearing GPU cache... @ 16:35:54\n",
      "Performing post-processing... @ 16:35:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████▉   | 64/92 [22:09<09:39, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:15\n",
      "Loading OH stacks onto cuda device... @ 16:36:15\n",
      "Performing cuda op... @ 16:36:15\n",
      "Loading back onto the cpu... @ 16:36:15\n",
      "Clearing GPU cache... @ 16:36:15\n",
      "Performing post-processing... @ 16:36:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|███████   | 65/92 [22:30<09:18, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:35\n",
      "Loading OH stacks onto cuda device... @ 16:36:35\n",
      "Performing cuda op... @ 16:36:35\n",
      "Loading back onto the cpu... @ 16:36:35\n",
      "Clearing GPU cache... @ 16:36:35\n",
      "Performing post-processing... @ 16:36:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 66/92 [22:51<08:58, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:36:56\n",
      "Loading OH stacks onto cuda device... @ 16:36:56\n",
      "Performing cuda op... @ 16:36:56\n",
      "Loading back onto the cpu... @ 16:36:56\n",
      "Clearing GPU cache... @ 16:36:56\n",
      "Performing post-processing... @ 16:36:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 67/92 [23:11<08:37, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:17\n",
      "Loading OH stacks onto cuda device... @ 16:37:17\n",
      "Performing cuda op... @ 16:37:17\n",
      "Loading back onto the cpu... @ 16:37:17\n",
      "Clearing GPU cache... @ 16:37:17\n",
      "Performing post-processing... @ 16:37:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▍  | 68/92 [23:32<08:16, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:38\n",
      "Loading OH stacks onto cuda device... @ 16:37:38\n",
      "Performing cuda op... @ 16:37:38\n",
      "Loading back onto the cpu... @ 16:37:38\n",
      "Clearing GPU cache... @ 16:37:38\n",
      "Performing post-processing... @ 16:37:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 69/92 [23:53<07:56, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:37:58\n",
      "Loading OH stacks onto cuda device... @ 16:37:58\n",
      "Performing cuda op... @ 16:37:58\n",
      "Loading back onto the cpu... @ 16:37:58\n",
      "Clearing GPU cache... @ 16:37:58\n",
      "Performing post-processing... @ 16:37:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 70/92 [24:13<07:35, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:38:19\n",
      "Loading OH stacks onto cuda device... @ 16:38:19\n",
      "Performing cuda op... @ 16:38:19\n",
      "Loading back onto the cpu... @ 16:38:19\n",
      "Clearing GPU cache... @ 16:38:19\n",
      "Performing post-processing... @ 16:38:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|███████▋  | 71/92 [24:34<07:14, 20.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:38:40\n",
      "Loading OH stacks onto cuda device... @ 16:38:40\n",
      "Performing cuda op... @ 16:38:40\n",
      "Loading back onto the cpu... @ 16:38:40\n",
      "Clearing GPU cache... @ 16:38:40\n",
      "Performing post-processing... @ 16:38:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 72/92 [24:55<06:54, 20.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:00\n",
      "Loading OH stacks onto cuda device... @ 16:39:00\n",
      "Performing cuda op... @ 16:39:00\n",
      "Loading back onto the cpu... @ 16:39:00\n",
      "Clearing GPU cache... @ 16:39:00\n",
      "Performing post-processing... @ 16:39:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████▉  | 73/92 [25:16<06:34, 20.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:21\n",
      "Loading OH stacks onto cuda device... @ 16:39:21\n",
      "Performing cuda op... @ 16:39:21\n",
      "Loading back onto the cpu... @ 16:39:21\n",
      "Clearing GPU cache... @ 16:39:21\n",
      "Performing post-processing... @ 16:39:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 74/92 [25:37<06:14, 20.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:39:42\n",
      "Loading OH stacks onto cuda device... @ 16:39:42\n",
      "Performing cuda op... @ 16:39:42\n",
      "Loading back onto the cpu... @ 16:39:42\n",
      "Clearing GPU cache... @ 16:39:42\n",
      "Performing post-processing... @ 16:39:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 75/92 [25:57<05:53, 20.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:03\n",
      "Loading OH stacks onto cuda device... @ 16:40:03\n",
      "Performing cuda op... @ 16:40:03\n",
      "Loading back onto the cpu... @ 16:40:03\n",
      "Clearing GPU cache... @ 16:40:03\n",
      "Performing post-processing... @ 16:40:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 76/92 [26:18<05:33, 20.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:24\n",
      "Loading OH stacks onto cuda device... @ 16:40:24\n",
      "Performing cuda op... @ 16:40:24\n",
      "Loading back onto the cpu... @ 16:40:24\n",
      "Clearing GPU cache... @ 16:40:24\n",
      "Performing post-processing... @ 16:40:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▎ | 77/92 [26:39<05:12, 20.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:40:45\n",
      "Loading OH stacks onto cuda device... @ 16:40:45\n",
      "Performing cuda op... @ 16:40:45\n",
      "Loading back onto the cpu... @ 16:40:45\n",
      "Clearing GPU cache... @ 16:40:45\n",
      "Performing post-processing... @ 16:40:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▍ | 78/92 [27:00<04:51, 20.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:06\n",
      "Loading OH stacks onto cuda device... @ 16:41:06\n",
      "Performing cuda op... @ 16:41:06\n",
      "Loading back onto the cpu... @ 16:41:06\n",
      "Clearing GPU cache... @ 16:41:06\n",
      "Performing post-processing... @ 16:41:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 79/92 [27:21<04:31, 20.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:27\n",
      "Loading OH stacks onto cuda device... @ 16:41:27\n",
      "Performing cuda op... @ 16:41:27\n",
      "Loading back onto the cpu... @ 16:41:27\n",
      "Clearing GPU cache... @ 16:41:27\n",
      "Performing post-processing... @ 16:41:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 80/92 [27:42<04:10, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:41:47\n",
      "Loading OH stacks onto cuda device... @ 16:41:47\n",
      "Performing cuda op... @ 16:41:47\n",
      "Loading back onto the cpu... @ 16:41:47\n",
      "Clearing GPU cache... @ 16:41:47\n",
      "Performing post-processing... @ 16:41:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 81/92 [28:03<03:49, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:08\n",
      "Loading OH stacks onto cuda device... @ 16:42:08\n",
      "Performing cuda op... @ 16:42:08\n",
      "Loading back onto the cpu... @ 16:42:08\n",
      "Clearing GPU cache... @ 16:42:08\n",
      "Performing post-processing... @ 16:42:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████▉ | 82/92 [28:23<03:28, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:29\n",
      "Loading OH stacks onto cuda device... @ 16:42:29\n",
      "Performing cuda op... @ 16:42:29\n",
      "Loading back onto the cpu... @ 16:42:29\n",
      "Clearing GPU cache... @ 16:42:29\n",
      "Performing post-processing... @ 16:42:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 83/92 [28:44<03:07, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:42:50\n",
      "Loading OH stacks onto cuda device... @ 16:42:50\n",
      "Performing cuda op... @ 16:42:50\n",
      "Loading back onto the cpu... @ 16:42:50\n",
      "Clearing GPU cache... @ 16:42:50\n",
      "Performing post-processing... @ 16:42:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|█████████▏| 84/92 [29:05<02:46, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:11\n",
      "Loading OH stacks onto cuda device... @ 16:43:11\n",
      "Performing cuda op... @ 16:43:11\n",
      "Loading back onto the cpu... @ 16:43:11\n",
      "Clearing GPU cache... @ 16:43:11\n",
      "Performing post-processing... @ 16:43:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 85/92 [29:26<02:26, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:32\n",
      "Loading OH stacks onto cuda device... @ 16:43:32\n",
      "Performing cuda op... @ 16:43:32\n",
      "Loading back onto the cpu... @ 16:43:32\n",
      "Clearing GPU cache... @ 16:43:32\n",
      "Performing post-processing... @ 16:43:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 86/92 [29:47<02:05, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:43:53\n",
      "Loading OH stacks onto cuda device... @ 16:43:53\n",
      "Performing cuda op... @ 16:43:53\n",
      "Loading back onto the cpu... @ 16:43:53\n",
      "Clearing GPU cache... @ 16:43:53\n",
      "Performing post-processing... @ 16:43:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▍| 87/92 [30:08<01:44, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:13\n",
      "Loading OH stacks onto cuda device... @ 16:44:13\n",
      "Performing cuda op... @ 16:44:13\n",
      "Loading back onto the cpu... @ 16:44:13\n",
      "Clearing GPU cache... @ 16:44:13\n",
      "Performing post-processing... @ 16:44:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 88/92 [30:29<01:23, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:34\n",
      "Loading OH stacks onto cuda device... @ 16:44:34\n",
      "Performing cuda op... @ 16:44:34\n",
      "Loading back onto the cpu... @ 16:44:34\n",
      "Clearing GPU cache... @ 16:44:34\n",
      "Performing post-processing... @ 16:44:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 89/92 [30:49<01:02, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:44:55\n",
      "Loading OH stacks onto cuda device... @ 16:44:55\n",
      "Performing cuda op... @ 16:44:55\n",
      "Loading back onto the cpu... @ 16:44:55\n",
      "Clearing GPU cache... @ 16:44:55\n",
      "Performing post-processing... @ 16:44:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 90/92 [31:10<00:41, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:16\n",
      "Loading OH stacks onto cuda device... @ 16:45:16\n",
      "Performing cuda op... @ 16:45:16\n",
      "Loading back onto the cpu... @ 16:45:16\n",
      "Clearing GPU cache... @ 16:45:16\n",
      "Performing post-processing... @ 16:45:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|█████████▉| 91/92 [31:31<00:20, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:37\n",
      "Loading OH stacks onto cuda device... @ 16:45:37\n",
      "Performing cuda op... @ 16:45:37\n",
      "Loading back onto the cpu... @ 16:45:37\n",
      "Clearing GPU cache... @ 16:45:37\n",
      "Performing post-processing... @ 16:45:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 92/92 [31:46<00:00, 19.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding processed chunk to chunk list. @ 16:45:52\n",
      "Consolidating chunks into a constant-length block (tensor). @ 16:45:52\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-59a2d2c4df80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mconstant_length_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordforms_CMs_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Consolidating chunks into a constant-length block (tensor).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mconstant_length_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstant_length_chunks\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mstampedNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding block to list of blocks.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mendNote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"l = {l}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset\n",
    "\n",
    "for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "    startNote(f\"l = {l}\")\n",
    "    \n",
    "    Ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "                                                     key_length=l,\n",
    "                                                     exact_length_only = False)\n",
    "                   for w in Ws_t]\n",
    "    print(f\"Size of Ws_OHs_tr_t = {sum(torch_nbytes(each) for each in Ws_OHs_tr_t) / 1e9}GB\")\n",
    "    \n",
    "    constant_length_chunks = []\n",
    "    chunk_size = 2500\n",
    "    chunks = chunkList(chunk_size, Ws_OHs_tr_t)\n",
    "    for chunk in tqdm(chunks, total=len(chunks)):\n",
    "        stampedNote(\"Loading OH stacks onto cuda device...\")\n",
    "        wordforms_OHs_tr_t_c = [each.cuda() for each in chunk]\n",
    "        stampedNote(\"Performing cuda op...\")\n",
    "#         wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "#                                                                   key_length=l, \n",
    "#                                                                   exact_length_only = False)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "        wordforms_CMs_c = [sourcePrefixStackToChannelMatrix_t_c(each)\n",
    "                           for each in wordforms_OHs_tr_t_c]\n",
    "        \n",
    "        stampedNote(\"Loading back onto the cpu...\")\n",
    "        wordforms_CMs_cpu = (each.cpu() for each in wordforms_CMs_c)\n",
    "        stampedNote(\"Performing post-processing...\")\n",
    "#         wordforms_CMs_n = np.array(list(par(delayed(makeCMbyWordformAndLength_t_c_post)(w_CM=each.numpy(), \n",
    "#                                                                                           key_length=l, \n",
    "#                                                                                           exact_length_only = False,\n",
    "#                                                                                           asType = 'ndarray')\n",
    "#                                        for each in wordforms_CMs_cpu)))\n",
    "#         wordforms_CMs_t = torch.tensor(wordforms_CMs_n)\n",
    "        wordforms_CMs_t = torch.stack([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "                                                                          key_length=l, \n",
    "                                                                          exact_length_only = False)\n",
    "                                       for each in wordforms_CMs_cpu])\n",
    "        stampedNote(\"Adding processed chunk to chunk list.\")\n",
    "        constant_length_chunks.append(wordforms_CMs_t)\n",
    "        stampedNote(\"Clearing GPU cache...\")\n",
    "        torch.cuda.empty_cache()\n",
    "    stampedNote('Consolidating chunks into a constant-length block (tensor).')\n",
    "    constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "    stampedNote('Adding block to list of blocks.\\n')\n",
    "    endNote(f\"l = {l}\")\n",
    "    cmsByLengthByWordformIndex_torch.append(constant_length_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:04:05.386079Z",
     "start_time": "2019-07-26T23:04:04.163Z"
    }
   },
   "outputs": [],
   "source": [
    "# offset = [torch.zeros((0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "\n",
    "# cmsByLengthByWordformIndex_torch = offset\n",
    "\n",
    "# for l in tqdm(wordlengthsInclEdges_range, total=len(wordlengthsInclEdges_range)):\n",
    "#     print(f\"l = {l}\")\n",
    "#     constant_length_chunks = []\n",
    "#     chunk_size = 100\n",
    "#     chunks_of_words = chunkList(chunk_size, Ws_t)\n",
    "#     for chunk in tqdm(chunks_of_words, total=len(chunks_of_words)):\n",
    "#         ws = chunk\n",
    "#         ws_OHs_tr_t = [makeCMbyWordformAndLength_t_c_pre(wordform=w, \n",
    "#                                                          key_length=l,\n",
    "#                                                          exact_length_only = False)\n",
    "#                        for w in ws]\n",
    "#         wordforms_OHs_tr_t_c = [each.cuda() for each in ws_OHs_tr_t]\n",
    "#         wordforms_CMs_c = [makeCMbyWordformAndLength_t_c_cudaStep(loaded_prep_result=each, \n",
    "#                                                                   key_length=l, \n",
    "#                                                                   exact_length_only = False)\n",
    "#                            for each in wordforms_OHs_tr_t_c]\n",
    "#         wordforms_CMs_cpu = [each.cpu() for each in wordforms_CMs_c]\n",
    "#         wordforms_CMs_t = torch.tensor([makeCMbyWordformAndLength_t_c_post(w_CM=each, \n",
    "#                                                                            key_length=l, \n",
    "#                                                                            exact_length_only = False)\n",
    "#                                         for each in wordforms_CMs_cpu])\n",
    "#         constant_length_chunks.append(wordforms_CMs_t)\n",
    "#     print('Consolidating chunks into a constant-length block (tensor).')\n",
    "#     constant_length_block = torch.stack([cm for chunk in constant_length_chunks for cm in chunk])\n",
    "#     print(f\"constant_length_block.shape = {constant_length_block.shape}\")\n",
    "#     print('Adding block to list of blocks.\\n')\n",
    "#     cmsByLengthByWordformIndex_torch.append(constant_length_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T23:04:05.386662Z",
     "start_time": "2019-07-26T23:04:04.591Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrates that makeChannelMatrixByWordformAndLength_t_c will run without errors, but slow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cmsByLengthByWordformIndex_torch = offset + [torch.tensor([makeChannelMatrixByWordformAndLength_t_c(w, l)\n",
    "                                                           for w in tqdm(Ws_t, \n",
    "                                                                         total=len(Ws_t))])\n",
    "                                             for l in tqdm(wordlengthsInclEdges_range, \n",
    "                                                           total=len(wordlengthsInclEdges_range))]\n",
    "# cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.867776Z",
     "start_time": "2019-07-24T21:54:07.698848Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "offset = [np.zeros(shape=(0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.902590Z",
     "start_time": "2019-07-24T21:54:22.869287Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in wordlengthsInclEdges_range:\n",
    "    assert all(cm.shape[1] == l - 2 for cm in cmsByLengthByWordformIndex[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:22.909309Z",
     "start_time": "2019-07-24T21:54:22.904096Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = offset + [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                                      for w in Ws_t])\n",
    "                                            for l in wordlengthsInclEdges_range]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.310885Z",
     "start_time": "2019-07-24T21:54:22.910819Z"
    }
   },
   "outputs": [],
   "source": [
    "exactCMsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l, exact_length_only = True)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "exactCMsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), exactCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment sequence (all prefixes or just wordforms) channel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `CMsByWordformIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " - `exactCMsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.318016Z",
     "start_time": "2019-07-24T21:54:24.312658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByWordformIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.329330Z",
     "start_time": "2019-07-24T21:54:24.319395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.341972Z",
     "start_time": "2019-07-24T21:54:24.330798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 38, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.015572096"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[2].shape\n",
    "cmsByLengthByWordformIndex[3].shape\n",
    "cmsByLengthByWordformIndex[10].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.697748Z",
     "start_time": "2019-07-24T21:54:24.343457Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))\n",
    "    pickle.dump(CMsByWordformIndex, open(o + 'CMs_by_wordform_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.759827Z",
     "start_time": "2019-07-24T21:54:24.699318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.766724Z",
     "start_time": "2019-07-24T21:54:24.761786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.803809Z",
     "start_time": "2019-07-24T21:54:24.768566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_prefix_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByPrefixIndex) if r else len(CMsByPrefixIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_prefix_index.pickle' if r else o + 'CMs_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_prefix_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.811621Z",
     "start_time": "2019-07-24T21:54:24.805592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(CMsByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'CMs_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:24.819439Z",
     "start_time": "2019-07-24T21:54:24.817389Z"
    }
   },
   "outputs": [],
   "source": [
    "# importDict(o + '.pW_C' + '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:30.653058Z",
     "start_time": "2019-07-24T21:54:24.821320Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    pickle.dump(exactCMsByLengthByWordformIndex, open(o + 'exact_CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:30.722681Z",
     "start_time": "2019-07-24T21:54:30.654567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByLengthByWordformIndex) if r else len(cmsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_length_by_wordform_index.pickle' if r else o + 'CMs_by_length_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:58.016185Z",
     "start_time": "2019-07-24T21:54:57.971893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "exact_CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(exactCMsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'exact_CMs_by_length_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     exact_CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:59.604073Z",
     "start_time": "2019-07-24T21:54:59.598079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:54:59.893777Z",
     "start_time": "2019-07-24T21:54:59.845739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy',\n",
       " 'Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json',\n",
       " 'p3Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy_metadata.json',\n",
       " 'pYX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle_metadata.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2Y012s.txt',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       " 'Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle_metadata.json']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.389549Z",
     "start_time": "2019-07-24T21:55:00.363581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving p3Y1X012_np to filepath 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy'\n"
     ]
    }
   ],
   "source": [
    "#if not r, export numpy representation of triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012' + '.npy', p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.593216Z",
     "start_time": "2019-07-24T21:55:00.588808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if not r:\n",
    "    CM_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "             'changes':'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         CM_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:00.840185Z",
     "start_time": "2019-07-24T21:55:00.836400Z"
    }
   },
   "outputs": [],
   "source": [
    "#if r, export .json of modified triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving extended, human-readable version of p3Y1X012 to filepath '{o + 'p3Y1X012_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X012_RE' + '.json', condProbDistAsDicts(p3Y1X012))\n",
    "          \n",
    "    print(f\"Saving extended, human-readable version of p3Y1X01 to filepath '{o + 'p3Y1X01_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X01_RE' + '.json', condProbDistAsDicts(p3Y1X01))\n",
    "\n",
    "#if r, export numpy representation of triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012_RE' + '.npy', p3Y1X012_np)\n",
    "    print(f\"Saving p3Y1X01_np to filepath '{o + 'p3Y1X01_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X01_RE' + '.npy', p3Y1X01_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:01.055288Z",
     "start_time": "2019-07-24T21:55:01.052060Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    CD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X012_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:55:01.318057Z",
     "start_time": "2019-07-24T21:55:01.314912Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    PD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X01_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X01_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X01_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
