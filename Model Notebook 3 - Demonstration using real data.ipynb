{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:43:56.473620Z",
     "start_time": "2018-03-31T23:43:56.462566Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivation/Problem-Statement\" data-toc-modified-id=\"Motivation/Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivation/Problem Statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Code-requirements\" data-toc-modified-id=\"Code-requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Code requirements</a></span></li><li><span><a href=\"#Exposition-/-other-notebooks\" data-toc-modified-id=\"Exposition-/-other-notebooks-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exposition / other notebooks</a></span></li></ul></li><li><span><a href=\"#Boilerplate-code-for-representing-and-manipulating-probability-distributions\" data-toc-modified-id=\"Boilerplate-code-for-representing-and-manipulating-probability-distributions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Boilerplate code for representing and manipulating probability distributions</a></span></li><li><span><a href=\"#Boilerplate-code-for-generating-model-code-given-model-inputs\" data-toc-modified-id=\"Boilerplate-code-for-generating-model-code-given-model-inputs-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Boilerplate code for generating model code given model inputs</a></span></li><li><span><a href=\"#Getting-data\" data-toc-modified-id=\"Getting-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Getting data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Converting-data-to-the-required-probability-distributions\" data-toc-modified-id=\"Converting-data-to-the-required-probability-distributions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Converting data to the required probability distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explorations-/-asides-/-sanity-checks\" data-toc-modified-id=\"Explorations-/-asides-/-sanity-checks-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Explorations / asides / sanity checks</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Testing</a></span></li></ul></li></ul></li><li><span><a href=\"#Generating-a-model\" data-toc-modified-id=\"Generating-a-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generating a model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Testing-the-model\" data-toc-modified-id=\"Testing-the-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Testing the model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation/Problem Statement\n",
    "\n",
    "The goal of this collection of notebooks ('Model Notebook k', $k \\in \\{0, 1, 2, 3\\}$) is to document/develop code for calculations related to an isolated word recognition task where:\n",
    "\n",
    " - A *lexicon* $\\mathcal{L}$ is a set of **wordforms**  $w$, where a **wordform** is a finite sequence $s_0, s_1, s_2 ... s_f$ of segments, and $s_0^i$ denotes the prefix of some wordform $w \\in \\mathcal{L}$ such that $f = |w| - 1$ and such that $w$ begins with segments $s_0, s_1, ..., s_i$, where $0 \\leq i \\leq f$.\n",
    " - In a single episode of the task, a **speaker** samples a wordform $w = s_0^f$ from $\\mathcal{L} \\sim p(s_0^f)$.\n",
    " - The speaker incrementally produces their intended wordform (one segment at a time, for our purposes) and the **listener** incrementally perceives $\\sigma_0^i \\sim p(\\sigma_0^i | s_0^i)$.\n",
    " - The listener considers what they have perceived so far $\\sigma_0^i$ and reasons about what the most likely actual intended wordform of the speaker is $\\sim p(\\hat{s_0^f}|\\sigma_0^i) \\propto p(\\sigma_0^i|s_0^f) p(s_0^f) = p(\\sigma_0^i|s_0^i) p(s_0^i)$.\n",
    "\n",
    "\n",
    "In particular, we want, for each possible intended wordform $s_0^f \\in \\mathcal{L}$, for each possible prefix $s_0^i$ of $s_0^f$, the listener's expected distribution over the speaker's intended wordform given the actual prefix produced so far $s_0^i$, where the expectation is taken with respect to possible perceived sequences $\\sigma_0^i$: \n",
    "\n",
    "$$p(\\hat{s_0^f}|s_0^i) = \\sum_\\limits{\\sigma_0^i} p(s_0^f|\\sigma_0^i)p(\\sigma_0^i|s_0^i) \\propto \\sum_\\limits{\\sigma_0^i} p(\\sigma_0^i|\\hat{s_0^f})p(\\hat{s_0^f})p(\\sigma_0^i|s_0^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code requirements\n",
    "\n",
    "The code for calculating \n",
    "$$p(\\hat{s_0^f}|s_0^i) = \\sum_\\limits{\\sigma_0^i} p(s_0^f|\\sigma_0^i)p(\\sigma_0^i|s_0^i) \\propto \\sum_\\limits{\\sigma_0^i} p(\\sigma_0^i|\\hat{s_0^f})p(\\hat{s_0^f})p(\\sigma_0^i|s_0^i)$$\n",
    "needs to take only two basic inputs:\n",
    "\n",
    " - a prior distribution over the lexicon $p(s_0^f) = p(\\hat{s_0^f})$.\n",
    " - an incrementally defined channel distribution $p(\\sigma_0^i|s_0^i) = p(\\sigma_0^i|\\hat{s_0^i})$, defined (for now) in terms of a uniphone channel distribution $p(\\sigma_i|s_i)$, where the distribution over what segment the listener perceives as the $i$th one depends (by assumption) *only* on the actual $i$th segment $s_i$ the speaker produced (if they've actually produced it at the time we're asking about).\n",
    "\n",
    "from which it needs to generate all relevant code and representations of probability distributions or means of estimating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposition / other notebooks\n",
    "\n",
    "*Notebook 0* introduces the model being developed and implemented, the organization of other notebooks in the collection, and introduces the adapted form of Peter Norvig's code for conveniently representing and manipulating probability distributions that I make use of in other notebooks.\n",
    "\n",
    "*Notebook 1* introduces the final model implementation and the derivations underlying its implementation and does so using randomly generated but highly constrained binary lexicons with a simple noise model. It is intended to introduce the model implementation/derivation in a context where the behavior and purpose (and basic correctness) of code is discernable by going through the notebook and the generated example.\n",
    "\n",
    "*Notebook 2* is more for testing purposes. It contains multiple implementations of the model. One is defined entirely in terms of the abstractions Peter Norvig provides -- abstractions that cannot scale but which were easy to use and whose correctness I have high confidence in. The second implementation is the one presented in notebooks 1 and 3. \n",
    "\n",
    "*This notebook*, *Notebook 3* is a demonstration notebook where real data is loaded and basic queries are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate code for representing and manipulating probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:19.791735Z",
     "start_time": "2018-03-31T23:44:19.443517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#from \n",
    "#    http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb\n",
    "#    http://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb\n",
    "#with slight modification.\n",
    "\n",
    "from fractions import Fraction\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "is_predicate = callable\n",
    "\n",
    "def P(event, space): \n",
    "    \"\"\"The probability of an event, given a sample space of equiprobable outcomes. \n",
    "    event: a collection of outcomes, or a predicate that is true of outcomes in the event. \n",
    "    space: a set of outcomes or a probability distribution of {outcome: frequency} pairs.\"\"\"\n",
    "    if is_predicate(event):\n",
    "        event = such_that(event, space)\n",
    "    if isinstance(space, ProbDist):\n",
    "        return sum(space[o] for o in space if o in event)\n",
    "    else:\n",
    "        return Fraction(len(event & space), len(space))\n",
    "    \n",
    "def such_that(predicate, space): \n",
    "    \"\"\"The outcomes in the sample pace for which the predicate is true.\n",
    "    If space is a set, return a subset {outcome,...};\n",
    "    if space is a ProbDist, return a ProbDist {outcome: frequency,...};\n",
    "    in both cases only with outcomes where predicate(element) is true.\"\"\"\n",
    "    if isinstance(space, ProbDist):\n",
    "        return ProbDist({o:space[o] for o in space if predicate(o)})\n",
    "    else:\n",
    "        return {o for o in space if predicate(o)}\n",
    "\n",
    "# class ProbDist(dict):\n",
    "class ProbDist(Counter):\n",
    "    \"A Probability Distribution; an {outcome: probability} mapping where probabilities sum to 1.\"\n",
    "    def __init__(self, mapping=(), **kwargs):\n",
    "        self.update(mapping, **kwargs)\n",
    "        total = sum(self.values())\n",
    "        if isinstance(total, int): \n",
    "            total = Fraction(total, 1)\n",
    "        for key in self: # Make probabilities sum to 1.\n",
    "            self[key] = self[key] / total\n",
    "            \n",
    "    def __and__(self, predicate): # Call this method by writing `probdist & predicate`\n",
    "        \"A new ProbDist, restricted to the outcomes of this ProbDist for which the predicate is true.\"\n",
    "        return ProbDist({e:self[e] for e in self if predicate(e)})\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        for k in self:\n",
    "            if isinstance(self[k], Fraction):\n",
    "                s+=\"{0}: {2}/{3} = {1}\\n\".format(transcriptionReprHack(k), float(self[k]), self[k].numerator, self[k].denominator)\n",
    "            else:\n",
    "                s+=\"{0}: {1}\\n\".format(transcriptionReprHack(k), float(self[k]))\n",
    "        return s\n",
    "\n",
    "dottedStringToTuple = lambda ds: tuple(ds.split('.'))\n",
    "tupleToDottedString = lambda t: '.'.join(t)\n",
    "\n",
    "def transcriptionReprHack(k):\n",
    "    if type(k) == type(tuple()):\n",
    "        if all(map(lambda el: type(el) == type(''), k)):\n",
    "            return tupleToDottedString(k)\n",
    "    return k.__repr__()    \n",
    "\n",
    "def Uniform(outcomes): return ProbDist({e: 1 for e in outcomes})\n",
    "\n",
    "def joint(A, B):\n",
    "    \"\"\"The joint distribution of two independent probability distributions. \n",
    "    Result is all entries of the form {(a, b): P(a) * P(b)}\"\"\"\n",
    "    return ProbDist({(a,b): A[a] * B[b]\n",
    "                    for a in A\n",
    "                    for b in B})\n",
    "\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def union(iterable):\n",
    "    return reduce(set.union, iterable)\n",
    "\n",
    "def joint2(iter_of_dists):\n",
    "    #ProbDist({(a,b): A[a] * B[b] for a in A for b in B})\n",
    "    #ProbDist({ab: A[ab[0]] * B[ab[1]] for ab in product(A,B)})\n",
    "    return ProbDist({each : prod(dist[each[i]] for i,dist in enumerate(iter_of_dists)) for each in list(product(*iter_of_dists))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:20.344627Z",
     "start_time": "2018-03-31T23:44:20.101292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bookkeeping - don't worry about this function...\n",
    "def getBoundaryVal(dist, i):\n",
    "    #p = {a:1/2, b:1/2} -> 2 items -> 2-1=1 boundaries where \n",
    "    #                   the boundary separating 'a' (item 0) from 'b' (item 1) is at 0+p(a)\n",
    "    #q = {0:1/3, 1:1/3, 2:1/6, 3:1/6} -> 4 items -> 4-1=3 boundaries where \n",
    "    #                   the boundary separating 0 from 1   is at 0 + q(0) = 1/3,\n",
    "    #                   the boundary separating 1 from 2   is at 0 + q(0) + q(1) = 2/3,\n",
    "    #                   the boundary separating 2 from 3   is at 0 + q(0) + q(1) + q(2) = 5/6,\n",
    "    #                ...the boundary separating i from i+1 is at \\sum_{j=0}^{j=i} q(j)\n",
    "    outcomes = list(dist.keys())\n",
    "    if i >= len(outcomes) - 1:\n",
    "        raise Exception(\"Boundary i = {0} out of bounds / does not exist for distribution {1} with {2} outcomes.\".format(i, dist, len(outcomes)))\n",
    "    if i == 0:\n",
    "        return dist[outcomes[0]]\n",
    "    return dist[outcomes[i]] + getBoundaryVal(dist, i-1)\n",
    "\n",
    "#bookkeeping - don't worry about this function...\n",
    "def getSampleOutcomeIndex(randReal, boundariesLeft, currIndex):\n",
    "#     print(\"boundariesLeft: {0}\".format(boundariesLeft))\n",
    "#     print(\"currIndex: {0}\".format(currIndex))\n",
    "    if boundariesLeft == [] or randReal <= boundariesLeft[0]:\n",
    "        return currIndex\n",
    "    return getSampleOutcomeIndex(randReal, boundariesLeft[1:], currIndex + 1)\n",
    "\n",
    "\n",
    "def sampleFrom(dist, num_samples = None):\n",
    "    \"\"\"\n",
    "    Given a distribution (either an {outcome: probability} mapping where the \n",
    "    probabilities sum to 1 or an implicit definition of a distribution via a thunk), \n",
    "    this returns a single sample from the distribution, unless num_samples is specified, \n",
    "    in which case a generator with num_samples samples is returned.\n",
    "    \"\"\"\n",
    "    if num_samples == None:\n",
    "        if callable(dist):\n",
    "            return dist()\n",
    "        elif isinstance(dist, ProbDist):\n",
    "            outcomes = list(dist.keys())\n",
    "        #     print(\"outcomes: {0}\".format(outcomes))\n",
    "\n",
    "            boundaries = [getBoundaryVal(dist, i) for i in range(len(outcomes)-1)]\n",
    "        #     print(\"boundaries: {0}\".format(boundaries))\n",
    "\n",
    "            randVal = random.random() #random real from unit interval\n",
    "        #     print(\"randval: {0}\".format(randVal))\n",
    "\n",
    "            sampledOutcomeIndex = getSampleOutcomeIndex(randVal, boundaries, 0)\n",
    "        #     print(\"sampledOutcomeIndex: {0}\".format(sampledOutcomeIndex))\n",
    "            if not (sampledOutcomeIndex >= 0 and sampledOutcomeIndex < len(outcomes)):\n",
    "                print('sampledOutcomeIndex: {0}'.format(sampledOutcomeIndex))\n",
    "                print('len(outcomes): {0}'.format(len(outcomes)))\n",
    "                if len(outcomes) == 0:\n",
    "                    print('len(outcomes) == 0! dist:')\n",
    "                    print(type(dist))\n",
    "                    print(dist)\n",
    "            assert(sampledOutcomeIndex >= 0 and sampledOutcomeIndex < len(outcomes))\n",
    "\n",
    "            sampledOutcome = outcomes[sampledOutcomeIndex]\n",
    "        #     print(\"sampledOutcome: {0}\".format(sampledOutcome))\n",
    "            return sampledOutcome\n",
    "    else:\n",
    "        if callable(dist):\n",
    "            return (dist() for each in range(num_samples))\n",
    "        elif isinstance(dist, ProbDist):\n",
    "            return (sampleFrom(dist, num_samples = None) for each in range(num_samples))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def frequencies(samples):\n",
    "    return Counter(samples)\n",
    "\n",
    "def makeSampler(dist):\n",
    "    \"\"\"\n",
    "    Given a ProbDist, returns a thunk that when called, returns one sample from dist.\n",
    "    \"\"\"\n",
    "    return lambda: sampleFrom(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:21.173564Z",
     "start_time": "2018-03-31T23:44:21.168308Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leftEdge = '⋊'\n",
    "rightEdge = '⋉'\n",
    "edgeSymbols = set([leftEdge, rightEdge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate code for generating model code given model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:23.400520Z",
     "start_time": "2018-03-31T23:44:22.966690Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating functions and values derived from p(X_0^f)...\n",
    "\n",
    "import itertools\n",
    "\n",
    "def getPrefixes(s):\n",
    "    return set(s[0:i] for i in range(1, len(s)+1))\n",
    "\n",
    "def getHasPrefix(prefix):\n",
    "    l = len(prefix)\n",
    "    hasAsPrefix = lambda full_input_seq: full_input_seq[0:l] == prefix\n",
    "    return hasAsPrefix\n",
    "  \n",
    "def getHasSuffix(suffix):\n",
    "    l = len(suffix)\n",
    "    hasAsSuffix = lambda full_input_seq: full_input_seq[-len(suffix):] == suffix\n",
    "    return hasAsSuffix\n",
    "\n",
    "def padInputSequenceWithBoundaries(inputSeq):\n",
    "    temp = list(inputSeq)\n",
    "    temp = tuple([leftEdge] + temp + [rightEdge])\n",
    "    return temp\n",
    "\n",
    "def trimBoundariesFromSequence(seq):\n",
    "    temp = list(seq)\n",
    "    if temp[0] == leftEdge:\n",
    "        temp = temp[1:]\n",
    "    if temp[-1] == rightEdge:\n",
    "        temp = temp[:-1]\n",
    "    return tuple(temp)\n",
    "\n",
    "def generateInputModel(inputDist, pad_with_boundaries = None):\n",
    "    if pad_with_boundaries == None:\n",
    "        pad_with_boundaries = False\n",
    "    model = {} #FIXME consider changing to a namedtuple/frozendict (possibly just before returning a value)\n",
    "    \n",
    "    if pad_with_boundaries:\n",
    "        old_map = list(inputDist.items())\n",
    "        old_inputs = list(map(lambda p: p[0], old_map))\n",
    "#         old_inputs = list(inputDist.keys())\n",
    "        old_values = [v for k,v in old_map]\n",
    "        new_inputs = list(map(padInputSequenceWithBoundaries, old_inputs))\n",
    "        assert(list(map(trimBoundariesFromSequence, new_inputs)) == old_inputs)\n",
    "        assert(list(zip(old_inputs, old_values)) == old_map)\n",
    "        new_map = dict(list(zip(new_inputs, old_values)))\n",
    "        inputDist = new_map\n",
    "    \n",
    "    model['inputDist'] = inputDist\n",
    "    model['p(X_0^f)'] = inputDist\n",
    "    \n",
    "    inputs = list(inputDist.keys())\n",
    "    model['inputs'] = inputs\n",
    "    \n",
    "    \n",
    "    inputAlphabet = set(itertools.chain.from_iterable([set(k) for k in inputs ]))\n",
    "#   print(inputAlphabet)\n",
    "#     if pad_with_boundaries and (leftEdge not in inputAlphabet):\n",
    "#         inputAlphabet += [leftEdge]\n",
    "#     if pad_with_boundaries and (rightEdge not in inputAlphabet):\n",
    "#         inputAlphabet += [rightEdge]\n",
    "    model['inputAlphabet'] = inputAlphabet\n",
    "    \n",
    "\n",
    "    prefixes = map(getPrefixes, inputs)\n",
    "    prefixes = set(itertools.chain.from_iterable(prefixes))\n",
    "#   print(\"<= 10 prefixes of lexicon:\")\n",
    "#   print(list(prefixes)[0:11])\n",
    "    model['prefixes'] = prefixes\n",
    "  \n",
    "    \n",
    "    # convenient calculation of p(x_0^i) as pmf, not as 'dist'\n",
    "    def prefixProb(input_prefix):\n",
    "        hasAsPrefix = getHasPrefix(input_prefix)\n",
    "        return P(hasAsPrefix, inputDist)\n",
    "    model['prefixProb'] = prefixProb\n",
    "    model['p(x_0^i)'] = prefixProb\n",
    "    \n",
    "    \n",
    "    # definition of p(X_0^f | x_0^i) as dist\n",
    "    def inputDist_givenPrefix(prefix):\n",
    "        hasPrefix = getHasPrefix(prefix)\n",
    "        return inputDist & hasPrefix\n",
    "    model['inputDist_givenPrefix'] = inputDist_givenPrefix\n",
    "    model['p(X_0^f | x_0^i)'] = inputDist_givenPrefix\n",
    "    \n",
    "    \n",
    "    # definition of p(x_{i+1}^f | x_0^i) as pmf, not as 'dist'\n",
    "    def suffixProb(input_suffix, input_prefix):\n",
    "        #p(X_0^f | x_0^i)\n",
    "        d = inputDist_givenPrefix(input_prefix)\n",
    "        isExactlyEqualTo = lambda s: s == tuple(list(input_prefix) + list(input_suffix))\n",
    "        return P(isExactlyEqualTo, d)\n",
    "    model['suffixProb'] = suffixProb\n",
    "    model['p(x_{i+1}^f | x_0^i)'] = suffixProb\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:29.151702Z",
     "start_time": "2018-03-31T23:44:28.803386Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating functions and values derived from p(Y_i|x_i)...\n",
    "\n",
    "\n",
    "def generateChannelModel(channelOutput_i_dist, inputModel, pad_with_boundaries = None):\n",
    "    if pad_with_boundaries == None:\n",
    "        pad_with_boundaries = False\n",
    "    \n",
    "    model = {} #FIXME consider changing to a namedtuple/frozendict (possibly just before returning a value)\n",
    "    \n",
    "    model['channelOutput_i_dist'] = channelOutput_i_dist\n",
    "    model['p(Y_i|x_i)'] = channelOutput_i_dist\n",
    "    \n",
    "    \n",
    "    # definition of p(Y_i) as a dist\n",
    "    sourceInputAlphabet = inputModel['inputAlphabet']\n",
    "    \n",
    "    \n",
    "    getKeys = lambda d: set(d.keys()) #assumes they're hashable\n",
    "#     getVals = lambda d: set(d.values()) #assumes they're hashable\n",
    "    channelInputAlphabet = getKeys(channelOutput_i_dist)\n",
    "    \n",
    "    getOutputSymbols = lambda cond_channel_dist: getKeys(cond_channel_dist)\n",
    "    outputSymbolsByInput = map(lambda eachInputSymbol: getOutputSymbols(channelOutput_i_dist[eachInputSymbol]), channelOutput_i_dist)\n",
    "    channelOutputAlphabet = reduce(set.union, outputSymbolsByInput)\n",
    "    \n",
    "    totalAlphabet = reduce(set.union, [channelOutputAlphabet, channelInputAlphabet, sourceInputAlphabet])\n",
    "    \n",
    "    \n",
    "#     channelOutput_i_marginal_dist = ProbDist({y: sum(channelOutput_i_dist(x)[y] for x in inputAlphabet) for y in inputAlphabet})\n",
    "    channelOutput_i_marginal_dist = ProbDist({y: sum(channelOutput_i_dist[x].get(y, 0.0) for x in channelInputAlphabet) for y in channelOutputAlphabet})\n",
    "    model['channelOutput_i_marginal_dist'] = channelOutput_i_marginal_dist\n",
    "    model['p(Y_i)'] = channelOutput_i_marginal_dist\n",
    "    \n",
    "    \n",
    "    # definition of p(y_0^j|x_0^i) as a pmf but not as a 'dist', assuming uniphone noise/channel model\n",
    "    prefixes = inputModel['prefixes']\n",
    "    def channelOutput_prefix_prob(inputPrefix, outputPrefix):\n",
    "        if len(outputPrefix) == len(inputPrefix):\n",
    "        #         |y_0^j|    ==      |x_0^i|\n",
    "#             slice_prob = lambda y_i, x_i: P({y_i}, channelOutput_i_dist(x_i))\n",
    "            slice_prob = lambda y_i, x_i: P({y_i}, channelOutput_i_dist[x_i])\n",
    "            return prod(slice_prob(y, x) for y,x in zip(outputPrefix, inputPrefix))\n",
    "        elif len(outputPrefix) < len(inputPrefix): #truncate excess part of inputPrefix\n",
    "                  #|y_0^j|     <      |x_0^i|\n",
    "            return channelOutput_prefix_prob(inputPrefix[:len(outputPrefix)], outputPrefix)\n",
    "        else:\n",
    "                  #|y_0^j|     >      |x_0^i|\n",
    "            hasInputPrefixAsPrefix = getHasPrefix(inputPrefix)\n",
    "            my_prefixes = (pref for pref in prefixes if len(pref) == len(outputPrefix) and hasInputPrefixAsPrefix(pref))\n",
    "            #         p(y_0^j|x_0^j)                                 p(x_0^j)          1   / p(x_0^i)\n",
    "            terms = ((channelOutput_prefix_prob(pref, outputPrefix), inputModel['prefixProb'](pref), 0.0 if inputModel['prefixProb'](inputPrefix) == 0.0 else 1.0 / inputModel['prefixProb'](inputPrefix) ) for pref in my_prefixes)\n",
    "            return sum(map(prod, terms))\n",
    "    model['p(y_0^j|x_0^i)'] = channelOutput_prefix_prob\n",
    "    model['channelOutput_prefix_prob'] = channelOutput_prefix_prob\n",
    "\n",
    "\n",
    "    # definition of sampler from p(Y_0^i|x_0^i), assuming uniphone noise/channel model\n",
    "    def channelOutput_prefix_sampler(inputPrefix):\n",
    "#         return tuple([sampleFrom(channelOutput_i_dist(x_i)) for x_i in inputPrefix])\n",
    "        return tuple([sampleFrom(channelOutput_i_dist[x_i]) for x_i in inputPrefix])\n",
    "    model['sample from p(Y_0^i|x_0^i)'] = channelOutput_prefix_sampler\n",
    "    model['channelOutput_prefix_sampler'] = channelOutput_prefix_sampler\n",
    "\n",
    "    \n",
    "    # calculation of p(y_0^i) as a pmf but not as a 'dist'\n",
    "    #   marginalizes over all prefixes of input sequences with length matching y_0^i \n",
    "    def channelOutput_prefix_marginal_prob(outputPrefix):\n",
    "\n",
    "        l = len(outputPrefix)\n",
    "        my_prefixes = (prefix for prefix in prefixes if len(prefix) == l)\n",
    "\n",
    "        #\\sum_{X_0^i} p(x_0^i) * p(y_0^i|x_0^i)\n",
    "        terms = ((inputModel['prefixProb'](input_prefix), channelOutput_prefix_prob(input_prefix, outputPrefix)) for input_prefix in my_prefixes)\n",
    "        probs = map(prod, terms)\n",
    "    #     probs = (prefixProb(input_prefix) * channelOutput_prefix_prob(input_prefix, outputPrefix) for input_prefix in my_prefixes)\n",
    "    #     probs = [prefixProb(input_prefix) * channelOutput_prefix_prob(input_prefix, outputPrefix) for input_prefix in my_prefixes] #only for testing; use generator exp otherwise\n",
    "    #     print('terms of \\sum_{X_0^i} p(x_0^i) * p(y_0^i|x_0^i):')\n",
    "    #     print(probs)\n",
    "        return sum(probs)\n",
    "    model['p(y_0^i)'] = channelOutput_prefix_marginal_prob\n",
    "    model['channelOutput_prefix_marginal_prob'] = channelOutput_prefix_marginal_prob\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:40.507601Z",
     "start_time": "2018-03-31T23:44:40.332836Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "#Hoeffding's inequality w.r.t. mc estimate p-hat(x-hat_0^j|x_0^i):\n",
    "# Let  X = p-hat(x-hat_0^j|x_0^i)\n",
    "#      𝛍 = p(x-hat_0^j|x_0^i)\n",
    "#      n = the number of samples that go into calculating X\n",
    "# then\n",
    "#      p(|X - 𝛍| > 𝛆) ≤ exp(2n𝛆²)\n",
    "#\n",
    "#  E.g. for n = 1000, 𝛆 = 0.01\n",
    "#      p(|X - 𝛍| > 𝛆) ≤ ≈0.82\n",
    "#\n",
    "#       for n = 1000, 𝛆 = 0.03\n",
    "#     p(|X - 𝛍| > 𝛆) ≤ ≈0.165\n",
    "#\n",
    "#       for n = 1000, 𝛆 = 0.05\n",
    "#     p(|X - 𝛍| > 𝛆) ≤ ≈0.0068\n",
    "#\n",
    "#       for n = 1000, 𝛆 = 0.1\n",
    "#     p(|X - 𝛍| > 𝛆) ≤ ≈2.06*10^-9\n",
    "def hoeffdingProb(epsilon, num_samples):\n",
    "    upperBound = exp(-2.0 * num_samples * epsilon * epsilon)\n",
    "    return upperBound\n",
    "\n",
    "def generateTotalModel(inputDist, channelOutput_i_dist, pad_with_boundaries = None):\n",
    "    if pad_with_boundaries == None:\n",
    "        pad_with_boundaries = False\n",
    "    \n",
    "    inputModel = generateInputModel(inputDist, pad_with_boundaries = pad_with_boundaries)\n",
    "    channelModel = generateChannelModel(channelOutput_i_dist, inputModel, pad_with_boundaries = pad_with_boundaries)\n",
    "    model = {**inputModel, **channelModel} #FIXME consider changing to a namedtuple/frozendict (possibly just before returning a value)\n",
    "\n",
    "    # receiver/listener model\n",
    "    #  = mc estimate of p(x-hat_0^j|x_0^i) as pmf, not as 'dist', assuming uniphone error probs\n",
    "    #                         x_0^i             x-hat_0^j\n",
    "    def est_channelInput_prob(true_inputPrefix, poss_inputPrefix, num_samples = None):\n",
    "        if num_samples == None:\n",
    "            num_samples = 1000\n",
    "\n",
    "    #     print('true x_0^i: {0}'.format(true_inputPrefix))\n",
    "    #     print('x-hat_0^j: {0}'.format(poss_inputPrefix))\n",
    "    #     print('num samples: {0}'.format(num_samples))\n",
    "\n",
    "        lenTruePrefix = len(true_inputPrefix) #|x_0^i|\n",
    "        lenPossPrefix = len(poss_inputPrefix) #|x-hat_0^j|\n",
    "\n",
    "        #if |x-hat_0^j|  < |x_0^i|, then calculate \\sum_\\limits{x-hat_0^i | x-hat_0^j is a prefix} p(x-hat_0^i|x_0^i)\n",
    "        if lenPossPrefix < lenTruePrefix:\n",
    "            hasPossPrefixAsPrefix = getHasPrefix(poss_inputPrefix)\n",
    "            my_prefixes = (pref for pref in model['prefixes'] if len(pref) == lenTruePrefix and hasPossPrefixAsPrefix(pref))\n",
    "            probs = (est_channelInput_prob(true_inputPrefix, pref) for pref in my_prefixes)\n",
    "            return sum(probs)\n",
    "\n",
    "        #if |x-hat_0^j| ≥ |x_0^i|, then proceed 'normally'...\n",
    "\n",
    "        #samples from p(Y_0^i|x_0^i)\n",
    "        samples = sampleFrom(lambda: model['channelOutput_prefix_sampler'](true_inputPrefix), num_samples)\n",
    "    #     samples = list(sampleFrom(lambda: channelOutput_prefix_sampler(true_inputPrefix), num_samples)) # only for debugging - otherwise use generator exp\n",
    "    #     print('samples from p(Y_0^i|x_0^i):')\n",
    "    #     print(samples)\n",
    "    #     likelihoods = [channelOutput_prefix_prob(poss_inputPrefix, outputPrefix) for outputPrefix in samples]\n",
    "    #     print('likelihoods p(y_0^i|x-hat_0^i):')\n",
    "    #     print(likelihoods)\n",
    "    #     priorOfOutputs = [channelOutput_prefix_marginal_prob(outputPrefix) for outputPrefix in samples]\n",
    "    #     print('priorOfOutputs p(y_0^i):')\n",
    "    #     print(priorOfOutputs)\n",
    "\n",
    "        #p(x-hat_0^j)\n",
    "        poss_inputPrefix_prob = model['prefixProb'](poss_inputPrefix)\n",
    "    #     print('p(x-hat_0^j) = {0}'.format( poss_inputPrefix_prob ))\n",
    "\n",
    "        #terms in \\sum_{samples y_0^i from p(Y_0^i|x_0^i)} p(y_0^i|x-hat_0^i) * p(x-hat_0^j) / p(y_0^i)\n",
    "#         terms = (model['channelOutput_prefix_prob'](poss_inputPrefix, outputPrefix)  * poss_inputPrefix_prob / model['channelOutput_prefix_marginal_prob'](outputPrefix) for outputPrefix in samples)\n",
    "        terms = ((model['channelOutput_prefix_prob'](poss_inputPrefix, outputPrefix), 0.0 if model['channelOutput_prefix_marginal_prob'](outputPrefix) == 0.0 else poss_inputPrefix_prob / model['channelOutput_prefix_marginal_prob'](outputPrefix) ) for outputPrefix in samples)\n",
    "    #     for outputPrefix in samples:\n",
    "    #       print('y_0^i: {0}'.format(outputPrefix))\n",
    "    #       print('term: {0} * {1} / {2}'.format(channelOutput_prefix_prob(poss_inputPrefix, outputPrefix), poss_inputPrefix_prob, channelOutput_prefix_marginal_prob(outputPrefix)))\n",
    "    #     terms = [channelOutput_prefix_prob(poss_inputPrefix, outputPrefix)  * poss_inputPrefix_prob / channelOutput_prefix_marginal_prob(outputPrefix) for outputPrefix in samples] #use only for debugging, otherwise use generator exp\n",
    "    #     print('terms in sum:')\n",
    "    #     print(terms)\n",
    "#         est = sum(terms) / num_samples\n",
    "        est = sum(map(prod,terms)) / num_samples\n",
    "\n",
    "        return est\n",
    "    model['p-hat(x-hat_0^j|x_0^i)'] = est_channelInput_prob\n",
    "    model['est_channelInput_prob'] = est_channelInput_prob\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:41.569801Z",
     "start_time": "2018-03-31T23:44:41.545410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericmeinhardt/Google Drive/Ideas/HH/recoverability/c2-jn'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:42.892452Z",
     "start_time": "2018-03-31T23:44:42.719558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD-aligned gate-3 diphone channel distribution.json\r\n",
      "IPhOD-aligned gate-6 diphone channel distribution.json\r\n",
      "IPhOD-aligned gate3 uniphone channel distribution.json\r\n",
      "IPhOD-aligned gate6 uniphone channel distribution.json\r\n",
      "IPhOD-aligned response diphones.txt\r\n",
      "IPhOD-aligned stimuli diphones.txt\r\n",
      "IPhOD-aligned stimuli uniphones.txt\r\n",
      "IPhOD-aligned trials just-gate3.csv\r\n",
      "IPhOD-aligned trials just-gate6.csv\r\n",
      "IPhOD-aligned uniphone channel distribution.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_phonWordToProb.json\r\n",
      "IPhOD_aligned_trials.csv\r\n",
      "IPhOD_noStress_lowercase_IPA_ALLDUPES.json\r\n",
      "IPhOD_noStress_lowercase_IPA_noDupes_consolidated_UTFesc.json\r\n",
      "IPhOD_noStress_lowercase_IPA_noDupes_consolidated_noUTFesc.json\r\n",
      "IPhOD_processed.json\r\n",
      "IPhOD_processed_UTFesc.json\r\n",
      "IPhOD_processed_noUTFesc.json\r\n",
      "IPhODv2.0_REALS.zip\r\n",
      "illegal triphones from IPhOD-aligned response diphones.txt\r\n",
      "illegal triphones from IPhOD-aligned stimuli diphones.txt\r\n",
      "triphones from IPhOD-aligned response diphones.txt\r\n",
      "triphones from IPhOD-aligned stimuli diphones.txt\r\n",
      "\r\n",
      "IPhODv2.0_REALS:\r\n",
      "2009_Dec01_Release_Readme.txt\r\n",
      "CMU_pronunciation_key.pdf\r\n",
      "IPhOD2_Words.txt\r\n",
      "IPhOD2_Words_IPA.csv\r\n",
      "IPhOD2_Words_IPA_prob.csv\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged.csv\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_phonWordToProb.json\r\n",
      "gpl.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T22:49:59.783850Z",
     "start_time": "2018-03-31T22:49:59.652098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammond's mysterious newdic.txt\r\n",
      "Hammond-aligned gate-3 diphone channel distribution.json\r\n",
      "Hammond-aligned gate-6 diphone channel distribution.json\r\n",
      "Hammond-aligned gate3 uniphone channel distribution.json\r\n",
      "Hammond-aligned gate6 uniphone channel distribution.json\r\n",
      "Hammond-aligned response diphones.txt\r\n",
      "Hammond-aligned stimuli diphones.txt\r\n",
      "Hammond-aligned stimuli uniphones.txt\r\n",
      "Hammond-aligned trials just-gate3.csv\r\n",
      "Hammond-aligned trials just-gate6.csv\r\n",
      "Hammond-aligned uniphone channel distribution.json\r\n",
      "Hammond_aligned_trials.csv\r\n",
      "Hammond_newdic_IPA.csv\r\n",
      "Hammond_newdic_IPA_aligned.csv\r\n",
      "Hammond_newdic_IPA_aligned_phonWordToNlprob.json\r\n",
      "Hammond_newdic_IPA_aligned_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:45.151621Z",
     "start_time": "2018-03-31T23:44:45.139328Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniphone_dist_fn_IPhOD = 'IPhOD-aligned uniphone channel distribution.json'\n",
    "uniphone_dist_fn_hammond = 'Hammond-aligned uniphone channel distribution.json'\n",
    "uniphone_dist_fn = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:45.774019Z",
     "start_time": "2018-03-31T23:44:45.766063Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_dist_fn_IPhOD = 'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_phonWordToProb.json'\n",
    "lexicon_dist_fn_hammond = 'Hammond_newdic_IPA_aligned_phonWordToProb.json'\n",
    "lexicon_dist_fn = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:12.045780Z",
     "start_time": "2018-03-31T23:45:12.040120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "which = 'IPhOD'\n",
    "# which = 'hammond'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:20.050556Z",
     "start_time": "2018-03-31T23:45:20.043008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if which == 'IPhOD':\n",
    "    uniphone_dist_fn = uniphone_dist_fn_IPhOD\n",
    "    lexicon_dist_fn = lexicon_dist_fn_IPhOD\n",
    "else:\n",
    "    uniphone_dist_fn = uniphone_dist_fn_hammond\n",
    "    lexicon_dist_fn = lexicon_dist_fn_hammond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:44:57.050570Z",
     "start_time": "2018-03-31T23:44:57.045764Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:29.403319Z",
     "start_time": "2018-03-31T23:45:29.395140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(uniphone_dist_fn, encoding='utf-8') as data_file:\n",
    "   uniphone_dist_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:31.840945Z",
     "start_time": "2018-03-31T23:45:31.826731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'aʊ': 0.0006578947368421052,\n",
       " 'b': 0.006048976608187134,\n",
       " 'd': 0.00034722222222222224,\n",
       " 'dʒ': 0.0006944444444444445,\n",
       " 'eɪ': 0.001388888888888889,\n",
       " 'f': 0.004422514619883041,\n",
       " 'g': 0.0010416666666666669,\n",
       " 'h': 0.01822002923976608,\n",
       " 'i': 0.00034722222222222224,\n",
       " 'k': 0.030957602339181284,\n",
       " 'l': 0.004093567251461988,\n",
       " 'm': 0.0010051169590643274,\n",
       " 'n': 0.0003289473684210526,\n",
       " 'oʊ': 0.0010233918128654971,\n",
       " 'p': 0.8784173976608189,\n",
       " 'r': 0.00034722222222222224,\n",
       " 's': 0.0006944444444444445,\n",
       " 't': 0.003654970760233918,\n",
       " 'tʃ': 0.0006944444444444445,\n",
       " 'u': 0.0003289473684210526,\n",
       " 'v': 0.0030884502923976607,\n",
       " 'w': 0.0006944444444444445,\n",
       " 'ŋ': 0.0006944444444444445,\n",
       " 'ɑ': 0.0013340643274853802,\n",
       " 'ɚ': 0.00034722222222222224,\n",
       " 'ɛ': 0.003801169590643275,\n",
       " 'ɪ': 0.002741228070175439,\n",
       " 'ʃ': 0.0013523391812865497,\n",
       " 'ʌ': 0.02814327485380117,\n",
       " 'θ': 0.0030884502923976603}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(uniphone_dist_in.keys())[0]\n",
    "test_k\n",
    "uniphone_dist_in[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:45.546930Z",
     "start_time": "2018-03-31T23:45:45.540537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniphone_dist = uniphone_dist_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:48.057077Z",
     "start_time": "2018-03-31T23:45:48.050222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ericmeinhardt/Google Drive/Ideas/HH/recoverability/c2-jn/IPhODv2.0_REALS\n"
     ]
    }
   ],
   "source": [
    "# %cd ..\n",
    "%cd IPhODv2.0_REALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:48.927651Z",
     "start_time": "2018-03-31T23:45:48.835396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(lexicon_dist_fn, encoding='utf-8') as data_file:\n",
    "   lexicon_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:50.344300Z",
     "start_time": "2018-03-31T23:45:50.337347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ericmeinhardt/Google Drive/Ideas/HH/recoverability/c2-jn\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:52.729180Z",
     "start_time": "2018-03-31T23:45:52.711342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ʃ.ʊ.r.ə.t.i.⋉'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.4555929067625541e-07"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(lexicon_in.keys())[0]\n",
    "test_k\n",
    "lexicon_in[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:53.411253Z",
     "start_time": "2018-03-31T23:45:53.218464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊', 'ʃ', 'ʊ', 'r', 'ə', 't', 'i', '⋉')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.4555929067625541e-07"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = {dottedStringToTuple(k):lexicon_in[k] for k in lexicon_in}\n",
    "dottedStringToTuple(test_k)\n",
    "lexicon[dottedStringToTuple(test_k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to the required probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:56.439738Z",
     "start_time": "2018-03-31T23:45:56.333287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('⋊',\n",
       "  'd',\n",
       "  'j',\n",
       "  'u',\n",
       "  'p',\n",
       "  'l',\n",
       "  'ə',\n",
       "  'k',\n",
       "  'eɪ',\n",
       "  't',\n",
       "  'ɪ',\n",
       "  'd',\n",
       "  '⋉'): 4.262807798377089e-07,\n",
       " ('⋊',\n",
       "  'f',\n",
       "  'ɑ',\n",
       "  'r',\n",
       "  't',\n",
       "  'ə',\n",
       "  'f',\n",
       "  'ə',\n",
       "  'k',\n",
       "  'eɪ',\n",
       "  'ʃ',\n",
       "  'ə',\n",
       "  'n',\n",
       "  '⋉'): 1.0397092191163634e-07,\n",
       " ('⋊', 'h', 'ɑ', 'ŋ', 'k', 'ɪ', 'ŋ', '⋉'): 3.1607160261137446e-06,\n",
       " ('⋊', 'k', 'æ', 'b', 'n', 'ə', 't', '⋉'): 4.330388897619654e-06,\n",
       " ('⋊',\n",
       "  'm',\n",
       "  'ə',\n",
       "  't',\n",
       "  'ɪ',\n",
       "  'r',\n",
       "  'i',\n",
       "  'ə',\n",
       "  'l',\n",
       "  'aɪ',\n",
       "  'z',\n",
       "  'ɪ',\n",
       "  'z',\n",
       "  '⋉'): 6.23825531469818e-08,\n",
       " ('⋊', 'r', 'æ', 'd', 'm', 'ə', 'n', '⋉'): 2.079418438232727e-08,\n",
       " ('⋊',\n",
       "  's',\n",
       "  'ɛ',\n",
       "  'k',\n",
       "  'ə',\n",
       "  'n',\n",
       "  'd',\n",
       "  'ɛ',\n",
       "  'r',\n",
       "  'i',\n",
       "  '⋉'): 4.262807798377089e-06,\n",
       " ('⋊',\n",
       "  's',\n",
       "  'ʌ',\n",
       "  'f',\n",
       "  'ə',\n",
       "  'k',\n",
       "  'eɪ',\n",
       "  'ʃ',\n",
       "  'ə',\n",
       "  'n',\n",
       "  '⋉'): 7.797819143372725e-07,\n",
       " ('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i', '⋉'): 5.302517017493452e-07,\n",
       " ('⋊', 'ə', 'n', 't', 'æ', 'ŋ', 'l', 'ɪ', 'ŋ', '⋉'): 1.0397092191163634e-07}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(X_0^f)\n",
    "inputDist = ProbDist(lexicon)\n",
    "\n",
    "phonWords = set(inputDist.keys())\n",
    "someWords = list(phonWords)[:10]\n",
    "{w:inputDist[w] for w in someWords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorations / asides / sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How many words in the lexicon have probability 0? (The answer should be that zero words do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:58.842733Z",
     "start_time": "2018-03-31T23:45:58.786365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroWords = {k for k in phonWords if inputDist[k] == 0}\n",
    "len(zeroWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How many words are there of each length (in phonemes, i.e. not counting word edge symbols.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLength(word):\n",
    "    '''\n",
    "    Returns the length of the word in a lexicon distribution, not including edgesymbols.\n",
    "    '''\n",
    "    return len(word) - 2\n",
    "\n",
    "len(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i', '⋉'))\n",
    "getLength(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i', '⋉'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myWords = list(inputDist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = list(map(getLength, myWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7828168794767985"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean, median\n",
    "mean(lengths)\n",
    "median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(6, 9137),\n",
       " (5, 8185),\n",
       " (7, 8153),\n",
       " (8, 6641),\n",
       " (4, 5556),\n",
       " (9, 4605),\n",
       " (10, 2940),\n",
       " (3, 2310),\n",
       " (11, 1783),\n",
       " (12, 1027),\n",
       " (13, 464),\n",
       " (2, 291),\n",
       " (14, 166),\n",
       " (15, 65),\n",
       " (16, 29),\n",
       " (1, 13),\n",
       " (17, 8),\n",
       " (18, 1),\n",
       " (20, 1),\n",
       " (28, 1)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthCounter = Counter(lengths)\n",
    "lengthCounter.keys()\n",
    "lengthCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "50337"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "51376"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_with_more_than_1000_words = {k for k in lengthCounter.keys() if lengthCounter[k] > 1000}\n",
    "len(lengths_with_more_than_1000_words)\n",
    "min(lengths_with_more_than_1000_words)\n",
    "max(lengths_with_more_than_1000_words)\n",
    "all(l >= min(lengths_with_more_than_1000_words) and l <= max(lengths_with_more_than_1000_words) for l in lengths_with_more_than_1000_words)\n",
    "sum(lengthCounter[k] for k in lengths_with_more_than_1000_words)\n",
    "sum(lengthCounter[k] for k in lengthCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 98% of the pronounciations in IPhOD are 3-12 phonemes long.\n",
    " - Just about 50% of the pronounciations in IPhOD are either 5, 6, or 7 phonemes long.\n",
    " - About 16% of the pronounciations in IPhOD are 4 phonemes long or shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME plotnine plot of length of word type vs. count of word types with that length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths = list(range(min(lengths) + 2, max(lengths) + 2 + 1))\n",
    "len(word_lengths)\n",
    "words_by_length = {l:[w for w in myWords if len(w) == l] for l in word_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('⋊',\n",
       "  'd',\n",
       "  'i',\n",
       "  'ɪ',\n",
       "  'n',\n",
       "  's',\n",
       "  't',\n",
       "  'ɪ',\n",
       "  't',\n",
       "  'u',\n",
       "  'ʃ',\n",
       "  'ə',\n",
       "  'n',\n",
       "  'ə',\n",
       "  'l',\n",
       "  'ə',\n",
       "  'z',\n",
       "  'eɪ',\n",
       "  'ʃ',\n",
       "  'ə',\n",
       "  'n',\n",
       "  '⋉')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_by_length[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How many unique prefixes of each length are there (not counting edge symbols, again)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51376"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "162198"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prefixes = list(map(getPrefixes, myWords))\n",
    "my_prefixes = set(itertools.chain.from_iterable(my_prefixes))\n",
    "len(myWords)\n",
    "len(my_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPrefixLength(prefix):\n",
    "    if prefix[0] == leftEdge:\n",
    "        adjustment = -1\n",
    "    if prefix[-1] == rightEdge:\n",
    "        adjustment = -2\n",
    "    return len(prefix) + adjustment\n",
    "len(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i', '⋉'))\n",
    "getPrefixLength(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i', '⋉'))\n",
    "len(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i'))\n",
    "getPrefixLength(('⋊', 'æ', 'b', 'ə', 'l', 'oʊ', 'n', 'i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.562713473655656"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixLengths = list(map(getPrefixLength, my_prefixes))\n",
    "mean(prefixLengths)\n",
    "median(prefixLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(6, 29673),\n",
       " (5, 29127),\n",
       " (7, 25047),\n",
       " (4, 20596),\n",
       " (8, 19251),\n",
       " (9, 13023),\n",
       " (10, 8076),\n",
       " (3, 7368),\n",
       " (11, 4726),\n",
       " (12, 2546),\n",
       " (13, 1120),\n",
       " (2, 913),\n",
       " (14, 411),\n",
       " (15, 164),\n",
       " (16, 68),\n",
       " (1, 51),\n",
       " (17, 19),\n",
       " (18, 4),\n",
       " (20, 3),\n",
       " (19, 2),\n",
       " (28, 2),\n",
       " (0, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "162198"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_lengthCounter = Counter(prefixLengths)\n",
    "prefix_lengthCounter.keys()\n",
    "prefix_lengthCounter.most_common()\n",
    "sum(prefix_lengthCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ≈94% of all unique prefixes in IPhOD are 3-10 phonemes long.\n",
    " - ≈84% of all unqiue prefixes in IPhOD are 4-9 phonemes long.\n",
    " - ≈52% of all unique prefixes in IPhOD are 5-7 phonemes long.\n",
    " - ≈18% of all unique prefixes (= prefix types) in IPhOD are 1-4 phonemes long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('⋊',\n",
       "  'æ',\n",
       "  'n',\n",
       "  't',\n",
       "  'aɪ',\n",
       "  'd',\n",
       "  'ɪ',\n",
       "  's',\n",
       "  'ə',\n",
       "  's',\n",
       "  't',\n",
       "  'æ',\n",
       "  'b',\n",
       "  'l',\n",
       "  'ɪ',\n",
       "  'ʃ',\n",
       "  'm',\n",
       "  'ə',\n",
       "  'n',\n",
       "  't',\n",
       "  'ɛ',\n",
       "  'r',\n",
       "  'i',\n",
       "  'ə')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_lengths = list(range(min(prefixLengths), max(lengths) + 2 + 1))\n",
    "len(prefix_lengths)\n",
    "prefixes_by_length = {l:[p for p in my_prefixes if len(p) == l] for l in prefix_lengths}\n",
    "prefixes_by_length[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How many neighbors does each unique full word and each unique prefix have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_shared_indices(wordA, wordB):\n",
    "    '''\n",
    "    Given two sequences, this function returns the number of indices where wordA and wordB have the same values. \n",
    "    '''\n",
    "    #NB if one word is longer than the other, zip truncates the extra values in the longer word - which is just fine.\n",
    "    equalVals = [pairedVals[0] == pairedVals[1] for pairedVals in zip(wordA, wordB)]\n",
    "    return sum(equalVals)\n",
    "\n",
    "def hammingDistance(wordA, wordB):\n",
    "    '''\n",
    "    Returns the Hamming distance between two sequences, where the Hamming distance between two words of unequal length is stipulated as undefined, which is to say 'None'.\n",
    "    '''\n",
    "    if len(wordA) != len(wordB):\n",
    "        return None\n",
    "    differentIndices = len(wordA) - num_shared_indices(wordA, wordB)\n",
    "    return differentIndices\n",
    "\n",
    "def isNeighbor(targetWord, possibleNeighbor, distanceThreshold = 1):\n",
    "    distance = hammingDistance(targetWord, possibleNeighbor)\n",
    "    if distance == None:\n",
    "        return False\n",
    "    return distance <= distanceThreshold\n",
    "\n",
    "def getNeighbors(targetWord, words, distanceThreshold = 1):\n",
    "#     sameLengthWords = [w for w in words if len(w) == len(targetWord)]\n",
    "#     return [w for w in sameLengthWords if isNeighbor(targetWord, w, distanceThreshold) and targetWord != w]\n",
    "    return [w for w in words if isNeighbor(targetWord, w, distanceThreshold) and targetWord != w]\n",
    "\n",
    "def numNeighbors(targetWord, words, distanceThreshold = 1):\n",
    "    return len(getNeighbors(targetWord, words, distanceThreshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3258330738087825"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 21456),\n",
       " (1, 10445),\n",
       " (2, 4535),\n",
       " (3, 2540),\n",
       " (4, 1718),\n",
       " (5, 1344),\n",
       " (6, 1156),\n",
       " (7, 918),\n",
       " (8, 784),\n",
       " (9, 696),\n",
       " (10, 631),\n",
       " (11, 524),\n",
       " (12, 508),\n",
       " (13, 380),\n",
       " (15, 344),\n",
       " (14, 337),\n",
       " (16, 312),\n",
       " (17, 273),\n",
       " (18, 257),\n",
       " (19, 223),\n",
       " (20, 187),\n",
       " (21, 180),\n",
       " (22, 170),\n",
       " (24, 153),\n",
       " (23, 148),\n",
       " (25, 137),\n",
       " (27, 129),\n",
       " (28, 128),\n",
       " (26, 124),\n",
       " (29, 119),\n",
       " (32, 101),\n",
       " (30, 97),\n",
       " (31, 76),\n",
       " (33, 66),\n",
       " (34, 36),\n",
       " (35, 35),\n",
       " (37, 29),\n",
       " (36, 26),\n",
       " (38, 21),\n",
       " (39, 16),\n",
       " (40, 6),\n",
       " (41, 6),\n",
       " (43, 4),\n",
       " (42, 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "51376"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullWord_neighborCounts = [numNeighbors(w, words_by_length[len(w)]) for w in myWords]\n",
    "mean(fullWord_neighborCounts)\n",
    "median(fullWord_neighborCounts)\n",
    "fullWord_neighborCounter = Counter(fullWord_neighborCounts)\n",
    "fullWord_neighborCounter.keys()\n",
    "fullWord_neighborCounter.most_common()\n",
    "sum(fullWord_neighborCounter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.020542793375998"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 45698),\n",
       " (1, 33872),\n",
       " (2, 18003),\n",
       " (3, 11104),\n",
       " (4, 7668),\n",
       " (5, 5686),\n",
       " (6, 4440),\n",
       " (7, 3531),\n",
       " (8, 2943),\n",
       " (9, 2542),\n",
       " (10, 2269),\n",
       " (11, 2006),\n",
       " (12, 1924),\n",
       " (13, 1685),\n",
       " (14, 1502),\n",
       " (15, 1344),\n",
       " (16, 1218),\n",
       " (17, 1093),\n",
       " (18, 1002),\n",
       " (19, 1001),\n",
       " (20, 862),\n",
       " (22, 813),\n",
       " (21, 800),\n",
       " (23, 733),\n",
       " (24, 657),\n",
       " (26, 607),\n",
       " (25, 587),\n",
       " (27, 554),\n",
       " (28, 491),\n",
       " (31, 447),\n",
       " (29, 433),\n",
       " (30, 409),\n",
       " (33, 383),\n",
       " (32, 372),\n",
       " (35, 346),\n",
       " (37, 324),\n",
       " (34, 307),\n",
       " (36, 291),\n",
       " (38, 284),\n",
       " (40, 259),\n",
       " (41, 251),\n",
       " (39, 238),\n",
       " (42, 207),\n",
       " (43, 192),\n",
       " (44, 181),\n",
       " (45, 134),\n",
       " (46, 127),\n",
       " (47, 117),\n",
       " (48, 63),\n",
       " (49, 49),\n",
       " (50, 39),\n",
       " (51, 32),\n",
       " (52, 30),\n",
       " (53, 11),\n",
       " (55, 11),\n",
       " (56, 8),\n",
       " (54, 7),\n",
       " (57, 4),\n",
       " (59, 3),\n",
       " (60, 2),\n",
       " (58, 1),\n",
       " (61, 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "162198"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_neighborCounts = [numNeighbors(p, prefixes_by_length[len(p)]) for p in my_prefixes]\n",
    "mean(prefix_neighborCounts)\n",
    "median(prefix_neighborCounts)\n",
    "prefix_neighborCounter = Counter(prefix_neighborCounts)\n",
    "prefix_neighborCounter.keys()\n",
    "prefix_neighborCounter.most_common()\n",
    "sum(prefix_neighborCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trim the lexicon down to 10 words for the sake of inspectability throughout testing in this section/immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:45:59.677838Z",
     "start_time": "2018-03-31T23:45:59.666372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbDist({('⋊',\n",
       "           'b',\n",
       "           'oʊ',\n",
       "           'l',\n",
       "           'ʃ',\n",
       "           'ə',\n",
       "           'v',\n",
       "           'ɪ',\n",
       "           'k',\n",
       "           's',\n",
       "           '⋉'): 0.006997455470737914,\n",
       "          ('⋊', 'k', 'oʊ', 'k', 'oʊ', '⋉'): 0.31075063613231546,\n",
       "          ('⋊',\n",
       "           'k',\n",
       "           'ɑ',\n",
       "           'r',\n",
       "           'b',\n",
       "           'j',\n",
       "           'ɚ',\n",
       "           'i',\n",
       "           'ʃ',\n",
       "           'ə',\n",
       "           'n',\n",
       "           '⋉'): 0.0006361323155216284,\n",
       "          ('⋊',\n",
       "           'k',\n",
       "           'ə',\n",
       "           'm',\n",
       "           'p',\n",
       "           'r',\n",
       "           'ɛ',\n",
       "           's',\n",
       "           't',\n",
       "           '⋉'): 0.028625954198473278,\n",
       "          ('⋊',\n",
       "           'n',\n",
       "           'æ',\n",
       "           'n',\n",
       "           'oʊ',\n",
       "           's',\n",
       "           'ɛ',\n",
       "           'k',\n",
       "           'ə',\n",
       "           'n',\n",
       "           'd',\n",
       "           '⋉'): 0.004452926208651399,\n",
       "          ('⋊', 'p', 'r', 'ɑ', 's', 'ɛ', 's', 't', '⋉'): 0.07983460559796436,\n",
       "          ('⋊', 's', 'k', 'i', 'm', 'z', '⋉'): 0.05597964376590331,\n",
       "          ('⋊', 's', 'ɪ', 'ŋ', 'ɚ', '⋉'): 0.4990458015267174,\n",
       "          ('⋊',\n",
       "           'ɪ',\n",
       "           'n',\n",
       "           'ɑ',\n",
       "           'k',\n",
       "           'j',\n",
       "           'ə',\n",
       "           'l',\n",
       "           'eɪ',\n",
       "           't',\n",
       "           '⋉'): 0.011132315521628498,\n",
       "          ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉'): 0.0025445292620865138})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's trim the lexicon down to 10 words for the sake of inspectability\n",
    "lexicon_actual = inputDist\n",
    "inputDist = ProbDist({w:inputDist[w] for w in someWords})\n",
    "inputDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:01.886518Z",
     "start_time": "2018-03-31T23:46:01.863549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋊.s.k.i.m.z.⋉: 0.05597964376590331\n",
      "⋊.θ.r.ʌ.s.s.⋉: 0.0025445292620865138\n",
      "⋊.s.ɪ.ŋ.ɚ.⋉: 0.4990458015267174\n",
      "⋊.k.ɑ.r.b.j.ɚ.i.ʃ.ə.n.⋉: 0.0006361323155216284\n",
      "⋊.p.r.ɑ.s.ɛ.s.t.⋉: 0.07983460559796436\n",
      "⋊.b.oʊ.l.ʃ.ə.v.ɪ.k.s.⋉: 0.006997455470737914\n",
      "⋊.k.ə.m.p.r.ɛ.s.t.⋉: 0.028625954198473278\n",
      "⋊.n.æ.n.oʊ.s.ɛ.k.ə.n.d.⋉: 0.004452926208651399\n",
      "⋊.k.oʊ.k.oʊ.⋉: 0.31075063613231546\n",
      "⋊.ɪ.n.ɑ.k.j.ə.l.eɪ.t.⋉: 0.011132315521628498\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('⋊', 's', 'k', 'i', 'm', 'z', '⋉')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.05597964376590331"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputDist)\n",
    "q = list(inputDist.keys())[0]\n",
    "q\n",
    "type(q)\n",
    "inputDist[q]\n",
    "type(inputDist[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:02.607107Z",
     "start_time": "2018-03-31T23:46:02.568616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aɪ'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ProbDist({'aɪ': 0.8234302484302483,\n",
       "          'aʊ': 0.0033783783783783825,\n",
       "          'b': 0.0012643325143325162,\n",
       "          'd': 0.0013513513513513532,\n",
       "          'eɪ': 0.11726214851214868,\n",
       "          'f': 0.00037878787878787933,\n",
       "          'g': 0.00037878787878787933,\n",
       "          'h': 0.0016635954135954156,\n",
       "          'i': 0.001515151515151517,\n",
       "          'j': 0.0020833333333333363,\n",
       "          'n': 0.00037878787878787933,\n",
       "          'oʊ': 0.0003378378378378383,\n",
       "          'r': 0.0029484029484029527,\n",
       "          't': 0.0007575757575757587,\n",
       "          'u': 0.00018939393939393966,\n",
       "          'æ': 0.005023205023205031,\n",
       "          'ð': 0.0003378378378378383,\n",
       "          'ŋ': 0.00037878787878787933,\n",
       "          'ɑ': 0.02136568386568389,\n",
       "          'ɔɪ': 0.0043355855855855925,\n",
       "          'ɚ': 0.0005272317772317779,\n",
       "          'ɛ': 0.0026566339066339106,\n",
       "          'ɪ': 0.0025081900081900116,\n",
       "          'ʌ': 0.00445331695331696,\n",
       "          'ʒ': 0.0003378378378378383,\n",
       "          'θ': 0.0007575757575757587})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(uniphone_dist_in.keys())[0]\n",
    "test_k\n",
    "\n",
    "uniphone_dist_in.update({rightEdge:{rightEdge: 1.0}})\n",
    "uniphone_dist_in.update({leftEdge:{leftEdge: 1.0}})\n",
    "\n",
    "#P(Y_i|X_i) as a dict mapping to ProbDists\n",
    "channelOutput_i_dist = {phone:ProbDist(uniphone_dist[phone]) for phone in uniphone_dist}\n",
    "channelOutput_i_dist[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:04.105589Z",
     "start_time": "2018-03-31T23:46:04.094647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Y_i|X_i = p):\n",
      "'d': 0.000347222222222222\n",
      "'p': 0.8784173976608184\n",
      "'tʃ': 0.000694444444444444\n",
      "'n': 0.0003289473684210524\n",
      "'θ': 0.003088450292397658\n",
      "'u': 0.0003289473684210524\n",
      "'ʃ': 0.0013523391812865489\n",
      "'l': 0.004093567251461985\n",
      "'b': 0.006048976608187129\n",
      "'m': 0.0010051169590643268\n",
      "'h': 0.01822002923976607\n",
      "'r': 0.000347222222222222\n",
      "'ŋ': 0.000694444444444444\n",
      "'oʊ': 0.0010233918128654965\n",
      "'k': 0.030957602339181264\n",
      "'s': 0.000694444444444444\n",
      "'t': 0.0036549707602339153\n",
      "'v': 0.0030884502923976586\n",
      "'ʌ': 0.028143274853801154\n",
      "'f': 0.004422514619883039\n",
      "'aʊ': 0.0006578947368421048\n",
      "'ɚ': 0.000347222222222222\n",
      "'w': 0.000694444444444444\n",
      "'i': 0.000347222222222222\n",
      "'eɪ': 0.001388888888888888\n",
      "'g': 0.0010416666666666662\n",
      "'dʒ': 0.000694444444444444\n",
      "'ɑ': 0.0013340643274853793\n",
      "'ɛ': 0.003801169590643272\n",
      "'ɪ': 0.002741228070175437\n",
      "\n",
      "p(Y_i|X_i = b):\n",
      "'d': 0.0264285714285714\n",
      "'p': 0.15612781954887195\n",
      "'tʃ': 0.001428571428571427\n",
      "'n': 0.005357142857142851\n",
      "'θ': 0.007142857142857135\n",
      "'z': 0.00035714285714285676\n",
      "'l': 0.004943609022556385\n",
      "'j': 0.0007142857142857135\n",
      "'m': 0.012857142857142845\n",
      "'ɪ': 0.015046992481202993\n",
      "'ŋ': 0.002142857142857141\n",
      "'t': 0.002142857142857141\n",
      "'æ': 0.0019736842105263137\n",
      "'ʊ': 0.0027725563909774408\n",
      "'ʌ': 0.03374999999999997\n",
      "'ɚ': 0.0013439849624060135\n",
      "'i': 0.002142857142857141\n",
      "'g': 0.02351503759398494\n",
      "'v': 0.023214285714285694\n",
      "'ɛ': 0.003486842105263154\n",
      "'ʒ': 0.0010432330827067659\n",
      "'u': 0.002499999999999997\n",
      "'w': 0.006071428571428565\n",
      "'ð': 0.0035150375939849584\n",
      "'h': 0.00848684210526315\n",
      "'r': 0.0030451127819548836\n",
      "'oʊ': 0.000686090225563909\n",
      "'k': 0.002471804511278193\n",
      "'ɑ': 0.004802631578947364\n",
      "'f': 0.002744360902255636\n",
      "'aʊ': 0.00035714285714285676\n",
      "'b': 0.6370300751879702\n",
      "'eɪ': 0.00035714285714285676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('p(Y_i|X_i = p):')\n",
    "print(channelOutput_i_dist['p'])\n",
    "print('p(Y_i|X_i = b):')\n",
    "print(channelOutput_i_dist['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** A peculiarity of the diphone gating trial data is that while [ə] is indeed a segment recognized in the original dataset, they didn't ask participants to differentiate it from [ʌ] -- indeed, North American/non-British linguists typically consider these to be stress variants of each other rather than distinct phonemes in (at least) N. American English.  One result of this is that there are exactly zero trials in which participants identified [ə] as [ə]. *In terms of the code below, this means that $p(Y = [ə]|X = [ə]) = 0$.* \n",
    "\n",
    "This matters because, in the code below, I often examine conditional distributions -- e.g. $p(y_0^j|x_0^i)$ or $p(\\hat{x}_0^j|x_0^i)$ where one of the two strings/events in question is a prefix of the other -- sometimes identical. The fact that $p(Y = [ə]|X = [ə]) = 0$ means that $p(y_0^j = s|x_0^i = s) = 0$ if [ə] is in the produced prefix $s$. *Don't be spooked by seeing zero probabilities like this - it's not evidence that something, somewhere is wrong.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:05.166565Z",
     "start_time": "2018-03-31T23:46:05.154677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbDist({'aɪ': 0.025272556390977443,\n",
       "          'aʊ': 0.005557644110275688,\n",
       "          'b': 0.006278195488721804,\n",
       "          'd': 0.002086466165413534,\n",
       "          'eɪ': 0.1606766917293233,\n",
       "          'f': 0.002261904761904762,\n",
       "          'g': 0.0014285714285714286,\n",
       "          'h': 0.003593358395989975,\n",
       "          'i': 0.005357142857142857,\n",
       "          'j': 0.0008959899749373433,\n",
       "          'n': 0.0010150375939849624,\n",
       "          'oʊ': 0.044251253132832055,\n",
       "          'p': 0.0006578947368421052,\n",
       "          'r': 0.0026879699248120296,\n",
       "          's': 0.0002380952380952381,\n",
       "          't': 0.0013721804511278195,\n",
       "          'u': 0.002086466165413534,\n",
       "          'w': 0.0003289473684210526,\n",
       "          'æ': 0.0019392230576441103,\n",
       "          'ɑ': 0.05714598997493732,\n",
       "          'ɔɪ': 0.030375939849624056,\n",
       "          'ɚ': 0.04531954887218044,\n",
       "          'ɛ': 0.10053884711779446,\n",
       "          'ɪ': 0.08782581453634083,\n",
       "          'ʊ': 0.044047619047619044,\n",
       "          'ʌ': 0.36676065162907273})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channelOutput_i_dist['ə']\n",
    "'ə' in channelOutput_i_dist['ə']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:07.378631Z",
     "start_time": "2018-03-31T23:46:07.225009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DERIVED VALUES/FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'channelOutput_i_dist': {'aɪ': ProbDist({'aɪ': 0.8234302484302483,\n",
       "            'aʊ': 0.0033783783783783825,\n",
       "            'b': 0.0012643325143325162,\n",
       "            'd': 0.0013513513513513532,\n",
       "            'eɪ': 0.11726214851214868,\n",
       "            'f': 0.00037878787878787933,\n",
       "            'g': 0.00037878787878787933,\n",
       "            'h': 0.0016635954135954156,\n",
       "            'i': 0.001515151515151517,\n",
       "            'j': 0.0020833333333333363,\n",
       "            'n': 0.00037878787878787933,\n",
       "            'oʊ': 0.0003378378378378383,\n",
       "            'r': 0.0029484029484029527,\n",
       "            't': 0.0007575757575757587,\n",
       "            'u': 0.00018939393939393966,\n",
       "            'æ': 0.005023205023205031,\n",
       "            'ð': 0.0003378378378378383,\n",
       "            'ŋ': 0.00037878787878787933,\n",
       "            'ɑ': 0.02136568386568389,\n",
       "            'ɔɪ': 0.0043355855855855925,\n",
       "            'ɚ': 0.0005272317772317779,\n",
       "            'ɛ': 0.0026566339066339106,\n",
       "            'ɪ': 0.0025081900081900116,\n",
       "            'ʌ': 0.00445331695331696,\n",
       "            'ʒ': 0.0003378378378378383,\n",
       "            'θ': 0.0007575757575757587}),\n",
       "  'aʊ': ProbDist({'aɪ': 0.002116454689984105,\n",
       "            'aʊ': 0.8571641494435609,\n",
       "            'b': 0.0010135135135135153,\n",
       "            'd': 0.0007054848966613682,\n",
       "            'dʒ': 0.00110294117647059,\n",
       "            'eɪ': 0.0006127450980392167,\n",
       "            'f': 0.00036764705882352995,\n",
       "            'g': 0.000551470588235295,\n",
       "            'h': 0.0010433227344992066,\n",
       "            'i': 0.0010731319554848982,\n",
       "            'j': 0.000551470588235295,\n",
       "            'l': 0.002053524112347645,\n",
       "            'm': 0.00036764705882352995,\n",
       "            'n': 0.0007352941176470599,\n",
       "            'oʊ': 0.025635930047694783,\n",
       "            'r': 0.0015798887122416558,\n",
       "            's': 0.00036764705882352995,\n",
       "            't': 0.0018498277689454187,\n",
       "            'tʃ': 0.00036764705882352995,\n",
       "            'u': 0.002038619501854799,\n",
       "            'v': 0.00036764705882352995,\n",
       "            'w': 0.000551470588235295,\n",
       "            'æ': 0.013364467408585075,\n",
       "            'ŋ': 0.00110294117647059,\n",
       "            'ɑ': 0.038925874403815626,\n",
       "            'ɔɪ': 0.005975092739798632,\n",
       "            'ɚ': 0.0034479332273449964,\n",
       "            'ɛ': 0.011979994700582957,\n",
       "            'ɪ': 0.0005365659777424491,\n",
       "            'ʃ': 0.00036764705882352995,\n",
       "            'ʊ': 0.00110294117647059,\n",
       "            'ʌ': 0.020243773184949687,\n",
       "            'ʒ': 0.00036764705882352995,\n",
       "            'θ': 0.00036764705882352995}),\n",
       "  'b': ProbDist({'aʊ': 0.00035714285714285676,\n",
       "            'b': 0.6370300751879702,\n",
       "            'd': 0.0264285714285714,\n",
       "            'eɪ': 0.00035714285714285676,\n",
       "            'f': 0.002744360902255636,\n",
       "            'g': 0.02351503759398494,\n",
       "            'h': 0.00848684210526315,\n",
       "            'i': 0.002142857142857141,\n",
       "            'j': 0.0007142857142857135,\n",
       "            'k': 0.002471804511278193,\n",
       "            'l': 0.004943609022556385,\n",
       "            'm': 0.012857142857142845,\n",
       "            'n': 0.005357142857142851,\n",
       "            'oʊ': 0.000686090225563909,\n",
       "            'p': 0.15612781954887195,\n",
       "            'r': 0.0030451127819548836,\n",
       "            't': 0.002142857142857141,\n",
       "            'tʃ': 0.001428571428571427,\n",
       "            'u': 0.002499999999999997,\n",
       "            'v': 0.023214285714285694,\n",
       "            'w': 0.006071428571428565,\n",
       "            'z': 0.00035714285714285676,\n",
       "            'æ': 0.0019736842105263137,\n",
       "            'ð': 0.0035150375939849584,\n",
       "            'ŋ': 0.002142857142857141,\n",
       "            'ɑ': 0.004802631578947364,\n",
       "            'ɚ': 0.0013439849624060135,\n",
       "            'ɛ': 0.003486842105263154,\n",
       "            'ɪ': 0.015046992481202993,\n",
       "            'ʊ': 0.0027725563909774408,\n",
       "            'ʌ': 0.03374999999999997,\n",
       "            'ʒ': 0.0010432330827067659,\n",
       "            'θ': 0.007142857142857135}),\n",
       "  'd': ProbDist({'aɪ': 0.000714285714285714,\n",
       "            'aʊ': 0.001015037593984962,\n",
       "            'b': 0.005902255639097741,\n",
       "            'd': 0.6806484962406019,\n",
       "            'dʒ': 0.0027443609022556378,\n",
       "            'eɪ': 0.001071428571428571,\n",
       "            'f': 0.001973684210526315,\n",
       "            'g': 0.012387218045112777,\n",
       "            'h': 0.005601503759398494,\n",
       "            'i': 0.001428571428571428,\n",
       "            'k': 0.013703007518796986,\n",
       "            'm': 0.002086466165413533,\n",
       "            'n': 0.004971804511278193,\n",
       "            'p': 0.00620300751879699,\n",
       "            'r': 0.01738721804511278,\n",
       "            's': 0.00032894736842105246,\n",
       "            't': 0.18222744360902243,\n",
       "            'tʃ': 0.002030075187969924,\n",
       "            'u': 0.0013721804511278189,\n",
       "            'v': 0.003101503759398495,\n",
       "            'w': 0.0006578947368421049,\n",
       "            'æ': 0.00032894736842105246,\n",
       "            'ð': 0.004887218045112779,\n",
       "            'ŋ': 0.001428571428571428,\n",
       "            'ɑ': 0.005733082706766914,\n",
       "            'ɚ': 0.00032894736842105246,\n",
       "            'ɛ': 0.004501879699248118,\n",
       "            'ɪ': 0.005930451127819546,\n",
       "            'ʊ': 0.001428571428571428,\n",
       "            'ʌ': 0.019859022556390972,\n",
       "            'ʒ': 0.000714285714285714,\n",
       "            'θ': 0.007302631578947364}),\n",
       "  'dʒ': ProbDist({'b': 0.0024574303405572743,\n",
       "            'd': 0.11546052631578935,\n",
       "            'dʒ': 0.5620162538699691,\n",
       "            'eɪ': 0.001760835913312693,\n",
       "            'g': 0.024613003095975226,\n",
       "            'h': 0.0007352941176470585,\n",
       "            'i': 0.00036764705882352924,\n",
       "            'l': 0.0007352941176470585,\n",
       "            'n': 0.0007352941176470585,\n",
       "            'oʊ': 0.001470588235294117,\n",
       "            'r': 0.0007352941176470585,\n",
       "            't': 0.016950464396284823,\n",
       "            'tʃ': 0.23407507739938088,\n",
       "            'u': 0.0014318885448916404,\n",
       "            'w': 0.0007352941176470585,\n",
       "            'ð': 0.005979102167182659,\n",
       "            'ŋ': 0.0007352941176470585,\n",
       "            'ɑ': 0.00036764705882352924,\n",
       "            'ɛ': 0.001470588235294117,\n",
       "            'ɪ': 0.009152476780185754,\n",
       "            'ʃ': 0.003889318885448915,\n",
       "            'ʊ': 0.0007352941176470585,\n",
       "            'ʌ': 0.0052825077399380775,\n",
       "            'ʒ': 0.005901702786377706,\n",
       "            'θ': 0.002205882352941176}),\n",
       "  'eɪ': ProbDist({'aɪ': 0.012286036036036046,\n",
       "            'aʊ': 0.00033783783783783813,\n",
       "            'b': 0.005912162162162167,\n",
       "            'd': 0.0008204633204633211,\n",
       "            'dʒ': 0.0010714285714285723,\n",
       "            'eɪ': 0.9221492921492919,\n",
       "            'f': 0.0003571428571428574,\n",
       "            'h': 0.0021541184041184063,\n",
       "            'i': 0.006618404118404125,\n",
       "            'j': 0.0017760617760617773,\n",
       "            'k': 0.0003571428571428574,\n",
       "            'n': 0.0003571428571428574,\n",
       "            'oʊ': 0.00033783783783783813,\n",
       "            'p': 0.00033783783783783813,\n",
       "            'r': 0.0003571428571428574,\n",
       "            's': 0.00033783783783783813,\n",
       "            't': 0.002461389961389963,\n",
       "            'u': 0.0007319819819819826,\n",
       "            'v': 0.0002252252252252254,\n",
       "            'æ': 0.004200450450450454,\n",
       "            'ð': 0.0005823680823680828,\n",
       "            'ɑ': 0.004000965250965255,\n",
       "            'ɔɪ': 0.0008638996138996147,\n",
       "            'ɚ': 0.0002252252252252254,\n",
       "            'ɛ': 0.019842342342342358,\n",
       "            'ɪ': 0.01096042471042472,\n",
       "            'ʊ': 0.00033783783783783813}),\n",
       "  'f': ProbDist({'aɪ': 0.0003472222222222222,\n",
       "            'dʒ': 0.0006944444444444444,\n",
       "            'eɪ': 0.0003472222222222222,\n",
       "            'f': 0.7521016081871346,\n",
       "            'g': 0.0006944444444444444,\n",
       "            'h': 0.007163742690058477,\n",
       "            'i': 0.001023391812865497,\n",
       "            'k': 0.0006944444444444444,\n",
       "            'l': 0.0006944444444444444,\n",
       "            'n': 0.0013523391812865495,\n",
       "            'oʊ': 0.0013888888888888887,\n",
       "            'p': 0.0036184210526315776,\n",
       "            'r': 0.0009868421052631577,\n",
       "            's': 0.0044773391812865495,\n",
       "            't': 0.004093567251461988,\n",
       "            'tʃ': 0.0006944444444444444,\n",
       "            'u': 0.0003472222222222222,\n",
       "            'v': 0.014236111111111109,\n",
       "            'w': 0.002704678362573099,\n",
       "            'ð': 0.004842836257309941,\n",
       "            'ɑ': 0.0006578947368421051,\n",
       "            'ɛ': 0.001023391812865497,\n",
       "            'ɪ': 0.0034539473684210517,\n",
       "            'ʃ': 0.0003472222222222222,\n",
       "            'ʊ': 0.0006944444444444444,\n",
       "            'ʌ': 0.019389619883040932,\n",
       "            'θ': 0.17192982456140346}),\n",
       "  'g': ProbDist({'b': 0.00859133126934984,\n",
       "            'd': 0.00965557275541795,\n",
       "            'dʒ': 0.002941176470588233,\n",
       "            'eɪ': 0.0018382352941176457,\n",
       "            'f': 0.0019736842105263146,\n",
       "            'g': 0.5911184210526316,\n",
       "            'h': 0.010603715170278631,\n",
       "            'i': 0.002089783281733745,\n",
       "            'j': 0.006249999999999996,\n",
       "            'k': 0.2359133126934983,\n",
       "            'l': 0.0026702786377708963,\n",
       "            'm': 0.0010642414860681107,\n",
       "            'n': 0.002708978328173373,\n",
       "            'oʊ': 0.0011029411764705878,\n",
       "            'p': 0.009616873065015477,\n",
       "            'r': 0.001644736842105262,\n",
       "            't': 0.024535603715170266,\n",
       "            'tʃ': 0.0021284829721362215,\n",
       "            'u': 0.0017608359133126927,\n",
       "            'v': 0.0003289473684210524,\n",
       "            'w': 0.0033668730650154776,\n",
       "            'æ': 0.0006578947368421048,\n",
       "            'ð': 0.0007352941176470583,\n",
       "            'ŋ': 0.007198142414860676,\n",
       "            'ɑ': 0.0033668730650154776,\n",
       "            'ɚ': 0.0006578947368421048,\n",
       "            'ɛ': 0.0007352941176470583,\n",
       "            'ɪ': 0.009113777089783276,\n",
       "            'ʊ': 0.002941176470588233,\n",
       "            'ʌ': 0.04496904024767799,\n",
       "            'ʒ': 0.0036764705882352915,\n",
       "            'θ': 0.0040441176470588204}),\n",
       "  'h': ProbDist({'aɪ': 0.0007352941176470586,\n",
       "            'aʊ': 0.0018939393939393936,\n",
       "            'b': 0.0022727272727272726,\n",
       "            'd': 0.001515151515151515,\n",
       "            'dʒ': 0.003787878787878787,\n",
       "            'eɪ': 0.008311051693404634,\n",
       "            'f': 0.0034090909090909085,\n",
       "            'g': 0.0007575757575757575,\n",
       "            'h': 0.762945632798574,\n",
       "            'i': 0.0029411764705882344,\n",
       "            'j': 0.0007575757575757575,\n",
       "            'k': 0.00374331550802139,\n",
       "            'l': 0.0022727272727272726,\n",
       "            'm': 0.0003"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 101552 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('DERIVED VALUES/FUNCTIONS:')\n",
    "\n",
    "total_model = generateTotalModel(inputDist, channelOutput_i_dist, pad_with_boundaries = False)\n",
    "total_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:09.205276Z",
     "start_time": "2018-03-31T23:46:09.196403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['channelOutput_i_dist',\n",
       " 'channelOutput_i_marginal_dist',\n",
       " 'channelOutput_prefix_marginal_prob',\n",
       " 'channelOutput_prefix_prob',\n",
       " 'channelOutput_prefix_sampler',\n",
       " 'est_channelInput_prob',\n",
       " 'inputAlphabet',\n",
       " 'inputDist',\n",
       " 'inputDist_givenPrefix',\n",
       " 'inputs',\n",
       " 'p(X_0^f | x_0^i)',\n",
       " 'p(X_0^f)',\n",
       " 'p(Y_i)',\n",
       " 'p(Y_i|x_i)',\n",
       " 'p(x_0^i)',\n",
       " 'p(x_{i+1}^f | x_0^i)',\n",
       " 'p(y_0^i)',\n",
       " 'p(y_0^i|x_0^i)',\n",
       " 'p-hat(x-hat_0^j|x_0^i)',\n",
       " 'prefixProb',\n",
       " 'prefixes',\n",
       " 'sample from p(Y_0^i|x_0^i)',\n",
       " 'suffixProb']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(total_model.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:10.276094Z",
     "start_time": "2018-03-31T23:46:10.266313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channelOutput_i_dist': dict,\n",
       " 'channelOutput_i_marginal_dist': __main__.ProbDist,\n",
       " 'channelOutput_prefix_marginal_prob': function,\n",
       " 'channelOutput_prefix_prob': function,\n",
       " 'channelOutput_prefix_sampler': function,\n",
       " 'est_channelInput_prob': function,\n",
       " 'inputAlphabet': set,\n",
       " 'inputDist': __main__.ProbDist,\n",
       " 'inputDist_givenPrefix': function,\n",
       " 'inputs': list,\n",
       " 'p(X_0^f | x_0^i)': function,\n",
       " 'p(X_0^f)': __main__.ProbDist,\n",
       " 'p(Y_i)': __main__.ProbDist,\n",
       " 'p(Y_i|x_i)': dict,\n",
       " 'p(x_0^i)': function,\n",
       " 'p(x_{i+1}^f | x_0^i)': function,\n",
       " 'p(y_0^i)': function,\n",
       " 'p(y_0^i|x_0^i)': function,\n",
       " 'p-hat(x-hat_0^j|x_0^i)': function,\n",
       " 'prefixProb': function,\n",
       " 'prefixes': set,\n",
       " 'sample from p(Y_0^i|x_0^i)': function,\n",
       " 'suffixProb': function}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:type(total_model[k]) for k in total_model.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:11.910325Z",
     "start_time": "2018-03-31T23:46:11.869368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an_input: ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')\n",
      "prefix_a: ('⋊', 'θ')\n",
      "prefix_b: ('⋊', 'θ', 'r')\n",
      "prefix_c: ('⋊', 'θ', 'r', 'ʌ')\n",
      "prefix_d: ('⋊', 'θ', 'r', 'ʌ', 's')\n",
      "suffix_a: ('s', 's', '⋉')\n",
      "suffix_b: ('s', '⋉')\n",
      "suffix_c: ('s', '⋉')\n"
     ]
    }
   ],
   "source": [
    "# print('TEST INPUTS:')\n",
    "an_input = total_model['inputs'][1]\n",
    "prefix_a = an_input[0:2]\n",
    "prefix_b = an_input[0:3]\n",
    "prefix_c = an_input[0:4]\n",
    "prefix_d = an_input[:-2]\n",
    "suffix_a = an_input[-3:]\n",
    "suffix_b = tuple([an_input[-2],'⋉'])\n",
    "suffix_c = an_input[-2:]\n",
    "\n",
    "print(\"{0}: {1}\".format('an_input', an_input))\n",
    "print(\"{0}: {1}\".format('prefix_a', prefix_a))\n",
    "print(\"{0}: {1}\".format('prefix_b', prefix_b))\n",
    "print(\"{0}: {1}\".format('prefix_c', prefix_c))\n",
    "print(\"{0}: {1}\".format('prefix_d', prefix_d))\n",
    "print(\"{0}: {1}\".format('suffix_a', suffix_a))\n",
    "print(\"{0}: {1}\".format('suffix_b', suffix_b))\n",
    "print(\"{0}: {1}\".format('suffix_c', suffix_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells below show properties of the input distribution $p(X_0^f)$ and calculations derived from it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:22.813598Z",
     "start_time": "2018-03-31T23:46:22.775741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some inputs:\n",
      "[('⋊', 's', 'k', 'i', 'm', 'z', '⋉'), ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉'), ('⋊', 's', 'ɪ', 'ŋ', 'ɚ', '⋉'), ('⋊', 'k', 'ɑ', 'r', 'b', 'j', 'ɚ', 'i', 'ʃ', 'ə', 'n', '⋉'), ('⋊', 'p', 'r', 'ɑ', 's', 'ɛ', 's', 't', '⋉'), ('⋊', 'b', 'oʊ', 'l', 'ʃ', 'ə', 'v', 'ɪ', 'k', 's', '⋉'), ('⋊', 'k', 'ə', 'm', 'p', 'r', 'ɛ', 's', 't', '⋉'), ('⋊', 'n', 'æ', 'n', 'oʊ', 's', 'ɛ', 'k', 'ə', 'n', 'd', '⋉'), ('⋊', 'k', 'oʊ', 'k', 'oʊ', '⋉'), ('⋊', 'ɪ', 'n', 'ɑ', 'k', 'j', 'ə', 'l', 'eɪ', 't', '⋉')]\n",
      "preview of p(X_0^f):\n",
      "{('⋊', 's', 'k', 'i', 'm', 'z', '⋉'): 0.05597964376590331, ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉'): 0.0025445292620865138, ('⋊', 's', 'ɪ', 'ŋ', 'ɚ', '⋉'): 0.4990458015267174, ('⋊', 'k', 'ɑ', 'r', 'b', 'j', 'ɚ', 'i', 'ʃ', 'ə', 'n', '⋉'): 0.0006361323155216284, ('⋊', 'p', 'r', 'ɑ', 's', 'ɛ', 's', 't', '⋉'): 0.07983460559796436, ('⋊', 'b', 'oʊ', 'l', 'ʃ', 'ə', 'v', 'ɪ', 'k', 's', '⋉'): 0.006997455470737914, ('⋊', 'k', 'ə', 'm', 'p', 'r', 'ɛ', 's', 't', '⋉'): 0.028625954198473278, ('⋊', 'n', 'æ', 'n', 'oʊ', 's', 'ɛ', 'k', 'ə', 'n', 'd', '⋉'): 0.004452926208651399, ('⋊', 'k', 'oʊ', 'k', 'oʊ', '⋉'): 0.31075063613231546, ('⋊', 'ɪ', 'n', 'ɑ', 'k', 'j', 'ə', 'l', 'eɪ', 't', '⋉'): 0.011132315521628498}\n",
      "input alphabet:\n",
      "{'⋉', 'd', 'p', 'n', 'θ', 'z', 'l', 'j', 'ŋ', 'm', 'ɪ', 't', 'æ', 'ʌ', 'ɚ', 'i', 'ɑ', 'ɛ', 'v', 'ʃ', 'ə', 'b', 'r', 'oʊ', 'k', 's', '⋊', 'eɪ'}\n",
      "input alphabet - edge symbols:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b',\n",
       " 'd',\n",
       " 'eɪ',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'oʊ',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'v',\n",
       " 'z',\n",
       " 'æ',\n",
       " 'ŋ',\n",
       " 'ɑ',\n",
       " 'ə',\n",
       " 'ɚ',\n",
       " 'ɛ',\n",
       " 'ɪ',\n",
       " 'ʃ',\n",
       " 'ʌ',\n",
       " 'θ'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<= 10 prefixes of lexicon:\n",
      "[('⋊', 's', 'k', 'i', 'm', 'z', '⋉'), ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉'), ('⋊', 's', 'ɪ', 'ŋ', 'ɚ', '⋉'), ('⋊', 'b', 'oʊ', 'l', 'ʃ', 'ə', 'v', 'ɪ'), ('⋊', 's', 'ɪ', 'ŋ', 'ɚ'), ('⋊', 'n'), ('⋊', 'k', 'ɑ', 'r', 'b', 'j', 'ɚ', 'i', 'ʃ', 'ə', 'n', '⋉'), ('⋊', 'p', 'r', 'ɑ', 's', 'ɛ', 's', 't', '⋉'), ('⋊', 'p', 'r'), ('⋊', 'b', 'oʊ', 'l'), ('⋊', 'b', 'oʊ', 'l', 'ʃ', 'ə', 'v', 'ɪ', 'k', 's', '⋉')]\n"
     ]
    }
   ],
   "source": [
    "someInputs = total_model['inputs'][:10]\n",
    "print('some inputs:')\n",
    "print(someInputs)\n",
    "print('preview of p(X_0^f):')\n",
    "print({k:total_model['p(X_0^f)'][k] for k in someInputs})\n",
    "assert(all([k in total_model['p(X_0^f)'] for k in someInputs]))\n",
    "\n",
    "print('input alphabet:')\n",
    "print(total_model['inputAlphabet'])\n",
    "print('input alphabet - edge symbols:')\n",
    "total_model['inputAlphabet'] - edgeSymbols\n",
    "\n",
    "print(\"<= 10 prefixes of lexicon:\")\n",
    "print(list(total_model['prefixes'])[0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...like \n",
    " - the probability the speaker's intended wordform has a particular prefix $p(x_0^i)$.\n",
    " - the distribution over full wordforms given a specific prefix $p(X_0^f|x_0^i)$.\n",
    " - the probability of a particular suffix given a particular prefix $p(x_{i+1}^f|x_0^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:24.571683Z",
     "start_time": "2018-03-31T23:46:24.476838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(X_0^f):\n",
      "⋊.s.k.i.m.z.⋉: 0.05597964376590331\n",
      "⋊.θ.r.ʌ.s.s.⋉: 0.0025445292620865138\n",
      "⋊.s.ɪ.ŋ.ɚ.⋉: 0.4990458015267174\n",
      "⋊.k.ɑ.r.b.j.ɚ.i.ʃ.ə.n.⋉: 0.0006361323155216284\n",
      "⋊.p.r.ɑ.s.ɛ.s.t.⋉: 0.07983460559796436\n",
      "⋊.b.oʊ.l.ʃ.ə.v.ɪ.k.s.⋉: 0.006997455470737914\n",
      "⋊.k.ə.m.p.r.ɛ.s.t.⋉: 0.028625954198473278\n",
      "⋊.n.æ.n.oʊ.s.ɛ.k.ə.n.d.⋉: 0.004452926208651399\n",
      "⋊.k.oʊ.k.oʊ.⋉: 0.31075063613231546\n",
      "⋊.ɪ.n.ɑ.k.j.ə.l.eɪ.t.⋉: 0.011132315521628498\n",
      "\n",
      "TEST PREFIX: ('⋊', 'θ')\n",
      "p(x_0^i = ('⋊', 'θ')) = 0.0025445292620865138\n",
      "p(X_0^f| x_0^i = ('⋊', 'θ')):\n",
      "⋊.θ.r.ʌ.s.s.⋉: 1.0\n",
      "\n",
      "TEST PREFIX: ('⋊', 'θ', 'r')\n",
      "p(x_0^i = ('⋊', 'θ', 'r')) = 0.0025445292620865138\n",
      "p(X_0^f| x_0^i = ('⋊', 'θ', 'r')):\n",
      "⋊.θ.r.ʌ.s.s.⋉: 1.0\n",
      "\n",
      "TEST PREFIX: ('⋊', 'θ', 'r', 'ʌ')\n",
      "p(x_0^i = ('⋊', 'θ', 'r', 'ʌ')) = 0.0025445292620865138\n",
      "p(X_0^f| x_0^i = ('⋊', 'θ', 'r', 'ʌ')):\n",
      "⋊.θ.r.ʌ.s.s.⋉: 1.0\n",
      "\n",
      "TEST PREFIX: ('⋊', 'θ', 'r', 'ʌ')\n",
      "TEST SUFFIX: ('s', 's', '⋉')\n",
      "Let j = i+1:\n",
      " p(x_j^f = ('s', 's', '⋉')|x_0^i = ('⋊', 'θ', 'r', 'ʌ')) = 1.0\n",
      "TEST PREFIX: ('⋊', 'θ', 'r', 'ʌ', 's')\n",
      "TEST SUFFIX: ('s', '⋉')\n",
      "Let j = i+1:\n",
      " p(x_j^f = ('s', '⋉')|x_0^i = ('⋊', 'θ', 'r', 'ʌ', 's')) = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('p(X_0^f):')\n",
    "# print('...')\n",
    "print(inputDist)\n",
    "test_prefix = prefix_a\n",
    "print(\"TEST PREFIX: {0}\".format(test_prefix))\n",
    "# print(\"{0} in dist:{1}\".format(test_prefix, test_prefix in total_model['prefixProb']))\n",
    "print('p(x_0^i = {0}) = {1}'.format(test_prefix, total_model['prefixProb'](test_prefix)))\n",
    "print('p(X_0^f| x_0^i = {0}):'.format(test_prefix))\n",
    "print(total_model['inputDist_givenPrefix'](test_prefix))\n",
    "test_prefix = prefix_b\n",
    "print(\"TEST PREFIX: {0}\".format(test_prefix))\n",
    "print('p(x_0^i = {0}) = {1}'.format(test_prefix, total_model['prefixProb'](test_prefix)))\n",
    "print('p(X_0^f| x_0^i = {0}):'.format(test_prefix))\n",
    "print(total_model['inputDist_givenPrefix'](test_prefix))\n",
    "test_prefix = prefix_c\n",
    "print(\"TEST PREFIX: {0}\".format(test_prefix))\n",
    "print('p(x_0^i = {0}) = {1}'.format(test_prefix, total_model['prefixProb'](test_prefix)))\n",
    "print('p(X_0^f| x_0^i = {0}):'.format(test_prefix))\n",
    "print(total_model['inputDist_givenPrefix'](test_prefix))\n",
    "\n",
    "test_suffix = suffix_a\n",
    "print(\"TEST PREFIX: {0}\".format(test_prefix))\n",
    "print('TEST SUFFIX: {0}'.format(test_suffix))\n",
    "print('Let j = i+1:')\n",
    "print(' p(x_j^f = {0}|x_0^i = {1}) = {2}'.format(test_suffix, test_prefix, total_model['suffixProb'](test_suffix, test_prefix)))\n",
    "test_prefix = prefix_d\n",
    "test_suffix = suffix_c\n",
    "print(\"TEST PREFIX: {0}\".format(test_prefix))\n",
    "print('TEST SUFFIX: {0}'.format(test_suffix))\n",
    "print('Let j = i+1:')\n",
    "print(' p(x_j^f = {0}|x_0^i = {1}) = {2}'.format(test_suffix, test_prefix, total_model['suffixProb'](test_suffix, test_prefix)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the channel distribution over (two examples of) a single segment $p(Y|X = x)$ and the marginal distribution $p(Y)$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:25.719101Z",
     "start_time": "2018-03-31T23:46:25.696790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Y_i|X_i = n):\n",
      "'aɪ': 0.0003289473684210526\n",
      "'d': 0.0006944444444444445\n",
      "'p': 0.0006944444444444445\n",
      "'tʃ': 0.0020833333333333337\n",
      "'ʒ': 0.0006944444444444445\n",
      "'n': 0.8828947368421052\n",
      "'θ': 0.0006944444444444445\n",
      "'u': 0.001736111111111111\n",
      "'z': 0.00034722222222222224\n",
      "'l': 0.003435672514619883\n",
      "'b': 0.0006578947368421052\n",
      "'ð': 0.0006944444444444445\n",
      "'ɑ': 0.003983918128654971\n",
      "'ŋ': 0.019060672514619886\n",
      "'h': 0.004093567251461989\n",
      "'ɪ': 0.008607456140350878\n",
      "'m': 0.02388523391812866\n",
      "'oʊ': 0.00034722222222222224\n",
      "'k': 0.0006944444444444445\n",
      "'æ': 0.0006944444444444445\n",
      "'j': 0.0010233918128654971\n",
      "'ʊ': 0.002722953216374269\n",
      "'ʌ': 0.025292397660818724\n",
      "'ɚ': 0.0010416666666666669\n",
      "'w': 0.007346491228070177\n",
      "'i': 0.0006944444444444445\n",
      "'g': 0.0020833333333333337\n",
      "'dʒ': 0.001388888888888889\n",
      "'v': 0.001388888888888889\n",
      "'ɛ': 0.0006944444444444445\n",
      "\n",
      "p(Y_i|X_i = oʊ):\n",
      "'aɪ': 0.0003378378378378383\n",
      "'p': 0.00036764705882352984\n",
      "'ʒ': 0.00036764705882352984\n",
      "'n': 0.0009191176470588246\n",
      "'θ': 0.00036764705882352984\n",
      "'u': 0.009994369369369382\n",
      "'l': 0.0036764705882352984\n",
      "'b': 0.0022257551669316407\n",
      "'ð': 0.00036764705882352984\n",
      "'m': 0.00036764705882352984\n",
      "'h': 0.00036764705882352984\n",
      "'r': 0.0009042130365659788\n",
      "'oʊ': 0.852550344462109\n",
      "'k': 0.0007352941176470597\n",
      "'ɔɪ': 0.02265169581346055\n",
      "'t': 0.0018382352941176492\n",
      "'ɪ': 0.0011029411764705897\n",
      "'ʌ': 0.02595223900370962\n",
      "'aʊ': 0.052389705882352984\n",
      "'ɚ': 0.010159976152623224\n",
      "'w': 0.000536565977742449\n",
      "'ʊ': 0.008677795442501334\n",
      "'eɪ': 0.0005216613672496032\n",
      "'g': 0.00036764705882352984\n",
      "'ɑ': 0.000536565977742449\n",
      "'ɛ': 0.001715686274509806\n",
      "\n",
      "p(Y_i):\n",
      "'⋉': 0.02439024390243903\n",
      "'d': 0.02538809224763328\n",
      "'p': 0.028890118803680868\n",
      "'tʃ': 0.028880786981696435\n",
      "'n': 0.024598273702872563\n",
      "'θ': 0.02693656411588989\n",
      "'z': 0.023239876785794922\n",
      "'l': 0.024064798028873204\n",
      "'j': 0.018651074285518383\n",
      "'ŋ': 0.022521445686844388\n",
      "'ɪ': 0.021805729551926598\n",
      "'m': 0.022320025205211577\n",
      "'ɔɪ': 0.025773698108884264\n",
      "'t': 0.025421552140268883\n",
      "'æ': 0.013845653637737786\n",
      "'ʊ': 0.010413149659775063\n",
      "'ʌ': 0.05511173240413619\n",
      "'ɚ': 0.027143278062194808\n",
      "'i': 0.0272691088827528\n",
      "'g': 0.01710842044165127\n",
      "'dʒ': 0.018991820609385084\n",
      "'v': 0.03348209362024438\n",
      "'ɛ': 0.027526404705259584\n",
      "'aɪ': 0.02694954729104761\n",
      "'ʒ': 0.018219901536612418\n",
      "'ʃ': 0.022373436854510553\n",
      "'u': 0.02677998977586433\n",
      "'w': 0.020658327698519482\n",
      "'ð': 0.007454425783170211\n",
      "'h': 0.022724847033381748\n",
      "'r': 0.024968706689229705\n",
      "'oʊ': 0.028088191120730504\n",
      "'k': 0.030359294982302755\n",
      "'s': 0.02369489708181848\n",
      "'ɑ': 0.035551200317502495\n",
      "'f': 0.025929518092214256\n",
      "'⋊': 0.02439024390243903\n",
      "'aʊ': 0.025073484350774958\n",
      "'b': 0.018683206749752084\n",
      "'eɪ': 0.044326839169458315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_phone_a = list(total_model['inputAlphabet'])[3]\n",
    "test_phone_b = list(total_model['inputAlphabet'])[23]\n",
    "\n",
    "print('p(Y_i|X_i = {0}):'.format(test_phone_a))\n",
    "# print(channelOutput_i_dist(0))\n",
    "print(channelOutput_i_dist[test_phone_a])\n",
    "print('p(Y_i|X_i = {0}):'.format(test_phone_b))\n",
    "# print(channelOutput_i_dist(1))\n",
    "print(channelOutput_i_dist[test_phone_b])\n",
    "\n",
    "print(\"p(Y_i):\")\n",
    "print(total_model['channelOutput_i_marginal_dist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are example calculations related to the following channel distribution over prefixes:\n",
    " - $p(y_0^i|x_0^i)$\n",
    " - $p(y_0^j|x_0^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:26.677146Z",
     "start_time": "2018-03-31T23:46:26.600585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(X_0^f):\n",
      "⋊.s.k.i.m.z.⋉: 0.05597964376590331\n",
      "⋊.θ.r.ʌ.s.s.⋉: 0.0025445292620865138\n",
      "⋊.s.ɪ.ŋ.ɚ.⋉: 0.4990458015267174\n",
      "⋊.k.ɑ.r.b.j.ɚ.i.ʃ.ə.n.⋉: 0.0006361323155216284\n",
      "⋊.p.r.ɑ.s.ɛ.s.t.⋉: 0.07983460559796436\n",
      "⋊.b.oʊ.l.ʃ.ə.v.ɪ.k.s.⋉: 0.006997455470737914\n",
      "⋊.k.ə.m.p.r.ɛ.s.t.⋉: 0.028625954198473278\n",
      "⋊.n.æ.n.oʊ.s.ɛ.k.ə.n.d.⋉: 0.004452926208651399\n",
      "⋊.k.oʊ.k.oʊ.⋉: 0.31075063613231546\n",
      "⋊.ɪ.n.ɑ.k.j.ə.l.eɪ.t.⋉: 0.011132315521628498\n",
      "\n",
      "TEST INPUT full: ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')\n",
      "TEST OUTPUT full: ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')\n",
      "p(y_0^i = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')| x_0^i = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')) = 0.33122571061530065\n",
      " \n",
      "TEST INPUT prefix: ('⋊', 'θ')\n",
      "TEST OUTPUT prefix: ('⋊', 'θ')\n",
      "p(y_0^i = ('⋊', 'θ')| x_0^i = ('⋊', 'θ')) = 0.6907205882352944\n",
      " \n",
      "TEST INPUT prefix: ('⋊', 'θ')\n",
      "TEST OUTPUT prefix: ('⋊', 'θ', 'r')\n",
      "p(y_0^j = ('⋊', 'θ', 'r')| x_0^i = ('⋊', 'θ')) = 0.5782879288175375\n",
      " \n",
      "TEST INPUT prefix: ('⋊', 'θ', 'r')\n",
      "TEST OUTPUT prefix: ('⋊', 'θ')\n",
      "p(y_0^j = ('⋊', 'θ')| x_0^i = ('⋊', 'θ', 'r')) = 0.6907205882352944\n"
     ]
    }
   ],
   "source": [
    "print(\"p(X_0^f):\")\n",
    "print(inputDist)\n",
    "  \n",
    "test_input_full = an_input\n",
    "test_output_full = an_input\n",
    "print(\"TEST INPUT full: {0}\".format(test_input_full))\n",
    "print(\"TEST OUTPUT full: {0}\".format(test_output_full))\n",
    "print('p(y_0^i = {0}| x_0^i = {1}) = {2}'.format(test_output_full, test_input_full, total_model['channelOutput_prefix_prob'](test_input_full, test_output_full)))\n",
    "# print('len({0}) = {1}'.format(trimBoundariesFromSequence(test_input_full), len(trimBoundariesFromSequence(test_input_full))))\n",
    "# print('p(no_error) = {0}'.format(channelOutput_i_dist[]))\n",
    "# print('{2}^(len({0})) = {1}'.format(trimBoundariesFromSequence(test_input_full), pNoError_i ** len(trimBoundariesFromSequence(test_input_full)), pNoError_i))\n",
    "\n",
    "print(' ')\n",
    "test_input_slice = prefix_a\n",
    "test_output_slice = prefix_a\n",
    "print(\"TEST INPUT prefix: {0}\".format(test_input_slice))\n",
    "print(\"TEST OUTPUT prefix: {0}\".format(test_output_slice))\n",
    "print('p(y_0^i = {0}| x_0^i = {1}) = {2}'.format(test_output_slice, test_input_slice, total_model['channelOutput_prefix_prob'](test_input_slice, test_output_slice)))\n",
    "\n",
    "print(' ')\n",
    "test_input_slice = prefix_a\n",
    "test_output_slice = prefix_b\n",
    "print(\"TEST INPUT prefix: {0}\".format(test_input_slice))\n",
    "print(\"TEST OUTPUT prefix: {0}\".format(test_output_slice))\n",
    "print('p(y_0^j = {0}| x_0^i = {1}) = {2}'.format(test_output_slice, test_input_slice, total_model['channelOutput_prefix_prob'](test_input_slice, test_output_slice)))\n",
    "\n",
    "print(' ')\n",
    "test_input_slice = prefix_b\n",
    "test_output_slice = prefix_a\n",
    "print(\"TEST INPUT prefix: {0}\".format(test_input_slice))\n",
    "print(\"TEST OUTPUT prefix: {0}\".format(test_output_slice))\n",
    "print('p(y_0^j = {0}| x_0^i = {1}) = {2}'.format(test_output_slice, test_input_slice, total_model['channelOutput_prefix_prob'](test_input_slice, test_output_slice)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are samples from $p(Y_0^i|x_0^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:34.274791Z",
     "start_time": "2018-03-31T23:46:27.577378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST INPUT full: ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')\n",
      "est. p(Y_0^i = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')|x_0^i = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')) based on 1,000 samples:\n",
      "333/1000\n",
      "Examples of output samples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['⋊.θ.r.ʌ.s.s.⋉',\n",
       " '⋊.θ.r.ʌ.s.s.⋉',\n",
       " '⋊.θ.r.ɑ.i.s.⋉',\n",
       " '⋊.p.r.ʌ.s.ɔɪ.⋉',\n",
       " '⋊.θ.r.ʌ.s.s.⋉',\n",
       " '⋊.θ.r.ʊ.s.s.⋉',\n",
       " '⋊.θ.r.ʊ.s.s.⋉',\n",
       " '⋊.f.r.ɚ.s.s.⋉',\n",
       " '⋊.θ.r.ɑ.s.s.⋉',\n",
       " '⋊.ʌ.r.ʌ.s.s.⋉']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_full = an_input\n",
    "print(\"TEST INPUT full: {0}\".format(test_input_full))\n",
    "outputSamples = list(sampleFrom(lambda : total_model['channelOutput_prefix_sampler'](test_input_full), 1000))\n",
    "outputSamplesDS = list(map(tupleToDottedString, outputSamples))\n",
    "outputSamplesDS_dist = ProbDist(Counter(outputSamplesDS))\n",
    "print('est. p(Y_0^i = {0}|x_0^i = {0}) based on 1,000 samples:'.format(an_input))\n",
    "print(outputSamplesDS_dist[tupleToDottedString(test_input_full)])\n",
    "print('Examples of output samples:')\n",
    "outputSamplesDS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:34.304431Z",
     "start_time": "2018-03-31T23:46:34.279831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST INPUT prefix: ('⋊', 'θ', 'r')\n",
      "10 samples from p(Y_0^i|x_0^i = ('⋊', 'θ', 'r')): [('⋊', 'f', 'æ'), ('⋊', 'θ', 'r'), ('⋊', 'θ', 'r'), ('⋊', 'θ', 'r'), ('⋊', 'dʒ', 'r'), ('⋊', 'ð', 'r'), ('⋊', 'ɛ', 'r'), ('⋊', 'θ', 'r'), ('⋊', 'θ', 'ɚ'), ('⋊', 'θ', 'r')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST INPUT prefix: {0}\".format(prefix_b))\n",
    "print('10 samples from p(Y_0^i|x_0^i = {0}): {1}'.format(prefix_b, list(sampleFrom(lambda : total_model['channelOutput_prefix_sampler'](prefix_b), 10)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example calculation of $p(y_0^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:34.320604Z",
     "start_time": "2018-03-31T23:46:34.308134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST OUTPUT prefix = ('⋊', 'θ', 'r')\n",
      "p(y_0^i = ('⋊', 'θ', 'r')) = 0.0016787690901115722\n"
     ]
    }
   ],
   "source": [
    "print('TEST OUTPUT prefix = {0}'.format(prefix_b))\n",
    "print('p(y_0^i = {0}) = {1}'.format(prefix_b, total_model['channelOutput_prefix_marginal_prob'](prefix_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, below are example calculations of $p(\\hat{x}_0^j| x_0^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:46:58.358658Z",
     "start_time": "2018-03-31T23:46:34.335768Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(X_0^f):\n",
      "⋊.s.k.i.m.z.⋉: 0.05597964376590331\n",
      "⋊.θ.r.ʌ.s.s.⋉: 0.0025445292620865138\n",
      "⋊.s.ɪ.ŋ.ɚ.⋉: 0.4990458015267174\n",
      "⋊.k.ɑ.r.b.j.ɚ.i.ʃ.ə.n.⋉: 0.0006361323155216284\n",
      "⋊.p.r.ɑ.s.ɛ.s.t.⋉: 0.07983460559796436\n",
      "⋊.b.oʊ.l.ʃ.ə.v.ɪ.k.s.⋉: 0.006997455470737914\n",
      "⋊.k.ə.m.p.r.ɛ.s.t.⋉: 0.028625954198473278\n",
      "⋊.n.æ.n.oʊ.s.ɛ.k.ə.n.d.⋉: 0.004452926208651399\n",
      "⋊.k.oʊ.k.oʊ.⋉: 0.31075063613231546\n",
      "⋊.ɪ.n.ɑ.k.j.ə.l.eɪ.t.⋉: 0.011132315521628498\n",
      "\n",
      "est p(x-hat_0^j = ('⋊', 1)| x_0^i = ('⋊', 'θ', 'r')) = 0\n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ', 'r')| x_0^i = ('⋊', 'θ', 'r')) = 0.7132299796807328\n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')| x_0^i = ('⋊', 'θ', 'r')) = 0.7215248357105462\n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ', 'r')| x_0^i = ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')) = 0.9999763433531295\n",
      " \n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ')| x_0^i = ('⋊', 'θ')) = 0.720068487711772\n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ')| x_0^i = ('⋊', 'θ', 'r')) = 0.7201675102912667\n",
      " \n",
      "est p(x-hat_0^j = ('⋊', 'θ', 'r')| x_0^i = ('⋊', 'θ')) = 0.7181326246871341\n"
     ]
    }
   ],
   "source": [
    "print(\"p(X_0^f):\")\n",
    "print(inputDist)\n",
    "\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(('⋊', 1), prefix_b, total_model['est_channelInput_prob'](prefix_b, ('⋊', 1))))\n",
    "print(' ')\n",
    "\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(prefix_b, prefix_b, total_model['est_channelInput_prob'](prefix_b, prefix_b)))\n",
    "print(' ')\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(an_input, prefix_b, total_model['est_channelInput_prob'](prefix_b, an_input)))\n",
    "print(' ')\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(prefix_b, an_input, total_model['est_channelInput_prob'](an_input, prefix_b)))\n",
    "\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(prefix_a, prefix_a, total_model['est_channelInput_prob'](prefix_b, prefix_b)))\n",
    "print(' ')\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(prefix_a, prefix_b, total_model['est_channelInput_prob'](prefix_b, prefix_b)))\n",
    "print(' ')\n",
    "print('est p(x-hat_0^j = {0}| x_0^i = {1}) = {2}'.format(prefix_b, prefix_a, total_model['est_channelInput_prob'](prefix_b, prefix_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to calculate the query $p(\\hat{X}_0^i = s|X_0^i = s)$, over random prefixes from our tiny ten word lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:50:18.061801Z",
     "start_time": "2018-03-31T23:50:18.031725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST INPUT full: ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prefixes of increasing length:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('⋊',),\n",
       " ('⋊', 'θ'),\n",
       " ('⋊', 'θ', 'r'),\n",
       " ('⋊', 'θ', 'r', 'ʌ'),\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's'),\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's', 's'),\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_full = an_input\n",
    "print(\"TEST INPUT full: {0}\".format(test_input_full))\n",
    "testPrefixes = sorted(list(getPrefixes(an_input)), key = lambda prefix: len(prefix))\n",
    "len(testPrefixes)\n",
    "print('Test prefixes of increasing length:')\n",
    "testPrefixes\n",
    "# testPrefixes - {tuple(['⋊'])}\n",
    "# testPrefixes = testPrefixes- {tuple(['⋊'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:48:11.406019Z",
     "start_time": "2018-03-31T23:47:23.601753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('⋊',): 0.15563607215881348,\n",
       " ('⋊', 'θ'): 1.1722090244293213,\n",
       " ('⋊', 'θ', 'r'): 2.2120699882507324,\n",
       " ('⋊', 'θ', 'r', 'ʌ'): 2.6993730068206787,\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's'): 3.2806639671325684,\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's', 's'): 3.9217159748077393,\n",
       " ('⋊', 'θ', 'r', 'ʌ', 's', 's', '⋉'): 3.872938871383667}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between successive queries (of increasing prefix lengths):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0165729522705078,\n",
       " 1.0398609638214111,\n",
       " 0.4873030185699463,\n",
       " 0.5812909603118896,\n",
       " 0.6410520076751709,\n",
       " -0.048777103424072266]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean difference:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6195504665374756"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/segment:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15563607215881348,\n",
       " 0.5861045122146606,\n",
       " 0.7373566627502441,\n",
       " 0.6748432517051697,\n",
       " 0.6561327934265136,\n",
       " 0.6536193291346232,\n",
       " 0.5532769816262382]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time/segment:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5738528004308947"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "def timeCall(thunk):\n",
    "    timeStart = time.time()\n",
    "    result = thunk()\n",
    "    timeEnd = time.time()\n",
    "    timePassed = timeEnd - timeStart\n",
    "    return timePassed\n",
    "\n",
    "\n",
    "times = dict()\n",
    "for eachPrefix in testPrefixes:\n",
    "#     print(eachPrefix)\n",
    "#     results[eachPrefix] = total_model['est_channelInput_prob'](eachPrefix, eachPrefix)\n",
    "    times[eachPrefix] = timeCall(lambda: total_model['est_channelInput_prob'](eachPrefix, eachPrefix))\n",
    "#     print('{0}: {1}'.format(eachPrefix, times[eachPrefix]))\n",
    "print('times:')\n",
    "times\n",
    "timeValues = [times[eachPrefix] for eachPrefix in testPrefixes]\n",
    "\n",
    "print('difference between successive queries (of increasing prefix lengths):')\n",
    "diffs = [trial_next_time - trial_time for trial_time, trial_next_time in zip(timeValues, timeValues[1:])]\n",
    "diffs\n",
    "print('mean difference:')\n",
    "statistics.mean(diffs)\n",
    "\n",
    "print('time/segment:')\n",
    "timePerSegment = [times[eachPrefix] / (1.0 * len(eachPrefix)) for eachPrefix in testPrefixes]\n",
    "timePerSegment\n",
    "print('mean time/segment:')\n",
    "statistics.mean(timePerSegment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to calculate $p(\\hat{X}_0^f = s|X_0^f = s)$ as the size of the lexicon grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T23:52:24.354533Z",
     "start_time": "2018-03-31T23:52:23.580370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "51376"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['channelOutput_i_dist',\n",
       " 'channelOutput_i_marginal_dist',\n",
       " 'channelOutput_prefix_marginal_prob',\n",
       " 'channelOutput_prefix_prob',\n",
       " 'channelOutput_prefix_sampler',\n",
       " 'est_channelInput_prob',\n",
       " 'inputAlphabet',\n",
       " 'inputDist',\n",
       " 'inputDist_givenPrefix',\n",
       " 'inputs',\n",
       " 'p(X_0^f | x_0^i)',\n",
       " 'p(X_0^f)',\n",
       " 'p(Y_i)',\n",
       " 'p(Y_i|x_i)',\n",
       " 'p(x_0^i)',\n",
       " 'p(x_{i+1}^f | x_0^i)',\n",
       " 'p(y_0^i)',\n",
       " 'p(y_0^i|x_0^i)',\n",
       " 'p-hat(x-hat_0^j|x_0^i)',\n",
       " 'prefixProb',\n",
       " 'prefixes',\n",
       " 'sample from p(Y_0^i|x_0^i)',\n",
       " 'suffixProb']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we want to restore the input distribution to the full one we imported...\n",
    "\n",
    "len(inputDist) #current input distribution\n",
    "len(lexicon_actual) #reference to the originally imported lexicon\n",
    "inputDist = lexicon_actual\n",
    "new_total_model = generateTotalModel(inputDist, channelOutput_i_dist, pad_with_boundaries = False)\n",
    "# new_total_model\n",
    "sorted(new_total_model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across $m$ $n$-word sublexicons of the actual lexicon, what is the average time to make $k$ random queries of $p(\\hat{X}_0^f = s|X_0^f = s)$ and where $s$ is chosen from a uniform distribution over words in the lexicon? Let's start with $m=3$ and $k=10$ and let $n$ vary over $\\{10, 100, 1000, 10000, 20000, 40000\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T00:58:37.729966Z",
     "start_time": "2018-04-01T00:09:38.634258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (s) for 10 queries: 42.809144020080566\n",
      "Time taken (s) for 10 queries: 54.05713701248169\n",
      "Time taken (s) for 10 queries: 62.51040315628052\n",
      "Mean time (over 3 sublexicons) to make 10 queries over a 10-word sublexicon: 53.12556139628092\n",
      "Time taken (s) for 10 queries: 103.55769801139832\n",
      "Time taken (s) for 10 queries: 99.49169898033142\n",
      "Time taken (s) for 10 queries: 101.20855593681335\n",
      "Mean time (over 3 sublexicons) to make 10 queries over a 100-word sublexicon: 101.4193176428477\n",
      "Time taken (s) for 10 queries: 442.6917288303375\n",
      "Time taken (s) for 10 queries: 557.3850648403168\n",
      "Time taken (s) for 10 queries: 492.1795291900635\n",
      "Mean time (over 3 sublexicons) to make 10 queries over a 1000-word sublexicon: 497.41877428690594\n",
      "Time taken (s) for 10 queries: 64854.661350011826\n",
      "Time taken (s) for 10 queries: 4353.231388092041\n",
      "Time taken (s) for 10 queries: 5082.366680145264\n",
      "Mean time (over 3 sublexicons) to make 10 queries over a 10000-word sublexicon: 24763.419806083042\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     }\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollectTimes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_ns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     }\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollectTimes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_ns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36mcollectTimes\u001b[0;34m(m, n, k)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrandom_sublexicons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetLexiconSampleOfSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenMatchingModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_sublexicons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mmean_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean time (over {0} sublexicons) to make {1} queries over a {2}-word sublexicon: {3}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrandom_sublexicons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetLexiconSampleOfSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenMatchingModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_sublexicons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mmean_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean time (over {0} sublexicons) to make {1} queries over a {2}-word sublexicon: {3}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36mgetTime\u001b[0;34m(model, a_k_val)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtimeStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmakeQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmakeRandomQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmakeQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_k_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtimeEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtimeTaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeEnd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimeStart\u001b[0m \u001b[0;31m#seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtimeStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmakeQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmakeRandomQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmakeQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_k_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtimeEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtimeTaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeEnd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimeStart\u001b[0m \u001b[0;31m#seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_k_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtimeStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmakeQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmakeRandomQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmakeQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_k_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtimeEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-0a0708517d8e>\u001b[0m in \u001b[0;36mmakeRandomQuery\u001b[0;34m(a_total_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# print('Starting query for {0}'.format(ds))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtimeStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_total_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p-hat(x-hat_0^j|x_0^i)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtimeEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtimeTaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeEnd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimeStart\u001b[0m \u001b[0;31m#seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3b605e82bde3>\u001b[0m in \u001b[0;36mest_channelInput_prob\u001b[0;34m(true_inputPrefix, poss_inputPrefix, num_samples)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m#     print(terms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#         est = sum(terms) / num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3b605e82bde3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#terms in \\sum_{samples y_0^i from p(Y_0^i|x_0^i)} p(y_0^i|x-hat_0^i) * p(x-hat_0^j) / p(y_0^i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#         terms = (model['channelOutput_prefix_prob'](poss_inputPrefix, outputPrefix)  * poss_inputPrefix_prob / model['channelOutput_prefix_marginal_prob'](outputPrefix) for outputPrefix in samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channelOutput_prefix_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposs_inputPrefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channelOutput_prefix_marginal_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mposs_inputPrefix_prob\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channelOutput_prefix_marginal_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutputPrefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m#     for outputPrefix in samples:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#       print('y_0^i: {0}'.format(outputPrefix))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5c2b3bf9e7b3>\u001b[0m in \u001b[0;36mchannelOutput_prefix_marginal_prob\u001b[0;34m(outputPrefix)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#     print('terms of \\sum_{X_0^i} p(x_0^i) * p(y_0^i|x_0^i):')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m#     print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p(y_0^i)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannelOutput_prefix_marginal_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channelOutput_prefix_marginal_prob'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannelOutput_prefix_marginal_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5c2b3bf9e7b3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#\\sum_{X_0^i} p(x_0^i) * p(y_0^i|x_0^i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prefixProb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannelOutput_prefix_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_prefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_prefixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m#     probs = (prefixProb(input_prefix) * channelOutput_prefix_prob(input_prefix, outputPrefix) for input_prefix in my_prefixes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5c2b3bf9e7b3>\u001b[0m in \u001b[0;36mchannelOutput_prefix_prob\u001b[0;34m(inputPrefix, outputPrefix)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#             slice_prob = lambda y_i, x_i: P({y_i}, channelOutput_i_dist(x_i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mslice_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannelOutput_i_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#truncate excess part of inputPrefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                   \u001b[0;31m#|y_0^j|     <      |x_0^i|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef97d726618d>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5c2b3bf9e7b3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#             slice_prob = lambda y_i, x_i: P({y_i}, channelOutput_i_dist(x_i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mslice_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannelOutput_i_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#truncate excess part of inputPrefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                   \u001b[0;31m#|y_0^j|     <      |x_0^i|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5c2b3bf9e7b3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y_i, x_i)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#         |y_0^j|    ==      |x_0^i|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#             slice_prob = lambda y_i, x_i: P({y_i}, channelOutput_i_dist(x_i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mslice_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannelOutput_i_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPrefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#truncate excess part of inputPrefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef97d726618d>\u001b[0m in \u001b[0;36mP\u001b[0;34m(event, space)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuch_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspace\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef97d726618d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuch_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProbDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspace\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_m = 3\n",
    "my_k = 10\n",
    "my_ns = [10, 100, 1000, 10000, 20000, 40000]\n",
    "\n",
    "def getLexiconSampleOfSize(full_lexicon, n):\n",
    "    some_entries = random.sample(phonWords, n) #sample n words without replacement from the lexicon\n",
    "    newLexicon = ProbDist({w:full_lexicon[w] for w in some_entries})\n",
    "    return newLexicon\n",
    "\n",
    "def genMatchingModel(random_sublexicon):\n",
    "    model = generateTotalModel(random_sublexicon, channelOutput_i_dist, pad_with_boundaries = False)\n",
    "    return model\n",
    "\n",
    "def makeRandomQuery(a_total_model):\n",
    "    random_word = random.choice(a_total_model['inputs'])\n",
    "    ds = tupleToDottedString(random_word)\n",
    "    # print('Starting query for {0}'.format(ds))\n",
    "    timeStart = time.time()\n",
    "    query_result = a_total_model['p-hat(x-hat_0^j|x_0^i)'](random_word, random_word)\n",
    "    timeEnd = time.time()\n",
    "    timeTaken = timeEnd - timeStart #seconds\n",
    "    # print('Finished query. Time taken (s): {0}'.format(timeTaken))\n",
    "    return (random_word, query_result, timeTaken)\n",
    "\n",
    "def getTime(model, a_k_val):\n",
    "    timeStart = time.time()\n",
    "    makeQuery = lambda: makeRandomQuery(model)\n",
    "    queries = [makeQuery() for each in range(a_k_val)]\n",
    "    timeEnd = time.time()\n",
    "    timeTaken = timeEnd - timeStart #seconds\n",
    "    print('Time taken (s) for {0} queries: {1}'.format(a_k_val, timeTaken))\n",
    "    return timeTaken\n",
    "\n",
    "def collectTimes(m, n, k):\n",
    "    parameters = {'m':m, 'n':n, 'k':k}\n",
    "    random_sublexicons = list(getLexiconSampleOfSize(inputDist, n) for each in range(m))\n",
    "    models = list(map(genMatchingModel, random_sublexicons))\n",
    "    times = list(map(lambda model: getTime(model, k), models))\n",
    "    mean_time = statistics.mean(times)\n",
    "    print('Mean time (over {0} sublexicons) to make {1} queries over a {2}-word sublexicon: {3}'.format(m, k, n, mean_time))\n",
    "    return {\n",
    "        'parameters':parameters, \n",
    "        'sublexicons':random_sublexicons, \n",
    "        'models':models, \n",
    "        'times':times, \n",
    "        'mean time':mean_time\n",
    "    }\n",
    "\n",
    "observations = [collectTimes(my_m, each_n, my_k) for each_n in my_ns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(Jupyter is buggy and slow; when corresponding code is run *outside* of a notebook, the mean time to make 10 queries is somewhere between 45-65 seconds no matter how large the (sub)lexicons in question are.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "188px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
