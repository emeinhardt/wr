{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:51.619649Z",
     "start_time": "2019-07-24T19:33:51.617445Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-representations\" data-toc-modified-id=\"numpy-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices\" data-toc-modified-id=\"Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Segment sequence (all prefixes or just wordforms) channel matrices</a></span></li><li><span><a href=\"#Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)\" data-toc-modified-id=\"Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional flag $f$ indicating whether to do calculations for both full wordforms and prefixes (`True`, default) or just full wordforms (`False`)\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix (if $f$, otherwise just for full source wordforms) and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes (if $f$, else full source wordforms) of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$).\n",
    "\n",
    "#FIXME update to reflect other exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:51.805180Z",
     "start_time": "2019-07-24T19:33:51.622958Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.126125Z",
     "start_time": "2019-07-24T19:33:51.806605Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.132404Z",
     "start_time": "2019-07-24T19:33:53.129659Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "# c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "\n",
    "w = ''\n",
    "# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'\n",
    "\n",
    "f = ''\n",
    "# f = 'True'\n",
    "\n",
    "p = ''\n",
    "# p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.138399Z",
     "start_time": "2019-07-24T19:33:53.133632Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.144805Z",
     "start_time": "2019-07-24T19:33:53.139557Z"
    }
   },
   "outputs": [],
   "source": [
    "if f == '':\n",
    "    f = 'True'\n",
    "\n",
    "if p == '':\n",
    "    r = False\n",
    "else:\n",
    "    r = True\n",
    "    print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.150801Z",
     "start_time": "2019-07-24T19:33:53.145943Z"
    }
   },
   "outputs": [],
   "source": [
    "if f == 'True':\n",
    "    f = True\n",
    "elif f == 'False':\n",
    "    f = False\n",
    "else:\n",
    "    raise Exception(f\"f must be either 'True' or 'False', got '{f}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.170949Z",
     "start_time": "2019-07-24T19:33:53.151890Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.175260Z",
     "start_time": "2019-07-24T19:33:53.173152Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.799810Z",
     "start_time": "2019-07-24T19:33:53.177040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.803254Z",
     "start_time": "2019-07-24T19:33:53.801245Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.948324Z",
     "start_time": "2019-07-24T19:33:53.804480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.get_device_name(1))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.952111Z",
     "start_time": "2019-07-24T19:33:53.949915Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:53.958336Z",
     "start_time": "2019-07-24T19:33:53.953422Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:54.209748Z",
     "start_time": "2019-07-24T19:33:53.959559Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:54.213443Z",
     "start_time": "2019-07-24T19:33:54.211269Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "    assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:54.245220Z",
     "start_time": "2019-07-24T19:33:54.214653Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.469850Z",
     "start_time": "2019-07-24T19:33:54.246751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 6403\n",
      "|Prefixes| = 21475\n",
      "|triphones| in lexicon = 5760\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ws)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.484007Z",
     "start_time": "2019-07-24T19:33:56.471363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 5760\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.554858Z",
     "start_time": "2019-07-24T19:33:56.485279Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.562230Z",
     "start_time": "2019-07-24T19:33:56.556139Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "    print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "    lexiconDiphones = lexiconTo2factors(Ws)\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "    X01s = lexiconDiphones\n",
    "    assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.572614Z",
     "start_time": "2019-07-24T19:33:56.563470Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "    # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "    for x01 in p3Y1X01:\n",
    "        p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "    # create new conditioning events\n",
    "    wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "    list(wordEndDiphones)[:5]\n",
    "\n",
    "    # create their distribution over outcomes\n",
    "    deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "    deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "    # add the new wordend conditioning events to the preview distribution\n",
    "    p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "    p3Y1X01['aʊ.s']['s']\n",
    "    p3Y1X01['ɑ.⋉']\n",
    "\n",
    "    # check that everything worked\n",
    "    for x01 in p3Y1X01:\n",
    "        assert rightEdge in p3Y1X01[x01]\n",
    "    #     if rightEdge not in p3Y1X01[x01]:\n",
    "    #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "    assert areNormalized(p3Y1X01)\n",
    "    assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "    #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "    # let's trim the preview model's conditioning events\n",
    "    p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "    p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "    X01s_RE = set(p3Y1X01.keys())\n",
    "    len(X01s_RE)\n",
    "    \n",
    "#     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.592570Z",
     "start_time": "2019-07-24T19:33:56.573827Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.615509Z",
     "start_time": "2019-07-24T19:33:56.595577Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.619882Z",
     "start_time": "2019-07-24T19:33:56.617802Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.626837Z",
     "start_time": "2019-07-24T19:33:56.621149Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    X01REmap = seqsToIndexMap(X01s_RE)\n",
    "    X01REOHs = seqMapToOneHots(X01REmap)\n",
    "    X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "    Y1s_RE = outcomes(p3Y1X01)\n",
    "    len(Y1s_RE)\n",
    "    Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "    print(Y1s_RE - Y1s)\n",
    "\n",
    "    Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "    Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "    Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "    OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.633523Z",
     "start_time": "2019-07-24T19:33:56.628027Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    for x012 in p3Y1X012:\n",
    "        p3Y1X012[x012].update({rightEdge:0.0})\n",
    "        assert rightEdge in p3Y1X012[x012]\n",
    "        assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "    outcomes(p3Y1X012) == Y1s\n",
    "    outcomes(p3Y1X012) == Y1s_RE\n",
    "    areNormalized(p3Y1X012)\n",
    "    uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.658957Z",
     "start_time": "2019-07-24T19:33:56.634924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2904, 1146])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 5760)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5760,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.857882Z",
     "start_time": "2019-07-24T19:33:56.660399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5760)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "if r:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "else:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.861898Z",
     "start_time": "2019-07-24T19:33:56.859484Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "    testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "    p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.867482Z",
     "start_time": "2019-07-24T19:33:56.863301Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.874631Z",
     "start_time": "2019-07-24T19:33:56.868819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.b.ɹ.ʌ.ð.ɚ.z.⋉'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.881173Z",
     "start_time": "2019-07-24T19:33:56.876059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.l.ɪ.b.ɚ.t'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.885745Z",
     "start_time": "2019-07-24T19:33:56.882594Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.892128Z",
     "start_time": "2019-07-24T19:33:56.887250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.i.ɛ.v.l.w.ɚ.j'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.902259Z",
     "start_time": "2019-07-24T19:33:56.893548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.n.t.ɛ.n.d'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.æ.oʊ.ʒ.w.p.g.u'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.909382Z",
     "start_time": "2019-07-24T19:33:56.903774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.n.ɪ', 'n.ɪ.n', 'ɪ.n.t', 'n.t.ɛ', 't.ɛ.n', 'ɛ.n.d')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.n.t.ɛ.n.d'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.914721Z",
     "start_time": "2019-07-24T19:33:56.910765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5522, 2236, 4423, 2151, 3039, 4209)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.921538Z",
     "start_time": "2019-07-24T19:33:56.916225Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.978355Z",
     "start_time": "2019-07-24T19:33:56.922969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.ɪ.n.t.ɛ.n.d'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(x0k)))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "    result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "# sourcePrefixToChannelMatrix_l(random_source_prefix, True)\n",
    "\n",
    "if r:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "        C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "    #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "    #                    for x012_idx in triphoneIndices] \n",
    "    #                   for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "            C = np.zeros((len(Y1s_RE), 1))\n",
    "    #         C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "            return C.reshape(len(Y1s_RE),1)\n",
    "    #         return C.reshape(len(Y1s),1)\n",
    "        return C\n",
    "else:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "        C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "            C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "#             return C.reshape(len(Y1s_RE),1)\n",
    "            return C.reshape(len(Y1s),1)\n",
    "        assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "        return C\n",
    "\n",
    "\n",
    "# sourcePrefixToChannelMatrix(random_source_prefix)\n",
    "\n",
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.989737Z",
     "start_time": "2019-07-24T19:33:56.980219Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    def sourcePrefixToPreviewVector(x0k):\n",
    "        xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "        if len(xp_t) < 2:\n",
    "            raise Exception('|x0k| must be > 1.')\n",
    "        if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "    #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "            uniformProb = 1.0 / len(Y1s_RE)\n",
    "            preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "            return preview_dist.reshape(len(Y1s_RE),1)\n",
    "\n",
    "        xi = xp_t[-2] #just-completed segment\n",
    "        xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "        xik_ds = t2ds((xi, xk))\n",
    "        preview_dist = p3Y1X01[xik_ds]\n",
    "    #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "        return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "    sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:56.997965Z",
     "start_time": "2019-07-24T19:33:56.991690Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0k)\n",
    "    def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "        # NB:\n",
    "        # if len(prefix) == n (including leftEdge), \n",
    "        # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "        p = prefix\n",
    "        if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "    #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "            return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "        else: #the extended channel matrix is garbage that should never be asked for\n",
    "            l = len(ds2t(p))\n",
    "            return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:57.003610Z",
     "start_time": "2019-07-24T19:33:56.999465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences = wordforms and prefixes\n"
     ]
    }
   ],
   "source": [
    "if f:\n",
    "    print('Source sequences = wordforms and prefixes')\n",
    "    source_seqs = prefixes_t #prefixes include full wordforms\n",
    "else:\n",
    "    print('Source sequences = just full wordforms')\n",
    "    source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:57.009657Z",
     "start_time": "2019-07-24T19:33:57.005184Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "                         for s in source_seqs]\n",
    "    xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "    xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.787581Z",
     "start_time": "2019-07-24T19:33:57.011301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "                     for s in source_seqs]\n",
    "CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.790664Z",
     "start_time": "2019-07-24T19:33:58.789028Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsOfLength(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         return {w for w in Ws if len(ds2t(w)) == l}\n",
    "#     return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.803161Z",
     "start_time": "2019-07-24T19:33:58.791891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.d.ɛ.m.ʌ.n.s.t.ɹ.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.d.ɪ.n.ɑ.m.ɪ.n.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ɪ.ŋ.⋉',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ʌ.d.⋉',\n",
       " '⋊.d.ɪ.s.ʌ.p.ɔɪ.n.t.m.ʌ.n.t.⋉',\n",
       " '⋊.d.ɪ.v.ɛ.l.ʌ.p.m.ʌ.n.t.s.⋉',\n",
       " '⋊.g.ɹ.æ.n.d.tʃ.ɪ.l.d.ɹ.ʌ.n.⋉',\n",
       " '⋊.k.ɑ.m.p.l.ɪ.k.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.k.ɑ.m.p.ɹ.ɪ.h.ɛ.n.ʃ.ʌ.n.⋉',\n",
       " '⋊.k.ɑ.n.s.ɪ.k.w.ɛ.n.s.ʌ.z.⋉',\n",
       " '⋊.k.ɑ.n.s.ɪ.k.w.ɛ.n.t.l.i.⋉',\n",
       " '⋊.k.ɑ.n.s.ʌ.n.t.ɹ.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.k.ɑ.z.m.ʌ.p.ɑ.l.ɪ.t.ʌ.n.⋉',\n",
       " '⋊.k.ʌ.m.j.u.n.ɪ.k.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.k.ʌ.m.j.u.n.ɪ.k.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.k.ʌ.n.t.ɪ.n.j.u.ʌ.s.l.i.⋉',\n",
       " '⋊.k.ʌ.n.t.ɪ.n.j.ʊ.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.k.ʌ.n.t.ɹ.ɪ.b.j.u.t.ɪ.ŋ.⋉',\n",
       " '⋊.k.ʌ.n.v.i.n.i.ʌ.n.s.ʌ.z.⋉',\n",
       " '⋊.m.ɛ.t.ɹ.ʌ.p.ɑ.l.ɪ.t.ʌ.n.⋉',\n",
       " '⋊.m.ʌ.n.ɪ.p.j.ʊ.l.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.oʊ.ɹ.g.ʌ.n.aɪ.z.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.p.i.d.i.ʌ.t.ɹ.ɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.p.ɑ.ɹ.t.ɪ.s.ɪ.p.eɪ.t.ɪ.ŋ.⋉',\n",
       " '⋊.p.ɑ.ɹ.t.ɪ.s.ɪ.p.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.p.ɑ.ɹ.t.ɪ.s.ɪ.p.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.p.ɹ.ɛ.z.b.ɪ.t.ɪ.ɹ.i.ʌ.n.⋉',\n",
       " '⋊.p.ɹ.ɪ.s.k.ɹ.ɪ.p.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.s.p.ɪ.ɹ.ɪ.tʃ.ʊ.æ.l.ʌ.t.i.⋉',\n",
       " '⋊.s.ʌ.b.s.k.ɹ.ɪ.p.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.t.ɛ.l.ʌ.m.ɑ.ɹ.k.ɛ.d.ɪ.ŋ.⋉',\n",
       " '⋊.æ.d.m.ɪ.n.ɪ.s.t.ɹ.eɪ.t.ɚ.⋉',\n",
       " '⋊.æ.d.v.ɚ.t.aɪ.z.m.ʌ.n.t.s.⋉',\n",
       " '⋊.ɛ.k.s.p.ɛ.k.t.eɪ.ʃ.ʌ.n.z.⋉',\n",
       " '⋊.ɛ.k.s.p.ɪ.ɹ.i.ʌ.n.s.ɪ.ŋ.⋉',\n",
       " '⋊.ɛ.k.s.p.ɪ.ɹ.i.ʌ.n.s.ʌ.z.⋉',\n",
       " '⋊.ɛ.p.ɪ.s.k.oʊ.p.eɪ.l.i.ʌ.n.⋉',\n",
       " '⋊.ɪ.n.d.i.ɛ.n.æ.p.oʊ.l.ɪ.s.⋉',\n",
       " '⋊.ɪ.n.k.ʌ.n.v.i.n.i.ʌ.n.t.⋉',\n",
       " '⋊.ɪ.n.t.ɚ.k.ʌ.n.ɛ.k.t.ɛ.d.⋉',\n",
       " '⋊.ɪ.n.t.ɚ.p.ɹ.ɪ.t.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.ɪ.n.t.ɹ.ʌ.d.ʌ.k.t.oʊ.ɹ.i.⋉',\n",
       " '⋊.ɪ.n.v.ɛ.s.t.ɪ.g.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.ɪ.s.t.æ.b.l.ɪ.ʃ.m.ʌ.n.t.⋉',\n",
       " '⋊.ɹ.i.ɪ.n.k.ɑ.ɹ.n.eɪ.ʃ.ʌ.n.⋉',\n",
       " '⋊.ɹ.ɛ.v.ʌ.l.u.ʃ.ʌ.n.aɪ.z.d.⋉',\n",
       " '⋊.ɹ.ɪ.d.ɪ.k.j.ʊ.l.ʌ.s.l.i.⋉',\n",
       " '⋊.ʌ.k.j.u.m.j.ʊ.l.eɪ.t.ʌ.d.⋉',\n",
       " '⋊.ʌ.n.f.oʊ.ɹ.tʃ.u.n.ʌ.t.l.i.⋉',\n",
       " '⋊.ʌ.n.s.k.ɹ.u.p.j.ʊ.l.ʌ.s.⋉',\n",
       " '⋊.ʌ.p.ɹ.ɑ.k.s.ɪ.m.ʌ.t.l.i.⋉'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16, Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.894686Z",
     "start_time": "2019-07-24T19:33:58.804417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{3: 11,\n",
       " 4: 144,\n",
       " 5: 808,\n",
       " 6: 1201,\n",
       " 7: 1183,\n",
       " 8: 974,\n",
       " 9: 779,\n",
       " 10: 555,\n",
       " 11: 354,\n",
       " 12: 211,\n",
       " 13: 98,\n",
       " 14: 51,\n",
       " 15: 25,\n",
       " 16: 5,\n",
       " 17: 2,\n",
       " 18: 1,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.898941Z",
     "start_time": "2019-07-24T19:33:58.895947Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.903700Z",
     "start_time": "2019-07-24T19:33:58.900255Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         maxL = max(wordlengthsInclEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "#     else:\n",
    "#         maxL = max(wordlengthsNotIncludingEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.990896Z",
     "start_time": "2019-07-24T19:33:58.904867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 6403,\n",
       " 4: 6392,\n",
       " 5: 6248,\n",
       " 6: 5440,\n",
       " 7: 4239,\n",
       " 8: 3056,\n",
       " 9: 2082,\n",
       " 10: 1303,\n",
       " 11: 748,\n",
       " 12: 394,\n",
       " 13: 183,\n",
       " 14: 85,\n",
       " 15: 34,\n",
       " 16: 9,\n",
       " 17: 4,\n",
       " 18: 2,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:58.997663Z",
     "start_time": "2019-07-24T19:33:58.992115Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        cm = sourcePrefixToChannelMatrix(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == cm.shape[1], f\"{cm.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        cm = np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)\n",
    "        assert key_length - 2 == cm.shape[1], f\"{cm.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:59.005865Z",
     "start_time": "2019-07-24T19:33:58.998741Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0f)\n",
    "    def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "        x0f = wordform\n",
    "        x0f_t = ds2t(x0f)\n",
    "        x0f_length = len(x0f_t)\n",
    "        if x0f_length == key_length:\n",
    "            return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "        elif x0f_length > key_length:\n",
    "    #         print('middle case')\n",
    "            #trim the wordform to be a prefix of length = key_length\n",
    "            x0k_t = x0f_t[:key_length]\n",
    "            x0k = t2ds(x0k_t)\n",
    "    #         print('x0k: {0}'.format(x0k))\n",
    "            return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "        else:\n",
    "            #grab the source \n",
    "            my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "            goal_l = key_length\n",
    "            return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "                          'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:33:59.012068Z",
     "start_time": "2019-07-24T19:33:59.006895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:35:44.919483Z",
     "start_time": "2019-07-24T19:35:44.915173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word lengths:\n",
      "{18}\n"
     ]
    }
   ],
   "source": [
    "if sorted(list(wordlengthsInclEdges)) != sorted(list(range(min(wordlengthsInclEdges), \\\n",
    "                                                           max(wordlengthsInclEdges)+1))):\n",
    "    print(\"Missing word lengths:\")\n",
    "    print({l for l in range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1) if l not in wordlengthsInclEdges})\n",
    "    wordlengthsInclEdges_range = list(range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1))\n",
    "else:\n",
    "    wordlengthsInclEdges_range = sorted(list(wordlengthsInclEdges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.627933Z",
     "start_time": "2019-07-24T19:33:59.018322Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "offset = [np.zeros(shape=(0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.659295Z",
     "start_time": "2019-07-24T19:34:10.629109Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in wordlengthsInclEdges_range:\n",
    "    assert all(cm.shape[1] == l - 2 for cm in cmsByLengthByWordformIndex[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.663382Z",
     "start_time": "2019-07-24T19:34:10.660597Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = offset + [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                                      for w in Ws_t])\n",
    "                                            for l in wordlengthsInclEdges_range]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment sequence (all prefixes or just wordforms) channel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.669474Z",
     "start_time": "2019-07-24T19:34:10.664859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.680666Z",
     "start_time": "2019-07-24T19:34:10.671077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 38, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.015572096"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[2].shape\n",
    "cmsByLengthByWordformIndex[3].shape\n",
    "cmsByLengthByWordformIndex[10].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.844702Z",
     "start_time": "2019-07-24T19:34:10.681996Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.930507Z",
     "start_time": "2019-07-24T19:34:10.846908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.935878Z",
     "start_time": "2019-07-24T19:34:10.932009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.975513Z",
     "start_time": "2019-07-24T19:34:10.937102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_prefix_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByPrefixIndex) if r else len(CMsByPrefixIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_prefix_index.pickle' if r else o + 'CMs_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_prefix_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:10.978525Z",
     "start_time": "2019-07-24T19:34:10.976847Z"
    }
   },
   "outputs": [],
   "source": [
    "# importDict(o + '.pW_C' + '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:13.855745Z",
     "start_time": "2019-07-24T19:34:10.979728Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:13.864332Z",
     "start_time": "2019-07-24T19:34:13.857648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_length_by_prefix_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByLengthByWordformIndex) if r else len(cmsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_length_by_prefix_index.pickle' if r else o + 'CMs_by_length_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_prefix_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.020385Z",
     "start_time": "2019-07-24T19:34:13.865768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.043777Z",
     "start_time": "2019-07-24T19:34:14.024896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy',\n",
       " 'Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json',\n",
       " 'p3Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2.npy_metadata.json',\n",
       " 'pYX.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X0X1X2Y012s.txt',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       " 'Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.067441Z",
     "start_time": "2019-07-24T19:34:14.045634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving p3Y1X012_np to filepath 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy'\n"
     ]
    }
   ],
   "source": [
    "#if not r, export numpy representation of triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012' + '.npy', p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.073540Z",
     "start_time": "2019-07-24T19:34:14.069159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if not r:\n",
    "    CM_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "             'changes':'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         CM_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.079065Z",
     "start_time": "2019-07-24T19:34:14.075019Z"
    }
   },
   "outputs": [],
   "source": [
    "#if r, export .json of modified triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving extended, human-readable version of p3Y1X012 to filepath '{o + 'p3Y1X012_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X012_RE' + '.json', condProbDistAsDicts(p3Y1X012))\n",
    "          \n",
    "    print(f\"Saving extended, human-readable version of p3Y1X01 to filepath '{o + 'p3Y1X01_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X01_RE' + '.json', condProbDistAsDicts(p3Y1X01))\n",
    "\n",
    "#if r, export numpy representation of triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012_RE' + '.npy', p3Y1X012_np)\n",
    "    print(f\"Saving p3Y1X01_np to filepath '{o + 'p3Y1X01_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X01_RE' + '.npy', p3Y1X01_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.086356Z",
     "start_time": "2019-07-24T19:34:14.080565Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    CD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X012_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:34:14.093614Z",
     "start_time": "2019-07-24T19:34:14.087811Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    PD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X01_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X01_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X01_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
