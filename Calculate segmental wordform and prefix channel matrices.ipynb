{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.468256Z",
     "start_time": "2019-05-30T22:07:05.464849Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-representations\" data-toc-modified-id=\"numpy-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional flag $f$ indicating whether to do calculations for both full wordforms and prefixes (`True`, default) or just full wordforms (`False`)\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix (if $f$, otherwise just for full source wordforms) and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes (if $f$, else full source wordforms) of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.481047Z",
     "start_time": "2019-05-30T22:07:05.473533Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.939429Z",
     "start_time": "2019-05-30T22:07:05.483137Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T17:53:19.682662Z",
     "start_time": "2019-05-31T17:53:19.679876Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "\n",
    "w = ''\n",
    "w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'\n",
    "\n",
    "f = ''\n",
    "f = 'True'\n",
    "\n",
    "p = ''\n",
    "p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.952471Z",
     "start_time": "2019-05-30T22:07:05.949058Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T17:55:08.909708Z",
     "start_time": "2019-05-31T17:55:08.905591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Including preview distribution in channel matrix calculations.\n"
     ]
    }
   ],
   "source": [
    "if f == '':\n",
    "    f = 'True'\n",
    "\n",
    "if p == '':\n",
    "    r = False\n",
    "else:\n",
    "    r = True\n",
    "    print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.967951Z",
     "start_time": "2019-05-30T22:07:05.960834Z"
    }
   },
   "outputs": [],
   "source": [
    "if f == 'True':\n",
    "    f = True\n",
    "elif f == 'false':\n",
    "    f = False\n",
    "else:\n",
    "    raise Exception(f\"f must be either 'True' or 'False', got '{f}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.979460Z",
     "start_time": "2019-05-30T22:07:05.970052Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:05.984219Z",
     "start_time": "2019-05-30T22:07:05.981460Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:06.322058Z",
     "start_time": "2019-05-30T22:07:05.986255Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:45:01.325712Z",
     "start_time": "2019-05-31T18:45:01.322838Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:13:51.506046Z",
     "start_time": "2019-05-30T23:13:49.211088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    print(torch.cuda.get_device_name(1))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:14:01.719540Z",
     "start_time": "2019-05-30T23:14:01.716405Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:14:13.989652Z",
     "start_time": "2019-05-30T23:14:13.985839Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:20:05.661487Z",
     "start_time": "2019-05-31T18:20:05.364975Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:07:24.283612Z",
     "start_time": "2019-05-31T18:07:24.229573Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "    assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:06.786540Z",
     "start_time": "2019-05-30T22:07:06.709304Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:31:43.048488Z",
     "start_time": "2019-05-30T22:31:39.924357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 6403\n",
      "|Prefixes| = 21475\n",
      "|triphones| in lexicon = 5760\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ws)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:20:22.736763Z",
     "start_time": "2019-05-31T18:20:22.724938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 5760\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:18:09.125267Z",
     "start_time": "2019-05-30T22:18:09.117689Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:07:44.702679Z",
     "start_time": "2019-05-31T18:07:44.604763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X012s| in channel model = 1323\n",
      "unmodelable lexicon diphones = \n",
      "{'s.⋉', '⋊.h', '⋊.ɚ', '⋊.ʃ', '⋊.s', 'oʊ.⋉', '⋊.æ', '⋊.ð', 'ɔɪ.⋉', 'd.⋉', 'b.⋉', '⋊.ɪ', 'ð.⋉', 'ɚ.⋉', 'ɹ.⋉', 'ʌ.⋉', '⋊.b', 't.⋉', '⋊.m', '⋊.g', 'p.⋉', 'æ.⋉', 'n.⋉', '⋊.k', '⋊.p', 'ʒ.⋉', 'dʒ.⋉', '⋊.t', 'v.⋉', 'aʊ.⋉', '⋊.dʒ', '⋊.n', '⋊.ɔɪ', '⋊.l', 'z.⋉', '⋊.ʌ', 'ŋ.⋉', 'ɛ.⋉', '⋊.w', '⋊.ɛ', 'f.⋉', '⋊.tʃ', 'ʃ.⋉', 'ɑ.⋉', '⋊.oʊ', 'aɪ.⋉', '⋊.v', 'u.⋉', 'i.⋉', 'tʃ.⋉', '⋊.z', '⋊.ɑ', 'eɪ.⋉', '⋊.aʊ', '⋊.j', '⋊.d', '⋊.i', '⋊.aɪ', '⋊.f', '⋊.ɹ', '⋊.eɪ', 'k.⋉', 'm.⋉', 'l.⋉', 'θ.⋉', '⋊.θ', 'g.⋉'}\n",
      "|X012s| in lexicon = 904\n"
     ]
    }
   ],
   "source": [
    "if r:\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "    print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "    lexiconDiphones = lexiconTo2factors(Ws)\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "    X01s = lexiconDiphones\n",
    "    assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:15:28.166051Z",
     "start_time": "2019-05-31T18:15:28.126087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s.⋉', 'w.⋉', 'oʊ.⋉', 'ɔɪ.⋉', 'd.⋉']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.007009776793949458"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'aɪ': 0.0,\n",
       " 'æ': 0.0,\n",
       " 'ʃ': 0.0,\n",
       " 'g': 0.0,\n",
       " 't': 0.0,\n",
       " 'v': 0.0,\n",
       " 'dʒ': 0.0,\n",
       " 'ɹ': 0.0,\n",
       " 'aʊ': 0.0,\n",
       " 'u': 0.0,\n",
       " 'oʊ': 0.0,\n",
       " 'ʒ': 0.0,\n",
       " 'k': 0.0,\n",
       " 'ð': 0.0,\n",
       " 'θ': 0.0,\n",
       " 'tʃ': 0.0,\n",
       " 'eɪ': 0.0,\n",
       " 'j': 0.0,\n",
       " 'm': 0.0,\n",
       " 'ɚ': 0.0,\n",
       " 'b': 0.0,\n",
       " 'p': 0.0,\n",
       " 'n': 0.0,\n",
       " 's': 0.0,\n",
       " 'w': 0.0,\n",
       " 'f': 0.0,\n",
       " 'ʊ': 0.0,\n",
       " 'i': 0.0,\n",
       " 'ɔɪ': 0.0,\n",
       " 'z': 0.0,\n",
       " 'ɪ': 0.0,\n",
       " 'ŋ': 0.0,\n",
       " 'ʌ': 0.0,\n",
       " 'ɑ': 0.0,\n",
       " 'ɛ': 0.0,\n",
       " 'h': 0.0,\n",
       " 'l': 0.0,\n",
       " 'd': 0.0,\n",
       " '⋉': 1.0}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmodelable lexicon diphones = \n",
      "{'⋊.h', '⋊.ɚ', '⋊.ð', '⋊.ʃ', '⋊.s', '⋊.æ', '⋊.ɪ', '⋊.b', '⋊.m', '⋊.g', '⋊.k', '⋊.p', '⋊.t', '⋊.dʒ', '⋊.n', '⋊.ɔɪ', '⋊.l', '⋊.ʌ', '⋊.w', '⋊.ɛ', '⋊.tʃ', '⋊.oʊ', '⋊.v', '⋊.z', '⋊.ɑ', '⋊.aʊ', '⋊.j', '⋊.d', '⋊.i', '⋊.aɪ', '⋊.f', '⋊.ɹ', '⋊.eɪ', '⋊.θ'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "    # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "    for x01 in p3Y1X01:\n",
    "        p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "    # create new conditioning events\n",
    "    wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "    list(wordEndDiphones)[:5]\n",
    "\n",
    "    # create their distribution over outcomes\n",
    "    deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "    deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "    # add the new wordend conditioning events to the preview distribution\n",
    "    p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "    p3Y1X01['aʊ.s']['s']\n",
    "    p3Y1X01['ɑ.⋉']\n",
    "\n",
    "    # check that everything worked\n",
    "    for x01 in p3Y1X01:\n",
    "        assert rightEdge in p3Y1X01[x01]\n",
    "    #     if rightEdge not in p3Y1X01[x01]:\n",
    "    #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "    assert areNormalized(p3Y1X01)\n",
    "    assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "    #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "    # let's trim the preview model's conditioning events\n",
    "    p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "    p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "    X01s_RE = set(p3Y1X01.keys())\n",
    "    len(X01s_RE)\n",
    "    \n",
    "#     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:22:21.176597Z",
     "start_time": "2019-05-30T22:22:21.173622Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:18:15.837368Z",
     "start_time": "2019-05-30T22:18:15.806087Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:26:35.947910Z",
     "start_time": "2019-05-30T22:26:35.945193Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:15:38.081001Z",
     "start_time": "2019-05-31T18:15:38.068517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'⋉'}\n"
     ]
    }
   ],
   "source": [
    "if r:\n",
    "    X01REmap = seqsToIndexMap(X01s_RE)\n",
    "    X01REOHs = seqMapToOneHots(X01REmap)\n",
    "    X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "    Y1s_RE = outcomes(p3Y1X01)\n",
    "    len(Y1s_RE)\n",
    "    Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "    print(Y1s_RE - Y1s)\n",
    "\n",
    "    Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "    Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "    Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "    OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:21:03.076031Z",
     "start_time": "2019-05-31T18:21:03.017771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if r:\n",
    "    for x012 in p3Y1X012:\n",
    "        p3Y1X012[x012].update({rightEdge:0.0})\n",
    "        assert rightEdge in p3Y1X012[x012]\n",
    "        assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "    outcomes(p3Y1X012) == Y1s\n",
    "    outcomes(p3Y1X012) == Y1s_RE\n",
    "    areNormalized(p3Y1X012)\n",
    "    uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:24:09.203521Z",
     "start_time": "2019-05-30T22:24:09.167277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2904, 1146])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 5760)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5760,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:21:10.365407Z",
     "start_time": "2019-05-31T18:21:10.170228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 5760)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:21:10.865988Z",
     "start_time": "2019-05-31T18:21:10.829541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 870)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:09.923501Z",
     "start_time": "2019-05-30T22:07:09.920732Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:09.930503Z",
     "start_time": "2019-05-30T22:07:09.925426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.k.ʌ.n.s.ɪ.d.ɚ.z.⋉'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:57.262480Z",
     "start_time": "2019-05-30T22:07:57.256956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.t.ɹ.æ.n.z.l'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:09.935762Z",
     "start_time": "2019-05-30T22:07:09.932379Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:07:09.942536Z",
     "start_time": "2019-05-30T22:07:09.937707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.z.u.b.eɪ.d.p.z.ʒ.ŋ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:15:37.250184Z",
     "start_time": "2019-05-30T22:15:37.239067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.w.ʌ.ʃ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.ɑ.ɪ.aɪ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:15:52.868109Z",
     "start_time": "2019-05-30T22:15:52.860726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.w.ʌ', 'w.ʌ.ʃ')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.w.ʌ.ʃ'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:18:27.226260Z",
     "start_time": "2019-05-30T22:18:27.221776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5627, 3445)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:43:58.475938Z",
     "start_time": "2019-05-30T22:43:58.470578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:37:11.115253Z",
     "start_time": "2019-05-31T18:37:11.096287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.w.ʌ.ʃ'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39, 2)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(x0k)))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "    result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "# sourcePrefixToChannelMatrix_l(random_source_prefix, True)\n",
    "\n",
    "if r:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "        C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "    #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "    #                    for x012_idx in triphoneIndices] \n",
    "    #                   for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "            C = np.zeros((len(Y1s_RE), 1))\n",
    "    #         C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "            return C.reshape(39,1)\n",
    "    #         return C.reshape(38,1)\n",
    "        return C\n",
    "else:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "        C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "            C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "#             return C.reshape(39,1)\n",
    "            return C.reshape(38,1)\n",
    "        return C\n",
    "\n",
    "\n",
    "# sourcePrefixToChannelMatrix(random_source_prefix)\n",
    "\n",
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:37:49.032649Z",
     "start_time": "2019-05-31T18:37:49.022031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01110786, 0.01110786, 0.01110786, 0.04004677, 0.01052324,\n",
       "       0.01110786, 0.01081555, 0.01110786, 0.27418883, 0.01110786,\n",
       "       0.01110786, 0.0695703 , 0.01110786, 0.01081555, 0.04033908,\n",
       "       0.01110786, 0.04004677, 0.01110786, 0.18649518, 0.01110786,\n",
       "       0.01110786, 0.01110786, 0.01110786, 0.01081555, 0.01110786,\n",
       "       0.01052324, 0.00818474, 0.01110786, 0.01110786, 0.01110786,\n",
       "       0.01110786, 0.01110786, 0.01110786, 0.01052324, 0.01110786,\n",
       "       0.01110786, 0.01081555, 0.01081555, 0.        ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if r:\n",
    "    def sourcePrefixToPreviewVector(x0k):\n",
    "        xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "        if len(xp_t) < 2:\n",
    "            raise Exception('|x0k| must be > 1.')\n",
    "        if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "    #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "            uniformProb = 1.0 / len(Y1s_RE)\n",
    "            preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "            return preview_dist.reshape(39,1)\n",
    "\n",
    "        xi = xp_t[-2] #just-completed segment\n",
    "        xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "        xik_ds = t2ds((xi, xk))\n",
    "        preview_dist = p3Y1X01[xik_ds]\n",
    "    #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "        return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "    sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:38:09.090921Z",
     "start_time": "2019-05-31T18:38:09.085952Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0k)\n",
    "    def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "        # NB:\n",
    "        # if len(prefix) == n (including leftEdge), \n",
    "        # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "        p = prefix\n",
    "        if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "    #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "            return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "        else: #the extended channel matrix is garbage that should never be asked for\n",
    "            l = len(ds2t(p))\n",
    "            return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:38:11.240407Z",
     "start_time": "2019-05-31T18:38:11.236244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences = wordforms and prefixes\n"
     ]
    }
   ],
   "source": [
    "if f:\n",
    "    print('Source sequences = wordforms and prefixes')\n",
    "    source_seqs = prefixes_t #prefixes include full wordforms\n",
    "else:\n",
    "    print('Source sequences = just full wordforms')\n",
    "    source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:39:08.728593Z",
     "start_time": "2019-05-31T18:39:08.092733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 6)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if r:\n",
    "    xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "                         for s in source_seqs]\n",
    "    xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "    xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:38:50.612488Z",
     "start_time": "2019-05-31T18:38:48.624027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 5)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "                     for s in source_seqs]\n",
    "CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:50:48.637918Z",
     "start_time": "2019-05-30T22:50:48.632542Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordformsOfLength(l, includingEdges = False):\n",
    "    if includingEdges:\n",
    "        return {w for w in Ws if len(ds2t(w)) == l}\n",
    "    return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:51:14.904034Z",
     "start_time": "2019-05-30T22:51:14.892606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.t.ɛ.l.ɪ.k.ʌ.m.j.u.n.ɪ.k.eɪ.ʃ.ʌ.n.⋉'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:51:16.826907Z",
     "start_time": "2019-05-30T22:51:16.743614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{3: 11,\n",
       " 4: 144,\n",
       " 5: 808,\n",
       " 6: 1201,\n",
       " 7: 1183,\n",
       " 8: 974,\n",
       " 9: 779,\n",
       " 10: 555,\n",
       " 11: 354,\n",
       " 12: 211,\n",
       " 13: 98,\n",
       " 14: 51,\n",
       " 15: 25,\n",
       " 16: 5,\n",
       " 17: 2,\n",
       " 18: 1,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:51:29.139697Z",
     "start_time": "2019-05-30T22:51:29.134366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:51:36.641063Z",
     "start_time": "2019-05-30T22:51:36.635999Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "    if includingEdges:\n",
    "        maxL = max(wordlengthsInclEdges)\n",
    "        return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "    else:\n",
    "        maxL = max(wordlengthsNotIncludingEdges)\n",
    "        return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T22:51:44.163808Z",
     "start_time": "2019-05-30T22:51:43.610771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 6403,\n",
       " 4: 6392,\n",
       " 5: 6248,\n",
       " 6: 5440,\n",
       " 7: 4239,\n",
       " 8: 3056,\n",
       " 9: 2082,\n",
       " 10: 1303,\n",
       " 11: 748,\n",
       " 12: 394,\n",
       " 13: 183,\n",
       " 14: 85,\n",
       " 15: 34,\n",
       " 16: 9,\n",
       " 17: 4,\n",
       " 18: 2,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:12:43.118097Z",
     "start_time": "2019-05-30T23:12:43.111444Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        return sourcePrefixToChannelMatrix(x0k)\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        return np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:40:43.779403Z",
     "start_time": "2019-05-31T18:40:43.772699Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0f)\n",
    "    def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "        x0f = wordform\n",
    "        x0f_t = ds2t(x0f)\n",
    "        x0f_length = len(x0f_t)\n",
    "        if x0f_length == key_length:\n",
    "            return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "        elif x0f_length > key_length:\n",
    "    #         print('middle case')\n",
    "            #trim the wordform to be a prefix of length = key_length\n",
    "            x0k_t = x0f_t[:key_length]\n",
    "            x0k = t2ds(x0k_t)\n",
    "    #         print('x0k: {0}'.format(x0k))\n",
    "            return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "        else:\n",
    "            #grab the source \n",
    "            my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "            goal_l = key_length\n",
    "            return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "                          'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:00:00.962318Z",
     "start_time": "2019-05-30T23:00:00.957906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:41:11.002213Z",
     "start_time": "2019-05-31T18:40:59.913539Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "cmsByLengthByWordformIndex = [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                         for w in Ws_t])\n",
    "                               for l in sorted(list(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:41:35.056700Z",
     "start_time": "2019-05-31T18:41:22.331926Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                             for w in Ws_t])\n",
    "                                   for l in sorted(list(wordlengthsInclEdges))]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:18:42.201995Z",
     "start_time": "2019-05-30T23:18:42.198322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T18:44:46.314101Z",
     "start_time": "2019-05-31T18:44:46.305115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 39, 1)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 39, 2)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.033961512"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[16].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:24:35.384102Z",
     "start_time": "2019-05-30T23:24:35.379805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:26:01.474372Z",
     "start_time": "2019-05-30T23:26:01.297718Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:26:10.764494Z",
     "start_time": "2019-05-30T23:26:10.758834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'p6Y0X01.json',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'p3Y1X01.json',\n",
       " 'pYX.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:28:25.489396Z",
     "start_time": "2019-05-30T23:28:25.440653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:30:09.377468Z",
     "start_time": "2019-05-30T23:30:09.372509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:30:58.714191Z",
     "start_time": "2019-05-30T23:30:55.814399Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T23:31:04.862760Z",
     "start_time": "2019-05-30T23:31:04.857867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'p6Y0X01.json',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'p3Y1X01.json',\n",
       " 'pYX.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
