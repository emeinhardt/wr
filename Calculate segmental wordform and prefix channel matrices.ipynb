{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:23.845339Z",
     "start_time": "2019-05-31T21:24:23.843315Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-representations\" data-toc-modified-id=\"numpy-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices\" data-toc-modified-id=\"Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Segment sequence (all prefixes or just wordforms) channel matrices</a></span></li><li><span><a href=\"#Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)\" data-toc-modified-id=\"Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional flag $f$ indicating whether to do calculations for both full wordforms and prefixes (`True`, default) or just full wordforms (`False`)\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix (if $f$, otherwise just for full source wordforms) and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes (if $f$, else full source wordforms) of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$).\n",
    "\n",
    "#FIXME update to reflect other exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:23.859414Z",
     "start_time": "2019-05-31T21:24:23.857459Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.314358Z",
     "start_time": "2019-05-31T21:24:23.863876Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.319910Z",
     "start_time": "2019-05-31T21:24:24.316507Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "\n",
    "w = ''\n",
    "w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'\n",
    "\n",
    "f = ''\n",
    "f = 'True'\n",
    "\n",
    "p = ''\n",
    "# p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.327567Z",
     "start_time": "2019-05-31T21:24:24.321661Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.333636Z",
     "start_time": "2019-05-31T21:24:24.330209Z"
    }
   },
   "outputs": [],
   "source": [
    "if f == '':\n",
    "    f = 'True'\n",
    "\n",
    "if p == '':\n",
    "    r = False\n",
    "else:\n",
    "    r = True\n",
    "    print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.339394Z",
     "start_time": "2019-05-31T21:24:24.336317Z"
    }
   },
   "outputs": [],
   "source": [
    "if f == 'True':\n",
    "    f = True\n",
    "elif f == 'false':\n",
    "    f = False\n",
    "else:\n",
    "    raise Exception(f\"f must be either 'True' or 'False', got '{f}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.348816Z",
     "start_time": "2019-05-31T21:24:24.341030Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:24.353144Z",
     "start_time": "2019-05-31T21:24:24.350586Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:27.043392Z",
     "start_time": "2019-05-31T21:24:24.354771Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:27.047593Z",
     "start_time": "2019-05-31T21:24:27.045194Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.259866Z",
     "start_time": "2019-05-31T21:24:27.049791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    print(torch.cuda.get_device_name(1))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.263949Z",
     "start_time": "2019-05-31T21:24:28.261355Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.270495Z",
     "start_time": "2019-05-31T21:24:28.265139Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.514622Z",
     "start_time": "2019-05-31T21:24:28.272151Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.518643Z",
     "start_time": "2019-05-31T21:24:28.516173Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "    assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:28.558409Z",
     "start_time": "2019-05-31T21:24:28.520468Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.555647Z",
     "start_time": "2019-05-31T21:24:28.559972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 6403\n",
      "|Prefixes| = 21475\n",
      "|triphones| in lexicon = 5760\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ws)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.570289Z",
     "start_time": "2019-05-31T21:24:30.557321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 5760\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.576953Z",
     "start_time": "2019-05-31T21:24:30.573906Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.624967Z",
     "start_time": "2019-05-31T21:24:30.580355Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "    print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "    lexiconDiphones = lexiconTo2factors(Ws)\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "    X01s = lexiconDiphones\n",
    "    assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.635449Z",
     "start_time": "2019-05-31T21:24:30.626428Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "    # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "    for x01 in p3Y1X01:\n",
    "        p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "    # create new conditioning events\n",
    "    wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "    list(wordEndDiphones)[:5]\n",
    "\n",
    "    # create their distribution over outcomes\n",
    "    deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "    deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "    # add the new wordend conditioning events to the preview distribution\n",
    "    p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "    p3Y1X01['aʊ.s']['s']\n",
    "    p3Y1X01['ɑ.⋉']\n",
    "\n",
    "    # check that everything worked\n",
    "    for x01 in p3Y1X01:\n",
    "        assert rightEdge in p3Y1X01[x01]\n",
    "    #     if rightEdge not in p3Y1X01[x01]:\n",
    "    #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "    assert areNormalized(p3Y1X01)\n",
    "    assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "    #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "    # let's trim the preview model's conditioning events\n",
    "    p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "    p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "    X01s_RE = set(p3Y1X01.keys())\n",
    "    len(X01s_RE)\n",
    "    \n",
    "#     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.643017Z",
     "start_time": "2019-05-31T21:24:30.637371Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.673648Z",
     "start_time": "2019-05-31T21:24:30.644883Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.677550Z",
     "start_time": "2019-05-31T21:24:30.675315Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.682478Z",
     "start_time": "2019-05-31T21:24:30.678866Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    X01REmap = seqsToIndexMap(X01s_RE)\n",
    "    X01REOHs = seqMapToOneHots(X01REmap)\n",
    "    X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "    Y1s_RE = outcomes(p3Y1X01)\n",
    "    len(Y1s_RE)\n",
    "    Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "    print(Y1s_RE - Y1s)\n",
    "\n",
    "    Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "    Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "    Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "    OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.688531Z",
     "start_time": "2019-05-31T21:24:30.684254Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    for x012 in p3Y1X012:\n",
    "        p3Y1X012[x012].update({rightEdge:0.0})\n",
    "        assert rightEdge in p3Y1X012[x012]\n",
    "        assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "    outcomes(p3Y1X012) == Y1s\n",
    "    outcomes(p3Y1X012) == Y1s_RE\n",
    "    areNormalized(p3Y1X012)\n",
    "    uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:30.717093Z",
     "start_time": "2019-05-31T21:24:30.690226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2904, 1146])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 5760)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5760,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:24:52.612236Z",
     "start_time": "2019-05-31T21:24:52.426626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5760)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "if r:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "else:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:04.418831Z",
     "start_time": "2019-05-31T21:25:04.415709Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "    testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "    p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:05.030809Z",
     "start_time": "2019-05-31T21:25:05.028123Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:05.423018Z",
     "start_time": "2019-05-31T21:25:05.418818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ʌ.t.æ.tʃ.t.⋉'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:05.800083Z",
     "start_time": "2019-05-31T21:25:05.794939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.s.t.ʌ.t.ɪ.s.t.ɪ.k.s.⋉'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:06.317360Z",
     "start_time": "2019-05-31T21:25:06.314541Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:07.087109Z",
     "start_time": "2019-05-31T21:25:07.082893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.j.l.æ.k.aʊ.w'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:07.535920Z",
     "start_time": "2019-05-31T21:25:07.526919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.æ.d.ʌ.l'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.z.j.oʊ.l'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:08.719019Z",
     "start_time": "2019-05-31T21:25:08.712289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.æ.d', 'æ.d.ʌ', 'd.ʌ.l')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.æ.d.ʌ.l'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:09.224403Z",
     "start_time": "2019-05-31T21:25:09.219024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5634, 3534, 553)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:09.792631Z",
     "start_time": "2019-05-31T21:25:09.786890Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:10.758901Z",
     "start_time": "2019-05-31T21:25:10.736060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.æ.d.ʌ.l'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(x0k)))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "    result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "# sourcePrefixToChannelMatrix_l(random_source_prefix, True)\n",
    "\n",
    "if r:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "        C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "    #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "    #                    for x012_idx in triphoneIndices] \n",
    "    #                   for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "            C = np.zeros((len(Y1s_RE), 1))\n",
    "    #         C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "            return C.reshape(39,1)\n",
    "    #         return C.reshape(38,1)\n",
    "        return C\n",
    "else:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "        C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "            C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "#             return C.reshape(39,1)\n",
    "            return C.reshape(38,1)\n",
    "        return C\n",
    "\n",
    "\n",
    "# sourcePrefixToChannelMatrix(random_source_prefix)\n",
    "\n",
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:11.964913Z",
     "start_time": "2019-05-31T21:25:11.957420Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    def sourcePrefixToPreviewVector(x0k):\n",
    "        xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "        if len(xp_t) < 2:\n",
    "            raise Exception('|x0k| must be > 1.')\n",
    "        if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "    #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "            uniformProb = 1.0 / len(Y1s_RE)\n",
    "            preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "            return preview_dist.reshape(39,1)\n",
    "\n",
    "        xi = xp_t[-2] #just-completed segment\n",
    "        xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "        xik_ds = t2ds((xi, xk))\n",
    "        preview_dist = p3Y1X01[xik_ds]\n",
    "    #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "        return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "    sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:12.939627Z",
     "start_time": "2019-05-31T21:25:12.934307Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0k)\n",
    "    def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "        # NB:\n",
    "        # if len(prefix) == n (including leftEdge), \n",
    "        # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "        p = prefix\n",
    "        if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "    #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "            return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "        else: #the extended channel matrix is garbage that should never be asked for\n",
    "            l = len(ds2t(p))\n",
    "            return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:13.716403Z",
     "start_time": "2019-05-31T21:25:13.711953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences = wordforms and prefixes\n"
     ]
    }
   ],
   "source": [
    "if f:\n",
    "    print('Source sequences = wordforms and prefixes')\n",
    "    source_seqs = prefixes_t #prefixes include full wordforms\n",
    "else:\n",
    "    print('Source sequences = just full wordforms')\n",
    "    source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:14.837595Z",
     "start_time": "2019-05-31T21:25:14.833395Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "                         for s in source_seqs]\n",
    "    xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "    xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:18.378760Z",
     "start_time": "2019-05-31T21:25:16.303353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "                     for s in source_seqs]\n",
    "CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:19.287215Z",
     "start_time": "2019-05-31T21:25:19.284295Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordformsOfLength(l, includingEdges = False):\n",
    "    if includingEdges:\n",
    "        return {w for w in Ws if len(ds2t(w)) == l}\n",
    "    return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:20.014496Z",
     "start_time": "2019-05-31T21:25:20.007347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.t.ɛ.l.ɪ.k.ʌ.m.j.u.n.ɪ.k.eɪ.ʃ.ʌ.n.⋉'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:20.601409Z",
     "start_time": "2019-05-31T21:25:20.527934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{3: 11,\n",
       " 4: 144,\n",
       " 5: 808,\n",
       " 6: 1201,\n",
       " 7: 1183,\n",
       " 8: 974,\n",
       " 9: 779,\n",
       " 10: 555,\n",
       " 11: 354,\n",
       " 12: 211,\n",
       " 13: 98,\n",
       " 14: 51,\n",
       " 15: 25,\n",
       " 16: 5,\n",
       " 17: 2,\n",
       " 18: 1,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:21.119398Z",
     "start_time": "2019-05-31T21:25:21.116550Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:21.614859Z",
     "start_time": "2019-05-31T21:25:21.609464Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "    if includingEdges:\n",
    "        maxL = max(wordlengthsInclEdges)\n",
    "        return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "    else:\n",
    "        maxL = max(wordlengthsNotIncludingEdges)\n",
    "        return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:22.668590Z",
     "start_time": "2019-05-31T21:25:22.106848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 6403,\n",
       " 4: 6392,\n",
       " 5: 6248,\n",
       " 6: 5440,\n",
       " 7: 4239,\n",
       " 8: 3056,\n",
       " 9: 2082,\n",
       " 10: 1303,\n",
       " 11: 748,\n",
       " 12: 394,\n",
       " 13: 183,\n",
       " 14: 85,\n",
       " 15: 34,\n",
       " 16: 9,\n",
       " 17: 4,\n",
       " 18: 2,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:22.674768Z",
     "start_time": "2019-05-31T21:25:22.670562Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        return sourcePrefixToChannelMatrix(x0k)\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        return np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:23.384912Z",
     "start_time": "2019-05-31T21:25:23.378129Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0f)\n",
    "    def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "        x0f = wordform\n",
    "        x0f_t = ds2t(x0f)\n",
    "        x0f_length = len(x0f_t)\n",
    "        if x0f_length == key_length:\n",
    "            return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "        elif x0f_length > key_length:\n",
    "    #         print('middle case')\n",
    "            #trim the wordform to be a prefix of length = key_length\n",
    "            x0k_t = x0f_t[:key_length]\n",
    "            x0k = t2ds(x0k_t)\n",
    "    #         print('x0k: {0}'.format(x0k))\n",
    "            return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "        else:\n",
    "            #grab the source \n",
    "            my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "            goal_l = key_length\n",
    "            return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "                          'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:23.891614Z",
     "start_time": "2019-05-31T21:25:23.887900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:34.862565Z",
     "start_time": "2019-05-31T21:25:24.625865Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "cmsByLengthByWordformIndex = [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                         for w in Ws_t])\n",
    "                               for l in sorted(list(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:34.867196Z",
     "start_time": "2019-05-31T21:25:34.864446Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                             for w in Ws_t])\n",
    "                                   for l in sorted(list(wordlengthsInclEdges))]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment sequence (all prefixes or just wordforms) channel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:34.874791Z",
     "start_time": "2019-05-31T21:25:34.868336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:34.885647Z",
     "start_time": "2019-05-31T21:25:34.877830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 38, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6403, 38, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.033090704"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[16].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:35.077382Z",
     "start_time": "2019-05-31T21:25:34.890878Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:35.132088Z",
     "start_time": "2019-05-31T21:25:35.084850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:35.137868Z",
     "start_time": "2019-05-31T21:25:35.133665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(38, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:38.073744Z",
     "start_time": "2019-05-31T21:25:35.138983Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_prefix_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:34.889807Z",
     "start_time": "2019-05-31T21:25:34.887225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T21:25:51.456579Z",
     "start_time": "2019-05-31T21:25:51.451511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pX0X1X2.npy',\n",
       " 'p6Y0X01.json',\n",
       " 'p3YX.json',\n",
       " 'p3Y0X01.json',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y01X01.json',\n",
       " 'p3Y1X01.json',\n",
       " 'pYX.json',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'gate6 trials.csv',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb',\n",
       " 'p6YX.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'gate3 trials.csv']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:04:17.965597Z",
     "start_time": "2019-05-31T22:04:17.933479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving p3Y1X012 to filepath 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X012.npy'\n"
     ]
    }
   ],
   "source": [
    "#if not r, export numpy representation of triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012' + '.npy', p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T22:13:15.390706Z",
     "start_time": "2019-05-31T22:13:15.386633Z"
    }
   },
   "outputs": [],
   "source": [
    "#if r, export .json of modified triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving extended, human-readable version of p3Y1X012 to filepath '{o + 'p3Y1X012_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X012_RE' + '.json', condProbDistAsDicts(p3Y1X012))\n",
    "          \n",
    "    print(f\"Saving extended, human-readable version of p3Y1X01 to filepath '{o + 'p3Y1X01_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X01_RE' + '.json', condProbDistAsDicts(p3Y1X01))\n",
    "\n",
    "#if r, export numpy representation of triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012_RE' + '.npy', p3Y1X012_np)\n",
    "    print(f\"Saving p3Y1X01_np to filepath '{o + 'p3Y1X01_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X01_RE' + '.npy', p3Y1X01_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
