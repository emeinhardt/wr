{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:10.941702Z",
     "start_time": "2019-07-25T23:23:10.934247Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#numpy-representations\" data-toc-modified-id=\"numpy-representations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><code>numpy</code> representations</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices\" data-toc-modified-id=\"Segment-sequence-(all-prefixes-or-just-wordforms)-channel-matrices-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Segment sequence (all prefixes or just wordforms) channel matrices</a></span></li><li><span><a href=\"#Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)\" data-toc-modified-id=\"Representations-of-$p_3(Y_1|X_0,-X_1;-X2)$-(and-$p_3(Y_1|X_0;-X_1)$)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    " - a filepath to a triphone channel model $c$\n",
    " - a filepath $w$ to a `.json` file specifying a conditional distribution $p(W|V)$ on segmental wordforms given orthographic ones\n",
    " - an output filepath prefix $o$\n",
    " - an optional filepath $p$ to a `.json` file specifying a 'preview' channel distribution to be included in calculated channel matrices.\n",
    "\n",
    "this notebook calculates a channel matrix for each source prefix and writes these channel matrices to file (with prefix given by $o$), with each file corresponding to a block of source prefixes of the same length. Within a block, the ordering of source prefixes/wordforms is given by alphabetically sorting the relevant set of prefixes (or just full wordforms, if $f$).\n",
    "\n",
    "#FIXME update to reflect other exports (including the channel matrix stacks acccctually used in subsequent notebooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `numpy`\n",
    " - `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:10.963684Z",
     "start_time": "2019-07-25T23:23:10.952171Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.584714Z",
     "start_time": "2019-07-25T23:23:10.968899Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.602006Z",
     "start_time": "2019-07-25T23:23:11.593195Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "c = ''\n",
    "# c = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'\n",
    "# c = \"CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json\"\n",
    "\n",
    "w = ''\n",
    "# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "# w = 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json'\n",
    "\n",
    "o = ''\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_'\n",
    "# o = 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_'\n",
    "\n",
    "p = ''\n",
    "# p = 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/p3Y1X01.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.619855Z",
     "start_time": "2019-07-25T23:23:11.608276Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_dir_exists(path.dirname(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.636349Z",
     "start_time": "2019-07-25T23:23:11.624738Z"
    }
   },
   "outputs": [],
   "source": [
    "if p == '':\n",
    "    r = False\n",
    "else:\n",
    "    r = True\n",
    "    print('Including preview distribution in channel matrix calculations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.670762Z",
     "start_time": "2019-07-25T23:23:11.641472Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:11.681056Z",
     "start_time": "2019-07-25T23:23:11.675406Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:13.026763Z",
     "start_time": "2019-07-25T23:23:11.690116Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:13.047641Z",
     "start_time": "2019-07-25T23:23:13.038935Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:13.075467Z",
     "start_time": "2019-07-25T23:23:13.053788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.get_device_name(1))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(1)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:13.088538Z",
     "start_time": "2019-07-25T23:23:13.081442Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "my_device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:13.103514Z",
     "start_time": "2019-07-25T23:23:13.094611Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda_ft = torch.cuda.FloatTensor\n",
    "cuda_dt = torch.cuda.DoubleTensor\n",
    "\n",
    "ft = torch.FloatTensor\n",
    "dt = torch.DoubleTensor\n",
    "\n",
    "my_ft = ft\n",
    "my_dt = dt\n",
    "\n",
    "torch.set_default_tensor_type(my_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:14.078582Z",
     "start_time": "2019-07-25T23:23:13.109396Z"
    }
   },
   "outputs": [],
   "source": [
    "p3Y1X012 = condDistsAsProbDists(importProbDist(c))\n",
    "\n",
    "assert uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:14.087266Z",
     "start_time": "2019-07-25T23:23:14.082659Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condDistsAsProbDists(importProbDist(p))\n",
    "    assert uniformOutcomes(pY1X01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:14.197144Z",
     "start_time": "2019-07-25T23:23:14.091198Z"
    }
   },
   "outputs": [],
   "source": [
    "pW_V = condDistsAsProbDists(importProbDist(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:23.752125Z",
     "start_time": "2019-07-25T23:23:14.201206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Wordforms| = 9172\n",
      "|Prefixes| = 42231\n",
      "|triphones| in lexicon = 7412\n"
     ]
    }
   ],
   "source": [
    "#extract segmental wordforms from w\n",
    "Ws = union(list(map(lambda d: set(conditions(d)), \n",
    "                    pW_V.values())))\n",
    "Ws_t = tuple(sorted(list(Ws)))\n",
    "print(f'|Wordforms| = {len(Ws)}')\n",
    "\n",
    "#extract prefixes from w\n",
    "Ps = union(map(getPrefixes, Ws))\n",
    "prefixes = Ps\n",
    "print(f'|Prefixes| = {len(Ps)}')\n",
    "Ps_t = tuple(sorted(list(Ws)))\n",
    "prefixes_t = Ps_t\n",
    "\n",
    "#extract inventory from w\n",
    "Xs = lexiconToInventory(Ws)\n",
    "    \n",
    "#extract triphones from w\n",
    "lexiconTriphones = lexiconTo3factors(Ws)\n",
    "print(f'|triphones| in lexicon = {len(lexiconTriphones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:23.782671Z",
     "start_time": "2019-07-25T23:23:23.756517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|triphones| in channel model = 7381\n",
      "|Y1s| = 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract triphones from c\n",
    "channelTriphones = set(p3Y1X012.keys())\n",
    "\n",
    "print(f'|triphones| in channel model = {len(channelTriphones)}')\n",
    "\n",
    "X012s = channelTriphones\n",
    "X012s_t = tuple(sorted(list(X012s)))\n",
    "\n",
    "#extract response phones\n",
    "Y1s = outcomes(p3Y1X012)\n",
    "Y1s_t = tuple(sorted(list(Y1s)))\n",
    "print(f'|Y1s| = {len(Y1s)}')\n",
    "\n",
    "leftEdge in Y1s\n",
    "rightEdge in Y1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:23.831288Z",
     "start_time": "2019-07-25T23:23:23.785639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "{'ŋ.⋉.⋉', 'aʊ.⋉.⋉', 'ʒ.⋉.⋉', 'i.⋉.⋉', 'eɪ.⋉.⋉', 'z.⋉.⋉', 'f.⋉.⋉', 't.⋉.⋉', 'b.⋉.⋉', 'oʊ.⋉.⋉', 'aɪ.⋉.⋉', 'm.⋉.⋉', 'ɹ.⋉.⋉', 's.⋉.⋉', 'd.⋉.⋉', 'g.⋉.⋉', 'θ.⋉.⋉', 'ɚ.⋉.⋉', 'ɑ.⋉.⋉', 'dʒ.⋉.⋉', 'ʃ.⋉.⋉', 'ð.⋉.⋉', 'tʃ.⋉.⋉', 'p.⋉.⋉', 'ɔɪ.⋉.⋉', 'ə.⋉.⋉', 'u.⋉.⋉', 'v.⋉.⋉', 'n.⋉.⋉', 'k.⋉.⋉', 'l.⋉.⋉'}\n"
     ]
    }
   ],
   "source": [
    "missing_from_channel = {triph for triph in lexiconTriphones if triph not in channelTriphones}\n",
    "if len(missing_from_channel) > 0:\n",
    "    print(len(missing_from_channel))\n",
    "    print(missing_from_channel)\n",
    "\n",
    "assert len(missing_from_channel) == 0 or all({rightEdge + '.' + rightEdge in triph for triph in missing_from_channel})\n",
    "# assert all({triph in channelTriphones for triph in lexiconTriphones})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are $x_0.⋉.⋉$ triphones in the lexicon triphone set, then we need to modify the central distribution:\n",
    " - we need to add all the relevant $x_0.⋉.⋉$ triphones to the conditions\n",
    " - the $p(Y1|\\cdot)$ distribution for each of them needs to have all its mass on ⋉\n",
    " - ⋉ needs to be added to the outcomes of every other kind of condition with probability zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:23.838924Z",
     "start_time": "2019-07-25T23:23:23.834588Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.180127Z",
     "start_time": "2019-07-25T23:23:23.841983Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(missing_from_channel) > 0:\n",
    "    assert not r\n",
    "    \n",
    "    #Convert p3Y1X012 back to a dictionary of dictionaries\n",
    "    p3Y1X012 = condProbDistAsDicts(p3Y1X012)\n",
    "    \n",
    "    #add the new conditions\n",
    "    for triph in missing_from_channel:\n",
    "        p3Y1X012[triph] = {y1:0.0 for y1 in Y1s}\n",
    "    \n",
    "    #add the new outcome, with appropriate probability mass\n",
    "    for x012 in p3Y1X012:\n",
    "        if x012 not in missing_from_channel:\n",
    "            p3Y1X012[x012][rightEdge] = 0.0\n",
    "        else:\n",
    "            p3Y1X012[x012][rightEdge] = 1.0\n",
    "    \n",
    "    #define a new set of outcomes and conditions\n",
    "    Y1s_RE = Y1s | {rightEdge}\n",
    "    Y1s_RE_t = tuple(sorted(list(Y1s_RE)))\n",
    "    \n",
    "    X012s_RE = X012s | missing_from_channel\n",
    "    X012s_RE_t = tuple(sorted(list(X012s_RE)))\n",
    "    \n",
    "    #define a placeholder for the old outcomes and conditions\n",
    "    Y1s_old = deepcopy(Y1s)\n",
    "    Y1s_t_old = deepcopy(Y1s_t)\n",
    "    \n",
    "    X012s_old = deepcopy(X012s)\n",
    "    X012s_t_old = deepcopy(X012s_t)\n",
    "    \n",
    "    #replace the old variables\n",
    "    Y1s = Y1s_RE\n",
    "    Y1s_t = Y1s_RE_t\n",
    "    \n",
    "    X012s = X012s_RE\n",
    "    X012s_t = X012s_RE_t\n",
    "    \n",
    "    assert areNormalized(p3Y1X012)\n",
    "    assert uniformOutcomes(p3Y1X012)\n",
    "    \n",
    "    #convert p3Y1X012 back to a dictionary of ProbDists\n",
    "    p3Y1X012 = condDistsAsProbDists(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.190236Z",
     "start_time": "2019-07-25T23:23:24.183342Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "    print(f'|X012s| in channel model = {len(channelDiphones)}')\n",
    "    \n",
    "    lexiconDiphones = lexiconTo2factors(Ws)\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    print(f'|X012s| in lexicon = {len(lexiconDiphones)}')\n",
    "    \n",
    "    X01s = lexiconDiphones\n",
    "    assert outcomes(p3Y1X01) == Y1s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gating trials that bear on $p(Y_{i+1}|X_i; X_{i+1} = ⋉)$, but a reasonable assumption is that there are plenty of good acoustic cues that any given segment $X_i$ is the end of the word (i.e. that $X_{i+1} = ⋉$) given the context of an isolated word recognition task, and that there are plenty of good acoustic cues that any given segment is NOT the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.205565Z",
     "start_time": "2019-07-25T23:23:24.193795Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01 = condProbDistAsDicts(p3Y1X01)\n",
    "    \n",
    "    # add ⋉ to the outcomes of every existing conditioning outcome\n",
    "    for x01 in p3Y1X01:\n",
    "        p3Y1X01[x01].update({rightEdge:0.0})\n",
    "\n",
    "    # create new conditioning events\n",
    "    wordEndDiphones = {x + '.' + rightEdge for x in Xs}\n",
    "    list(wordEndDiphones)[:5]\n",
    "\n",
    "    # create their distribution over outcomes\n",
    "    deltaDist = {y1:0.0 for y1 in Y1s}\n",
    "    deltaDist.update({rightEdge:1.0})\n",
    "\n",
    "    # add the new wordend conditioning events to the preview distribution\n",
    "    p3Y1X01.update({wordEnd:deltaDist for wordEnd in wordEndDiphones})\n",
    "    p3Y1X01['aʊ.s']['s']\n",
    "    p3Y1X01['ɑ.⋉']\n",
    "\n",
    "    # check that everything worked\n",
    "    for x01 in p3Y1X01:\n",
    "        assert rightEdge in p3Y1X01[x01]\n",
    "    #     if rightEdge not in p3Y1X01[x01]:\n",
    "    #         p3Y1X01[x01][rightEdge] = 0.0\n",
    "\n",
    "    assert areNormalized(p3Y1X01)\n",
    "    assert uniformOutcomes(p3Y1X01)\n",
    "\n",
    "    channelDiphones = set(p3Y1X01.keys())\n",
    "\n",
    "    unmodelableLexiconDiphones = {diph for diph in lexiconDiphones if diph not in channelDiphones}\n",
    "    print(f'unmodelable lexicon diphones = \\n{unmodelableLexiconDiphones}')\n",
    "    assert all({diph in channelDiphones for diph in lexiconDiphones if ds2t(diph)[0] != leftEdge and ds2t(diph)[1] != rightEdge})\n",
    "    \n",
    "    #we'll worry about left-edge initial diphones later\n",
    "    \n",
    "    # let's trim the preview model's conditioning events\n",
    "    p3Y1X01 = {x01:p3Y1X01[x01] for x01 in p3Y1X01 if x01 in lexiconDiphones}\n",
    "    \n",
    "    p3Y1X01 = condDistsAsProbDists(p3Y1X01)\n",
    "    \n",
    "    X01s_RE = set(p3Y1X01.keys())\n",
    "    len(X01s_RE)\n",
    "    \n",
    "#     print(X01s_RE - X01s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `numpy` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.222339Z",
     "start_time": "2019-07-25T23:23:24.218018Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmap = seqsToIndexMap(Xs)\n",
    "XOHmap = seqsToOneHotMap(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.322724Z",
     "start_time": "2019-07-25T23:23:24.228547Z"
    }
   },
   "outputs": [],
   "source": [
    "X012map = seqsToIndexMap(X012s)\n",
    "# X012OHs = seqMapToOneHots(X012map)\n",
    "X012OHmap = seqsToOneHotMap(X012s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.329841Z",
     "start_time": "2019-07-25T23:23:24.325785Z"
    }
   },
   "outputs": [],
   "source": [
    "Y1map = seqsToIndexMap(Y1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.340084Z",
     "start_time": "2019-07-25T23:23:24.333392Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    X01REmap = seqsToIndexMap(X01s_RE)\n",
    "    X01REOHs = seqMapToOneHots(X01REmap)\n",
    "    X01REOHmap = seqsToOneHotMap(X01s_RE)\n",
    "    \n",
    "    Y1s_RE = outcomes(p3Y1X01)\n",
    "    len(Y1s_RE)\n",
    "    Y1s_RE_list = sorted(list(Y1s_RE))\n",
    "\n",
    "    print(Y1s_RE - Y1s)\n",
    "\n",
    "    Y1REmap = seqsToIndexMap(Y1s_RE)\n",
    "\n",
    "    Y1REOHs = seqMapToOneHots(Y1REmap)\n",
    "    Y1REOHmap = seqsToOneHotMap(Y1s_RE)\n",
    "    OHY1REmap = oneHotToSeqMap(Y1s_RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `r` is `True`, then to ensure uniformity of event spaces between the triphone channel distribution and the preview distribution, we'll add a $⋉$ outcome (with probability 0.0) to each conditional distribution in the triphone channel distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.349794Z",
     "start_time": "2019-07-25T23:23:24.343610Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    for x012 in p3Y1X012:\n",
    "        p3Y1X012[x012].update({rightEdge:0.0})\n",
    "        assert rightEdge in p3Y1X012[x012]\n",
    "        assert p3Y1X012[x012][rightEdge] == 0.0\n",
    "\n",
    "    outcomes(p3Y1X012) == Y1s\n",
    "    outcomes(p3Y1X012) == Y1s_RE\n",
    "    areNormalized(p3Y1X012)\n",
    "    uniformOutcomes(p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.400677Z",
     "start_time": "2019-07-25T23:23:24.352966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  6, 12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('t.i.f', 'i.f.l')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3756, 1449])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 7412)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7412,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dsToUniphoneIndices(ds, uniphoneToIndexMap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToIndexMap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToUniphoneOHs(ds, uniphoneToOHmap):\n",
    "    uniphoneSeq = ds2t(ds)\n",
    "    return np.array([uniphoneToOHmap[uniphone] for uniphone in uniphoneSeq])\n",
    "\n",
    "def dsToTriphoneSeq(ds):\n",
    "    return dsToKfactorSequence(3, ds)\n",
    "\n",
    "def dsToTriphoneIndices(ds, triphoneToIndexMap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToIndexMap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "def dsToTriphoneOHs(ds, triphoneToOHmap):\n",
    "    triphoneSeq = dsToTriphoneSeq(ds)\n",
    "    return np.array([triphoneToOHmap[triphone] for triphone in triphoneSeq])\n",
    "\n",
    "dsToUniphoneIndices('t.i.f.l', Xmap)\n",
    "dsToUniphoneOHs('t.i.f.l', XOHmap)\n",
    "dsToTriphoneSeq('t.i.f.l')\n",
    "dsToTriphoneIndices('t.i.f.l', X012map)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap).shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0].shape\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[0][5528]\n",
    "dsToTriphoneOHs('t.i.f.l', X012OHmap)[1][5352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.712924Z",
     "start_time": "2019-07-25T23:23:24.403974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 7412)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3Y1X012_np = condDistFamilyToNP(p3Y1X012)\n",
    "if r:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1REmap, p3Y1X012)\n",
    "else:\n",
    "    testNPcondDist(p3Y1X012_np, X012map, Y1map, p3Y1X012)\n",
    "p3Y1X012_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.720219Z",
     "start_time": "2019-07-25T23:23:24.716070Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    p3Y1X01_np = condDistFamilyToNP(p3Y1X01)\n",
    "    testNPcondDist(p3Y1X01_np, X01REmap, Y1REmap, p3Y1X01)\n",
    "    p3Y1X01_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.731094Z",
     "start_time": "2019-07-25T23:23:24.723346Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.744492Z",
     "start_time": "2019-07-25T23:23:24.733929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.v.æ.l.ə.d.eɪ.t.⋉.⋉'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_wordform = choice(list(Ws))\n",
    "random_source_wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.759159Z",
     "start_time": "2019-07-25T23:23:24.747850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.t.ɪ.l.ɚ.⋉.⋉'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.768202Z",
     "start_time": "2019-07-25T23:23:24.762702Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomPrefix(l, alphabet=Xs):\n",
    "    return randomString(alphabet, l, hasLeftEdge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.782527Z",
     "start_time": "2019-07-25T23:23:24.771110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.ɹ.s.tʃ.ɹ.h.b.l.v.b'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_channel_prefix2 = randomPrefix(len(ds2t(random_source_wordform))-1, alphabet=Y1s)\n",
    "random_channel_prefix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.807593Z",
     "start_time": "2019-07-25T23:23:24.786011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.u.t.ɹ.i.ə'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.u.ʌ.⋉.w.ð.æ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_source_prefix = getRandomKey(pX0i)\n",
    "random_source_prefix = choice(list(Ps))\n",
    "while ds2t(random_source_prefix)[-1] == rightEdge:\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "while len(ds2t(random_source_prefix)) > len(ds2t(random_source_wordform)):\n",
    "#     random_source_prefix = getRandomKey(pX0i)\n",
    "    random_source_prefix = choice(list(Ps))\n",
    "random_source_prefix\n",
    "random_channel_prefix = randomPrefix(len(ds2t(random_source_prefix))-1, alphabet=Y1s)\n",
    "random_channel_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.822622Z",
     "start_time": "2019-07-25T23:23:24.811305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('⋊.n.u', 'n.u.t', 'u.t.ɹ', 't.ɹ.i', 'ɹ.i.ə')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'⋊.n.u.t.ɹ.i.ə'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphones(x0k):\n",
    "    xp_t = ds2t(x0k) #\"x prefix\"\n",
    "    \n",
    "#     xi = xp_t[-2] #just-completed segment\n",
    "#     xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "    \n",
    "#     xik_ds = t2ds((xi, xk))\n",
    "#     preview_dist = p3Y1X01[xik_ds]\n",
    "    \n",
    "    x012s = dsToKfactorSequence(3, t2ds(xp_t))\n",
    "    return x012s\n",
    "\n",
    "random_triphoneSeq = sourcePrefixToTriphones(random_source_prefix)\n",
    "random_triphoneSeq\n",
    "threeFactorSequenceToDS(random_triphoneSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.833577Z",
     "start_time": "2019-07-25T23:23:24.825966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7128, 2764, 4179, 3963, 6238)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sourcePrefixToTriphoneIndices(x0k):\n",
    "    triphoneSequence = sourcePrefixToTriphones(x0k)\n",
    "    return tuple(map(lambda x012: X012map[x012], triphoneSequence))\n",
    "\n",
    "sourcePrefixToTriphoneIndices(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.847067Z",
     "start_time": "2019-07-25T23:23:24.836948Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = np.zeros((len(Y1s), 1))\n",
    "blah[-1] = 1.0\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.925922Z",
     "start_time": "2019-07-25T23:23:24.850111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.n.u.t.ɹ.i.ə'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "def sourcePrefixToChannelMatrix_l(x0k, debug=False):\n",
    "    triphoneOHs = dsToTriphoneOHs(x0k, X012OHmap)\n",
    "    if debug:\n",
    "        print('x0k = {0}'.format(x0k))\n",
    "        print('|x0k| = {0}'.format(len(x0k)))\n",
    "        print('triphoneIdxs = {0}'.format(sourcePrefixToTriphoneIndices(x0k)))\n",
    "        print('triphoneOHs.shape = {0}'.format(triphoneOHs.shape))\n",
    "        print('p3Y1X012_np.shape = {0}'.format(p3Y1X012_np.shape))\n",
    "        print('result = p3Y1X012_np * triphoneOHs.T')\n",
    "    result = np.matmul(p3Y1X012_np, triphoneOHs.T)\n",
    "    return result\n",
    "# sourcePrefixToChannelMatrix_l(random_source_prefix, True)\n",
    "\n",
    "if r:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "        C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "    #     C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "    #                    for x012_idx in triphoneIndices] \n",
    "    #                   for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "            C = np.zeros((len(Y1s_RE), 1))\n",
    "    #         C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "            return C.reshape(len(Y1s_RE),1)\n",
    "    #         return C.reshape(len(Y1s),1)\n",
    "        return C\n",
    "else:\n",
    "    def sourcePrefixToChannelMatrix(x0k):\n",
    "        triphoneIndices = sourcePrefixToTriphoneIndices(x0k)\n",
    "#         C = np.array([[p3Y1X012_np[Y1REmap[y1], x012_idx] for x012_idx in triphoneIndices] for y1 in sorted(Y1s_RE)])\n",
    "        C = np.array([[p3Y1X012_np[Y1map[y1], x012_idx] \n",
    "                       for x012_idx in triphoneIndices] \n",
    "                      for y1 in Y1s_t])\n",
    "        if x0k == leftEdge or (len(ds2t(x0k)) == 2 and ds2t(x0k)[0] == leftEdge):\n",
    "#             C = np.zeros((len(Y1s_RE), 1))\n",
    "            C = np.zeros((len(Y1s), 1))\n",
    "            C[-1] = 1.0\n",
    "#             return C.reshape(len(Y1s_RE),1)\n",
    "            return C.reshape(len(Y1s),1)\n",
    "        assert len(triphoneIndices) == len(dsToKfactorSequence(3, x0k)), f\"{len(triphoneIndices)} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0k = {x0k}\\n\\t {dsToKfactorSequence(3, x0k)}\\n\\t {triphoneIndices}\"\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == C.shape[1], f\"{C.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\"\n",
    "        return C\n",
    "\n",
    "\n",
    "# sourcePrefixToChannelMatrix(random_source_prefix)\n",
    "\n",
    "random_source_prefix\n",
    "sourcePrefixToChannelMatrix_l(random_source_prefix).shape\n",
    "print(sourcePrefixToChannelMatrix_l(random_source_prefix) == sourcePrefixToChannelMatrix(random_source_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.938160Z",
     "start_time": "2019-07-25T23:23:24.930346Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:24.958415Z",
     "start_time": "2019-07-25T23:23:24.942433Z"
    }
   },
   "outputs": [],
   "source": [
    "random_source_prefixes = choices(Ps_t, k=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:33.590253Z",
     "start_time": "2019-07-25T23:23:24.962596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 ms ± 26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "sourcePrefixToChannelMatrix_l(choice(random_source_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:33.609062Z",
     "start_time": "2019-07-25T23:23:33.596173Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    def sourcePrefixToPreviewVector(x0k):\n",
    "        xp_t = ds2t(x0k) #\"x prefix\"\n",
    "\n",
    "        if len(xp_t) < 2:\n",
    "            raise Exception('|x0k| must be > 1.')\n",
    "        if len(xp_t) == 2 and xp_t[0] == leftEdge:\n",
    "    #         raise Exception(\"There's no gating data that bears on this calculation, nor is it that interesting.\")\n",
    "            uniformProb = 1.0 / len(Y1s_RE)\n",
    "            preview_dist = uniformProb * np.ones((len(Y1s_RE), 1))#garbage\n",
    "            return preview_dist.reshape(len(Y1s_RE),1)\n",
    "\n",
    "        xi = xp_t[-2] #just-completed segment\n",
    "        xk = xp_t[-1] #upcoming segment that we only get coarticulatory information about\n",
    "\n",
    "        xik_ds = t2ds((xi, xk))\n",
    "        preview_dist = p3Y1X01[xik_ds]\n",
    "    #     assert Y1s_RE == set(preview_dist.keys()) #comment out once you are reasonably confident this is true by construction\n",
    "\n",
    "        return np.array([preview_dist[y1] for y1 in sorted(Y1s_RE)])\n",
    "\n",
    "    sourcePrefixToPreviewVector(random_source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:33.658176Z",
     "start_time": "2019-07-25T23:23:33.616470Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0k)\n",
    "    def makeExtendedChannelMatrixByPrefix(prefix):\n",
    "        # NB:\n",
    "        # if len(prefix) == n (including leftEdge), \n",
    "        # then the extended channel matrix will have dimensions 39 x (n-1)\n",
    "\n",
    "        p = prefix\n",
    "        if prefix != leftEdge:# and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "    #     if prefix != leftEdge and not (len(ds2t(p)) == 2 and ds2t(p)[0] == leftEdge):\n",
    "            return np.hstack( (sourcePrefixToChannelMatrix(p) , sourcePrefixToPreviewVector(p).reshape(39,1)))\n",
    "        else: #the extended channel matrix is garbage that should never be asked for\n",
    "            l = len(ds2t(p))\n",
    "            return np.zeros((39, l-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:33.697549Z",
     "start_time": "2019-07-25T23:23:33.662513Z"
    }
   },
   "outputs": [],
   "source": [
    "# if f:\n",
    "#     print('Source sequences = wordforms and prefixes')\n",
    "#     source_seqs = prefixes_t #prefixes include full wordforms\n",
    "# else:\n",
    "#     print('Source sequences = just full wordforms')\n",
    "#     source_seqs = Ws_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:33.726880Z",
     "start_time": "2019-07-25T23:23:33.701420Z"
    }
   },
   "outputs": [],
   "source": [
    "# if r:\n",
    "#     xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(s)\n",
    "#                          for s in source_seqs]\n",
    "#     xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "#     xCMsByPrefixIndex[3].shape\n",
    "if r:\n",
    "    xCMsByPrefixIndex = [makeExtendedChannelMatrixByPrefix(p)\n",
    "                         for p in prefixes_t]\n",
    "    xCMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in xCMsByPrefixIndex[1:]]\n",
    "\n",
    "    xCMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:43.070787Z",
     "start_time": "2019-07-25T23:23:33.738443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(s)\n",
    "#                      for s in source_seqs]\n",
    "# CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "# CMsByPrefixIndex[3].shape\n",
    "CMsByPrefixIndex = [sourcePrefixToChannelMatrix_l(p)\n",
    "                     for p in prefixes_t]\n",
    "CMsByPrefixIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByPrefixIndex[1:]]\n",
    "\n",
    "CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.242205Z",
     "start_time": "2019-07-25T23:23:43.082195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMsByWordformIndex = [sourcePrefixToChannelMatrix_l(w)\n",
    "                     for w in Ws_t]\n",
    "CMsByWordformIndex_torch = [None] + [torch.from_numpy(each) for each in CMsByWordformIndex[1:]]\n",
    "\n",
    "CMsByWordformIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.253181Z",
     "start_time": "2019-07-25T23:23:52.247497Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsOfLength(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         return {w for w in Ws if len(ds2t(w)) == l}\n",
    "#     return {w for w in Ws if len(ds2t(w)) == l + 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.314213Z",
     "start_time": "2019-07-25T23:23:52.256646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'⋊.aɪ.s.ə.l.eɪ.ʃ.ɪ.n.ɪ.z.m.⋉.⋉',\n",
       " '⋊.b.aɪ.oʊ.k.ɛ.m.ə.s.t.ɹ.i.⋉.⋉',\n",
       " '⋊.b.ɪ.b.l.i.ɑ.g.ɹ.ə.f.i.⋉.⋉',\n",
       " '⋊.d.ɑ.k.j.ʊ.m.ɛ.n.t.ɚ.i.⋉.⋉',\n",
       " '⋊.d.ə.l.æ.p.ə.d.eɪ.t.ɪ.d.⋉.⋉',\n",
       " '⋊.d.ɛ.m.ə.n.s.t.ɹ.eɪ.t.ɚ.⋉.⋉',\n",
       " '⋊.d.ɪ.s.k.w.ə.z.ɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.d.ɪ.s.t.ɹ.ɪ.b.j.u.t.ɚ.⋉.⋉',\n",
       " '⋊.d.ɪ.s.ə.b.i.d.i.ə.n.s.⋉.⋉',\n",
       " '⋊.d.ɪ.s.ɪ.d.v.æ.n.t.ɪ.dʒ.⋉.⋉',\n",
       " '⋊.d.ɪ.s.ɪ.n.t.ɪ.g.ɹ.eɪ.t.⋉.⋉',\n",
       " '⋊.d.ɪ.t.ɚ.m.ə.n.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.dʒ.i.oʊ.p.ɑ.l.ə.t.ɪ.k.s.⋉.⋉',\n",
       " '⋊.dʒ.ʊ.ɹ.ɪ.s.d.ɪ.k.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.f.ɑ.ɹ.m.æ.l.d.ə.h.aɪ.d.⋉.⋉',\n",
       " '⋊.f.ɪ.l.ɪ.n.θ.ɹ.ɑ.p.ɪ.k.⋉.⋉',\n",
       " '⋊.h.aɪ.p.ə.k.ɑ.n.d.ɹ.i.ə.⋉.⋉',\n",
       " '⋊.h.ɑ.s.p.ə.t.æ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.h.ɪ.s.t.ɚ.ɛ.k.t.ə.m.i.⋉.⋉',\n",
       " '⋊.j.u.n.ə.f.ɑ.ɹ.m.ə.t.i.⋉.⋉',\n",
       " '⋊.j.u.n.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.j.ʊ.n.æ.n.ə.m.ə.s.l.i.⋉.⋉',\n",
       " '⋊.j.ʊ.ɹ.ə.n.æ.l.ɪ.s.ɪ.s.⋉.⋉',\n",
       " '⋊.k.aɪ.ɹ.oʊ.p.ɹ.æ.k.t.ɪ.k.⋉.⋉',\n",
       " '⋊.k.aʊ.n.t.ɚ.b.æ.l.ɪ.n.s.⋉.⋉',\n",
       " '⋊.k.j.u.m.j.ʊ.l.ɪ.t.ɪ.v.⋉.⋉',\n",
       " '⋊.k.w.ɑ.n.t.ɪ.t.eɪ.t.ɪ.v.⋉.⋉',\n",
       " '⋊.k.æ.l.k.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.æ.l.ɪ.s.θ.ɛ.n.ɪ.k.s.⋉.⋉',\n",
       " '⋊.k.æ.t.ɪ.s.t.ɹ.ɑ.f.ɪ.k.⋉.⋉',\n",
       " '⋊.k.ɑ.m.j.u.n.ɪ.s.t.ɪ.k.⋉.⋉',\n",
       " '⋊.k.ɑ.m.p.l.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.m.p.ə.n.s.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.d.ə.m.ɪ.n.i.ə.m.⋉.⋉',\n",
       " '⋊.k.ɑ.n.d.ɛ.m.n.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.d.ɛ.n.s.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.s.t.ɚ.n.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.s.t.ɪ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.s.t.ɪ.t.u.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.n.s.ɪ.k.ɹ.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.z.m.ə.p.ɑ.l.ɪ.t.n.⋉.⋉',\n",
       " '⋊.k.ɑ.ŋ.g.ɹ.ɪ.g.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ɑ.ɹ.b.oʊ.h.aɪ.d.ɹ.eɪ.t.⋉.⋉',\n",
       " '⋊.k.ə.n.g.ɹ.æ.tʃ.ə.l.eɪ.t.⋉.⋉',\n",
       " '⋊.k.ə.n.s.p.ɪ.k.j.u.ə.s.⋉.⋉',\n",
       " '⋊.k.ə.n.s.t.ɹ.ʌ.k.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.k.ə.n.s.ɚ.v.ə.t.oʊ.ɹ.i.⋉.⋉',\n",
       " '⋊.k.ə.n.s.ɚ.v.ə.t.ɪ.z.m.⋉.⋉',\n",
       " '⋊.k.ə.n.s.ɛ.k.j.u.t.ɪ.v.⋉.⋉',\n",
       " '⋊.k.ə.n.t.æ.m.ə.n.ɪ.n.t.⋉.⋉',\n",
       " '⋊.k.ə.n.t.ɪ.n.dʒ.ɪ.n.s.i.⋉.⋉',\n",
       " '⋊.k.ə.n.t.ɹ.ɪ.b.j.u.t.ɚ.⋉.⋉',\n",
       " '⋊.k.ɹ.ɛ.d.ə.b.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.k.ɹ.ɪ.m.ə.n.ɑ.l.ə.dʒ.i.⋉.⋉',\n",
       " '⋊.k.ɹ.ɪ.s.æ.n.θ.ə.m.ə.m.⋉.⋉',\n",
       " '⋊.l.ɪ.ŋ.g.w.ɪ.s.t.ɪ.k.s.⋉.⋉',\n",
       " '⋊.m.aɪ.k.ɹ.ɪ.s.k.ɑ.p.ɪ.k.⋉.⋉',\n",
       " '⋊.m.æ.g.n.ɪ.f.ɪ.s.ɪ.n.t.⋉.⋉',\n",
       " '⋊.m.æ.l.n.u.t.ɹ.ɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.m.æ.n.j.u.s.k.ɹ.ɪ.p.t.⋉.⋉',\n",
       " '⋊.m.ɑ.d.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.m.ɑ.n.s.t.ɹ.ɑ.s.ɪ.t.i.⋉.⋉',\n",
       " '⋊.m.ɛ.t.ɹ.ə.p.ɑ.l.ɪ.t.n.⋉.⋉',\n",
       " '⋊.m.ɪ.l.ɪ.t.ɚ.ɪ.s.t.ɪ.k.⋉.⋉',\n",
       " '⋊.n.oʊ.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.i.d.i.ə.t.ɹ.ɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.oʊ.s.t.g.ɹ.æ.dʒ.u.ɪ.t.⋉.⋉',\n",
       " '⋊.p.oʊ.s.t.m.ɪ.s.t.ɹ.ɪ.s.⋉.⋉',\n",
       " '⋊.p.ɑ.ɹ.n.ə.g.ɹ.æ.f.ɪ.k.⋉.⋉',\n",
       " '⋊.p.ɑ.ɹ.s.ə.m.oʊ.n.i.ə.s.⋉.⋉',\n",
       " '⋊.p.ɹ.i.z.ɛ.n.t.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.ɹ.oʊ.f.ə.l.æ.k.t.ɪ.k.⋉.⋉',\n",
       " '⋊.p.ɹ.oʊ.t.æ.g.ɪ.n.ɪ.s.t.⋉.⋉',\n",
       " '⋊.p.ɹ.ɑ.b.l.ə.m.æ.t.ɪ.k.⋉.⋉',\n",
       " '⋊.p.ɹ.ɑ.b.ə.b.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.p.ɹ.ɑ.k.l.ə.m.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.ɹ.ɑ.s.t.ɪ.t.u.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.ɹ.ɑ.s.ɪ.k.j.u.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.p.ɹ.ə.m.ɪ.s.k.j.u.ɪ.s.⋉.⋉',\n",
       " '⋊.p.ɹ.ɪ.d.ɪ.k.ə.m.ə.n.t.⋉.⋉',\n",
       " '⋊.p.ɹ.ɪ.z.ʌ.m.p.tʃ.u.ə.s.⋉.⋉',\n",
       " '⋊.p.ɹ.ɪ.ɹ.ɛ.k.w.ə.z.ɪ.t.⋉.⋉',\n",
       " '⋊.s.aɪ.k.ə.s.ə.m.æ.t.ɪ.k.⋉.⋉',\n",
       " '⋊.s.m.ɑ.ɹ.g.ɪ.s.b.ɑ.ɹ.d.⋉.⋉',\n",
       " '⋊.s.p.ɑ.n.t.ɪ.n.i.ə.t.i.⋉.⋉',\n",
       " '⋊.s.p.ɛ.k.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.p.ɛ.k.t.æ.k.j.ʊ.l.ɚ.⋉.⋉',\n",
       " '⋊.s.t.æ.t.ɪ.s.t.ɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.t.ɪ.m.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.t.ɪ.p.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.t.ɹ.eɪ.t.f.ɑ.ɹ.w.ɚ.d.⋉.⋉',\n",
       " '⋊.s.u.p.ɚ.s.t.ɹ.ʌ.k.tʃ.ɚ.⋉.⋉',\n",
       " '⋊.s.æ.t.ɪ.s.f.æ.k.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.ə.b.s.t.æ.n.tʃ.i.eɪ.t.⋉.⋉',\n",
       " '⋊.s.ɚ.t.ɪ.f.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.s.ɛ.n.s.ə.b.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.s.ɛ.n.s.ɪ.t.ɪ.v.ə.t.i.⋉.⋉',\n",
       " '⋊.s.ɪ.g.n.ɪ.f.ɪ.k.ɪ.n.s.⋉.⋉',\n",
       " '⋊.s.ʌ.b.s.t.ɪ.t.u.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.t.ɑ.k.s.ɪ.k.ɑ.l.ə.dʒ.i.⋉.⋉',\n",
       " '⋊.t.ɛ.k.n.ə.k.æ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.t.ɹ.æ.n.k.w.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.t.ɹ.ɪ.b.j.ʊ.l.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.æ.b.n.ɑ.ɹ.m.æ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.æ.d.v.ɚ.t.aɪ.z.m.ə.n.t.⋉.⋉',\n",
       " '⋊.æ.n.θ.ɹ.ə.p.ɑ.l.ə.dʒ.i.⋉.⋉',\n",
       " '⋊.æ.s.t.ɹ.ə.f.ɪ.z.ɪ.k.s.⋉.⋉',\n",
       " '⋊.ɑ.b.dʒ.ɛ.k.t.ɪ.v.ə.t.i.⋉.⋉',\n",
       " '⋊.ɑ.n.t.ɹ.ə.p.ɹ.ə.n.ʊ.ɹ.⋉.⋉',\n",
       " '⋊.ɑ.t.ə.b.aɪ.ɑ.g.ɹ.ə.f.i.⋉.⋉',\n",
       " '⋊.ɑ.ɹ.g.ɪ.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ə.f.oʊ.ɹ.m.ɛ.n.tʃ.ɪ.n.d.⋉.⋉',\n",
       " '⋊.ə.l.ɛ.k.t.ɹ.ɪ.s.ɪ.t.i.⋉.⋉',\n",
       " '⋊.ə.p.ɛ.n.d.ɪ.s.aɪ.t.ɪ.s.⋉.⋉',\n",
       " '⋊.ɛ.k.s.p.l.ɔɪ.t.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɛ.k.s.p.l.ɪ.n.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɛ.k.s.p.ɛ.k.t.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɛ.k.s.t.ɹ.ə.v.ɚ.t.ɪ.d.⋉.⋉',\n",
       " '⋊.ɛ.l.ɪ.dʒ.ɪ.b.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.ɛ.n.v.aɪ.ɹ.ə.n.m.ə.n.t.⋉.⋉',\n",
       " '⋊.ɪ.g.z.ɑ.ɹ.b.ə.t.ɪ.n.t.⋉.⋉',\n",
       " '⋊.ɪ.k.s.t.ɹ.æ.p.ə.l.eɪ.t.⋉.⋉',\n",
       " '⋊.ɪ.m.j.u.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɪ.m.p.ɹ.ə.p.ɹ.aɪ.ə.t.i.⋉.⋉',\n",
       " '⋊.ɪ.n.d.ɑ.k.t.ɹ.ə.n.eɪ.t.⋉.⋉',\n",
       " '⋊.ɪ.n.d.ə.p.ɛ.n.d.ɪ.n.s.⋉.⋉',\n",
       " '⋊.ɪ.n.d.ə.p.ɛ.n.d.ɪ.n.t.⋉.⋉',\n",
       " '⋊.ɪ.n.f.l.æ.m.ə.t.oʊ.ɹ.i.⋉.⋉',\n",
       " '⋊.ɪ.n.f.ɪ.l.t.ɹ.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɪ.n.f.ɪ.ɹ.i.ɑ.ɹ.ə.t.i.⋉.⋉',\n",
       " '⋊.ɪ.n.h.j.u.m.æ.n.ɪ.t.i.⋉.⋉',\n",
       " '⋊.ɪ.n.k.ɑ.m.p.ə.t.ɪ.n.t.⋉.⋉',\n",
       " '⋊.ɪ.n.k.ɪ.n.s.ɪ.d.ɚ.ɪ.t.⋉.⋉',\n",
       " '⋊.ɪ.n.s.t.ə.b.ɪ.l.ɪ.t.i.⋉.⋉',\n",
       " '⋊.ɪ.n.t.ɹ.ə.d.ʌ.k.t.ɚ.i.⋉.⋉',\n",
       " '⋊.ɹ.oʊ.d.ə.d.ɛ.n.d.ɹ.ə.n.⋉.⋉',\n",
       " '⋊.ɹ.æ.m.ə.f.ə.k.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɹ.ɛ.dʒ.ɪ.s.t.ɹ.eɪ.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɹ.ɛ.k.t.æ.ŋ.g.j.ʊ.l.ɚ.⋉.⋉',\n",
       " '⋊.ɹ.ɛ.s.ə.p.ɹ.ɑ.s.ɪ.t.i.⋉.⋉',\n",
       " '⋊.ɹ.ɛ.t.ɹ.ə.b.j.u.ʃ.ɪ.n.⋉.⋉',\n",
       " '⋊.ɹ.ɪ.h.ə.b.ɪ.l.ɪ.t.eɪ.t.⋉.⋉',\n",
       " '⋊.ɹ.ɪ.k.ɪ.n.s.t.ɹ.ʌ.k.t.⋉.⋉',\n",
       " '⋊.ʌ.n.d.ɚ.ɛ.s.t.ə.m.eɪ.t.⋉.⋉',\n",
       " '⋊.ʌ.n.m.ɪ.t.ɪ.g.eɪ.t.ɪ.d.⋉.⋉',\n",
       " '⋊.ʌ.n.s.aɪ.ɪ.n.t.ɪ.f.ɪ.k.⋉.⋉',\n",
       " '⋊.ʌ.n.ə.k.ʌ.m.p.ə.n.i.d.⋉.⋉',\n",
       " '⋊.ʌ.n.ɪ.k.s.p.ɛ.k.t.ɪ.d.⋉.⋉',\n",
       " '⋊.ʌ.n.ɪ.n.t.ɚ.ʌ.p.t.ɪ.d.⋉.⋉',\n",
       " '⋊.ʌ.n.ɪ.n.t.ɹ.ɪ.s.t.ɪ.d.⋉.⋉'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordformsOfLength(16, Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.498085Z",
     "start_time": "2019-07-25T23:23:52.318329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{4: 5,\n",
       " 5: 173,\n",
       " 6: 1266,\n",
       " 7: 1790,\n",
       " 8: 1646,\n",
       " 9: 1328,\n",
       " 10: 1052,\n",
       " 11: 828,\n",
       " 12: 508,\n",
       " 13: 329,\n",
       " 14: 150,\n",
       " 15: 66,\n",
       " 16: 24,\n",
       " 17: 6,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges = set(len(ds2t(w)) for w in Ws)\n",
    "wordlengthsInclEdges\n",
    "numWordsOfExactlyLength = {l:len(wordformsOfLength(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "numWordsOfExactlyLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.507506Z",
     "start_time": "2019-07-25T23:23:52.501104Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsNotIncludingEdges = {each-2 for each in wordlengthsInclEdges}\n",
    "wordlengthsNotIncludingEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.514799Z",
     "start_time": "2019-07-25T23:23:52.510565Z"
    }
   },
   "outputs": [],
   "source": [
    "# def wordformsAtLeastLlong(l, includingEdges = False):\n",
    "#     if includingEdges:\n",
    "#         maxL = max(wordlengthsInclEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])\n",
    "#     else:\n",
    "#         maxL = max(wordlengthsNotIncludingEdges)\n",
    "#         return union([wordformsOfLength(eachl, includingEdges) for eachl in range(l, maxL+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.659361Z",
     "start_time": "2019-07-25T23:23:52.517403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 9172,\n",
       " 5: 9167,\n",
       " 6: 8994,\n",
       " 7: 7728,\n",
       " 8: 5938,\n",
       " 9: 4292,\n",
       " 10: 2964,\n",
       " 11: 1912,\n",
       " 12: 1084,\n",
       " 13: 576,\n",
       " 14: 247,\n",
       " 15: 97,\n",
       " 16: 31,\n",
       " 17: 7,\n",
       " 19: 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthFreqs = {l:len(wordformsAtLeastLlong(l, Ws, True)) for l in wordlengthsInclEdges}\n",
    "lengthFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.673334Z",
     "start_time": "2019-07-25T23:23:52.662905Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns p(Y0i|x0f), padded if necessary\n",
    "def makeChannelMatrixByWordformAndLength(wordform, key_length, exact_length_only = False):\n",
    "    x0f = wordform\n",
    "    x0f_t = ds2t(x0f)\n",
    "    x0f_length = len(x0f_t)\n",
    "    if x0f_length == key_length:\n",
    "        return sourcePrefixToChannelMatrix(x0f)\n",
    "    elif exact_length_only:\n",
    "        cm = np.zeros(shape=(len(Y1s), key_length - 2))\n",
    "        return cm\n",
    "    elif x0f_length > key_length:\n",
    "#         print('middle case')\n",
    "        #trim the wordform to be a prefix of length = key_length\n",
    "        x0k_t = x0f_t[:key_length]\n",
    "#         assert len(x0k_t) == key_length\n",
    "        x0k = t2ds(x0k_t)\n",
    "#         print('x0k: {0}'.format(x0k))\n",
    "        cm = sourcePrefixToChannelMatrix(x0k)\n",
    "        assert len(dsToKfactorSequence(3, x0k)) == cm.shape[1], f\"{cm.shape[1]} != {len(dsToKfactorSequence(3, x0k))}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm\n",
    "    else:\n",
    "        #grab the source \n",
    "        my_CM = sourcePrefixToChannelMatrix(x0f)\n",
    "        goal_l = key_length\n",
    "        #extend the channel matrix with padding\n",
    "        cm = np.pad(my_CM, ((0,0), (0, goal_l - my_CM.shape[1] - 2)), \n",
    "                      'constant', constant_values=0.0)\n",
    "        assert key_length - 2 == cm.shape[1], f\"{cm.shape[1]} != {key_length - 2}\\n\\t x0f = {wordform}\\n\\t key_length = {key_length}\"\n",
    "        return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.689403Z",
     "start_time": "2019-07-25T23:23:52.676089Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    # returns p(Y0K|x0f)\n",
    "    def makeExtendedChannelMatrixByWordformAndLength(wordform, key_length):\n",
    "        x0f = wordform\n",
    "        x0f_t = ds2t(x0f)\n",
    "        x0f_length = len(x0f_t)\n",
    "        if x0f_length == key_length:\n",
    "            return makeExtendedChannelMatrixByPrefix(x0f)\n",
    "        elif x0f_length > key_length:\n",
    "    #         print('middle case')\n",
    "            #trim the wordform to be a prefix of length = key_length\n",
    "            x0k_t = x0f_t[:key_length]\n",
    "            x0k = t2ds(x0k_t)\n",
    "    #         print('x0k: {0}'.format(x0k))\n",
    "            return makeExtendedChannelMatrixByPrefix(x0k)\n",
    "        else:\n",
    "            #grab the source \n",
    "            my_xCM = makeExtendedChannelMatrixByPrefix(x0f)\n",
    "            goal_l = key_length\n",
    "            return np.pad(my_xCM, ((0,0), (0, goal_l - my_xCM.shape[1] - 1)), \n",
    "                          'constant', constant_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.702658Z",
     "start_time": "2019-07-25T23:23:52.692177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlengthsInclEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:23:52.719356Z",
     "start_time": "2019-07-25T23:23:52.705654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing word lengths:\n",
      "{18}\n"
     ]
    }
   ],
   "source": [
    "if sorted(list(wordlengthsInclEdges)) != sorted(list(range(min(wordlengthsInclEdges), \\\n",
    "                                                           max(wordlengthsInclEdges)+1))):\n",
    "    print(\"Missing word lengths:\")\n",
    "    print({l for l in range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1) if l not in wordlengthsInclEdges})\n",
    "    wordlengthsInclEdges_range = list(range(min(wordlengthsInclEdges), max(wordlengthsInclEdges)+1))\n",
    "else:\n",
    "    wordlengthsInclEdges_range = sorted(list(wordlengthsInclEdges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:23.612744Z",
     "start_time": "2019-07-25T23:23:52.722661Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~17s on wittgenstein under load\n",
    "offset = [np.zeros(shape=(0,0)) for each in range(min(wordlengthsInclEdges))]\n",
    "cmsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "cmsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), cmsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:23.671128Z",
     "start_time": "2019-07-25T23:24:23.617871Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in wordlengthsInclEdges_range:\n",
    "    assert all(cm.shape[1] == l - 2 for cm in cmsByLengthByWordformIndex[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:23.703494Z",
     "start_time": "2019-07-25T23:24:23.674444Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    xCMsByLengthByWordformIndex = offset + [np.array([makeExtendedChannelMatrixByWordformAndLength(w, l)\n",
    "                                                      for w in Ws_t])\n",
    "                                            for l in wordlengthsInclEdges_range]\n",
    "    xCMsByLengthByWordformIndex_torch = list(map(lambda xCM: torch.from_numpy(xCM).type(my_ft), xCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:26.648366Z",
     "start_time": "2019-07-25T23:24:23.706879Z"
    }
   },
   "outputs": [],
   "source": [
    "exactCMsByLengthByWordformIndex = offset + [np.array([makeChannelMatrixByWordformAndLength(w, l, exact_length_only = True)\n",
    "                                                 for w in Ws_t])\n",
    "                                       for l in wordlengthsInclEdges_range]\n",
    "exactCMsByLengthByWordformIndex_torch = list(map(lambda cm: torch.from_numpy(cm).type(my_ft), exactCMsByLengthByWordformIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment sequence (all prefixes or just wordforms) channel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save \n",
    " - `CMsByPrefixIndex`\n",
    " - `CMsByWordformIndex`\n",
    " - `cmsByLengthByWordformIndex`\n",
    " - `exactCMsByLengthByWordformIndex`\n",
    " \n",
    "(and/or their extended analogues, if `r`) to disk, and when importing, we will need to know\n",
    " - the set/sequence of key strings (prefixes or just wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:26.659328Z",
     "start_time": "2019-07-25T23:24:26.652154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByWordformIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:26.672416Z",
     "start_time": "2019-07-25T23:24:26.662993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CMsByPrefixIndex)\n",
    "# CMsByPrefixIndex.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:26.696438Z",
     "start_time": "2019-07-25T23:24:26.675770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.022893312"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmsByLengthByWordformIndex)\n",
    "cmsByLengthByWordformIndex[0].shape\n",
    "cmsByLengthByWordformIndex[1].shape\n",
    "cmsByLengthByWordformIndex[2].shape\n",
    "cmsByLengthByWordformIndex[3].shape\n",
    "cmsByLengthByWordformIndex[10].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.404434Z",
     "start_time": "2019-07-25T23:24:26.699762Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByPrefixIndex, open(o + 'xCMs_by_prefix_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(CMsByPrefixIndex, open(o + 'CMs_by_prefix_index.pickle', 'wb'))\n",
    "    pickle.dump(CMsByWordformIndex, open(o + 'CMs_by_wordform_index.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.676095Z",
     "start_time": "2019-07-25T23:24:27.434577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in = pickle.load(open(o + 'CMs_by_prefix_index.pickle', 'rb'))\n",
    "    len(CMsByPrefixIndex_in)\n",
    "\n",
    "    assert all(np.array_equal(CMsByPrefixIndex_in[i], CMsByPrefixIndex[i]) for i in range(len(CMsByPrefixIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.694433Z",
     "start_time": "2019-07-25T23:24:27.681828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39, 8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not r:\n",
    "    CMsByPrefixIndex_in[3].shape\n",
    "    CMsByPrefixIndex[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.715639Z",
     "start_time": "2019-07-25T23:24:27.699133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_prefix_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByPrefixIndex) if r else len(CMsByPrefixIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_prefix_index.pickle' if r else o + 'CMs_by_prefix_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_prefix_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.732944Z",
     "start_time": "2019-07-25T23:24:27.720228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(CMsByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'CMs_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:27.743049Z",
     "start_time": "2019-07-25T23:24:27.737415Z"
    }
   },
   "outputs": [],
   "source": [
    "# importDict(o + '.pW_C' + '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.440843Z",
     "start_time": "2019-07-25T23:24:27.747719Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    pickle.dump(xCMsByLengthByWordformIndex, open(o + 'xCMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "else:\n",
    "    pickle.dump(cmsByLengthByWordformIndex, open(o + 'CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    pickle.dump(exactCMsByLengthByWordformIndex, open(o + 'exact_CMs_by_length_by_wordform_index.pickle', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.505587Z",
     "start_time": "2019-07-25T23:24:38.445959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_length_by_wordform_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_length_by_wordform_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(xCMsByLengthByWordformIndex) if r else len(cmsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted wordforms of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'P':{'from fp':w,\n",
    "         'changes':'(x)CMs constructed from sorted prefixes of W',\n",
    "         'size':len(Ps_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'xCMs_by_length_by_wordform_index.pickle' if r else o + 'CMs_by_length_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.525819Z",
     "start_time": "2019-07-25T23:24:38.510175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODexact_CMs_by_length_by_prefix_index.pickle\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODexact_CMs_by_length_by_prefix_index.pickle_metadata.json\n"
     ]
    }
   ],
   "source": [
    "exact_CMs_by_length_by_wordform_idx_md = {\n",
    "    'r':r,\n",
    "    'length':len(exactCMsByLengthByWordformIndex),\n",
    "    'W':{'from fp':w,\n",
    "         'changes':'CMs constructed from sorted wordforms of W',\n",
    "         'size':len(Ws_t)},\n",
    "    'C':{'from fp':c,\n",
    "         'changes':'None'}\n",
    "}\n",
    "\n",
    "my_fp = o + 'exact_CMs_by_length_by_wordform_index.pickle'\n",
    "exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                     my_fp,\n",
    "                     None,\n",
    "                     exact_CMs_by_length_by_wordform_idx_md,\n",
    "                     'Step 4e',\n",
    "                     'Calculate segmental wordform and prefix channel matrices',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.539135Z",
     "start_time": "2019-07-25T23:24:38.530429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_OD'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.611696Z",
     "start_time": "2019-07-25T23:24:38.543896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_wordform_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2.npy_metadata.json',\n",
       " 'Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate LTR_newdic_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb',\n",
       " 'p3Y1X01.json',\n",
       " 'gate6 trials.csv',\n",
       " 'Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'pY1X0X1X2.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2.json',\n",
       " 'pYX.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2.npy',\n",
       " 'f6_Y0Y1_X0X1.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle',\n",
       " 'Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'p6Y1X01.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012Y012s.txt',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_length_by_wordform_index.pickle_metadata.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_prefix_index.pickle_metadata.json',\n",
       " 'pX0X1X2.npy_metadata.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X012.npy_metadata.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'p6Y0X01.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODexact_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X012.npy',\n",
       " 'f3_Y0Y1_X0X1.json',\n",
       " 'gate3 trials.csv',\n",
       " 'p3Y0X01.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_wordform_index.pickle_metadata.json',\n",
       " 'p6YX.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_wordform_index.pickle_metadata.json',\n",
       " 'pX0X1X2.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_exact_CMs_by_length_by_wordform_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_length_by_wordform_index.pickle',\n",
       " 'p3Y01X01.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODexact_CMs_by_length_by_prefix_index.pickle_metadata.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy_metadata.json',\n",
       " 'p6Y01X01.json',\n",
       " 'Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.01.ipynb',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_prefix_index.pickle',\n",
       " 'Generating  uniform triphone lexicon dist.ipynb',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_CMs_by_length_by_wordform_index.pickle_metadata.json',\n",
       " 'pX0X1X2.npy',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODCMs_by_prefix_index.pickle',\n",
       " 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'p3YX.json']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(path.dirname(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations of $p_3(Y_1|X_0, X_1; X2)$ (and $p_3(Y_1|X_0; X_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.652132Z",
     "start_time": "2019-07-25T23:24:38.616367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving p3Y1X012_np to filepath 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODp3Y1X012.npy'\n"
     ]
    }
   ],
   "source": [
    "#if not r, export numpy representation of triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012' + '.npy', p3Y1X012_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.688967Z",
     "start_time": "2019-07-25T23:24:38.671870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODp3Y1X012.npy\n",
      " to \n",
      "\tCM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_ODp3Y1X012.npy_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if not r:\n",
    "    CM_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "             'changes':'None' if len(missing_from_channel) == 0 else 'adjustment of conditions and outcomes as noted in adjacent dictionaries'},\n",
    "        'X012s':{'from fp':c,\n",
    "                 'changes':\"None\" if len(missing_from_channel) == 0 else \"For every stimulus triphone x_0.x_1.⋉ in the original, added x1.⋉.⋉\"},\n",
    "        'Y1s':{'from fp':c,\n",
    "               'changes':\"None\" if len(missing_from_channel) == 0 else \"Added ⋉, where p(⋉|x012) = 1 iff x_1 == ⋉ and otherwise = 0\"}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         CM_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not r, export json representation of (modified!) triphone channel distribution\n",
    "if not r:\n",
    "    print(f\"Saving p3Y1X012 to filepath '{o + 'p3Y1X012' + '.json'}'\")\n",
    "    exportProbDist(o + 'p3Y1X012' + '.json', p3Y1X012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.706667Z",
     "start_time": "2019-07-25T23:24:38.694780Z"
    }
   },
   "outputs": [],
   "source": [
    "#if r, export .json of modified triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving extended, human-readable version of p3Y1X012 to filepath '{o + 'p3Y1X012_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X012_RE' + '.json', condProbDistAsDicts(p3Y1X012))\n",
    "          \n",
    "    print(f\"Saving extended, human-readable version of p3Y1X01 to filepath '{o + 'p3Y1X01_RE' + '.json'}'\")\n",
    "    exportDict(o + 'p3Y1X01_RE' + '.json', condProbDistAsDicts(p3Y1X01))\n",
    "\n",
    "#if r, export numpy representation of triphone channel distribution and preview distribution\n",
    "if r:\n",
    "    print(f\"Saving p3Y1X012_np to filepath '{o + 'p3Y1X012_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X012_RE' + '.npy', p3Y1X012_np)\n",
    "    print(f\"Saving p3Y1X01_np to filepath '{o + 'p3Y1X01_RE' + '.npy'}'\")\n",
    "    np.save(o + 'p3Y1X01_RE' + '.npy', p3Y1X01_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.726166Z",
     "start_time": "2019-07-25T23:24:38.712036Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    CD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X012_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X012_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X012_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T23:24:38.745696Z",
     "start_time": "2019-07-25T23:24:38.731527Z"
    }
   },
   "outputs": [],
   "source": [
    "if r:\n",
    "    PD_md = {\n",
    "        'r':r,\n",
    "        'C':{'from fp':c,\n",
    "         'changes':\"Added ⋉ to the outcomes of every existing conditioning outcome; added new conditioning events X⋉\" if r else 'None'}\n",
    "    }\n",
    "\n",
    "    my_fp = o + 'p3Y1X01_RE' + '.npy'\n",
    "    exportMatrixMetadata(my_fp + '_metadata.json',\n",
    "                         my_fp,\n",
    "                         p3Y1X01_np,\n",
    "                         PD_md,\n",
    "                         'Step 4e',\n",
    "                         'Calculate segmental wordform and prefix channel matrices',\n",
    "                         {'Comment':f\"See also corresponding .json file @ {o + 'p3Y1X01_RE' + '.json'}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
