{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:26.550687Z",
     "start_time": "2019-05-15T20:27:26.546770Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span><ul class=\"toc-item\"><li><span><a href=\"#Papermill---command-line\" data-toc-modified-id=\"Papermill---command-line-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Papermill - command line</a></span></li><li><span><a href=\"#Old-School\" data-toc-modified-id=\"Old-School-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Old School</a></span></li></ul></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Import-Files-to-Project\" data-toc-modified-id=\"Import-Files-to-Project-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import Files to Project</a></span><ul class=\"toc-item\"><li><span><a href=\"#Projection-mapping\" data-toc-modified-id=\"Projection-mapping-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Projection mapping</a></span></li><li><span><a href=\"#Case-A:-gating-data\" data-toc-modified-id=\"Case-A:-gating-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Case A: gating data</a></span></li><li><span><a href=\"#Case-B:-transcription-lexicon\" data-toc-modified-id=\"Case-B:-transcription-lexicon-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Case B: transcription lexicon</a></span></li></ul></li><li><span><a href=\"#Apply-Projection\" data-toc-modified-id=\"Apply-Projection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Apply Projection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Case-A:-gating-data\" data-toc-modified-id=\"Case-A:-gating-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Case A: gating data</a></span></li><li><span><a href=\"#Case-B:-transcription-lexicon\" data-toc-modified-id=\"Case-B:-transcription-lexicon-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Case B: transcription lexicon</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-conditions\" data-toc-modified-id=\"Pre-conditions-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Pre-conditions</a></span></li><li><span><a href=\"#Processing\" data-toc-modified-id=\"Processing-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Processing</a></span></li><li><span><a href=\"#Post-conditions\" data-toc-modified-id=\"Post-conditions-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Post-conditions</a></span></li></ul></li></ul></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Case-A:-gating-data\" data-toc-modified-id=\"Case-A:-gating-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Case A: gating data</a></span></li><li><span><a href=\"#Case-B:-transcription-lexicon\" data-toc-modified-id=\"Case-B:-transcription-lexicon-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Case B: transcription lexicon</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Let \n",
    " - $g$ be a gating data file\n",
    " - $l$ be a transcribed lexicon relation file\n",
    "\n",
    "The notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` will import both, calculate the segmental inventories $\\Sigma_g$, $\\Sigma_l$ used in each, and \n",
    " - calculate the portion $\\overline{g}$ of $g$'s inventory $\\Sigma_g$ unique to $g$ relative to $l$\n",
    " - calculate the portion $\\overline{l}$ of $l$'s inventory $\\Sigma_l$ unique to $l$ relative to $g$\n",
    "\n",
    "After running that notebook, *you* (or someone else) then (in $\\S$5-6) defined dictionaries (eventually exported) for mapping \n",
    " - $\\overline{g} \\rightarrow \\Sigma_g \\cap \\Sigma_l$\n",
    " - $\\overline{l} \\rightarrow \\Sigma_g \\cap \\Sigma_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This* notebook takes three arguments:\n",
    " - $p$: *one* of those projection mappings as an argument.\n",
    " - $g$ or $l$: (as another argument) the appropriate file (**g**ating data or transcribed **l**exicon) to apply the projection mapping to.\n",
    " - $o$: a filename or filepath for the projected version of the second argument, to be computed by this notebook.\n",
    " \n",
    "It then applies the projection mapping to the input file and produces the output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papermill - command line\n",
    "\n",
    "This notebook is intended to be used with the [`papermill`](https://papermill.readthedocs.io/en/latest/) package.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let \n",
    " - `p` = `./LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json`\n",
    " - `l` = `./LTR_Buckeye/LTR_Buckeye.tsv`\n",
    "then\n",
    "```\n",
    "papermill \"Align transcriptions.ipynb\" \"GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb\" -p l \"./LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\" -p o \"./LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\"\n",
    "```\n",
    "will \n",
    " - create a new notebook `GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb` that records data processing (but not, if it runs successfully, requiring any action or intervention from you) \n",
    "\n",
    "...and output `./LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv`, a projected version of \n",
    " - `./LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json`\n",
    "based on the projection defined in \n",
    " - `./LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have or want to use this notebook as intended, edit the filenames/paths in the cell below with the top comment #\"PARAMETERS CELL\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:33:19.913512Z",
     "start_time": "2019-05-15T20:33:19.762071Z"
    }
   },
   "outputs": [],
   "source": [
    "from string_utils import *\n",
    "from boilerplate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:28.141045Z",
     "start_time": "2019-05-15T20:27:28.134765Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:28.281633Z",
     "start_time": "2019-05-15T20:27:28.268775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:28.521443Z",
     "start_time": "2019-05-15T20:27:28.404316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1 initial directory setup.txt'\r\n",
      "'2a alignment_paths_and_cmds.sh'\r\n",
      "'Align transcriptions.ipynb'\r\n",
      " boilerplate.py\r\n",
      "'Gating Data - Transcription Lexicon Alignment Maker.ipynb'\r\n",
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye.tsv alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed.tsv alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed.tsv alignment definition.ipynb'\r\n",
      " \u001b[01;34mLTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mLTR_Buckeye_aligned_w_GD_AmE_destressed\u001b[0m/\r\n",
      " \u001b[01;34mLTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mLTR_CMU_destressed_aligned_w_GD_AmE_destressed\u001b[0m/\r\n",
      " \u001b[01;34mLTR_CMU_stressed\u001b[0m/\r\n",
      " \u001b[01;34mLTR_newdic_destressed\u001b[0m/\r\n",
      " \u001b[01;34mLTR_newdic_destressed_aligned_w_GD_AmE_destressed\u001b[0m/\r\n",
      " \u001b[01;34mold\u001b[0m/\r\n",
      "'Processing Driver Notebook.ipynb'\r\n",
      " \u001b[01;34m__pycache__\u001b[0m/\r\n",
      " string_utils.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:28.549563Z",
     "start_time": "2019-05-15T20:27:28.546034Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS CELL\n",
    "#\n",
    "# This is the Paremeters Cell that Papermill looks at and modifies\n",
    "# \n",
    "# go to View->Cell Toolbar->Tags to see what's going on\n",
    "\n",
    "p = \"\"\n",
    "# p = \"./GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\"\n",
    "# p = \"./LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\"\n",
    "\n",
    "g = \"\"\n",
    "# g = \"./GD_AmE/AmE-diphones-IPA-annotated-columns.csv\"\n",
    "\n",
    "l = \"\"\n",
    "# l = \"./LTR_Buckeye/LTR_Buckeye.tsv\"\n",
    "\n",
    "o = \"\"\n",
    "# o = \"./GD_AmE_destressed_aligned_w_LTR_Buckeye/AmE-diphones_aligned_w_LTR_Buckeye.tsv\"\n",
    "# o = \"./LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:28.706572Z",
     "start_time": "2019-05-15T20:27:28.700646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./LTR_Buckeye_aligned_w_GD_AmE_destressed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_dp = path.dirname(p)\n",
    "projection_dp\n",
    "projection_fn = path.basename(p)\n",
    "projection_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:29.188546Z",
     "start_time": "2019-05-15T20:27:29.184480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_on_gating_data = True if g != \"\" else False\n",
    "working_on_gating_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:29.895760Z",
     "start_time": "2019-05-15T20:27:29.888957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./LTR_Buckeye'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye.tsv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if working_on_gating_data:\n",
    "    data_dp = path.dirname(g)\n",
    "    data_fn = path.basename(g)\n",
    "else:\n",
    "    data_dp = path.dirname(l)\n",
    "    data_fn = path.basename(l)\n",
    "    \n",
    "data_dp\n",
    "data_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:32.770409Z",
     "start_time": "2019-05-15T20:27:32.767728Z"
    }
   },
   "outputs": [],
   "source": [
    "if not working_on_gating_data:\n",
    "    assert l != \"\", \"Either l or g must not be the empty string.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:33.195321Z",
     "start_time": "2019-05-15T20:27:33.192488Z"
    }
   },
   "outputs": [],
   "source": [
    "assert o != \"\", \"Must supply a filename or filepath for the projected version of the input, got o={0} instead\".format(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:33.611674Z",
     "start_time": "2019-05-15T20:27:33.607941Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dirpath = path.dirname(o)\n",
    "output_fn = path.basename(o)\n",
    "if not path.exists(output_dirpath):\n",
    "    makedirs(output_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Files to Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:37.588886Z",
     "start_time": "2019-05-15T20:27:37.580920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'m̩': 'm', 'n̩': 'n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(p, encoding='utf-8') as data_file:\n",
    "    projection_mapping = json.loads(data_file.read())\n",
    "len(projection_mapping)\n",
    "projection_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:38.612158Z",
     "start_time": "2019-05-15T20:27:38.608271Z"
    }
   },
   "outputs": [],
   "source": [
    "if projection_mapping == dict():\n",
    "    print(\"Projection mapping is empty: no segments will be changed in creating {0}.\".format(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case A: gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:39.267599Z",
     "start_time": "2019-05-15T20:27:39.257501Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDiphoneGatingTrials(filename, print_fields = True):\n",
    "    '''\n",
    "    Opens filename in the current working directory and returns the trials as a \n",
    "    list of dictionaries, plus the fieldnames in the order present in the file.\n",
    "    '''\n",
    "    diphone_fields = []\n",
    "    diphoneTrials = []\n",
    "    diphoneDataInFilename = filename\n",
    "    with open(diphoneDataInFilename, newline='') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        diphone_fields = my_reader.fieldnames\n",
    "        if print_fields:\n",
    "            print(\"fieldnames: {0}\".format(diphone_fields))\n",
    "        for row in my_reader:\n",
    "            #print(row)\n",
    "            diphoneTrials.append(row)\n",
    "    return {'trials': diphoneTrials, 'fields':diphone_fields}\n",
    "\n",
    "def writeProcessedDataToCSV(theTrials, theFieldnames, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, delimiter='\\t',fieldnames=theFieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(theTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:39.447923Z",
     "start_time": "2019-05-15T20:27:39.444213Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDestressedDiphone(row):\n",
    "    return row['diphoneInSeg']\n",
    "\n",
    "def getStressedDiphone(row):\n",
    "    return row['diphoneInWStress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:39.606033Z",
     "start_time": "2019-05-15T20:27:39.597802Z"
    }
   },
   "outputs": [],
   "source": [
    "sound_fields = ['Prec_context', 'CorrAns1', 'CorrAns2', 'Resp1', 'Resp2',\n",
    "                'diphoneInSeg', 'diphoneInWStress', 'diphoneOutSeg',\n",
    "                'prefixSeg', 'prefixWStress',\n",
    "                'suffixSeg', 'suffixWStress',\n",
    "                'stimulusSeg', 'stimulusWProsody']\n",
    "diphone_fields = ['CorrAns1', 'CorrAns2', 'Resp1', 'Resp2',\n",
    "                  'diphoneInSeg', 'diphoneInWStress', 'diphoneOutSeg']\n",
    "#                 'stimulusSeg', 'stimulusWProsody']\n",
    "\n",
    "def getSoundFields(row):\n",
    "    return project_dict(row, sound_fields)\n",
    "\n",
    "def getDiphoneFields(row, include_full_stimulus_column = False):\n",
    "    if not include_full_stimulus_column:\n",
    "        return project_dict(row, diphone_fields)\n",
    "    return project_dict(row, diphone_fields + ['stimulusSeg', 'stimulusWProsody'])\n",
    "\n",
    "core_sound_fields = ['Prec_context', 'CorrAns1', 'CorrAns2', 'Resp1', 'Resp2']\n",
    "\n",
    "def getSounds(row):\n",
    "    return set(project_dict(row, core_sound_fields).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:39.744247Z",
     "start_time": "2019-05-15T20:27:39.734999Z"
    }
   },
   "outputs": [],
   "source": [
    "def getStimSeg1(row, which_stress):\n",
    "    seg = row['CorrAns1']\n",
    "    if which_stress == 'destressed':\n",
    "        return seg\n",
    "    elif which_stress == 'stressed':\n",
    "        s = row['seg1_stress']\n",
    "        if s == '2' or s == 2:\n",
    "            return seg\n",
    "        else:\n",
    "            return seg + str(s)\n",
    "    else:\n",
    "        assert which_stress in ['destressed', 'stressed'], '{0} is an invalid choice about stress representations'.format(which_stress)\n",
    "\n",
    "def getStimSeg2(row, which_stress):\n",
    "    seg = row['CorrAns2']\n",
    "    if which_stress == 'destressed':\n",
    "        return seg\n",
    "    elif which_stress == 'stressed':\n",
    "        s = row['seg2_stress']\n",
    "        if s == '2' or s == 2:\n",
    "            return seg\n",
    "        else:\n",
    "            return seg + str(s)\n",
    "    else:\n",
    "        assert which_stress in ['destressed', 'stressed'], '{0} is an invalid choice about stress representations'.format(which_stress)\n",
    "        \n",
    "def removeConsStress(stringRep):\n",
    "    return ''.join([c for c in stringRep if c != \"2\"])\n",
    "\n",
    "def removeStress(stringRep):\n",
    "    return ''.join([c for c in stringRep if c != \"0\" and c != \"1\" and c != \"2\"])\n",
    "\n",
    "def replaceSyllableBoundaries(stringRep):\n",
    "    return stringRep.replace('-','.')\n",
    "\n",
    "def justSegments(stringRep):\n",
    "    return replaceSyllableBoundaries(removeStress(stringRep))\n",
    "\n",
    "def getDiphonesInAsStr(row, which_stress):\n",
    "    if which_stress == 'destressed':\n",
    "        return row['diphoneInSeg']\n",
    "    elif which_stress == 'stressed': \n",
    "        #we remove consonant stress annotations because there are none in IPhOD (and probably none in Hammond's newdic, either)\n",
    "        assert removeStress(row['diphoneInWStress']) == row['diphoneInSeg'], '{0} and {1} have segmental mismatch'.format(row['diphoneIn'], row['diphoneInWStress'])\n",
    "        return removeConsStress(row['diphoneInWStress'])\n",
    "    else:\n",
    "        assert which_stress in ['destressed', 'stressed'], '{0} is an invalid choice about stress representations'.format(which_stress)\n",
    "        \n",
    "def getDiphonesOutAsStr(row):\n",
    "    return row['diphoneOutSeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:39.879632Z",
     "start_time": "2019-05-15T20:27:39.873388Z"
    }
   },
   "outputs": [],
   "source": [
    "def mergeXintoY(sound_x,sound_y,the_dict, exact_match = True):\n",
    "    '''\n",
    "    Replace every instance of sound X with one of sound Y \n",
    "    in all sound fields of the_dict.\n",
    "    \n",
    "    If exact_match is True, then a field's value must be exactly\n",
    "    and entirely equal to sound_X; otherwise, this function will\n",
    "    substitute any instance (substring) of sound_X in the sound\n",
    "    fields of the_dict.\n",
    "    '''\n",
    "    for key in the_dict.keys():\n",
    "        if exact_match:\n",
    "            if sound_x == the_dict[key] and key in sound_fields:\n",
    "#                 if key != 'Prec_context':\n",
    "#                     print(\"{2}:{0}⟶{1}.\".format(the_dict[key], sound_y, key))\n",
    "                the_dict.update({key: sound_y})\n",
    "        else: #use carefully...\n",
    "            if sound_x in the_dict[key] and key in sound_fields:\n",
    "                old_str = the_dict[key]\n",
    "                new_str = old_str.replace(sound_x, sound_y)\n",
    "#                 if key != 'Prec_context':\n",
    "#                     print(\"{2}:{0}⟶{1}.\".format(old_str, new_str, key))\n",
    "                the_dict.update({key: new_str})\n",
    "    return the_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:40.019671Z",
     "start_time": "2019-05-15T20:27:40.015978Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    # diphoneDataInFilename = \"diphones-IPA-annotated-columns.csv\"\n",
    "    diphoneDataInFP = g\n",
    "\n",
    "    file_data = getDiphoneGatingTrials(diphoneDataInFP)\n",
    "    rows_in = file_data['trials']\n",
    "    the_fields = file_data['fields']\n",
    "    rows_in[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case B: transcription lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:41.094417Z",
     "start_time": "2019-05-15T20:27:40.321819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('Orthographic_Wordform', 'i'), ('Transcription', 'aɪ')]),\n",
       " OrderedDict([('Orthographic_Wordform', 'uh'), ('Transcription', 'ʌ')]),\n",
       " OrderedDict([('Orthographic_Wordform', 'grew'), ('Transcription', 'g.ɹ.u')]),\n",
       " OrderedDict([('Orthographic_Wordform', 'up'), ('Transcription', 'ʌ.p')]),\n",
       " OrderedDict([('Orthographic_Wordform', 'in'), ('Transcription', 'ɪ.n')])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_rows_in = []\n",
    "if not working_on_gating_data:\n",
    "    transcribed_lexicon_fp = l\n",
    "    with open(transcribed_lexicon_fp) as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE, quotechar='@')\n",
    "        for row in my_reader:\n",
    "            #print(row)\n",
    "            lexicon_rows_in.append(row)\n",
    "    lexicon_rows_in[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:42.335807Z",
     "start_time": "2019-05-15T20:27:42.025216Z"
    }
   },
   "outputs": [],
   "source": [
    "from funcy import compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case A: gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:42.566885Z",
     "start_time": "2019-05-15T20:27:42.561661Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    if len(projection_mapping) == 0:\n",
    "        print('No segments to project.')\n",
    "    else:\n",
    "        for seg_to_project in projection_mapping:\n",
    "            print('Seg to project: {0}'.format(seg_to_project))\n",
    "            print('\\t# rows w/ seg: {0}'.format(len([r for r in rows_in \n",
    "                                                     if seg_to_project in getSoundFields(r).values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:43.014643Z",
     "start_time": "2019-05-15T20:27:43.009668Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    if len(projection_mapping) == 0:\n",
    "        print('No segments to project.')\n",
    "    else:\n",
    "        row_alteration_functions = []\n",
    "        for seg_to_project in projection_mapping:\n",
    "            project_to = projection_mapping[seg_to_project]\n",
    "            row_alteration_functions.append(lambda row: mergeXintoY(seg_to_project, project_to, row, exact_match = True))\n",
    "        row_alteration_function = compose(*tuple(row_alteration_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:43.286713Z",
     "start_time": "2019-05-15T20:27:43.282818Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    if len(projection_mapping) == 0:\n",
    "        print('No segments to project.')\n",
    "        projected_rows = rows_in\n",
    "    else:\n",
    "        projected_rows = list(map(row_alteration_function,\n",
    "                                  rows_in))\n",
    "        len(rows_in)\n",
    "        len(projected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:43.909028Z",
     "start_time": "2019-05-15T20:27:43.904253Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    if len(projection_mapping) == 0:\n",
    "        print('No segments to project.')\n",
    "    else:\n",
    "        for seg_to_project in projection_mapping:\n",
    "            print('Seg to project: {0}'.format(seg_to_project))\n",
    "            badRows = [r for r in projected_rows if seg_to_project in getSoundFields(r).values()]\n",
    "            print('\\t# rows w/ seg: {0}'.format(len(badRows)))\n",
    "            assert len(badRows) == 0, '{0} still found after it should have been removed'.format(seg_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:44.315416Z",
     "start_time": "2019-05-15T20:27:44.312685Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data and projection_mapping == dict():\n",
    "    assert rows_in == projected_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case B: transcription lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:45.956932Z",
     "start_time": "2019-05-15T20:27:45.696399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785/216062 total rows have segment m̩\n",
      "989/216062 total rows have segment n̩\n"
     ]
    }
   ],
   "source": [
    "if len(projection_mapping) == 0:\n",
    "    print('No segments to remap.')\n",
    "else:\n",
    "    total_num_rows = len(lexicon_rows_in)\n",
    "    rows_with = {}\n",
    "    for segment_to_remap in projection_mapping:\n",
    "        rows_with[segment_to_remap] = list(filter(lambda row: segment_to_remap in ds2t(row['Transcription']),\n",
    "                                           lexicon_rows_in))\n",
    "        print('{0}/{1} total rows have segment {2}'.format(len(rows_with[segment_to_remap]), total_num_rows, segment_to_remap))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:27:52.277976Z",
     "start_time": "2019-05-15T20:27:52.273331Z"
    }
   },
   "outputs": [],
   "source": [
    "# def subInDS(dottedString, to_replace, replacement):\n",
    "#     '''\n",
    "#     Replace each instance of symbol 'to_replace' \n",
    "#     with 'replacement' symbol in 'dottedString'.\n",
    "#     '''\n",
    "#     old_symbol_tuple = dottedStringToTuple( dottedString )\n",
    "\n",
    "#     replacer = lambda symb: symb if symb != to_replace else replacement\n",
    "#     new_symbol_tuple = tuple( map(replacer, old_symbol_tuple) )\n",
    "\n",
    "#     dottedSymbols = tupleToDottedString( new_symbol_tuple ) \n",
    "#     return dottedSymbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:33:23.347316Z",
     "start_time": "2019-05-15T20:33:23.340947Z"
    }
   },
   "outputs": [],
   "source": [
    "def replaceXwithYinRow(x, y, row):\n",
    "    new_row = modify_dict(row, 'Transcription', subInDS( row['Transcription'], x, y))\n",
    "    return new_row\n",
    "\n",
    "def makeReplacer(x,y):\n",
    "    def replaceXwithYin(row):\n",
    "        new_row = modify_dict(row, 'Transcription', subInDS( row['Transcription'], x, y))\n",
    "        return new_row\n",
    "    return replaceXwithYin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:33:24.722670Z",
     "start_time": "2019-05-15T20:33:23.480910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216062"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "216062"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_projection_functions = []\n",
    "if not working_on_gating_data:\n",
    "    for segment_to_remap in projection_mapping:\n",
    "        lexicon_projection_functions.append(makeReplacer(segment_to_remap, projection_mapping[segment_to_remap]))\n",
    "    lexicon_projection_function = compose(*tuple(lexicon_projection_functions))\n",
    "#         projected_lexicon_rows = list(map(lambda row: replaceXwithYinRow(x, y, row),\n",
    "#                                           lexicon_rows_in))\n",
    "    projected_lexicon_rows = list(map(lexicon_projection_function,\n",
    "                                      lexicon_rows_in))\n",
    "    len(lexicon_rows_in)\n",
    "    len(projected_lexicon_rows)\n",
    "    \n",
    "    if len(projection_mapping) == 0:\n",
    "        assert lexicon_rows_in == projected_lexicon_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:08.662103Z",
     "start_time": "2019-05-15T20:34:08.399398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/216062 total rows have segment m̩\n",
      "m̩ removed from processed lexicon.\n",
      "0/216062 total rows have segment n̩\n",
      "n̩ removed from processed lexicon.\n"
     ]
    }
   ],
   "source": [
    "if len(projection_mapping) == 0:\n",
    "    print('No segments to remap.')\n",
    "else:\n",
    "    total_num_rows = len(projected_lexicon_rows)\n",
    "    rows_with = {}\n",
    "    for segment_to_remap in projection_mapping:\n",
    "        rows_with[segment_to_remap] = list(filter(lambda row: segment_to_remap in ds2t(row['Transcription']),\n",
    "                                           projected_lexicon_rows))\n",
    "        print('{0}/{1} total rows have segment {2}'.format(len(rows_with[segment_to_remap]), total_num_rows, segment_to_remap))\n",
    "        if len(rows_with[segment_to_remap]) > 0:\n",
    "            print('{0} still contained in processed lexicon!'.format(segment_to_remap))\n",
    "        if len(rows_with[segment_to_remap]) == 0:\n",
    "            print('{0} removed from processed lexicon.'.format(segment_to_remap))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case A: gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:16.455436Z",
     "start_time": "2019-05-15T20:34:16.452114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:17.048485Z",
     "start_time": "2019-05-15T20:34:17.003935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:29.912132Z",
     "start_time": "2019-05-15T20:34:29.908568Z"
    }
   },
   "outputs": [],
   "source": [
    "if working_on_gating_data:\n",
    "    print('Output dir before writing to file:')\n",
    "    print(listdir(output_dirpath))\n",
    "    print(' ')\n",
    "    \n",
    "    writeProcessedDataToCSV(projected_rows, the_fields, o)\n",
    "    \n",
    "    print('Output dir *after* writing to file:')\n",
    "    print(listdir(output_dirpath))\n",
    "    print(' ')\n",
    "    print('Wrote o={0} to file.'.format(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case B: transcription lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:33.366397Z",
     "start_time": "2019-05-15T20:34:33.362080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:34:37.836805Z",
     "start_time": "2019-05-15T20:34:37.832853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:38:04.593507Z",
     "start_time": "2019-05-15T20:38:04.174323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir before writing to file:\n",
      "['alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json']\n",
      " \n",
      "Output dir *after* writing to file:\n",
      "['alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json', 'LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "if not working_on_gating_data:\n",
    "    print('Output dir before writing to file:')\n",
    "    print(listdir(output_dirpath))\n",
    "    print(' ')\n",
    "    \n",
    "    with open(o, 'w', newline='\\n') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Orthographic_Wordform', 'Transcription'], delimiter='\\t', quoting=csv.QUOTE_NONE, quotechar='@')\n",
    "\n",
    "        writer.writeheader()\n",
    "        writer.writerows(projected_lexicon_rows)\n",
    "    \n",
    "    print('Output dir *after* writing to file:')\n",
    "    print(listdir(output_dirpath))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
