{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:31:15.835368Z",
     "start_time": "2019-03-12T02:31:15.832241Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview-/-requirements\" data-toc-modified-id=\"Overview-/-requirements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview / requirements</a></span></li><li><span><a href=\"#Overhead\" data-toc-modified-id=\"Overhead-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overhead</a></span></li><li><span><a href=\"#Choose-parameters\" data-toc-modified-id=\"Choose-parameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Choose parameters</a></span></li><li><span><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hammond's-newdic\" data-toc-modified-id=\"Hammond's-newdic-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Hammond's newdic</a></span><ul class=\"toc-item\"><li><span><a href=\"#SUBTLEX\" data-toc-modified-id=\"SUBTLEX-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>SUBTLEX</a></span></li></ul></li><li><span><a href=\"#COCA\" data-toc-modified-id=\"COCA-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>COCA</a></span></li><li><span><a href=\"#IPhOD\" data-toc-modified-id=\"IPhOD-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>IPhOD</a></span></li></ul></li><li><span><a href=\"#IPhOD-processing...\" data-toc-modified-id=\"IPhOD-processing...-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>IPhOD processing...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-data-that-definitely-won't-end-up-in-the-lexicon-distribution...\" data-toc-modified-id=\"Remove-data-that-definitely-won't-end-up-in-the-lexicon-distribution...-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Remove data that definitely won't end up in the lexicon distribution...</a></span></li><li><span><a href=\"#Remove-words-with-triphones-and-diphones-that-a-channel-distribution-isn't-definable-for\" data-toc-modified-id=\"Remove-words-with-triphones-and-diphones-that-a-channel-distribution-isn't-definable-for-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Remove words with triphones and diphones that a channel distribution isn't definable for</a></span></li><li><span><a href=\"#Choose-wordforms-for-the-IPhOD-lexicon-distribution\" data-toc-modified-id=\"Choose-wordforms-for-the-IPhOD-lexicon-distribution-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Choose wordforms for the IPhOD lexicon distribution</a></span></li><li><span><a href=\"#Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-IPhOD\" data-toc-modified-id=\"Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-IPhOD-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Calculate frequencies, probabilities, and informativities of phonological wordforms in IPhOD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-probability-mass-among-phonological-realizations-of-the-same-orthographic-word\" data-toc-modified-id=\"Split-probability-mass-among-phonological-realizations-of-the-same-orthographic-word-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Split probability mass among phonological realizations of the same orthographic word</a></span></li><li><span><a href=\"#Calculate-probability-of-a-phonological-word-(marginalizing-over-orthographic-wordforms)\" data-toc-modified-id=\"Calculate-probability-of-a-phonological-word-(marginalizing-over-orthographic-wordforms)-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Calculate probability of a phonological word (marginalizing over orthographic wordforms)</a></span></li></ul></li><li><span><a href=\"#Define-representation-for-export\" data-toc-modified-id=\"Define-representation-for-export-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Define representation for export</a></span></li><li><span><a href=\"#Export-/-import\" data-toc-modified-id=\"Export-/-import-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Export / import</a></span><ul class=\"toc-item\"><li><span><a href=\"#...import...\" data-toc-modified-id=\"...import...-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>...import...</a></span></li></ul></li></ul></li><li><span><a href=\"#Hammond's-newdic-processing...\" data-toc-modified-id=\"Hammond's-newdic-processing...-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Hammond's newdic processing...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-clutter...\" data-toc-modified-id=\"Remove-clutter...-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Remove clutter...</a></span></li><li><span><a href=\"#Remove-words-with-triphones-that-a-channel-distribution-isn't-definable-for\" data-toc-modified-id=\"Remove-words-with-triphones-that-a-channel-distribution-isn't-definable-for-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Remove words with triphones that a channel distribution isn't definable for</a></span></li><li><span><a href=\"#Choose-wordforms-for-the-Hammond-lexicon-distribution\" data-toc-modified-id=\"Choose-wordforms-for-the-Hammond-lexicon-distribution-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Choose wordforms for the Hammond lexicon distribution</a></span></li><li><span><a href=\"#Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-Hammond's-newdic\" data-toc-modified-id=\"Calculate-frequencies,-probabilities,-and-informativities-of-phonological-wordforms-in-Hammond's-newdic-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Calculate frequencies, probabilities, and informativities of phonological wordforms in Hammond's newdic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-up-probability-of-orthographic-words-in-SUBTLEX\" data-toc-modified-id=\"Look-up-probability-of-orthographic-words-in-SUBTLEX-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Look up probability of orthographic words in SUBTLEX</a></span></li><li><span><a href=\"#Look-up-probability-of-orthographic-words-in-IPhOD\" data-toc-modified-id=\"Look-up-probability-of-orthographic-words-in-IPhOD-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Look-up probability of orthographic words in IPhOD</a></span></li><li><span><a href=\"#Look-up-COCA-unigram-frequencies\" data-toc-modified-id=\"Look-up-COCA-unigram-frequencies-6.4.3\"><span class=\"toc-item-num\">6.4.3&nbsp;&nbsp;</span>Look up COCA unigram frequencies</a></span></li><li><span><a href=\"#Map-each-remaining-phonological-wordform-in-Hammond's-newdic-to-a-frequency/probability-(marginalizing-over-orthographic-wordforms)\" data-toc-modified-id=\"Map-each-remaining-phonological-wordform-in-Hammond's-newdic-to-a-frequency/probability-(marginalizing-over-orthographic-wordforms)-6.4.4\"><span class=\"toc-item-num\">6.4.4&nbsp;&nbsp;</span>Map each remaining phonological wordform in Hammond's newdic to a frequency/probability (marginalizing over orthographic wordforms)</a></span></li><li><span><a href=\"#Set-a-minimum-probability-threshold-for-inclusion\" data-toc-modified-id=\"Set-a-minimum-probability-threshold-for-inclusion-6.4.5\"><span class=\"toc-item-num\">6.4.5&nbsp;&nbsp;</span>Set a minimum probability threshold for inclusion</a></span></li></ul></li><li><span><a href=\"#Define-representation-for-export\" data-toc-modified-id=\"Define-representation-for-export-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Define representation for export</a></span></li><li><span><a href=\"#Export...\" data-toc-modified-id=\"Export...-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Export...</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import...\" data-toc-modified-id=\"Import...-6.6.1\"><span class=\"toc-item-num\">6.6.1&nbsp;&nbsp;</span>Import...</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview / requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the ultimate goal of these notebooks is to process data for a word recognition model that can map from a segmental transcription of the incrementally produced prefix of a speaker's intended wordform to a listener's beliefs about what the speaker's actual intended wordform is. (See other notebooks - especially the collection documenting the model implementation -  for more details.) \n",
    "\n",
    "This requires a lexicon of transcribed wordforms, the ability to assign a prior probability to each wordform, and a model of coarticulation and noise/errors in the listening process. Where previous notebooks in this collection have each only transformed what is structurally the same dataset, this notebook transforms those structures into one of the two types of inputs to the word recognition model - a prior distribution over wordforms (a lexicon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, I am running Python 3.6.5, Jupyter 5.5.0, and otherwise Anaconda 5.2. More specifically, this notebook assumes the current working directory contains\n",
    " - a copy of Hammond's mysterious 'newdic' transcribed lexicon of English http://dingo.sbs.arizona.edu/~hammond/lsasummer11/newdic, processed by two previous notebooks ('Notebook 1b', `Notebook 2b`)\n",
    "    - `Hammond_newdic_IPA_aligned.csv`\n",
    " - a copy of the COCA unigrams data for American English: `.\\COCA\\1-gram\\w1.txt`\n",
    " - a copy of the $\\text{SUBTLEX}_{\\text{US}}$ database file `SUBTLEX-US frequency list with PoS information text version.txt` (available from e.g. https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus).\n",
    " - a copy of the data associated with IPhOD (available from http://www.iphod.com/), processed by two previous notebooks (`Notebook 1b`, `Notebook 2b`):\n",
    "   - `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv`\n",
    " - files from `Notebook 2b` and `Notebook 3a` indicating what stressed and unstressed triphones are in each of the two lexicons and what stressed and unstressed stimuli triphones are constructable from gating data aligned with each of the two lexicons:\n",
    "  - These outputs of `Notebook 2a` are required:\n",
    "     - `Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `IPhOD-aligned_destressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `IPhOD-aligned_stressed stimuli diphone-based illegal triphones.txt`\n",
    "     - `Hammond-aligned_destressed stimuli illegal diphones.txt`\n",
    "     - `IPhOD-aligned_destressed stimuli illegal diphones.txt`\n",
    "     - `IPhOD-aligned_stressed stimuli illegal diphones.txt`\n",
    "\n",
    "This last set of files are required because use of either lexicon with a triphone noise model requires words to be removed if they contain triphones whose channel distribution can't be estimated or modeled from the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can produce twelve .json files, each containing a list of dictionaries mapping wordforms to frequencies:\n",
    " 1. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 2. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json`\n",
    " 3. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 4. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_noBadTriphones_phonWordToProb.json`\n",
    " 5. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 6. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_hasBadTriphones_phonWordToProb.json`\n",
    " 7. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 8. `IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressed_hasBadTriphones_phonWordToProb.json`\n",
    " 9. `Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json`\n",
    " 10. `Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json`\n",
    " 11. `Hammond_newdic_IPA_aligned_destressed_hasBadTriphones_phonWordToNlprob.json`\n",
    " 12. `Hammond_newdic_IPA_aligned_destressed_hasBadTriphones_phonWordToProb.json`\n",
    "\n",
    " \n",
    "That is, for each of two lexicon datasources there are two choices with respect to removing wordforms containing triphones that the aligned triphone channel distribution cannot model, two choices with respect to exporting probabilities or negative log probabilities, and for IPhOD there are two choices with respect to stress (stressed wordform transcriptions vs. destressed wordform transcriptions). (**FIXME/TODO:** For the time being I do not have a stressed version of the aligned Hammond lexicon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:31:31.365971Z",
     "start_time": "2019-03-12T02:31:31.331419Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:31.970882Z",
     "start_time": "2019-03-12T03:12:31.922191Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import *\n",
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:22.179615Z",
     "start_time": "2019-02-14T20:48:22.177400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "\n",
    "# def union(Ss):\n",
    "#     return reduce(set.union, Ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:22.621288Z",
     "start_time": "2019-02-14T20:48:22.615444Z"
    }
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def getRandomKey(a_dict, printKey = False):\n",
    "#     randKey = random.choice(list(a_dict.keys()))\n",
    "#     if printKey:\n",
    "#         print('Random key: {0}'.format(randKey))\n",
    "#     return randKey\n",
    "\n",
    "# def testRandomKey(a_dict, printKey = True, printVal = True):\n",
    "#     randKey = getRandomKey(a_dict)\n",
    "#     if printKey:\n",
    "#         print('Random key: {0}'.format(randKey))\n",
    "#     if printVal:\n",
    "#         print('value ⟶ {0}'.format(a_dict[randKey]))\n",
    "#     return {'key': randKey, 'val': a_dict[randKey]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.006771Z",
     "start_time": "2019-02-14T20:48:22.998494Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tupleToDottedString(pair): \n",
    "#     return '.'.join(pair)\n",
    "\n",
    "# def dottedStringToTuple(s): \n",
    "#     return tuple(s.split('.'))\n",
    "\n",
    "# t2ds = tupleToDottedString\n",
    "# ds2t = dottedStringToTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.401888Z",
     "start_time": "2019-02-14T20:48:23.396703Z"
    }
   },
   "outputs": [],
   "source": [
    "# def importSeqs(seq_fn):\n",
    "#     phoneSeqsAsStr = []\n",
    "#     with open(seq_fn, 'r') as the_file:\n",
    "#         for row in the_file:\n",
    "#             phoneSeqsAsStr.append(row.rstrip('\\r\\n'))\n",
    "#     return set(phoneSeqsAsStr)\n",
    "\n",
    "# def exportSeqs(seq_fn, seqs):\n",
    "#     with open(seq_fn, 'w') as the_file:\n",
    "#         for seq in seqs:\n",
    "#             the_file.write(seq + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T20:48:23.735404Z",
     "start_time": "2019-02-14T20:48:23.729993Z"
    }
   },
   "outputs": [],
   "source": [
    "# leftEdge = '⋊'\n",
    "# rightEdge = '⋉'\n",
    "# edgeSymbols = {leftEdge, rightEdge}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:09.930812Z",
     "start_time": "2019-03-12T02:32:09.926856Z"
    }
   },
   "outputs": [],
   "source": [
    "def padInputSequenceWithBoundaries(inputSeq):\n",
    "    temp = list(dottedStringToTuple(inputSeq))\n",
    "    temp = tuple([leftEdge] + temp + [rightEdge])\n",
    "    return tupleToDottedString(temp)\n",
    "\n",
    "def trimBoundariesFromSequence(seq):\n",
    "    temp = list(dottedStringToTuple(seq))\n",
    "    if temp[0] == leftEdge:\n",
    "        temp = temp[1:]\n",
    "    if temp[-1] == rightEdge:\n",
    "        temp = temp[:-1]\n",
    "    return tupleToDottedString(tuple(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:12.721146Z",
     "start_time": "2019-03-12T02:32:12.717963Z"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import takewhile, product\n",
    "\n",
    "# def dsToKfactors(k, ds):\n",
    "#     seq = ds2t(ds)\n",
    "#     l = len(seq)\n",
    "#     if k > l:\n",
    "#         return tuple()\n",
    "#     kFactor_start_indices = takewhile(lambda pair: pair[0] <= l-k, enumerate(seq))\n",
    "#     kFactors = tuple(seq[index[0]:index[0]+k] for index in kFactor_start_indices)\n",
    "#     return set(map(t2ds, kFactors))\n",
    "\n",
    "# def dsTo2factors(ds):\n",
    "#     return dsToKfactors(2, ds)\n",
    "# def dsTo3factors(ds):\n",
    "#     return dsToKfactors(3, ds)\n",
    "\n",
    "# def lexiconToKfactors(DSs, k):\n",
    "#     myDsToKfactors = lambda ds: dsToKfactors(k, ds)\n",
    "#     return union(map(set, map(myDsToKfactors, DSs)))\n",
    "\n",
    "# def lexiconTo2factors(DSs):\n",
    "#     return union(map(set, map(dsTo2factors, DSs)))\n",
    "# def lexiconTo3factors(DSs):\n",
    "#     return union(map(set, map(dsTo3factors, DSs)))\n",
    "\n",
    "\n",
    "# def compareKfactors(DSs_A, DSs_B, k):\n",
    "#     A = lexiconToKfactors(DSs_A, k)\n",
    "#     B = lexiconToKfactors(DSs_B, k)\n",
    "#     return {\"A == B\":A == B, \"A - B\": A - B, \"B - A\": B - A}\n",
    "\n",
    "# def sameKfactors(DSs_A, DSs_B, k):\n",
    "#     return compareKfactors(DSs_A, DSs_B, k)[\"A == B\"]\n",
    "\n",
    "# def hasIllicitKfactors(W, illicit_k_factors):\n",
    "#     if type(W) == str:      \n",
    "#         # gather the k-factors into an immutable data structure\n",
    "#         illicit_kfs = tuple(illicit_k_factors)\n",
    "#         # get the set of k-factor lengths (values of k) among the illicit_kfs\n",
    "#         illicit_factor_lengths = set([len(ds2t(kf)) for kf in illicit_kfs])\n",
    "#         # map each k to the set of k-factors of dotted string ds\n",
    "#         kFactorSets = {kf_l:dsToKfactors(kf_l, W) for kf_l in illicit_factor_lengths}\n",
    "#         illegal_kfactors_discovered = tuple(ikf for ikf in illicit_kfs if ikf in kFactorSets[len(ds2t(ikf))])\n",
    "#         if illegal_kfactors_discovered == tuple():\n",
    "#             return False\n",
    "#         return illegal_kfactors_discovered\n",
    "#     else:\n",
    "#         myFunc = lambda w: hasIllicitKfactors(w, illicit_k_factors)\n",
    "#         results = tuple(map(myFunc, W))\n",
    "#         if not any(results):\n",
    "#             return False\n",
    "#         return set(t2ds(each) for each in results if each != False)\n",
    "\n",
    "# def sigmaK(sigma, k):\n",
    "#     return product(sigma, repeat=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:15.099146Z",
     "start_time": "2019-03-12T02:32:15.096014Z"
    }
   },
   "outputs": [],
   "source": [
    "# my_epsilon = 1e-13\n",
    "\n",
    "# def norm(dist):\n",
    "#     return sum(dist.values())\n",
    "\n",
    "# def norms(dists):\n",
    "#     return map(norm, dists)\n",
    "\n",
    "# def isNormalized(dist, epsilon = None):\n",
    "#     if epsilon == None:\n",
    "#         epsilon = my_epsilon\n",
    "#     return abs(norm(dist) - 1) < my_epsilon\n",
    "\n",
    "# def areNormalized(dists, epsilon = None):\n",
    "#     if epsilon == None:\n",
    "#         epsilon = my_epsilon\n",
    "#     return all(map(lambda k: isNormalized(dists[k]), dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:17.775448Z",
     "start_time": "2019-03-12T02:32:17.772575Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json, codecs\n",
    "\n",
    "# def exportProbDist(fn, dist):\n",
    "#     with codecs.open(fn, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(dist, f, ensure_ascii = False, indent = 4)\n",
    "        \n",
    "# def importProbDist(fn):\n",
    "#     with open(fn, encoding='utf-8') as data_file:\n",
    "#         dist_in = json.loads(data_file.read())\n",
    "#     return dist_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:48.874281Z",
     "start_time": "2019-03-12T02:32:48.863240Z"
    }
   },
   "outputs": [],
   "source": [
    "diphone_analyses = ('destressed stimuli', 'stressed stimuli', 'destressed response')\n",
    "\n",
    "def importNphoneAnalysis(N, which_align):\n",
    "    assert which_align in {'unaligned', 'Hammond-aligned', 'IPhOD-aligned'}\n",
    "    assert N in {1,2,3}\n",
    "\n",
    "    which_infix = {1:'',\n",
    "                   2:'',\n",
    "                   3:'diphone-based'}[N]\n",
    "    which_suffix = {1:{'licit':'',\n",
    "                       'illicit':''},\n",
    "                    2:{'licit':'',\n",
    "                       'illicit':'illegal'},\n",
    "                    3:{'licit':'constructible',\n",
    "                       'illicit':'illegal'}}[N]\n",
    "    which_n = {1:'uniphones',\n",
    "               2:'diphones',\n",
    "               3:'triphones'}[N]\n",
    "    file_ext = '.txt'\n",
    "\n",
    "    which_licit = {1:('licit',),\n",
    "                   2:('licit', 'illicit'),\n",
    "                   3:('licit', 'illicit')}[N]\n",
    "\n",
    "    which_stress_which_diph = diphone_analyses\n",
    "\n",
    "    analysis = dict()\n",
    "    for each_licit in which_licit:\n",
    "#         print('each_licit = {0}'.format(each_licit))\n",
    "        analysis[each_licit] = dict()\n",
    "        for each_stress_each_diph in which_stress_which_diph:\n",
    "#             print('each_stress_each_diph = {0}'.format(each_stress_each_diph))\n",
    "            my_suff = ' '.join([each for each in [each_stress_each_diph, which_infix, which_suffix[each_licit], which_n] if each != ''])\n",
    "            analysis_fn = which_align + '_' + my_suff + file_ext\n",
    "#             analysis_fn = which_align + '_' + ' '.join([each_stress_each_diph, which_infix, which_suffix[each_licit], which_n]) + file_ext\n",
    "            print('Importing: ' + analysis_fn)\n",
    "            analysis[each_licit][each_stress_each_diph] = importSeqs(analysis_fn)\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:52.560406Z",
     "start_time": "2019-03-12T02:32:52.557190Z"
    }
   },
   "outputs": [],
   "source": [
    "# def project_dict(the_dict, keys_to_keep):\n",
    "#     new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "#     return new_dict\n",
    "\n",
    "# def edit_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable (because it returns a value) but stateful(= in-place) dictionary update.\n",
    "#     '''\n",
    "#     the_dict.update({the_key: the_new_value})\n",
    "#     return the_dict\n",
    "\n",
    "# def modify_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable and (naively-implemented) non-mutating dictionary update.\n",
    "#     '''\n",
    "#     new_dict = {k:the_dict[k] for k in the_dict}\n",
    "#     new_dict.update({the_key: the_new_value})\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:56.720255Z",
     "start_time": "2019-03-12T02:32:56.717182Z"
    }
   },
   "outputs": [],
   "source": [
    "# from time import localtime, strftime\n",
    "# def stamp():\n",
    "#     return strftime('%H:%M:%S', localtime())\n",
    "\n",
    "# def processDataWProgressUpdates(f, data):\n",
    "#     print('Start @ {0}'.format(stamp()))\n",
    "#     l = len(data)\n",
    "#     benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "#     benchmarkIndices = [round(each/100.0 * l) for each in benchmarkPercentages]\n",
    "#     for i, d in enumerate(data):\n",
    "#         if i in benchmarkIndices:\n",
    "#             print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, l, i/l, d, stamp()))\n",
    "#         f(d)\n",
    "#     print('Finish @ {0}'.format(stamp()))\n",
    "        \n",
    "# def constructDictWProgressUpdates(f, data, a_dict):\n",
    "#     def g(d):\n",
    "#         a_dict.update({d:f(d)})\n",
    "#     processDataWProgressUpdates(g, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one of the two lines below uncommented to determine whether or not wordforms with unmodelable triphones will be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:32:59.984185Z",
     "start_time": "2019-03-12T02:32:59.981304Z"
    }
   },
   "outputs": [],
   "source": [
    "# which_filter = 'hasBadTriphones'\n",
    "which_filter = 'noBadTriphones'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one of the two lines uncommented to determine whether wordforms in the output files will have stressed or unstressed representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:01.979770Z",
     "start_time": "2019-03-12T02:33:01.976961Z"
    }
   },
   "outputs": [],
   "source": [
    "which_stress = 'destressed'\n",
    "# which_stress = 'stressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:03.111482Z",
     "start_time": "2019-03-12T02:33:03.099257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed_noBadTriphones'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which = which_stress + '_' + which_filter\n",
    "which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:06.398313Z",
     "start_time": "2019-03-12T02:33:06.394657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:06.556925Z",
     "start_time": "2019-03-12T02:33:06.554129Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hammond's newdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:07.171336Z",
     "start_time": "2019-03-12T02:33:07.051927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:10.033531Z",
     "start_time": "2019-03-12T02:33:10.028997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_fn = 'Hammond_newdic_IPA_aligned.csv'\n",
    "hammond_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:10.278285Z",
     "start_time": "2019-03-12T02:33:10.180388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['Transcription', 'stressInfoA', 'stressInfoB', 'Orthography', 'Frequency', 'PoSs'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'ə'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_newdic = []\n",
    "with open(hammond_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        hammond_newdic.append(row)\n",
    "\n",
    "len(hammond_newdic)\n",
    "hammond_newdic[0].keys()\n",
    "hammond_newdic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:11.037222Z",
     "start_time": "2019-03-12T02:33:10.932133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing: Hammond-aligned_destressed stimuli diphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli illegal diphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli illegal diphones.txt\n",
      "Importing: Hammond-aligned_destressed response illegal diphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphone-based constructible triphones.txt\n",
      "Importing: Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: Hammond-aligned_destressed response diphone-based illegal triphones.txt\n"
     ]
    }
   ],
   "source": [
    "diphoneAnalysis_h = importNphoneAnalysis(2, 'Hammond-aligned')\n",
    "triphoneAnalysis_h = importNphoneAnalysis(3, 'Hammond-aligned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBTLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:18.390695Z",
     "start_time": "2019-03-12T02:33:17.853924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['Word', 'FREQcount', 'CDcount', 'FREQlow', 'Cdlow', 'SUBTLWF', 'Lg10WF', 'SUBTLCD', 'Lg10CD', 'Dom_PoS_SUBTLEX', 'Freq_dom_PoS_SUBTLEX', 'Percentage_dom_PoS', 'All_PoS_SUBTLEX', 'All_freqs_SUBTLEX'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Word', 'a'),\n",
       "             ('FREQcount', '1041179'),\n",
       "             ('CDcount', '8382'),\n",
       "             ('FREQlow', '976941'),\n",
       "             ('Cdlow', '8380'),\n",
       "             ('SUBTLWF', '20415.27'),\n",
       "             ('Lg10WF', '6.0175'),\n",
       "             ('SUBTLCD', '99.93'),\n",
       "             ('Lg10CD', '3.9234'),\n",
       "             ('Dom_PoS_SUBTLEX', 'Article'),\n",
       "             ('Freq_dom_PoS_SUBTLEX', '993445'),\n",
       "             ('Percentage_dom_PoS', '0.96'),\n",
       "             ('All_PoS_SUBTLEX',\n",
       "              'Article.Adverb.Letter.To.Noun.Preposition.Adjective'),\n",
       "             ('All_freqs_SUBTLEX', '993445.33186.6441.744.257.52.5')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtlex_us_db_fn = 'SUBTLEX-US frequency list with PoS information text version.txt'\n",
    "subtlex = []\n",
    "with open(subtlex_us_db_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        subtlex.append(row)\n",
    "\n",
    "len(subtlex)\n",
    "subtlex[0].keys()\n",
    "subtlex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:33:18.975677Z",
     "start_time": "2019-03-12T02:33:18.948833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "74286"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWords_SX = [r['Word'] for r in subtlex]\n",
    "len(orthWords_SX)\n",
    "len(set(orthWords_SX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:45:43.561379Z",
     "start_time": "2019-03-12T02:33:19.439434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start @ 19:33:19\n",
      "743 | 743/74286 = 0.010001884608136122 | adjournment | 19:33:26\n",
      "1486 | 1486/74286 = 0.020003769216272244 | alec | 19:33:33\n",
      "2229 | 2229/74286 = 0.030005653824408368 | angleworm | 19:33:39\n",
      "3714 | 3714/74286 = 0.04999596155399402 | attaching | 19:33:53\n",
      "7429 | 7429/74286 = 0.10000538459467463 | boulders | 19:34:28\n",
      "14857 | 14857/74286 = 0.1999973077026627 | cpu | 19:36:08\n",
      "22286 | 22286/74286 = 0.30000269229733734 | establishes | 19:37:38\n",
      "29714 | 29714/74286 = 0.3999946154053254 | having | 19:38:48\n",
      "37143 | 37143/74286 = 0.5 | licked | 19:39:58\n",
      "44572 | 44572/74286 = 0.6000053845946747 | ontario | 19:41:08\n",
      "52000 | 52000/74286 = 0.6999973077026627 | raffia | 19:42:16\n",
      "59429 | 59429/74286 = 0.8000026922973373 | skag | 19:43:25\n",
      "66857 | 66857/74286 = 0.8999946154053253 | topmost | 19:44:35\n",
      "70572 | 70572/74286 = 0.950004038446006 | ust | 19:45:09\n",
      "71315 | 71315/74286 = 0.9600059230541421 | vigilantes | 19:45:16\n",
      "72057 | 72057/74286 = 0.9699943461755917 | warrior | 19:45:23\n",
      "72800 | 72800/74286 = 0.9799962307837278 | whithersoever | 19:45:29\n",
      "73543 | 73543/74286 = 0.9899981153918639 | worm | 19:45:36\n",
      "Finish @ 19:45:43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49719560.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rowWword(w): \n",
    "    return [r for r in subtlex if r['Word'] == w][0]\n",
    "# orthwordToSXF = {w:rowWword(w)['FREQcount'] for w in orthWords_SX}\n",
    "\n",
    "orthwordToSXF = dict()\n",
    "def wToSXF(w):\n",
    "    return float(rowWword(w)['FREQcount'])\n",
    "\n",
    "#13m on an Intel i7-5650U (MacBook Air)\n",
    "#12.5m [wittgenstein/cython]\n",
    "constructDictWProgressUpdates(wToSXF, orthWords_SX, orthwordToSXF)\n",
    "\n",
    "sum(orthwordToSXF.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:45:43.565443Z",
     "start_time": "2019-03-12T02:45:43.563446Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:45:43.576232Z",
     "start_time": "2019-03-12T02:45:43.566427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:46:59.020188Z",
     "start_time": "2019-03-12T02:46:58.660417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram\tREADME\n",
      "w1cs_c.txt  w1cs.txt  w1.txt\n",
      "Single words occurring three times or more in the\n",
      "Corpus of Contemporary American English (http://corpus.byu.edu/coca)\n",
      "Not case sensitive; no part of speech\n",
      "\n",
      "\n",
      "For more information on full word frequency and n-grams lists, see:\n",
      "http://www.wordfrequency.info\n",
      "http://www.ngrams.info\n",
      "\n",
      "\n",
      "freq\tword1\t\n",
      "-----\t-----\n",
      "\n",
      "\n",
      "9738579\ta\n",
      "40\ta&e\n",
      "3\ta&j\n",
      "395\ta&m\n",
      "7\ta&m-commerce\n",
      "16\ta&m-corpus\n"
     ]
    }
   ],
   "source": [
    "!ls COCA\n",
    "!ls COCA/1-gram\n",
    "!head -20 ./COCA/1-gram/w1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:00.359800Z",
     "start_time": "2019-03-12T02:46:59.783306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Single words occurring three times or more in the',),\n",
       " ('Corpus of Contemporary American English (http://corpus.byu.edu/coca)',),\n",
       " ('Not case sensitive; no part of speech',),\n",
       " ('',),\n",
       " ('',),\n",
       " ('For more information on full word frequency and n-grams lists, see:',),\n",
       " ('http://www.wordfrequency.info',),\n",
       " ('http://www.ngrams.info',),\n",
       " ('',),\n",
       " ('',),\n",
       " ('freq', 'word1', ''),\n",
       " ('-----', '-----'),\n",
       " ('',),\n",
       " ('',),\n",
       " ('9738579', 'a'),\n",
       " ('40', 'a&e'),\n",
       " ('3', 'a&j'),\n",
       " ('395', 'a&m'),\n",
       " ('7', 'a&m-commerce'),\n",
       " ('16', 'a&m-corpus')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_unigrams_rel_path = './COCA/1-gram/'\n",
    "coca_unigrams_fn = 'w1.txt'\n",
    "\n",
    "coca_unigrams = []\n",
    "\n",
    "with open(coca_unigrams_rel_path + coca_unigrams_fn, 'r') as the_file:\n",
    "    for row in the_file:\n",
    "        coca_unigrams.append(tuple(row.rstrip('\\r\\n').split('\\t')))\n",
    "\n",
    "coca_unigrams[0:20]\n",
    "coca_unigrams = coca_unigrams[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:02.412913Z",
     "start_time": "2019-03-12T02:47:02.406930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('40', 'a&e'),\n",
       " ('3', 'a&j'),\n",
       " ('395', 'a&m'),\n",
       " ('7', 'a&m-commerce'),\n",
       " ('16', 'a&m-corpus')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "486687"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coca_unigrams[:5]\n",
    "len(coca_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:02.561040Z",
     "start_time": "2019-03-12T02:47:02.558316Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:03.216937Z",
     "start_time": "2019-03-12T02:47:02.693777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a&e', '40')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(reversed(coca_unigrams[0]))\n",
    "\n",
    "coca_unigrams = [tuple(reversed(pair)) for pair in coca_unigrams]\n",
    "coca_unigrams = list(map(lambda pair: (pair[0], float(pair[1])),\n",
    "                         coca_unigrams))\n",
    "coca_unigrams = dict(coca_unigrams)\n",
    "coca_unigram_counts = Counter(coca_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:03.221481Z",
     "start_time": "2019-03-12T02:47:03.219096Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:03.229474Z",
     "start_time": "2019-03-12T02:47:03.222826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covet'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "349.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_orthWord = choice(hammond_newdic)['Orthography']\n",
    "random_orthWord\n",
    "coca_unigram_counts[random_orthWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:03.240211Z",
     "start_time": "2019-03-12T02:47:03.231404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'410,428,505.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(sum(coca_unigram_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPhOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:05.290698Z",
     "start_time": "2019-03-12T02:47:05.160809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IPhOD2_Words_IPA.csv\r\n",
      " IPhOD2_Words_IPA_prob_caughtCotMerged.csv\r\n",
      " IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv\r\n",
      " IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressedTriphones.txt\r\n",
      " IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_stressedTriphones.txt\r\n",
      " IPhOD2_Words_IPA_prob.csv\r\n",
      " IPhOD2_Words.txt\r\n",
      "'IPhOD-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'IPhOD-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'IPhOD-aligned_destressed response diphones.txt'\r\n",
      "'IPhOD-aligned_destressed response illegal diphones.txt'\r\n",
      "'IPhOD-aligned_destressed response uniphones.txt'\r\n",
      "'IPhOD-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'IPhOD-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'IPhOD-aligned_destressed stimuli diphones.txt'\r\n",
      "'IPhOD-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'IPhOD-aligned_destressed stimuli uniphones.txt'\r\n",
      "'IPhOD-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'IPhOD-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'IPhOD-aligned_stressed stimuli diphones.txt'\r\n",
      "'IPhOD-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'IPhOD-aligned_stressed stimuli uniphones.txt'\r\n",
      " IPhOD_aligned_trials.csv\r\n",
      "\r\n",
      "IPhOD2v2.0_REALS:\r\n",
      "2009_Dec01_Release_Readme.txt  gpl.txt\r\n",
      "CMU_pronunciation_key.pdf      IPhOD2_Words.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:05.320663Z",
     "start_time": "2019-03-12T02:47:05.317719Z"
    }
   },
   "outputs": [],
   "source": [
    "# my_lexicon_fn = 'IPhOD2_Words.txt'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA.csv'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA_prob.csv'\n",
    "# my_lexicon_fn = 'IPhOD2_Words_IPA_prob_caughtCotMerged.csv'\n",
    "my_lexicon_fn = 'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:12.189201Z",
     "start_time": "2019-03-12T02:47:11.019326Z"
    }
   },
   "outputs": [],
   "source": [
    "lexicon = []\n",
    "with open(my_lexicon_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        lexicon.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:12.379635Z",
     "start_time": "2019-03-12T02:47:12.356721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Indx', 'NPhon', 'NSyll', 'Nlprob', 'Prob', 'SCDcnt', 'SFreq', 'StTrn', 'UnTrn', 'Word', 'strBPAV', 'strCBPAV', 'strCDEN', 'strCLCPOSPAV', 'strCPOSPAV', 'strCTPAV', 'strDENS', 'strFBPAV', 'strFDEN', 'strFLCPOSPAV', 'strFPOSPAV', 'strFTPAV', 'strLBPAV', 'strLCPOSPAV', 'strLDEN', 'strLLCPOSPAV', 'strLPOSPAV', 'strLTPAV', 'strPOSPAV', 'strTPAV', 'unsBPAV', 'unsCBPAV', 'unsCDEN', 'unsCLCPOSPAV', 'unsCPOSPAV', 'unsCTPAV', 'unsDENS', 'unsFBPAV', 'unsFDEN', 'unsFLCPOSPAV', 'unsFPOSPAV', 'unsFTPAV', 'unsLBPAV', 'unsLCPOSPAV', 'unsLDEN', 'unsLLCPOSPAV', 'unsLPOSPAV', 'unsLTPAV', 'unsPOSPAV', 'unsTPAV'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('Indx', '1'),\n",
       "              ('NPhon', '1'),\n",
       "              ('NSyll', '1'),\n",
       "              ('Nlprob', '6.10491267350873'),\n",
       "              ('Prob', '0.014529081648642661'),\n",
       "              ('SCDcnt', '8382'),\n",
       "              ('SFreq', '20415.27'),\n",
       "              ('StTrn', 'ə0'),\n",
       "              ('UnTrn', 'ə'),\n",
       "              ('Word', 'a'),\n",
       "              ('strBPAV', '0'),\n",
       "              ('strCBPAV', '0'),\n",
       "              ('strCDEN', '114167'),\n",
       "              ('strCLCPOSPAV', '0.10933994'),\n",
       "              ('strCPOSPAV', '0.03349570'),\n",
       "              ('strCTPAV', '0'),\n",
       "              ('strDENS', '21'),\n",
       "              ('strFBPAV', '0'),\n",
       "              ('strFDEN', '145035.32'),\n",
       "              ('strFLCPOSPAV', '0.21988359'),\n",
       "              ('strFPOSPAV', '0.06061931'),\n",
       "              ('strFTPAV', '0'),\n",
       "              ('strLBPAV', '0'),\n",
       "              ('strLCPOSPAV', '0.06666667'),\n",
       "              ('strLDEN', '77.56'),\n",
       "              ('strLLCPOSPAV', '0.07556101'),\n",
       "              ('strLPOSPAV', '0.03338777'),\n",
       "              ('strLTPAV', '0'),\n",
       "              ('strPOSPAV', '0.03588891'),\n",
       "              ('strTPAV', '0'),\n",
       "              ('unsBPAV', '0'),\n",
       "              ('unsCBPAV', '0'),\n",
       "              ('unsCDEN', '136251'),\n",
       "              ('unsCLCPOSPAV', '0.15987477'),\n",
       "              ('unsCPOSPAV', '0.04159586'),\n",
       "              ('unsCTPAV', '0'),\n",
       "              ('unsDENS', '26'),\n",
       "              ('unsFBPAV', '0'),\n",
       "              ('unsFDEN', '150377.45'),\n",
       "              ('unsFLCPOSPAV', '0.22760866'),\n",
       "              ('unsFPOSPAV', '0.07192868'),\n",
       "              ('unsFTPAV', '0'),\n",
       "              ('unsLBPAV', '0'),\n",
       "              ('unsLCPOSPAV', '0.07692308'),\n",
       "              ('unsLDEN', '91.03'),\n",
       "              ('unsLLCPOSPAV', '0.12570126'),\n",
       "              ('unsLPOSPAV', '0.03999447'),\n",
       "              ('unsLTPAV', '0'),\n",
       "              ('unsPOSPAV', '0.04449866'),\n",
       "              ('unsTPAV', '0')]),\n",
       " OrderedDict([('Indx', '2'),\n",
       "              ('NPhon', '1'),\n",
       "              ('NSyll', '1'),\n",
       "              ('Nlprob', '6.10491267350873'),\n",
       "              ('Prob', '0.014529081648642661'),\n",
       "              ('SCDcnt', '8382'),\n",
       "              ('SFreq', '20415.27'),\n",
       "              ('StTrn', 'eɪ1'),\n",
       "              ('UnTrn', 'eɪ'),\n",
       "              ('Word', 'a'),\n",
       "              ('strBPAV', '0'),\n",
       "              ('strCBPAV', '0'),\n",
       "              ('strCDEN', '145321'),\n",
       "              ('strCLCPOSPAV', '0.11115314'),\n",
       "              ('strCPOSPAV', '0.00458059'),\n",
       "              ('strCTPAV', '0'),\n",
       "              ('strDENS', '44'),\n",
       "              ('strFBPAV', '0'),\n",
       "              ('strFDEN', '89743.84'),\n",
       "              ('strFLCPOSPAV', '0.21993981'),\n",
       "              ('strFPOSPAV', '0.02042537'),\n",
       "              ('strFTPAV', '0'),\n",
       "              ('strLBPAV', '0'),\n",
       "              ('strLCPOSPAV', '0.06666667'),\n",
       "              ('strLDEN', '116.4'),\n",
       "              ('strLLCPOSPAV', '0.09186536'),\n",
       "              ('strLPOSPAV', '0.00398874'),\n",
       "              ('strLTPAV', '0'),\n",
       "              ('strPOSPAV', '0.00286029'),\n",
       "              ('strTPAV', '0'),\n",
       "              ('unsBPAV', '0'),\n",
       "              ('unsCBPAV', '0'),\n",
       "              ('unsCDEN', '145321'),\n",
       "              ('unsCLCPOSPAV', '0.11115314'),\n",
       "              ('unsCPOSPAV', '0.00473232'),\n",
       "              ('unsCTPAV', '0'),\n",
       "              ('unsDENS', '44'),\n",
       "              ('unsFBPAV', '0'),\n",
       "              ('unsFDEN', '89743.84'),\n",
       "              ('unsFLCPOSPAV', '0.21993981'),\n",
       "              ('unsFPOSPAV', '0.02151505'),\n",
       "              ('unsFTPAV', '0'),\n",
       "              ('unsLBPAV', '0'),\n",
       "              ('unsLCPOSPAV', '0.07692308'),\n",
       "              ('unsLDEN', '116.4'),\n",
       "              ('unsLLCPOSPAV', '0.09186536'),\n",
       "              ('unsLPOSPAV', '0.00425179'),\n",
       "              ('unsLTPAV', '0'),\n",
       "              ('unsPOSPAV', '0.00333062'),\n",
       "              ('unsTPAV', '0')]),\n",
       " OrderedDict([('Indx', '3'),\n",
       "              ('NPhon', '7'),\n",
       "              ('NSyll', '3'),\n",
       "              ('Nlprob', '21.451420046618022'),\n",
       "              ('Prob', '3.487218149862776e-07'),\n",
       "              ('SCDcnt', '23'),\n",
       "              ('SFreq', '0.49'),\n",
       "              ('StTrn', 't.r.ɪ2.p.ə0.l.eɪ1'),\n",
       "              ('UnTrn', 't.r.ɪ.p.ə.l.eɪ'),\n",
       "              ('Word', 'Aaa'),\n",
       "              ('strBPAV', '0.00479395'),\n",
       "              ('strCBPAV', '0.00393286'),\n",
       "              ('strCDEN', '0'),\n",
       "              ('strCLCPOSPAV', '0.06229064'),\n",
       "              ('strCPOSPAV', '0.04821133'),\n",
       "              ('strCTPAV', '0.00011896'),\n",
       "              ('strDENS', '0'),\n",
       "              ('strFBPAV', '0.00221983'),\n",
       "              ('strFDEN', '0'),\n",
       "              ('strFLCPOSPAV', '0.06127196'),\n",
       "              ('strFPOSPAV', '0.04222206'),\n",
       "              ('strFTPAV', '0.00010726'),\n",
       "              ('strLBPAV', '0.00441717'),\n",
       "              ('strLCPOSPAV', '0.05360625'),\n",
       "              ('strLDEN', '0'),\n",
       "              ('strLLCPOSPAV', '0.05900326'),\n",
       "              ('strLPOSPAV', '0.05090395'),\n",
       "              ('strLTPAV', '0.00011330'),\n",
       "              ('strPOSPAV', '0.05032317'),\n",
       "              ('strTPAV', '0.00012595'),\n",
       "              ('unsBPAV', '0.00663829'),\n",
       "              ('unsCBPAV', '0.00546635'),\n",
       "              ('unsCDEN', '395'),\n",
       "              ('unsCLCPOSPAV', '0.06879694'),\n",
       "              ('unsCPOSPAV', '0.05521636'),\n",
       "              ('unsCTPAV', '0.00027130'),\n",
       "              ('unsDENS', '4'),\n",
       "              ('unsFBPAV', '0.00318710'),\n",
       "              ('unsFDEN', '10.94'),\n",
       "              ('unsFLCPOSPAV', '0.06801497'),\n",
       "              ('unsFPOSPAV', '0.04886090'),\n",
       "              ('unsFTPAV', '0.00018534'),\n",
       "              ('unsLBPAV', '0.00630382'),\n",
       "              ('unsLCPOSPAV', '0.06060924'),\n",
       "              ('unsLDEN', '1.57'),\n",
       "              ('unsLLCPOSPAV', '0.06617590'),\n",
       "              ('unsLPOSPAV', '0.05853472'),\n",
       "              ('unsLTPAV', '0.00030978'),\n",
       "              ('unsPOSPAV', '0.05873163'),\n",
       "              ('unsTPAV', '0.00034453')]),\n",
       " OrderedDict([('Indx', '4'),\n",
       "              ('NPhon', '7'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '21.708577886115147'),\n",
       "              ('Prob', '2.91787641110967e-07'),\n",
       "              ('SCDcnt', '12'),\n",
       "              ('SFreq', '0.41'),\n",
       "              ('StTrn', 'ɑ1.r.d.v.ɑ2.r.k'),\n",
       "              ('UnTrn', 'ɑ.r.d.v.ɑ.r.k'),\n",
       "              ('Word', 'aardvark'),\n",
       "              ('strBPAV', '0.00137863'),\n",
       "              ('strCBPAV', '0.00149961'),\n",
       "              ('strCDEN', '0'),\n",
       "              ('strCLCPOSPAV', '0.02592236'),\n",
       "              ('strCPOSPAV', '0.02692287'),\n",
       "              ('strCTPAV', '0.00008005'),\n",
       "              ('strDENS', '0'),\n",
       "              ('strFBPAV', '0.00123232'),\n",
       "              ('strFDEN', '0'),\n",
       "              ('strFLCPOSPAV', '0.02559907'),\n",
       "              ('strFPOSPAV', '0.02499488'),\n",
       "              ('strFTPAV', '0.00004571'),\n",
       "              ('strLBPAV', '0.00153373'),\n",
       "              ('strLCPOSPAV', '0.03081185'),\n",
       "              ('strLDEN', '0'),\n",
       "              ('strLLCPOSPAV', '0.02938683'),\n",
       "              ('strLPOSPAV', '0.02946981'),\n",
       "              ('strLTPAV', '0.00007731'),\n",
       "              ('strPOSPAV', '0.03109908'),\n",
       "              ('strTPAV', '0.00007832'),\n",
       "              ('unsBPAV', '0.00280566'),\n",
       "              ('unsCBPAV', '0.00305301'),\n",
       "              ('unsCDEN', '0'),\n",
       "              ('unsCLCPOSPAV', '0.02731816'),\n",
       "              ('unsCPOSPAV', '0.02777683'),\n",
       "              ('unsCTPAV', '0.00013870'),\n",
       "              ('unsDENS', '0'),\n",
       "              ('unsFBPAV', '0.00267029'),\n",
       "              ('unsFDEN', '0'),\n",
       "              ('unsFLCPOSPAV', '0.02696933'),\n",
       "              ('unsFPOSPAV', '0.02591157'),\n",
       "              ('unsFTPAV', '0.00008230'),\n",
       "              ('unsLBPAV', '0.00305252'),\n",
       "              ('unsLCPOSPAV', '0.03238795'),\n",
       "              ('unsLDEN', '0'),\n",
       "              ('unsLLCPOSPAV', '0.03075498'),\n",
       "              ('unsLPOSPAV', '0.03064735'),\n",
       "              ('unsLTPAV', '0.00016417'),\n",
       "              ('unsPOSPAV', '0.03289830'),\n",
       "              ('unsTPAV', '0.00016205')]),\n",
       " OrderedDict([('Indx', '5'),\n",
       "              ('NPhon', '4'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '16.54944494142362'),\n",
       "              ('Prob', '1.042607059091626e-05'),\n",
       "              ('SCDcnt', '162'),\n",
       "              ('SFreq', '14.65'),\n",
       "              ('StTrn', 'ɛ1.r.ə0.n'),\n",
       "              ('UnTrn', 'ɛ.r.ə.n'),\n",
       "              ('Word', 'Aaron'),\n",
       "              ('strBPAV', '0.01189711'),\n",
       "              ('strCBPAV', '0.01036897'),\n",
       "              ('strCDEN', '6911'),\n",
       "              ('strCLCPOSPAV', '0.04227526'),\n",
       "              ('strCPOSPAV', '0.03915159'),\n",
       "              ('strCTPAV', '0.00058309'),\n",
       "              ('strDENS', '15'),\n",
       "              ('strFBPAV', '0.00866527'),\n",
       "              ('strFDEN', '334.45'),\n",
       "              ('strFLCPOSPAV', '0.04215606'),\n",
       "              ('strFPOSPAV', '0.03575674'),\n",
       "              ('strFTPAV', '0.00034252'),\n",
       "              ('strLBPAV', '0.01203257'),\n",
       "              ('strLCPOSPAV', '0.03705601'),\n",
       "              ('strLDEN', '12.03'),\n",
       "              ('strLLCPOSPAV', '0.04189901'),\n",
       "              ('strLPOSPAV', '0.04301820'),\n",
       "              ('strLTPAV', '0.00069347'),\n",
       "              ('strPOSPAV', '0.04428639'),\n",
       "              ('strTPAV', '0.00062271'),\n",
       "              ('unsBPAV', '0.01381465'),\n",
       "              ('unsCBPAV', '0.01306627'),\n",
       "              ('unsCDEN', '12975'),\n",
       "              ('unsCLCPOSPAV', '0.04980887'),\n",
       "              ('unsCPOSPAV', '0.04384883'),\n",
       "              ('unsCTPAV', '0.00078667'),\n",
       "              ('unsDENS', '17'),\n",
       "              ('unsFBPAV', '0.01149921'),\n",
       "              ('unsFDEN', '685.02'),\n",
       "              ('unsFLCPOSPAV', '0.07209335'),\n",
       "              ('unsFPOSPAV', '0.04509124'),\n",
       "              ('unsFTPAV', '0.00048959'),\n",
       "              ('unsLBPAV', '0.01402346'),\n",
       "              ('unsLCPOSPAV', '0.04132635'),\n",
       "              ('unsLDEN', '14.59'),\n",
       "              ('unsLLCPOSPAV', '0.04660694'),\n",
       "              ('unsLPOSPAV', '0.04759894'),\n",
       "              ('unsLTPAV', '0.00087101'),\n",
       "              ('unsPOSPAV', '0.05007211'),\n",
       "              ('unsTPAV', '0.00085112')]),\n",
       " OrderedDict([('Indx', '6'),\n",
       "              ('NPhon', '2'),\n",
       "              ('NSyll', '1'),\n",
       "              ('Nlprob', '19.866457545896868'),\n",
       "              ('Prob', '1.0461654449588329e-06'),\n",
       "              ('SCDcnt', '51'),\n",
       "              ('SFreq', '1.47'),\n",
       "              ('StTrn', 'æ1.b'),\n",
       "              ('UnTrn', 'æ.b'),\n",
       "              ('Word', 'Ab'),\n",
       "              ('strBPAV', '0.00070116'),\n",
       "              ('strCBPAV', '0.00044297'),\n",
       "              ('strCDEN', '42291'),\n",
       "              ('strCLCPOSPAV', '0.02506846'),\n",
       "              ('strCPOSPAV', '0.00976395'),\n",
       "              ('strCTPAV', '0'),\n",
       "              ('strDENS', '31'),\n",
       "              ('strFBPAV', '0.00018963'),\n",
       "              ('strFDEN', '8952.86'),\n",
       "              ('strFLCPOSPAV', '0.01015147'),\n",
       "              ('strFPOSPAV', '0.01385802'),\n",
       "              ('strFTPAV', '0'),\n",
       "              ('strLBPAV', '0.00072701'),\n",
       "              ('strLCPOSPAV', '0.02735712'),\n",
       "              ('strLDEN', '34.26'),\n",
       "              ('strLLCPOSPAV', '0.02190067'),\n",
       "              ('strLPOSPAV', '0.00999835'),\n",
       "              ('strLTPAV', '0'),\n",
       "              ('strPOSPAV', '0.01083006'),\n",
       "              ('strTPAV', '0'),\n",
       "              ('unsBPAV', '0.00095919'),\n",
       "              ('unsCBPAV', '0.00056616'),\n",
       "              ('unsCDEN', '42291'),\n",
       "              ('unsCLCPOSPAV', '0.02646400'),\n",
       "              ('unsCPOSPAV', '0.01137853'),\n",
       "              ('unsCTPAV', '0'),\n",
       "              ('unsDENS', '32'),\n",
       "              ('unsFBPAV', '0.00026165'),\n",
       "              ('unsFDEN', '8952.86'),\n",
       "              ('unsFLCPOSPAV', '0.01155083'),\n",
       "              ('unsFPOSPAV', '0.01497176'),\n",
       "              ('unsFTPAV', '0'),\n",
       "              ('unsLBPAV', '0.00086282'),\n",
       "              ('unsLCPOSPAV', '0.02814570'),\n",
       "              ('unsLDEN', '34.26'),\n",
       "              ('unsLLCPOSPAV', '0.02263862'),\n",
       "              ('unsLPOSPAV', '0.01296026'),\n",
       "              ('unsLTPAV', '0'),\n",
       "              ('unsPOSPAV', '0.01565681'),\n",
       "              ('unsTPAV', '0')]),\n",
       " OrderedDict([('Indx', '7'),\n",
       "              ('NPhon', '3'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '19.866457545896868'),\n",
       "              ('Prob', '1.0461654449588329e-06'),\n",
       "              ('SCDcnt', '51'),\n",
       "              ('SFreq', '1.47'),\n",
       "              ('StTrn', 'eɪ1.b.i1'),\n",
       "              ('UnTrn', 'eɪ.b.i'),\n",
       "              ('Word', 'Ab'),\n",
       "              ('strBPAV', '0.00041198'),\n",
       "              ('strCBPAV', '0.00082426'),\n",
       "              ('strCDEN', '10612'),\n",
       "              ('strCLCPOSPAV', '0.00384819'),\n",
       "              ('strCPOSPAV', '0.00788689'),\n",
       "              ('strCTPAV', '0.00000154'),\n",
       "              ('strDENS', '5'),\n",
       "              ('strFBPAV', '0.00165320'),\n",
       "              ('strFDEN', '5844.9'),\n",
       "              ('strFLCPOSPAV', '0.00240143'),\n",
       "              ('strFPOSPAV', '0.01126537'),\n",
       "              ('strFTPAV', '0.00000067'),\n",
       "              ('strLBPAV', '0.00055885'),\n",
       "              ('strLCPOSPAV', '0.01008029'),\n",
       "              ('strLDEN', '7.72'),\n",
       "              ('strLLCPOSPAV', '0.00591462'),\n",
       "              ('strLPOSPAV', '0.00840710'),\n",
       "              ('strLTPAV', '0.00000498'),\n",
       "              ('strPOSPAV', '0.00801384'),\n",
       "              ('strTPAV', '0.00000529'),\n",
       "              ('unsBPAV', '0.00070620'),\n",
       "              ('unsCBPAV', '0.00136259'),\n",
       "              ('unsCDEN', '24070'),\n",
       "              ('unsCLCPOSPAV', '0.00812329'),\n",
       "              ('unsCPOSPAV', '0.01186590'),\n",
       "              ('unsCTPAV', '0.00021987'),\n",
       "              ('unsDENS', '13'),\n",
       "              ('unsFBPAV', '0.00232742'),\n",
       "              ('unsFDEN', '7348.75'),\n",
       "              ('unsFLCPOSPAV', '0.00477102'),\n",
       "              ('unsFPOSPAV', '0.01460524'),\n",
       "              ('unsFTPAV', '0.00037520'),\n",
       "              ('unsLBPAV', '0.00092384'),\n",
       "              ('unsLCPOSPAV', '0.01656140'),\n",
       "              ('unsLDEN', '17.72'),\n",
       "              ('unsLLCPOSPAV', '0.01086358'),\n",
       "              ('unsLPOSPAV', '0.01128913'),\n",
       "              ('unsLTPAV', '0.00007660'),\n",
       "              ('unsPOSPAV', '0.01070701'),\n",
       "              ('unsTPAV', '0.00003745')]),\n",
       " OrderedDict([('Indx', '8'),\n",
       "              ('NPhon', '4'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '22.20814889560566'),\n",
       "              ('Prob', '2.0638638029800102e-07'),\n",
       "              ('SCDcnt', '15'),\n",
       "              ('SFreq', '0.29'),\n",
       "              ('StTrn', 'ə0.b.æ1.k'),\n",
       "              ('UnTrn', 'ə.b.æ.k'),\n",
       "              ('Word', 'aback'),\n",
       "              ('strBPAV', '0.00249014'),\n",
       "              ('strCBPAV', '0.00213390'),\n",
       "              ('strCDEN', '10230'),\n",
       "              ('strCLCPOSPAV', '0.02497310'),\n",
       "              ('strCPOSPAV', '0.02256442'),\n",
       "              ('strCTPAV', '0.00011024'),\n",
       "              ('strDENS', '2'),\n",
       "              ('strFBPAV', '0.00187808'),\n",
       "              ('strFDEN', '2084.71'),\n",
       "              ('strFLCPOSPAV', '0.03203707'),\n",
       "              ('strFPOSPAV', '0.02766156'),\n",
       "              ('strFTPAV', '0.00026386'),\n",
       "              ('strLBPAV', '0.00236581'),\n",
       "              ('strLCPOSPAV', '0.01996641'),\n",
       "              ('strLDEN', '5.18'),\n",
       "              ('strLLCPOSPAV', '0.02284313'),\n",
       "              ('strLPOSPAV', '0.02597716'),\n",
       "              ('strLTPAV', '0.00008609'),\n",
       "              ('strPOSPAV', '0.02795435'),\n",
       "              ('strTPAV', '0.00008732'),\n",
       "              ('unsBPAV', '0.00307521'),\n",
       "              ('unsCBPAV', '0.00245509'),\n",
       "              ('unsCDEN', '10230'),\n",
       "              ('unsCLCPOSPAV', '0.02633703'),\n",
       "              ('unsCPOSPAV', '0.02486323'),\n",
       "              ('unsCTPAV', '0.00012145'),\n",
       "              ('unsDENS', '2'),\n",
       "              ('unsFBPAV', '0.00213486'),\n",
       "              ('unsFDEN', '2084.71'),\n",
       "              ('unsFLCPOSPAV', '0.03296305'),\n",
       "              ('unsFPOSPAV', '0.03071776'),\n",
       "              ('unsFTPAV', '0.00027855'),\n",
       "              ('unsLBPAV', '0.00283104'),\n",
       "              ('unsLCPOSPAV', '0.02081854'),\n",
       "              ('unsLDEN', '5.18'),\n",
       "              ('unsLLCPOSPAV', '0.02364720'),\n",
       "              ('unsLPOSPAV', '0.02822531'),\n",
       "              ('unsLTPAV', '0.00011872'),\n",
       "              ('unsPOSPAV', '0.03124957'),\n",
       "              ('unsTPAV', '0.00013788')]),\n",
       " OrderedDict([('Indx', '9'),\n",
       "              ('NPhon', '6'),\n",
       "              ('NSyll', '3'),\n",
       "              ('Nlprob', '22.481167390012075'),\n",
       "              ('Prob', '1.708025216259319e-07'),\n",
       "              ('SCDcnt', '10'),\n",
       "              ('SFreq', '0.24'),\n",
       "              ('StTrn', 'æ1.b.ə0.k.ə0.s'),\n",
       "              ('UnTrn', 'æ.b.ə.k.ə.s'),\n",
       "              ('Word', 'abacus'),\n",
       "              ('strBPAV', '0.00465286'),\n",
       "              ('strCBPAV', '0.00339918'),\n",
       "              ('strCDEN', '0'),\n",
       "              ('strCLCPOSPAV', '0.06252302'),\n",
       "              ('strCPOSPAV', '0.04239733'),\n",
       "              ('strCTPAV', '0.00009941'),\n",
       "              ('strDENS', '0'),\n",
       "              ('strFBPAV', '0.00206951'),\n",
       "              ('strFDEN', '0'),\n",
       "              ('strFLCPOSPAV', '0.06041806'),\n",
       "              ('strFPOSPAV', '0.04110196'),\n",
       "              ('strFTPAV', '0.00005130'),\n",
       "              ('strLBPAV', '0.00421699'),\n",
       "              ('strLCPOSPAV', '0.05596900'),\n",
       "              ('strLDEN', '0'),\n",
       "              ('strLLCPOSPAV', '0.06275189'),\n",
       "              ('strLPOSPAV', '0.04569719'),\n",
       "              ('strLTPAV', '0.00013019'),\n",
       "              ('strPOSPAV', '0.04516777'),\n",
       "              ('strTPAV', '0.00012216'),\n",
       "              ('unsBPAV', '0.00533857'),\n",
       "              ('unsCBPAV', '0.00449214'),\n",
       "              ('unsCDEN', '0'),\n",
       "              ('unsCLCPOSPAV', '0.06697153'),\n",
       "              ('unsCPOSPAV', '0.04642964'),\n",
       "              ('unsCTPAV', '0.00014595'),\n",
       "              ('unsDENS', '0'),\n",
       "              ('unsFBPAV', '0.00386514'),\n",
       "              ('unsFDEN', '0'),\n",
       "              ('unsFLCPOSPAV', '0.06527492'),\n",
       "              ('unsFPOSPAV', '0.04798881'),\n",
       "              ('unsFTPAV', '0.00007602'),\n",
       "              ('unsLBPAV', '0.00510929'),\n",
       "              ('unsLCPOSPAV', '0.05996655'),\n",
       "              ('unsLDEN', '0'),\n",
       "              ('unsLLCPOSPAV', '0.06654066'),\n",
       "              ('unsLPOSPAV', '0.04963739'),\n",
       "              ('unsLTPAV', '0.00018753'),\n",
       "              ('unsPOSPAV', '0.04967565'),\n",
       "              ('unsTPAV', '0.00017278')]),\n",
       " OrderedDict([('Indx', '10'),\n",
       "              ('NPhon', '7'),\n",
       "              ('NSyll', '4'),\n",
       "              ('Nlprob', '21.393704548761736'),\n",
       "              ('Prob', '3.629553584551053e-07'),\n",
       "              ('SCDcnt', '11'),\n",
       "              ('SFreq', '0.51'),\n",
       "              ('StTrn', 'æ2.b.ə0.l.oʊ1.n.i0'),\n",
       "              ('UnTrn', 'æ.b.ə.l.oʊ.n.i'),\n",
       "              ('Word', 'abalone'),\n",
       "              ('strBPAV', '0.00415311'),\n",
       "              ('strCBPAV', '0.00355559'),\n",
       "              ('strCDEN', '183'),\n",
       "              ('strCLCPOSPAV', '0.03858267'),\n",
       "              ('strCPOSPAV', '0.02737455'),\n",
       "              ('strCTPAV', '0.00028476'),\n",
       "              ('strDENS', '1'),\n",
       "              ('strFBPAV', '0.00229702'),\n",
       "              ('strFDEN', '4.96'),\n",
       "              ('strFLCPOSPAV', '0.03827206'),\n",
       "              ('strFPOSPAV', '0.02593441'),\n",
       "              ('strFTPAV', '0.00016788'),\n",
       "              ('strLBPAV', '0.00385323'),\n",
       "              ('strLCPOSPAV', '0.03361684'),\n",
       "              ('strLDEN', '1.07'),\n",
       "              ('strLLCPOSPAV', '0.03598374'),\n",
       "              ('strLPOSPAV', '0.02823468'),\n",
       "              ('strLTPAV', '0.00030101'),\n",
       "              ('strPOSPAV', '0.02835650'),\n",
       "              ('strTPAV', '0.00036410'),\n",
       "              ('unsBPAV', '0.00467179'),\n",
       "              ('unsCBPAV', '0.00404593'),\n",
       "              ('unsCDEN', '183'),\n",
       "              ('unsCLCPOSPAV', '0.04384638'),\n",
       "              ('unsCPOSPAV', '0.03328168'),\n",
       "              ('unsCTPAV', '0.00029948'),\n",
       "              ('unsDENS', '1'),\n",
       "              ('unsFBPAV', '0.00303990'),\n",
       "              ('unsFDEN', '4.96')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 20000 exceeded with 20473 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lexicon[0].keys()\n",
    "lexicon[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:13.362492Z",
     "start_time": "2019-03-12T02:47:13.260472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing: IPhOD-aligned_destressed stimuli diphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli illegal diphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli illegal diphones.txt\n",
      "Importing: IPhOD-aligned_destressed response illegal diphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphone-based constructible triphones.txt\n",
      "Importing: IPhOD-aligned_destressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: IPhOD-aligned_stressed stimuli diphone-based illegal triphones.txt\n",
      "Importing: IPhOD-aligned_destressed response diphone-based illegal triphones.txt\n"
     ]
    }
   ],
   "source": [
    "#FIXME load illegal diphones and triphones\n",
    "diphoneAnalysis_i = importNphoneAnalysis(2, 'IPhOD-aligned')\n",
    "triphoneAnalysis_i = importNphoneAnalysis(3, 'IPhOD-aligned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPhOD processing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a json file mapping phonological wordforms to frequencies or probabilities for a word recognition model we need to\n",
    " 1. Remove data we don't want or can't use in the word recognition model.\n",
    " 2. Choose a set of phonological wordforms (unstressed or stressed) to map to frequencies/probabilities.\n",
    " 3. Filter the phonological wordforms based on the illegal diphones and triphones.\n",
    " 3. Calculate frequencies/probabilities, dealing with the vagaries of IPhOD (and remembering to normalize):\n",
    "   - The wordform frequency of each entry is a corpus frequency of the associated orthographic word.\n",
    "   - Some distinct orthographic wordforms share a phonological wordform.\n",
    " 4. Phonological wordforms need to have word edge symbols added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data that definitely won't end up in the lexicon distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:17.252923Z",
     "start_time": "2019-03-12T02:47:17.251005Z"
    }
   },
   "outputs": [],
   "source": [
    "# def project_dict(the_dict, keys_to_keep):\n",
    "#     new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "#     return new_dict\n",
    "# project_dict({'Name':'Joe','ID':123,'Job':'clerk'},['Job','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:18.123591Z",
     "start_time": "2019-03-12T02:47:17.793148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim_lexicon = list(map(lambda d: project_dict(d, ['StTrn', 'UnTrn', 'Word', 'SFreq','Prob','Nlprob']), lexicon))\n",
    "slim_lexicon[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words with triphones and diphones that a channel distribution isn't definable for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:18.406414Z",
     "start_time": "2019-03-12T02:47:18.398662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16389"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illegalDiphones_i = diphoneAnalysis_i['illicit'][which_stress + ' stimuli']\n",
    "len(illegalDiphones_i)\n",
    "illegalTriphones_i = triphoneAnalysis_i['illicit'][which_stress + ' stimuli']\n",
    "len(illegalTriphones_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:18.698420Z",
     "start_time": "2019-03-12T02:47:18.689045Z"
    }
   },
   "outputs": [],
   "source": [
    "def badNphone(nphone, n, badNphones):\n",
    "    if n == 2:\n",
    "        return nphone in badNphones\n",
    "    if n == 3:\n",
    "        return nphone in badNphones\n",
    "    raise Exception('n must be 2 or 3.')\n",
    "\n",
    "def containsBadNphones(row, n, getNphones, badNphones):\n",
    "    phs = getNphones(row, n)\n",
    "    return any([badNphone(ph, n, badNphones) for ph in phs])\n",
    "\n",
    "def rowsWBadNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if containsBadNphones(r, n, getNphones, badNphones)]\n",
    "\n",
    "def observedBadNphones(rows, n, getNphones, badNphones):\n",
    "    return union([getNphones(r, n) for r in rows if containsBadNphones(r, n, getNphones, badNphones)])\n",
    "\n",
    "def onlyRowsWithGoodNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if not containsBadNphones(r, n, getNphones, badNphones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:18.939530Z",
     "start_time": "2019-03-12T02:47:18.934493Z"
    }
   },
   "outputs": [],
   "source": [
    "badDiphones = set()\n",
    "badTriphones = set()\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    badDiphones = set()\n",
    "    badTriphones = set()\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    badDiphones = set(illegalDiphones_i)\n",
    "    badTriphones = set(illegalTriphones_i)\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} instead\".format(which_filter))\n",
    "    \n",
    "def getNphones_i(IPhOD_row, n):\n",
    "#     strTrnTuple = dottedStringToTuple(IPhOD_row['StTrn'])\n",
    "#     unTrnTuple = dottedStringToTuple(IPhOD_row['UnTrn'])\n",
    "\n",
    "    strNFactors = dsToKfactors(n, IPhOD_row['StTrn'])\n",
    "    unstrNFactors = dsToKfactors(n, IPhOD_row['UnTrn'])\n",
    "#     print(unstrNFactors)\n",
    "    \n",
    "#     uniqueStrNphones = set(strNFactors)\n",
    "#     uniqueUnstrNphones = set(unstrNFactors)\n",
    "    if which_stress == 'destressed':\n",
    "        return unstrNFactors\n",
    "    elif which_stress == 'stressed':\n",
    "        return strNFactors\n",
    "    else:\n",
    "        raise Exception('Bad which_stress arg.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:19.750270Z",
     "start_time": "2019-03-12T02:47:19.146335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.034776975754210626"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.017018805955438324"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Aaron',\n",
       " 'aberrant',\n",
       " 'acerbic',\n",
       " 'actuarial',\n",
       " 'actuaries',\n",
       " 'actuary',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'aerial',\n",
       " 'aerials',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aerobatics',\n",
       " 'aerobic',\n",
       " 'aerobically',\n",
       " 'aerodromes',\n",
       " 'aerodynamic',\n",
       " 'aerodynamically',\n",
       " 'aerodynamics',\n",
       " 'Aerodyne',\n",
       " 'aeronautical',\n",
       " 'aeronautical',\n",
       " 'Aeronautics',\n",
       " 'aerosol',\n",
       " 'Aerosols',\n",
       " 'aerospace',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'aficionado',\n",
       " 'ageratum',\n",
       " 'agrarian',\n",
       " 'airbag',\n",
       " 'airbags',\n",
       " 'airbase',\n",
       " 'airboat',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aircrafts',\n",
       " 'aircrafts',\n",
       " 'Aircrew',\n",
       " 'airdrop',\n",
       " 'aired',\n",
       " 'Airedale',\n",
       " 'Airedales',\n",
       " 'airfare',\n",
       " 'airfares',\n",
       " 'airfield',\n",
       " 'airfields',\n",
       " 'airflow',\n",
       " 'airfoil',\n",
       " 'airhead',\n",
       " 'airing',\n",
       " 'airless',\n",
       " 'airlift',\n",
       " 'airlifted',\n",
       " 'airlifting',\n",
       " 'Airlifts',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airliners',\n",
       " 'Airlines',\n",
       " 'airlock',\n",
       " 'airlocks',\n",
       " 'airmail',\n",
       " 'Airman',\n",
       " 'airmen',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airship',\n",
       " 'airships',\n",
       " 'airspace',\n",
       " 'airspeed',\n",
       " 'airstrike',\n",
       " 'airstrikes',\n",
       " 'airstrip',\n",
       " 'airstrips',\n",
       " 'airtight',\n",
       " 'airtime',\n",
       " 'airwave',\n",
       " 'airwaves',\n",
       " 'airway',\n",
       " 'Airways',\n",
       " 'airworthy',\n",
       " 'airy',\n",
       " 'Algerians',\n",
       " 'alphanumeric',\n",
       " 'Altair',\n",
       " 'America',\n",
       " 'America',\n",
       " 'American',\n",
       " 'American',\n",
       " 'Americana',\n",
       " 'Americanism',\n",
       " 'Americanization',\n",
       " 'Americanize',\n",
       " 'Americanized',\n",
       " 'Americans',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Americas',\n",
       " 'ancillary',\n",
       " 'Angeline',\n",
       " 'Antares',\n",
       " 'antiaircraft',\n",
       " 'antidisestablishmentarianism',\n",
       " 'anywhere',\n",
       " 'anywhere',\n",
       " 'apothecary',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'Apparently',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'aquarium',\n",
       " 'aquariums',\n",
       " 'Aquarius',\n",
       " 'Ara',\n",
       " 'Arab',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabic',\n",
       " 'Arable',\n",
       " 'Arabs',\n",
       " 'Arabs',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'Ariadne',\n",
       " 'arid',\n",
       " 'arid',\n",
       " 'Ariel',\n",
       " 'Aries',\n",
       " 'aristocracy',\n",
       " 'Aristotle',\n",
       " 'arithmetic',\n",
       " 'Arizona',\n",
       " 'armchair',\n",
       " 'armchairs',\n",
       " 'aromatic',\n",
       " 'arrant',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'arrow',\n",
       " 'arrow',\n",
       " 'Arrowhead',\n",
       " 'Arrowhead',\n",
       " 'arrowheads',\n",
       " 'arrowheads',\n",
       " 'arrowroot',\n",
       " 'arrows',\n",
       " 'arrows',\n",
       " 'Arthurian',\n",
       " 'ary',\n",
       " 'asparagus',\n",
       " 'atmospheric',\n",
       " 'atmospherics',\n",
       " 'austerity',\n",
       " 'authoritarian',\n",
       " 'authoritarianism',\n",
       " 'auxiliary',\n",
       " 'aviary',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'backstairs',\n",
       " 'Banbury',\n",
       " 'barbarian',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbaric',\n",
       " 'barbarity',\n",
       " 'barbarity',\n",
       " 'Barbera',\n",
       " 'Barbero',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bares',\n",
       " 'barest',\n",
       " 'baring',\n",
       " 'Barish',\n",
       " 'barite',\n",
       " 'baritone',\n",
       " 'baritones',\n",
       " 'barium',\n",
       " 'barometric',\n",
       " 'Baron',\n",
       " 'Baron',\n",
       " 'Baroness',\n",
       " 'baronet',\n",
       " 'baronet',\n",
       " 'baronets',\n",
       " 'baronets',\n",
       " 'barons',\n",
       " 'barons',\n",
       " 'barrack',\n",
       " 'barrack',\n",
       " 'barracks',\n",
       " 'barracks',\n",
       " 'barracuda',\n",
       " 'Barragan',\n",
       " 'Barre',\n",
       " 'barrel',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barreled',\n",
       " 'barreling',\n",
       " 'barreling',\n",
       " 'barrels',\n",
       " 'barrels',\n",
       " 'barren',\n",
       " 'barren',\n",
       " 'Barrens',\n",
       " 'Barret',\n",
       " 'barricade',\n",
       " 'barricade',\n",
       " 'barricaded',\n",
       " 'barricaded',\n",
       " 'barricades',\n",
       " 'barricades',\n",
       " 'barrier',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barriers',\n",
       " 'Barringer',\n",
       " 'Barrios',\n",
       " 'barrister',\n",
       " 'barrister',\n",
       " 'barristers',\n",
       " 'barristers',\n",
       " 'Barrow',\n",
       " 'Barrow',\n",
       " 'Barrows',\n",
       " 'Barrows',\n",
       " 'Barry',\n",
       " 'Barry',\n",
       " 'Bavarian',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'bearer',\n",
       " 'bearers',\n",
       " 'bearing',\n",
       " 'bearings',\n",
       " 'bearish',\n",
       " 'bears',\n",
       " 'beneficiaries',\n",
       " 'beneficiary',\n",
       " 'beret',\n",
       " 'beret',\n",
       " 'Berets',\n",
       " 'Berets',\n",
       " 'Bering',\n",
       " 'berries',\n",
       " 'Berrigan',\n",
       " 'Berry',\n",
       " 'Berryman',\n",
       " 'Beryl',\n",
       " 'Beware',\n",
       " 'beyond',\n",
       " 'beyond',\n",
       " 'bier',\n",
       " 'billionaire',\n",
       " 'billionaires',\n",
       " 'blackberries',\n",
       " 'blackberry',\n",
       " 'Blair',\n",
       " 'blare',\n",
       " 'blared',\n",
       " 'blares',\n",
       " 'blaring',\n",
       " 'Bluebeard',\n",
       " 'blueberries',\n",
       " 'blueberry',\n",
       " 'Bolero',\n",
       " 'bracero',\n",
       " 'Breweries',\n",
       " 'buccaneer',\n",
       " 'Buccaneers',\n",
       " 'budgetary',\n",
       " 'Bulgaria',\n",
       " 'Bulgarian',\n",
       " 'Bulgarians',\n",
       " 'burial',\n",
       " 'burials',\n",
       " 'buried',\n",
       " 'buries',\n",
       " 'Burroughs',\n",
       " 'bury',\n",
       " 'burying',\n",
       " 'caballero',\n",
       " 'cairn',\n",
       " 'cairns',\n",
       " 'caldera',\n",
       " 'caldera',\n",
       " 'canaries',\n",
       " 'canary',\n",
       " 'Canberra',\n",
       " 'Canterbury',\n",
       " 'capillaries',\n",
       " 'capillary',\n",
       " 'caramel',\n",
       " 'caramelize',\n",
       " 'caramelized',\n",
       " 'carat',\n",
       " 'carats',\n",
       " 'caravan',\n",
       " 'caravan',\n",
       " 'caravans',\n",
       " 'caravans',\n",
       " 'caravel',\n",
       " 'caraway',\n",
       " 'caraway',\n",
       " 'cardiopulmonary',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'carefree',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'caregiver',\n",
       " 'caregivers',\n",
       " 'careless',\n",
       " 'carelessly',\n",
       " 'carelessness',\n",
       " 'carer',\n",
       " 'cares',\n",
       " 'caretaker',\n",
       " 'caretakers',\n",
       " 'Carey',\n",
       " 'Caribbean',\n",
       " 'Caribe',\n",
       " 'caribou',\n",
       " 'caricature',\n",
       " 'caricaturist',\n",
       " 'Carillon',\n",
       " 'caring',\n",
       " 'Carioca',\n",
       " 'carob',\n",
       " 'Carol',\n",
       " 'Carol',\n",
       " 'Carole',\n",
       " 'Carole',\n",
       " 'Carolina',\n",
       " 'Carolinas',\n",
       " 'Caroline',\n",
       " 'carols',\n",
       " 'Carolyn',\n",
       " 'carotene',\n",
       " 'carousel',\n",
       " 'Carrel',\n",
       " 'carriage',\n",
       " 'carriage',\n",
       " 'carriages',\n",
       " 'carriages',\n",
       " 'carried',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrier',\n",
       " 'carriers',\n",
       " 'carriers',\n",
       " 'carries',\n",
       " 'carries',\n",
       " 'carrion',\n",
       " 'Carroll',\n",
       " 'Carroll',\n",
       " 'carrot',\n",
       " 'carrot',\n",
       " 'carrots',\n",
       " 'carrots',\n",
       " 'Carrousel',\n",
       " 'Carrow',\n",
       " 'carry',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'carrying',\n",
       " 'Caryl',\n",
       " 'cautionary',\n",
       " 'cavalier',\n",
       " 'cemeteries',\n",
       " 'cemetery',\n",
       " 'cemetery',\n",
       " 'centenary',\n",
       " 'Cera',\n",
       " 'cerebral',\n",
       " 'ceremonial',\n",
       " 'ceremonies',\n",
       " 'ceremony',\n",
       " 'Cervantes',\n",
       " 'chair',\n",
       " 'chaired',\n",
       " 'chairing',\n",
       " 'Chairman',\n",
       " 'chairmanship',\n",
       " 'chairmen',\n",
       " 'chairperson',\n",
       " 'chairs',\n",
       " 'chairwoman',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characteristically',\n",
       " 'characteristics',\n",
       " 'characterization',\n",
       " 'characterizations',\n",
       " 'characterize',\n",
       " 'characterized',\n",
       " 'characterizes',\n",
       " 'characterizing',\n",
       " 'characters',\n",
       " 'characters',\n",
       " 'chariot',\n",
       " 'chariots',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'charitable',\n",
       " 'charitably',\n",
       " 'charities',\n",
       " 'charities',\n",
       " 'charity',\n",
       " 'Charon',\n",
       " 'chemotherapy',\n",
       " 'Cher',\n",
       " 'Cherie',\n",
       " 'cherish',\n",
       " 'cherished',\n",
       " 'cherishes',\n",
       " 'cherishing',\n",
       " 'Cherokee',\n",
       " 'Cherokees',\n",
       " 'cherries',\n",
       " 'cherry',\n",
       " 'cherub',\n",
       " 'cherubs',\n",
       " 'Cheung',\n",
       " 'childbearing',\n",
       " 'childcare',\n",
       " 'Chimera',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'clairvoyance',\n",
       " 'clairvoyant',\n",
       " 'Clare',\n",
       " 'Clarence',\n",
       " 'Clarendon',\n",
       " 'clarification',\n",
       " 'clarifications',\n",
       " 'clarified',\n",
       " 'clarifies',\n",
       " 'clarify',\n",
       " 'clarifying',\n",
       " 'clarinet',\n",
       " 'clarinetist',\n",
       " 'Clarion',\n",
       " 'clarity',\n",
       " 'clarity',\n",
       " 'Clary',\n",
       " 'Cleric',\n",
       " 'clerical',\n",
       " 'clerical',\n",
       " 'clerics',\n",
       " 'coherently',\n",
       " 'cometary',\n",
       " 'commentaries',\n",
       " 'commentary',\n",
       " 'commissaries',\n",
       " 'commissary',\n",
       " 'companero',\n",
       " 'comparable',\n",
       " 'comparative',\n",
       " 'comparatively',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'compares',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'comparisons',\n",
       " 'Concerto',\n",
       " 'Concertos',\n",
       " 'concessionary',\n",
       " 'concierge',\n",
       " 'confectionary',\n",
       " 'confectionery',\n",
       " 'confrere',\n",
       " 'confreres',\n",
       " 'consortium',\n",
       " 'Constabulary',\n",
       " 'contemporaries',\n",
       " 'contemporary',\n",
       " 'contrariness',\n",
       " 'contrary',\n",
       " 'contrary',\n",
       " 'cookware',\n",
       " 'copyright',\n",
       " 'copyrighted',\n",
       " 'copyrights',\n",
       " 'copywriter',\n",
       " 'copywriters',\n",
       " 'corollary',\n",
       " 'coronary',\n",
       " 'Corsair',\n",
       " 'corticosteroids',\n",
       " 'counterrevolutionary',\n",
       " 'counterrevolutionary',\n",
       " 'counterterrorism',\n",
       " 'counterterrorism',\n",
       " 'Counterterrorist',\n",
       " 'Counterterrorist',\n",
       " 'cranberries',\n",
       " 'cranberry',\n",
       " 'culinary',\n",
       " 'curry',\n",
       " 'customarily',\n",
       " 'customary',\n",
       " 'dairies',\n",
       " 'dairy',\n",
       " 'dare',\n",
       " 'dared',\n",
       " 'Daredevil',\n",
       " 'daredevils',\n",
       " 'dares',\n",
       " 'daresay',\n",
       " 'daring',\n",
       " 'daycare',\n",
       " 'deathbed',\n",
       " 'debonair',\n",
       " 'declarant',\n",
       " 'declaratory',\n",
       " 'declare',\n",
       " 'declared',\n",
       " 'declares',\n",
       " 'declaring',\n",
       " 'Delaware',\n",
       " 'Delftware',\n",
       " 'demerit',\n",
       " 'demerits',\n",
       " 'denarii',\n",
       " 'Deoxyribonucleic',\n",
       " 'deregulate',\n",
       " 'deregulating',\n",
       " 'deregulation',\n",
       " 'derelict',\n",
       " 'dereliction',\n",
       " 'derelicts',\n",
       " 'derelicts',\n",
       " 'derivation',\n",
       " 'Derrick',\n",
       " 'derringer',\n",
       " 'Derry',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairs',\n",
       " 'deuterium',\n",
       " 'dewberry',\n",
       " 'dexterity',\n",
       " 'dexterity',\n",
       " 'dictionaries',\n",
       " 'dictionary',\n",
       " 'dietary',\n",
       " 'dignitaries',\n",
       " 'dignitary',\n",
       " 'dinnerware',\n",
       " 'directing',\n",
       " 'direction',\n",
       " 'directional',\n",
       " 'directionless',\n",
       " 'directions',\n",
       " 'directive',\n",
       " 'directives',\n",
       " 'directly',\n",
       " 'directness',\n",
       " 'director',\n",
       " 'Directorate',\n",
       " 'directorial',\n",
       " 'directories',\n",
       " 'directors',\n",
       " 'directorship',\n",
       " 'directory',\n",
       " 'directs',\n",
       " 'disappear',\n",
       " 'disappearance',\n",
       " 'disappearances',\n",
       " 'disappeared',\n",
       " 'disappearing',\n",
       " 'disappears',\n",
       " 'disciplinarian',\n",
       " 'disciplinary',\n",
       " 'discretionary',\n",
       " 'disparage',\n",
       " 'disparaged',\n",
       " 'disparages',\n",
       " 'disparaging',\n",
       " 'disparagingly',\n",
       " 'disparate',\n",
       " 'disparities',\n",
       " 'disparity',\n",
       " 'dispensary',\n",
       " 'disrepair',\n",
       " 'dissimilarity',\n",
       " 'diuretic',\n",
       " 'diuretics',\n",
       " 'diversionary',\n",
       " 'diversionary',\n",
       " 'doctrinaire',\n",
       " 'doer',\n",
       " 'doorknob',\n",
       " 'doorknobs',\n",
       " 'downstairs',\n",
       " 'dram',\n",
       " 'drams',\n",
       " 'dromedary',\n",
       " 'Drury',\n",
       " 'dysentery',\n",
       " 'earless',\n",
       " 'earmark',\n",
       " 'earmarked',\n",
       " 'earmarks',\n",
       " 'earmuff',\n",
       " 'earmuffs',\n",
       " 'earphone',\n",
       " 'earphones',\n",
       " 'earpiece',\n",
       " 'earpieces',\n",
       " 'earplug',\n",
       " 'earplugs',\n",
       " 'earring',\n",
       " 'earrings',\n",
       " 'ears',\n",
       " 'earshot',\n",
       " 'earthbound',\n",
       " 'earwax',\n",
       " 'egalitarian',\n",
       " 'elsewhere',\n",
       " 'Embarcadero',\n",
       " 'embarrass',\n",
       " 'embarrassed',\n",
       " 'embarrasses',\n",
       " 'embarrassing',\n",
       " 'embarrassingly',\n",
       " 'embarrassment',\n",
       " 'embarrassments',\n",
       " 'emeritus',\n",
       " 'emissaries',\n",
       " 'emissary',\n",
       " 'endear',\n",
       " 'endeared',\n",
       " 'endearing',\n",
       " 'engineered',\n",
       " 'ensnare',\n",
       " 'ensnared',\n",
       " 'ensnares',\n",
       " 'era',\n",
       " 'erasable',\n",
       " 'erase',\n",
       " 'erased',\n",
       " 'eraser',\n",
       " 'erasers',\n",
       " 'erases',\n",
       " 'erasing',\n",
       " 'Eric',\n",
       " 'Erica',\n",
       " 'Erika',\n",
       " 'Erin',\n",
       " 'erode',\n",
       " 'eroding',\n",
       " 'errand',\n",
       " 'errands',\n",
       " 'errant',\n",
       " 'erratically',\n",
       " 'erred',\n",
       " 'erring',\n",
       " 'erroneous',\n",
       " 'erroneously',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'errs',\n",
       " 'ersatz',\n",
       " 'ersatz',\n",
       " 'erudite',\n",
       " 'erudition',\n",
       " 'erupt',\n",
       " 'erupted',\n",
       " 'erupted',\n",
       " 'erupting',\n",
       " 'eruption',\n",
       " 'eruptions',\n",
       " 'eruptive',\n",
       " 'erupts',\n",
       " 'erupts',\n",
       " 'Escudero',\n",
       " 'esoteric',\n",
       " 'esoteric',\n",
       " 'estuary',\n",
       " 'Euro',\n",
       " 'eurodollar',\n",
       " 'everywhere',\n",
       " 'everywhere',\n",
       " 'evidentiary',\n",
       " 'evidentiary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'exclusionary',\n",
       " 'expeditionary',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimental',\n",
       " 'experimentally',\n",
       " 'experimentally',\n",
       " 'experimentation',\n",
       " 'experimented',\n",
       " 'experimenter',\n",
       " 'experimenting',\n",
       " 'experimenting',\n",
       " 'experiments',\n",
       " 'extramarital',\n",
       " 'extraordinaire',\n",
       " 'extraordinarily',\n",
       " 'extraordinary',\n",
       " 'extraordinary',\n",
       " 'eyewear',\n",
       " 'Eyrie',\n",
       " 'Fahrenheit',\n",
       " 'fair',\n",
       " 'Fairbanks',\n",
       " 'faire',\n",
       " 'fairer',\n",
       " 'fairest',\n",
       " 'fairground',\n",
       " 'fairgrounds',\n",
       " 'fairies',\n",
       " 'fairly',\n",
       " 'fairness',\n",
       " 'fairs',\n",
       " 'fairway',\n",
       " 'Fairways',\n",
       " 'fairy',\n",
       " 'fairyland',\n",
       " 'familiarity',\n",
       " 'fanfare',\n",
       " 'Faraday',\n",
       " 'fare',\n",
       " 'fared',\n",
       " 'fares',\n",
       " 'farewell',\n",
       " 'farewells',\n",
       " 'faring',\n",
       " 'Faro',\n",
       " 'Farrel',\n",
       " 'farrier',\n",
       " 'farris',\n",
       " 'Farrow',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'fer',\n",
       " 'feral',\n",
       " 'feria',\n",
       " 'fermium',\n",
       " 'Ferreiro',\n",
       " 'Ferrel',\n",
       " 'Ferrer',\n",
       " 'ferret',\n",
       " 'ferreting',\n",
       " 'ferrets',\n",
       " 'ferried',\n",
       " 'Ferrier',\n",
       " 'ferries',\n",
       " 'Ferris',\n",
       " 'ferrite',\n",
       " 'ferrous',\n",
       " 'ferry',\n",
       " 'ferryboat',\n",
       " 'ferryboats',\n",
       " 'ferryman',\n",
       " 'fiduciary',\n",
       " 'flair',\n",
       " 'flare',\n",
       " 'flared',\n",
       " 'flares',\n",
       " 'flaring',\n",
       " 'Flashgun',\n",
       " 'flatware',\n",
       " 'footwear',\n",
       " 'forbearance',\n",
       " 'forebear',\n",
       " 'forebears',\n",
       " 'formulary',\n",
       " 'forswear',\n",
       " 'foursquare',\n",
       " 'fragmentary',\n",
       " 'frere',\n",
       " 'Freres',\n",
       " 'Freres',\n",
       " 'functionaries',\n",
       " 'functionary',\n",
       " 'funerary',\n",
       " 'Gara',\n",
       " 'Gare',\n",
       " 'Gareth',\n",
       " 'Garibaldi',\n",
       " 'garish',\n",
       " 'Garret',\n",
       " 'garrets',\n",
       " 'Garrick',\n",
       " 'Garrison',\n",
       " 'garrisoned',\n",
       " 'garrulous',\n",
       " 'Gary',\n",
       " 'Gary',\n",
       " 'Generalissimo',\n",
       " 'generic',\n",
       " 'generically',\n",
       " 'generics',\n",
       " 'Gerald',\n",
       " 'geriatric',\n",
       " 'geriatrics',\n",
       " 'Gerontology',\n",
       " 'gerrymandering',\n",
       " 'glare',\n",
       " 'glared',\n",
       " 'glares',\n",
       " 'glaring',\n",
       " 'glassware',\n",
       " 'Glengarry',\n",
       " 'Goodyear',\n",
       " 'gooseberries',\n",
       " 'gooseberry',\n",
       " 'grandeur',\n",
       " 'grandparent',\n",
       " 'grandparent',\n",
       " 'grandparent',\n",
       " 'grandparents',\n",
       " 'grandparents',\n",
       " 'grandparents',\n",
       " 'graybeards',\n",
       " 'gregarious',\n",
       " 'guarantee',\n",
       " 'guaranteed',\n",
       " 'guaranteeing',\n",
       " 'guarantees',\n",
       " 'guarantor',\n",
       " 'guarantors',\n",
       " 'Guaranty',\n",
       " 'Guarneri',\n",
       " 'guerre',\n",
       " 'guru',\n",
       " 'gurus',\n",
       " 'Gutierrez',\n",
       " 'hackberry',\n",
       " 'hair',\n",
       " 'haircut',\n",
       " 'haircuts',\n",
       " 'hairdo',\n",
       " 'hairdos',\n",
       " 'hairdresser',\n",
       " 'hairdressers',\n",
       " 'Hairdressing',\n",
       " 'haired',\n",
       " 'hairless',\n",
       " 'hairline',\n",
       " 'hairs',\n",
       " 'hairspray',\n",
       " 'hairstyle',\n",
       " 'hairy',\n",
       " 'Hanbury',\n",
       " 'hardware',\n",
       " 'Hare',\n",
       " 'harebrained',\n",
       " 'harem',\n",
       " 'hares',\n",
       " 'Haring',\n",
       " 'Harold',\n",
       " 'Harr',\n",
       " 'harried',\n",
       " 'Harriet',\n",
       " 'Harris',\n",
       " 'Harrison',\n",
       " 'Harrow',\n",
       " 'harrowing',\n",
       " 'Harry',\n",
       " 'healthcare',\n",
       " 'hear',\n",
       " 'hearer',\n",
       " 'hearers',\n",
       " 'hearing',\n",
       " 'hearings',\n",
       " 'hears',\n",
       " 'hearsay',\n",
       " 'heiress',\n",
       " 'heiresses',\n",
       " 'heirloom',\n",
       " 'heirlooms',\n",
       " 'heirs',\n",
       " 'Herald',\n",
       " 'heralded',\n",
       " 'heralding',\n",
       " 'heraldry',\n",
       " 'heralds',\n",
       " 'herbarium',\n",
       " 'herbarium',\n",
       " 'Herder',\n",
       " 'here',\n",
       " 'hereditary',\n",
       " 'Hereford',\n",
       " 'heresy',\n",
       " 'heretic',\n",
       " 'heritage',\n",
       " 'heritage',\n",
       " 'heritages',\n",
       " 'hero',\n",
       " 'heroes',\n",
       " 'heroin',\n",
       " 'heroine',\n",
       " 'heroines',\n",
       " 'heroism',\n",
       " 'heron',\n",
       " 'herons',\n",
       " 'Herr',\n",
       " 'herring',\n",
       " 'herrings',\n",
       " 'Hertz',\n",
       " 'hilarious',\n",
       " 'Hilariously',\n",
       " 'hilarity',\n",
       " 'Homeric',\n",
       " 'honoraria',\n",
       " 'honorarium',\n",
       " 'honorary',\n",
       " 'housewares',\n",
       " 'Huckleberry',\n",
       " 'Huerta',\n",
       " 'Huerta',\n",
       " 'humanitarian',\n",
       " 'humanitarian',\n",
       " 'humanitarians',\n",
       " 'humanitarians',\n",
       " 'Hungarian',\n",
       " 'Hungarians',\n",
       " 'Huron',\n",
       " 'hurricane',\n",
       " 'hydrotherapy',\n",
       " 'hyperbaric',\n",
       " 'hysteria',\n",
       " 'hysteric',\n",
       " 'hysterical',\n",
       " 'hysterically',\n",
       " 'hysterics',\n",
       " 'illusionary',\n",
       " 'imaginary',\n",
       " 'immunotherapy',\n",
       " 'impair',\n",
       " 'impaired',\n",
       " 'impairing',\n",
       " 'impairment',\n",
       " 'impairs',\n",
       " 'imperative',\n",
       " 'imperatives',\n",
       " 'imperialistic',\n",
       " 'imperil',\n",
       " 'imperils',\n",
       " 'incendiary',\n",
       " 'inflationary',\n",
       " 'inherent',\n",
       " 'inherently',\n",
       " 'inherit',\n",
       " 'inheritable',\n",
       " 'inheritance',\n",
       " 'inherited',\n",
       " 'inheriting',\n",
       " 'inheritor',\n",
       " 'inherits',\n",
       " 'initiative',\n",
       " 'initiatives',\n",
       " 'interdisciplinary',\n",
       " 'interferon',\n",
       " 'intermarry',\n",
       " 'intermediaries',\n",
       " 'intermediaries',\n",
       " 'intermediary',\n",
       " 'intermediary',\n",
       " 'interplanetary',\n",
       " 'interrogate',\n",
       " 'interrogated',\n",
       " 'interrogating',\n",
       " 'interrogation',\n",
       " 'interrogations',\n",
       " 'interrogator',\n",
       " 'interrogatories',\n",
       " 'interrogators',\n",
       " 'interrogatory',\n",
       " 'invariable']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadTriphs = rowsWBadNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "len(hasBadTriphs)\n",
    "len(lexicon)\n",
    "len(hasBadTriphs)/len(lexicon) #proportion of the lexicon (by type) that has bad triphs\n",
    "sum([float(entry['Prob']) for entry in hasBadTriphs]) #prob mass of bad triph words by token\n",
    "list(map(lambda r: r['Word'], hasBadTriphs[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:20.392652Z",
     "start_time": "2019-03-12T02:47:19.752495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1887"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03492504164353137"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.01714713558335328"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Aaron',\n",
       " 'aberrant',\n",
       " 'acerbic',\n",
       " 'actuarial',\n",
       " 'actuaries',\n",
       " 'actuary',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'Aer',\n",
       " 'aerial',\n",
       " 'aerials',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aerobatics',\n",
       " 'aerobic',\n",
       " 'aerobically',\n",
       " 'aerodromes',\n",
       " 'aerodynamic',\n",
       " 'aerodynamically',\n",
       " 'aerodynamics',\n",
       " 'Aerodyne',\n",
       " 'aeronautical',\n",
       " 'aeronautical',\n",
       " 'Aeronautics',\n",
       " 'aerosol',\n",
       " 'Aerosols',\n",
       " 'aerospace',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'aficionado',\n",
       " 'ageratum',\n",
       " 'agrarian',\n",
       " 'air',\n",
       " 'airbag',\n",
       " 'airbags',\n",
       " 'airbase',\n",
       " 'airboat',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aircrafts',\n",
       " 'aircrafts',\n",
       " 'Aircrew',\n",
       " 'airdrop',\n",
       " 'aired',\n",
       " 'Airedale',\n",
       " 'Airedales',\n",
       " 'airfare',\n",
       " 'airfares',\n",
       " 'airfield',\n",
       " 'airfields',\n",
       " 'airflow',\n",
       " 'airfoil',\n",
       " 'airhead',\n",
       " 'airing',\n",
       " 'airless',\n",
       " 'airlift',\n",
       " 'airlifted',\n",
       " 'airlifting',\n",
       " 'Airlifts',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airliners',\n",
       " 'Airlines',\n",
       " 'airlock',\n",
       " 'airlocks',\n",
       " 'airmail',\n",
       " 'Airman',\n",
       " 'airmen',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airship',\n",
       " 'airships',\n",
       " 'airspace',\n",
       " 'airspeed',\n",
       " 'airstrike',\n",
       " 'airstrikes',\n",
       " 'airstrip',\n",
       " 'airstrips',\n",
       " 'airtight',\n",
       " 'airtime',\n",
       " 'airwave',\n",
       " 'airwaves',\n",
       " 'airway',\n",
       " 'Airways',\n",
       " 'airworthy',\n",
       " 'airy',\n",
       " 'Algerians',\n",
       " 'alphanumeric',\n",
       " 'Altair',\n",
       " 'America',\n",
       " 'America',\n",
       " 'American',\n",
       " 'American',\n",
       " 'Americana',\n",
       " 'Americanism',\n",
       " 'Americanization',\n",
       " 'Americanize',\n",
       " 'Americanized',\n",
       " 'Americans',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Americas',\n",
       " 'ancillary',\n",
       " 'Angeline',\n",
       " 'Antares',\n",
       " 'antiaircraft',\n",
       " 'antidisestablishmentarianism',\n",
       " 'anywhere',\n",
       " 'anywhere',\n",
       " 'apothecary',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'Apparently',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'aquarium',\n",
       " 'aquariums',\n",
       " 'Aquarius',\n",
       " 'Ara',\n",
       " 'Arab',\n",
       " 'Arab',\n",
       " 'Arabic',\n",
       " 'Arabic',\n",
       " 'Arable',\n",
       " 'Arabs',\n",
       " 'Arabs',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'Ariadne',\n",
       " 'arid',\n",
       " 'arid',\n",
       " 'Ariel',\n",
       " 'Aries',\n",
       " 'aristocracy',\n",
       " 'Aristotle',\n",
       " 'arithmetic',\n",
       " 'Arizona',\n",
       " 'armchair',\n",
       " 'armchairs',\n",
       " 'aromatic',\n",
       " 'arrant',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'arrow',\n",
       " 'arrow',\n",
       " 'Arrowhead',\n",
       " 'Arrowhead',\n",
       " 'arrowheads',\n",
       " 'arrowheads',\n",
       " 'arrowroot',\n",
       " 'arrows',\n",
       " 'arrows',\n",
       " 'Arthurian',\n",
       " 'ary',\n",
       " 'asparagus',\n",
       " 'atmospheric',\n",
       " 'atmospherics',\n",
       " 'austerity',\n",
       " 'authoritarian',\n",
       " 'authoritarianism',\n",
       " 'auxiliary',\n",
       " 'aviary',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'backstairs',\n",
       " 'Banbury',\n",
       " 'barbarian',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbaric',\n",
       " 'barbarity',\n",
       " 'barbarity',\n",
       " 'Barbera',\n",
       " 'Barbero',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bares',\n",
       " 'barest',\n",
       " 'baring',\n",
       " 'Barish',\n",
       " 'barite',\n",
       " 'baritone',\n",
       " 'baritones',\n",
       " 'barium',\n",
       " 'barometric',\n",
       " 'Baron',\n",
       " 'Baron',\n",
       " 'Baroness',\n",
       " 'baronet',\n",
       " 'baronet',\n",
       " 'baronets',\n",
       " 'baronets',\n",
       " 'barons',\n",
       " 'barons',\n",
       " 'barrack',\n",
       " 'barrack',\n",
       " 'barracks',\n",
       " 'barracks',\n",
       " 'barracuda',\n",
       " 'Barragan',\n",
       " 'Barre',\n",
       " 'barrel',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barreled',\n",
       " 'barreling',\n",
       " 'barreling',\n",
       " 'barrels',\n",
       " 'barrels',\n",
       " 'barren',\n",
       " 'barren',\n",
       " 'Barrens',\n",
       " 'Barret',\n",
       " 'barricade',\n",
       " 'barricade',\n",
       " 'barricaded',\n",
       " 'barricaded',\n",
       " 'barricades',\n",
       " 'barricades',\n",
       " 'barrier',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barriers',\n",
       " 'Barringer',\n",
       " 'Barrios',\n",
       " 'barrister',\n",
       " 'barrister',\n",
       " 'barristers',\n",
       " 'barristers',\n",
       " 'Barrow',\n",
       " 'Barrow',\n",
       " 'Barrows',\n",
       " 'Barrows',\n",
       " 'Barry',\n",
       " 'Barry',\n",
       " 'Bavarian',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'bearer',\n",
       " 'bearers',\n",
       " 'bearing',\n",
       " 'bearings',\n",
       " 'bearish',\n",
       " 'bears',\n",
       " 'beneficiaries',\n",
       " 'beneficiary',\n",
       " 'beret',\n",
       " 'beret',\n",
       " 'Berets',\n",
       " 'Berets',\n",
       " 'Bering',\n",
       " 'berries',\n",
       " 'Berrigan',\n",
       " 'Berry',\n",
       " 'Berryman',\n",
       " 'Beryl',\n",
       " 'Beware',\n",
       " 'beyond',\n",
       " 'beyond',\n",
       " 'bier',\n",
       " 'billionaire',\n",
       " 'billionaires',\n",
       " 'blackberries',\n",
       " 'blackberry',\n",
       " 'Blair',\n",
       " 'blare',\n",
       " 'blared',\n",
       " 'blares',\n",
       " 'blaring',\n",
       " 'Bluebeard',\n",
       " 'blueberries',\n",
       " 'blueberry',\n",
       " 'Bolero',\n",
       " 'bracero',\n",
       " 'Breweries',\n",
       " 'buccaneer',\n",
       " 'Buccaneers',\n",
       " 'budgetary',\n",
       " 'Bulgaria',\n",
       " 'Bulgarian',\n",
       " 'Bulgarians',\n",
       " 'burial',\n",
       " 'burials',\n",
       " 'buried',\n",
       " 'buries',\n",
       " 'Burroughs',\n",
       " 'bury',\n",
       " 'burying',\n",
       " 'caballero',\n",
       " 'cairn',\n",
       " 'cairns',\n",
       " 'caldera',\n",
       " 'caldera',\n",
       " 'canaries',\n",
       " 'canary',\n",
       " 'Canberra',\n",
       " 'Canterbury',\n",
       " 'capillaries',\n",
       " 'capillary',\n",
       " 'caramel',\n",
       " 'caramelize',\n",
       " 'caramelized',\n",
       " 'carat',\n",
       " 'carats',\n",
       " 'caravan',\n",
       " 'caravan',\n",
       " 'caravans',\n",
       " 'caravans',\n",
       " 'caravel',\n",
       " 'caraway',\n",
       " 'caraway',\n",
       " 'cardiopulmonary',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'carefree',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'caregiver',\n",
       " 'caregivers',\n",
       " 'careless',\n",
       " 'carelessly',\n",
       " 'carelessness',\n",
       " 'carer',\n",
       " 'cares',\n",
       " 'caretaker',\n",
       " 'caretakers',\n",
       " 'Carey',\n",
       " 'Caribbean',\n",
       " 'Caribe',\n",
       " 'caribou',\n",
       " 'caricature',\n",
       " 'caricaturist',\n",
       " 'Carillon',\n",
       " 'caring',\n",
       " 'Carioca',\n",
       " 'carob',\n",
       " 'Carol',\n",
       " 'Carol',\n",
       " 'Carole',\n",
       " 'Carole',\n",
       " 'Carolina',\n",
       " 'Carolinas',\n",
       " 'Caroline',\n",
       " 'carols',\n",
       " 'Carolyn',\n",
       " 'carotene',\n",
       " 'carousel',\n",
       " 'Carrel',\n",
       " 'carriage',\n",
       " 'carriage',\n",
       " 'carriages',\n",
       " 'carriages',\n",
       " 'carried',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrier',\n",
       " 'carriers',\n",
       " 'carriers',\n",
       " 'carries',\n",
       " 'carries',\n",
       " 'carrion',\n",
       " 'Carroll',\n",
       " 'Carroll',\n",
       " 'carrot',\n",
       " 'carrot',\n",
       " 'carrots',\n",
       " 'carrots',\n",
       " 'Carrousel',\n",
       " 'Carrow',\n",
       " 'carry',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'carrying',\n",
       " 'Caryl',\n",
       " 'cautionary',\n",
       " 'cavalier',\n",
       " 'cemeteries',\n",
       " 'cemetery',\n",
       " 'cemetery',\n",
       " 'centenary',\n",
       " 'Cera',\n",
       " 'cerebral',\n",
       " 'ceremonial',\n",
       " 'ceremonies',\n",
       " 'ceremony',\n",
       " 'Cervantes',\n",
       " 'chair',\n",
       " 'chaired',\n",
       " 'chairing',\n",
       " 'Chairman',\n",
       " 'chairmanship',\n",
       " 'chairmen',\n",
       " 'chairperson',\n",
       " 'chairs',\n",
       " 'chairwoman',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characteristically',\n",
       " 'characteristics',\n",
       " 'characterization',\n",
       " 'characterizations',\n",
       " 'characterize',\n",
       " 'characterized',\n",
       " 'characterizes',\n",
       " 'characterizing',\n",
       " 'characters',\n",
       " 'characters',\n",
       " 'chariot',\n",
       " 'chariots',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'charitable',\n",
       " 'charitably',\n",
       " 'charities',\n",
       " 'charities',\n",
       " 'charity',\n",
       " 'Charon',\n",
       " 'chemotherapy',\n",
       " 'Cher',\n",
       " 'Cherie',\n",
       " 'cherish',\n",
       " 'cherished',\n",
       " 'cherishes',\n",
       " 'cherishing',\n",
       " 'Cherokee',\n",
       " 'Cherokees',\n",
       " 'cherries',\n",
       " 'cherry',\n",
       " 'cherub',\n",
       " 'cherubs',\n",
       " 'Cheung',\n",
       " 'childbearing',\n",
       " 'childcare',\n",
       " 'Chimera',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'clairvoyance',\n",
       " 'clairvoyant',\n",
       " 'Clare',\n",
       " 'Clarence',\n",
       " 'Clarendon',\n",
       " 'clarification',\n",
       " 'clarifications',\n",
       " 'clarified',\n",
       " 'clarifies',\n",
       " 'clarify',\n",
       " 'clarifying',\n",
       " 'clarinet',\n",
       " 'clarinetist',\n",
       " 'Clarion',\n",
       " 'clarity',\n",
       " 'clarity',\n",
       " 'Clary',\n",
       " 'Cleric',\n",
       " 'clerical',\n",
       " 'clerical',\n",
       " 'clerics',\n",
       " 'coherently',\n",
       " 'cometary',\n",
       " 'commentaries',\n",
       " 'commentary',\n",
       " 'commissaries',\n",
       " 'commissary',\n",
       " 'companero',\n",
       " 'comparable',\n",
       " 'comparative',\n",
       " 'comparatively',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'compares',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'comparisons',\n",
       " 'Concerto',\n",
       " 'Concertos',\n",
       " 'concessionary',\n",
       " 'concierge',\n",
       " 'confectionary',\n",
       " 'confectionery',\n",
       " 'confrere',\n",
       " 'confreres',\n",
       " 'consortium',\n",
       " 'Constabulary',\n",
       " 'contemporaries',\n",
       " 'contemporary',\n",
       " 'contrariness',\n",
       " 'contrary',\n",
       " 'contrary',\n",
       " 'cookware',\n",
       " 'copyright',\n",
       " 'copyrighted',\n",
       " 'copyrights',\n",
       " 'copywriter',\n",
       " 'copywriters',\n",
       " 'corollary',\n",
       " 'coronary',\n",
       " 'Corsair',\n",
       " 'corticosteroids',\n",
       " 'counterrevolutionary',\n",
       " 'counterrevolutionary',\n",
       " 'counterterrorism',\n",
       " 'counterterrorism',\n",
       " 'Counterterrorist',\n",
       " 'Counterterrorist',\n",
       " 'cranberries',\n",
       " 'cranberry',\n",
       " 'culinary',\n",
       " 'curry',\n",
       " 'customarily',\n",
       " 'customary',\n",
       " 'dairies',\n",
       " 'dairy',\n",
       " 'dare',\n",
       " 'dared',\n",
       " 'Daredevil',\n",
       " 'daredevils',\n",
       " 'dares',\n",
       " 'daresay',\n",
       " 'daring',\n",
       " 'daycare',\n",
       " 'deathbed',\n",
       " 'debonair',\n",
       " 'declarant',\n",
       " 'declaratory',\n",
       " 'declare',\n",
       " 'declared',\n",
       " 'declares',\n",
       " 'declaring',\n",
       " 'Delaware',\n",
       " 'Delftware',\n",
       " 'demerit',\n",
       " 'demerits',\n",
       " 'denarii',\n",
       " 'Deoxyribonucleic',\n",
       " 'deregulate',\n",
       " 'deregulating',\n",
       " 'deregulation',\n",
       " 'derelict',\n",
       " 'dereliction',\n",
       " 'derelicts',\n",
       " 'derelicts',\n",
       " 'derivation',\n",
       " 'Derrick',\n",
       " 'derringer',\n",
       " 'Derry',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairs',\n",
       " 'deuterium',\n",
       " 'dewberry',\n",
       " 'dexterity',\n",
       " 'dexterity',\n",
       " 'dictionaries',\n",
       " 'dictionary',\n",
       " 'dietary',\n",
       " 'dignitaries',\n",
       " 'dignitary',\n",
       " 'dinnerware',\n",
       " 'directing',\n",
       " 'direction',\n",
       " 'directional',\n",
       " 'directionless',\n",
       " 'directions',\n",
       " 'directive',\n",
       " 'directives',\n",
       " 'directly',\n",
       " 'directness',\n",
       " 'director',\n",
       " 'Directorate',\n",
       " 'directorial',\n",
       " 'directories',\n",
       " 'directors',\n",
       " 'directorship',\n",
       " 'directory',\n",
       " 'directs',\n",
       " 'disappear',\n",
       " 'disappearance',\n",
       " 'disappearances',\n",
       " 'disappeared',\n",
       " 'disappearing',\n",
       " 'disappears',\n",
       " 'disciplinarian',\n",
       " 'disciplinary',\n",
       " 'discretionary',\n",
       " 'disparage',\n",
       " 'disparaged',\n",
       " 'disparages',\n",
       " 'disparaging',\n",
       " 'disparagingly',\n",
       " 'disparate',\n",
       " 'disparities',\n",
       " 'disparity',\n",
       " 'dispensary',\n",
       " 'disrepair',\n",
       " 'dissimilarity',\n",
       " 'diuretic',\n",
       " 'diuretics',\n",
       " 'diversionary',\n",
       " 'diversionary',\n",
       " 'doctrinaire',\n",
       " 'doer',\n",
       " 'doorknob',\n",
       " 'doorknobs',\n",
       " 'downstairs',\n",
       " 'dram',\n",
       " 'drams',\n",
       " 'dromedary',\n",
       " 'Drury',\n",
       " 'dysentery',\n",
       " 'ear',\n",
       " 'earless',\n",
       " 'earmark',\n",
       " 'earmarked',\n",
       " 'earmarks',\n",
       " 'earmuff',\n",
       " 'earmuffs',\n",
       " 'earphone',\n",
       " 'earphones',\n",
       " 'earpiece',\n",
       " 'earpieces',\n",
       " 'earplug',\n",
       " 'earplugs',\n",
       " 'earring',\n",
       " 'earrings',\n",
       " 'ears',\n",
       " 'earshot',\n",
       " 'earthbound',\n",
       " 'earwax',\n",
       " 'egalitarian',\n",
       " 'elsewhere',\n",
       " 'Embarcadero',\n",
       " 'embarrass',\n",
       " 'embarrassed',\n",
       " 'embarrasses',\n",
       " 'embarrassing',\n",
       " 'embarrassingly',\n",
       " 'embarrassment',\n",
       " 'embarrassments',\n",
       " 'emeritus',\n",
       " 'emissaries',\n",
       " 'emissary',\n",
       " 'endear',\n",
       " 'endeared',\n",
       " 'endearing',\n",
       " 'engineered',\n",
       " 'ensnare',\n",
       " 'ensnared',\n",
       " 'ensnares',\n",
       " 'era',\n",
       " 'erasable',\n",
       " 'erase',\n",
       " 'erased',\n",
       " 'eraser',\n",
       " 'erasers',\n",
       " 'erases',\n",
       " 'erasing',\n",
       " 'ere',\n",
       " 'Eric',\n",
       " 'Erica',\n",
       " 'Erika',\n",
       " 'Erin',\n",
       " 'erode',\n",
       " 'eroding',\n",
       " 'err',\n",
       " 'errand',\n",
       " 'errands',\n",
       " 'errant',\n",
       " 'erratically',\n",
       " 'erred',\n",
       " 'erring',\n",
       " 'erroneous',\n",
       " 'erroneously',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'errs',\n",
       " 'ersatz',\n",
       " 'ersatz',\n",
       " 'erudite',\n",
       " 'erudition',\n",
       " 'erupt',\n",
       " 'erupted',\n",
       " 'erupted',\n",
       " 'erupting',\n",
       " 'eruption',\n",
       " 'eruptions',\n",
       " 'eruptive',\n",
       " 'erupts',\n",
       " 'erupts',\n",
       " 'Escudero',\n",
       " 'esoteric',\n",
       " 'esoteric',\n",
       " 'estuary',\n",
       " 'Euro',\n",
       " 'eurodollar',\n",
       " 'everywhere',\n",
       " 'everywhere',\n",
       " 'evidentiary',\n",
       " 'evidentiary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'evolutionary',\n",
       " 'exclusionary',\n",
       " 'expeditionary',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimental',\n",
       " 'experimentally',\n",
       " 'experimentally',\n",
       " 'experimentation',\n",
       " 'experimented',\n",
       " 'experimenter',\n",
       " 'experimenting',\n",
       " 'experimenting',\n",
       " 'experiments',\n",
       " 'extramarital',\n",
       " 'extraordinaire',\n",
       " 'extraordinarily',\n",
       " 'extraordinary',\n",
       " 'extraordinary',\n",
       " 'eyewear',\n",
       " 'Eyre',\n",
       " 'Eyrie',\n",
       " 'Fahrenheit',\n",
       " 'fair',\n",
       " 'Fairbanks',\n",
       " 'faire',\n",
       " 'fairer',\n",
       " 'fairest',\n",
       " 'fairground',\n",
       " 'fairgrounds',\n",
       " 'fairies',\n",
       " 'fairly',\n",
       " 'fairness',\n",
       " 'fairs',\n",
       " 'fairway',\n",
       " 'Fairways',\n",
       " 'fairy',\n",
       " 'fairyland',\n",
       " 'familiarity',\n",
       " 'fanfare',\n",
       " 'Faraday',\n",
       " 'fare',\n",
       " 'fared',\n",
       " 'fares',\n",
       " 'farewell',\n",
       " 'farewells',\n",
       " 'faring',\n",
       " 'Faro',\n",
       " 'Farrel',\n",
       " 'farrier',\n",
       " 'farris',\n",
       " 'Farrow',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'February',\n",
       " 'fer',\n",
       " 'feral',\n",
       " 'feria',\n",
       " 'fermium',\n",
       " 'Ferreiro',\n",
       " 'Ferrel',\n",
       " 'Ferrer',\n",
       " 'ferret',\n",
       " 'ferreting',\n",
       " 'ferrets',\n",
       " 'ferried',\n",
       " 'Ferrier',\n",
       " 'ferries',\n",
       " 'Ferris',\n",
       " 'ferrite',\n",
       " 'ferrous',\n",
       " 'ferry',\n",
       " 'ferryboat',\n",
       " 'ferryboats',\n",
       " 'ferryman',\n",
       " 'fiduciary',\n",
       " 'flair',\n",
       " 'flare',\n",
       " 'flared',\n",
       " 'flares',\n",
       " 'flaring',\n",
       " 'Flashgun',\n",
       " 'flatware',\n",
       " 'footwear',\n",
       " 'forbearance',\n",
       " 'forebear',\n",
       " 'forebears',\n",
       " 'formulary',\n",
       " 'forswear',\n",
       " 'foursquare',\n",
       " 'fragmentary',\n",
       " 'frere',\n",
       " 'Freres',\n",
       " 'Freres',\n",
       " 'functionaries',\n",
       " 'functionary',\n",
       " 'funerary',\n",
       " 'Gara',\n",
       " 'Gare',\n",
       " 'Gareth',\n",
       " 'Garibaldi',\n",
       " 'garish',\n",
       " 'Garret',\n",
       " 'garrets',\n",
       " 'Garrick',\n",
       " 'Garrison',\n",
       " 'garrisoned',\n",
       " 'garrulous',\n",
       " 'Gary',\n",
       " 'Gary',\n",
       " 'Generalissimo',\n",
       " 'generic',\n",
       " 'generically',\n",
       " 'generics',\n",
       " 'Gerald',\n",
       " 'geriatric',\n",
       " 'geriatrics',\n",
       " 'Gerontology',\n",
       " 'gerrymandering',\n",
       " 'glare',\n",
       " 'glared',\n",
       " 'glares',\n",
       " 'glaring',\n",
       " 'glassware',\n",
       " 'Glengarry',\n",
       " 'Goodyear',\n",
       " 'gooseberries',\n",
       " 'gooseberry',\n",
       " 'grandeur',\n",
       " 'grandparent',\n",
       " 'grandparent',\n",
       " 'grandparent',\n",
       " 'grandparents',\n",
       " 'grandparents',\n",
       " 'grandparents',\n",
       " 'graybeards',\n",
       " 'gregarious',\n",
       " 'guarantee',\n",
       " 'guaranteed',\n",
       " 'guaranteeing',\n",
       " 'guarantees',\n",
       " 'guarantor',\n",
       " 'guarantors',\n",
       " 'Guaranty',\n",
       " 'Guarneri',\n",
       " 'guerre',\n",
       " 'guru',\n",
       " 'gurus',\n",
       " 'Gutierrez',\n",
       " 'hackberry',\n",
       " 'hair',\n",
       " 'haircut',\n",
       " 'haircuts',\n",
       " 'hairdo',\n",
       " 'hairdos',\n",
       " 'hairdresser',\n",
       " 'hairdressers',\n",
       " 'Hairdressing',\n",
       " 'haired',\n",
       " 'hairless',\n",
       " 'hairline',\n",
       " 'hairs',\n",
       " 'hairspray',\n",
       " 'hairstyle',\n",
       " 'hairy',\n",
       " 'Hanbury',\n",
       " 'hardware',\n",
       " 'Hare',\n",
       " 'harebrained',\n",
       " 'harem',\n",
       " 'hares',\n",
       " 'Haring',\n",
       " 'Harold',\n",
       " 'Harr',\n",
       " 'harried',\n",
       " 'Harriet',\n",
       " 'Harris',\n",
       " 'Harrison',\n",
       " 'Harrow',\n",
       " 'harrowing',\n",
       " 'Harry',\n",
       " 'healthcare',\n",
       " 'hear',\n",
       " 'hearer',\n",
       " 'hearers',\n",
       " 'hearing',\n",
       " 'hearings',\n",
       " 'hears',\n",
       " 'hearsay',\n",
       " 'heir',\n",
       " 'heiress',\n",
       " 'heiresses',\n",
       " 'heirloom',\n",
       " 'heirlooms',\n",
       " 'heirs',\n",
       " 'Herald',\n",
       " 'heralded',\n",
       " 'heralding',\n",
       " 'heraldry',\n",
       " 'heralds',\n",
       " 'herbarium',\n",
       " 'herbarium',\n",
       " 'Herder',\n",
       " 'here',\n",
       " 'hereditary',\n",
       " 'Hereford',\n",
       " 'heresy',\n",
       " 'heretic',\n",
       " 'heritage',\n",
       " 'heritage',\n",
       " 'heritages',\n",
       " 'hero',\n",
       " 'heroes',\n",
       " 'heroin',\n",
       " 'heroine',\n",
       " 'heroines',\n",
       " 'heroism',\n",
       " 'heron',\n",
       " 'herons',\n",
       " 'Herr',\n",
       " 'herring',\n",
       " 'herrings',\n",
       " 'Hertz',\n",
       " 'hilarious',\n",
       " 'Hilariously',\n",
       " 'hilarity',\n",
       " 'Homeric',\n",
       " 'honoraria',\n",
       " 'honorarium',\n",
       " 'honorary',\n",
       " 'housewares',\n",
       " 'Huckleberry',\n",
       " 'Huerta',\n",
       " 'Huerta',\n",
       " 'humanitarian',\n",
       " 'humanitarian',\n",
       " 'humanitarians',\n",
       " 'humanitarians',\n",
       " 'Hungarian',\n",
       " 'Hungarians',\n",
       " 'Huron',\n",
       " 'hurricane',\n",
       " 'hydrotherapy',\n",
       " 'hyperbaric',\n",
       " 'hysteria',\n",
       " 'hysteric',\n",
       " 'hysterical',\n",
       " 'hysterically',\n",
       " 'hysterics',\n",
       " 'illusionary',\n",
       " 'imaginary',\n",
       " 'immunotherapy',\n",
       " 'impair',\n",
       " 'impaired',\n",
       " 'impairing',\n",
       " 'impairment',\n",
       " 'impairs',\n",
       " 'imperative',\n",
       " 'imperatives',\n",
       " 'imperialistic',\n",
       " 'imperil',\n",
       " 'imperils',\n",
       " 'incendiary',\n",
       " 'inflationary',\n",
       " 'inherent',\n",
       " 'inherently',\n",
       " 'inherit',\n",
       " 'inheritable',\n",
       " 'inheritance',\n",
       " 'inherited',\n",
       " 'inheriting',\n",
       " 'inheritor',\n",
       " 'inherits',\n",
       " 'initiative',\n",
       " 'initiatives',\n",
       " 'interdisciplinary',\n",
       " 'interferon',\n",
       " 'intermarry',\n",
       " 'intermediaries',\n",
       " 'intermediaries',\n",
       " 'intermediary',\n",
       " 'intermediary',\n",
       " 'interplanetary',\n",
       " 'interrogate',\n",
       " 'interrogated',\n",
       " 'interrogating']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadDiphs = rowsWBadNphones(lexicon, 2, getNphones_i, badDiphones)\n",
    "len(hasBadDiphs)\n",
    "len(lexicon)\n",
    "len(hasBadDiphs)/len(lexicon) #proportion of the lexicon (by type) that has bad triphs\n",
    "sum([float(entry['Prob']) for entry in hasBadDiphs]) #prob mass of bad triph words by token\n",
    "list(map(lambda r: r['Word'], hasBadDiphs[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:20.395937Z",
     "start_time": "2019-03-12T02:47:20.393940Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME see how many words there are like this and what they're like, also check sum of probability of such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:21.692560Z",
     "start_time": "2019-03-12T02:47:20.397499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observedBadNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "len(observedBadNphones(lexicon, 3, getNphones_i, badTriphones))\n",
    "# observedBadNphones(lexicon, 2, getNphones_i, badDiphones)\n",
    "len(observedBadNphones(lexicon, 2, getNphones_i, badDiphones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:22.880508Z",
     "start_time": "2019-03-12T02:47:21.694209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "52143"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noBadTriphs_IPhOD = onlyRowsWithGoodNphones(lexicon, 3, getNphones_i, badTriphones)\n",
    "noBadDiphsOrTriphs_IPhOD = onlyRowsWithGoodNphones(noBadTriphs_IPhOD, 2, getNphones_i, badDiphones)\n",
    "# [r for r in lexicon if not containsBadTriphones(r)]\n",
    "len(lexicon)\n",
    "len(noBadTriphs_IPhOD)\n",
    "len(noBadDiphsOrTriphs_IPhOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:22.885854Z",
     "start_time": "2019-03-12T02:47:22.881926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noBadTriphones'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_filter\n",
    "\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    lexicon_filtered = lexicon\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    lexicon_filtered = noBadTriphs_IPhOD\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} intsead\".format(which_filter))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose wordforms for the IPhOD lexicon distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we want to export a json file( = a list of dictionaries) mapping each phonological wordform to a number representing a frequency or probability or negative log probability of that phonological wordform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:22.892811Z",
     "start_time": "2019-03-12T02:47:22.887056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'UnTrn'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_stress\n",
    "\n",
    "if which_stress == 'destressed':\n",
    "    wordform_field = 'UnTrn'\n",
    "elif which_stress == 'stressed':\n",
    "    wordform_field = 'StTrn'\n",
    "else:\n",
    "    raise Exception(\"'which_stress' must be one of 'destressed' or 'stressed'; got {0} instead\".format(which_stress))\n",
    "\n",
    "wordform_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:22.926609Z",
     "start_time": "2019-03-12T02:47:22.895355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "49601"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2550"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonologicalWords = [entry[wordform_field] for entry in lexicon_filtered]\n",
    "len(phonologicalWords)\n",
    "len(set(phonologicalWords))\n",
    "len(phonologicalWords) - len(set(phonologicalWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate frequencies, probabilities, and informativities of phonological wordforms in IPhOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to frequency/probability/negative log probability: unfortunately, each entry represents a phonological representation of an orthographic word, and the probability is based on the orthographic word; multiple phonological representations share an orthographic representation (and therefore a frequency/probability/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:22.960468Z",
     "start_time": "2019-03-12T02:47:22.928017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46328"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWords = [entry['Word'] for entry in lexicon_filtered]\n",
    "len(orthWords)\n",
    "len(set(orthWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many orthographic words are associated with multiple phonological representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:23.378172Z",
     "start_time": "2019-03-12T02:47:23.375269Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:23.768137Z",
     "start_time": "2019-03-12T02:47:23.721510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "41202"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5126"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthWordCounter = Counter(orthWords)\n",
    "sum(orthWordCounter.values())\n",
    "uniqueWords = {k for k in orthWordCounter if orthWordCounter[k] == 1}\n",
    "dupedWords = {k for k in orthWordCounter if orthWordCounter[k] > 1}\n",
    "len(uniqueWords)\n",
    "len(dupedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:24.029992Z",
     "start_time": "2019-03-12T02:47:24.000427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ages'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('Indx', '1011'),\n",
       "              ('NPhon', '4'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '16.947188818009725'),\n",
       "              ('Prob', '7.913850168668177e-06'),\n",
       "              ('SCDcnt', '472'),\n",
       "              ('SFreq', '11.12'),\n",
       "              ('StTrn', 'eɪ1.dʒ.ə0.z'),\n",
       "              ('UnTrn', 'eɪ.dʒ.ə.z'),\n",
       "              ('Word', 'ages'),\n",
       "              ('strBPAV', '0.00141955'),\n",
       "              ('strCBPAV', '0.00132695'),\n",
       "              ('strCDEN', '1331'),\n",
       "              ('strCLCPOSPAV', '0.03327800'),\n",
       "              ('strCPOSPAV', '0.01649463'),\n",
       "              ('strCTPAV', '0.00008526'),\n",
       "              ('strDENS', '6'),\n",
       "              ('strFBPAV', '0.00147871'),\n",
       "              ('strFDEN', '34.05'),\n",
       "              ('strFLCPOSPAV', '0.02050019'),\n",
       "              ('strFPOSPAV', '0.01659288'),\n",
       "              ('strFTPAV', '0.00004504'),\n",
       "              ('strLBPAV', '0.00152468'),\n",
       "              ('strLCPOSPAV', '0.04191295'),\n",
       "              ('strLDEN', '4.22'),\n",
       "              ('strLLCPOSPAV', '0.03906567'),\n",
       "              ('strLPOSPAV', '0.01680775'),\n",
       "              ('strLTPAV', '0.00010453'),\n",
       "              ('strPOSPAV', '0.01457023'),\n",
       "              ('strTPAV', '0.00006351'),\n",
       "              ('unsBPAV', '0.00154588'),\n",
       "              ('unsCBPAV', '0.00163194'),\n",
       "              ('unsCDEN', '1331'),\n",
       "              ('unsCLCPOSPAV', '0.04062797'),\n",
       "              ('unsCPOSPAV', '0.02022112'),\n",
       "              ('unsCTPAV', '0.00008381'),\n",
       "              ('unsDENS', '6'),\n",
       "              ('unsFBPAV', '0.00236137'),\n",
       "              ('unsFDEN', '34.05'),\n",
       "              ('unsFLCPOSPAV', '0.05030303'),\n",
       "              ('unsFPOSPAV', '0.02498111'),\n",
       "              ('unsFTPAV', '0.00004662'),\n",
       "              ('unsLBPAV', '0.00171907'),\n",
       "              ('unsLCPOSPAV', '0.04607600'),\n",
       "              ('unsLDEN', '4.22'),\n",
       "              ('unsLLCPOSPAV', '0.04380238'),\n",
       "              ('unsLPOSPAV', '0.01991086'),\n",
       "              ('unsLTPAV', '0.00010196'),\n",
       "              ('unsPOSPAV', '0.01773388'),\n",
       "              ('unsTPAV', '0.00006128')]),\n",
       " OrderedDict([('Indx', '1012'),\n",
       "              ('NPhon', '4'),\n",
       "              ('NSyll', '2'),\n",
       "              ('Nlprob', '16.947188818009725'),\n",
       "              ('Prob', '7.913850168668177e-06'),\n",
       "              ('SCDcnt', '472'),\n",
       "              ('SFreq', '11.12'),\n",
       "              ('StTrn', 'eɪ1.dʒ.ɪ0.z'),\n",
       "              ('UnTrn', 'eɪ.dʒ.ɪ.z'),\n",
       "              ('Word', 'ages'),\n",
       "              ('strBPAV', '0.00168020'),\n",
       "              ('strCBPAV', '0.00134055'),\n",
       "              ('strCDEN', '1852'),\n",
       "              ('strCLCPOSPAV', '0.03599720'),\n",
       "              ('strCPOSPAV', '0.01459982'),\n",
       "              ('strCTPAV', '0.00007708'),\n",
       "              ('strDENS', '13'),\n",
       "              ('strFBPAV', '0.00250917'),\n",
       "              ('strFDEN', '47.69'),\n",
       "              ('strFLCPOSPAV', '0.02756869'),\n",
       "              ('strFPOSPAV', '0.01710143'),\n",
       "              ('strFTPAV', '0.00003152'),\n",
       "              ('strLBPAV', '0.00155298'),\n",
       "              ('strLCPOSPAV', '0.04138076'),\n",
       "              ('strLDEN', '7.06'),\n",
       "              ('strLLCPOSPAV', '0.03846880'),\n",
       "              ('strLPOSPAV', '0.01252308'),\n",
       "              ('strLTPAV', '0.00011924'),\n",
       "              ('strPOSPAV', '0.00954696'),\n",
       "              ('strTPAV', '0.00011996'),\n",
       "              ('unsBPAV', '0.00211912'),\n",
       "              ('unsCBPAV', '0.00173804'),\n",
       "              ('unsCDEN', '1852'),\n",
       "              ('unsCLCPOSPAV', '0.04432904'),\n",
       "              ('unsCPOSPAV', '0.01949784'),\n",
       "              ('unsCTPAV', '0.00007588'),\n",
       "              ('unsDENS', '13'),\n",
       "              ('unsFBPAV', '0.00288464'),\n",
       "              ('unsFDEN', '47.69'),\n",
       "              ('unsFLCPOSPAV', '0.03658328'),\n",
       "              ('unsFPOSPAV', '0.02087041'),\n",
       "              ('unsFTPAV', '0.00003266'),\n",
       "              ('unsLBPAV', '0.00197198'),\n",
       "              ('unsLCPOSPAV', '0.04745206'),\n",
       "              ('unsLDEN', '7.06'),\n",
       "              ('unsLLCPOSPAV', '0.04572186'),\n",
       "              ('unsLPOSPAV', '0.01863121'),\n",
       "              ('unsLTPAV', '0.00011675'),\n",
       "              ('unsPOSPAV', '0.01601939'),\n",
       "              ('unsTPAV', '0.00011831')])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duped_word_ex = list(dupedWords)[0]\n",
    "duped_word_ex\n",
    "some_entries = [entry for entry in lexicon_filtered if entry['Word'] == duped_word_ex]\n",
    "some_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split probability mass among phonological realizations of the same orthographic word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since frequencies are only available for orthographic wordforms, we'll assume that each of the $n$ phonological wordforms associated with the same orthographic wordform $o$ with frequency $f_o$ each have frequency $\\frac{f_o}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:24.574097Z",
     "start_time": "2019-03-12T02:47:24.571531Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# def edit_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable (because it returns a value) but stateful(= in-place) dictionary update.\n",
    "#     '''\n",
    "#     the_dict.update({the_key: the_new_value})\n",
    "#     return the_dict\n",
    "\n",
    "# def modify_dict(the_dict, the_key, the_new_value):\n",
    "#     '''\n",
    "#     Composable and (naively-implemented) non-mutating dictionary update.\n",
    "#     '''\n",
    "#     new_dict = {k:the_dict[k] for k in the_dict}\n",
    "#     new_dict.update({the_key: the_new_value})\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T02:47:24.965081Z",
     "start_time": "2019-03-12T02:47:24.963086Z"
    }
   },
   "outputs": [],
   "source": [
    "# from time import localtime, strftime\n",
    "# def stamp():\n",
    "#     return strftime('%H:%M:%S', localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:21.921933Z",
     "start_time": "2019-03-12T02:47:25.345857Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start @ 19:47:25\n",
      "463 | 463/46328 = 0.009993956138836125 | invaluable | 19:47:33\n",
      "927 | 927/46328 = 0.02000949749611466 | bore | 19:47:41\n",
      "1390 | 1390/46328 = 0.030003453634950786 | fringe | 19:47:50\n",
      "2316 | 2316/46328 = 0.049991365912623036 | rust | 19:48:07\n",
      "4633 | 4633/46328 = 0.10000431704368848 | Raider | 19:48:50\n",
      "9266 | 9266/46328 = 0.20000863408737696 | simplifying | 19:50:14\n",
      "13898 | 13898/46328 = 0.29999136591262304 | psyches | 19:51:37\n",
      "18531 | 18531/46328 = 0.3999956829563115 | pearly | 19:53:01\n",
      "23164 | 23164/46328 = 0.5 | Burrows | 19:54:25\n",
      "27797 | 27797/46328 = 0.6000043170436885 | Hy | 19:55:48\n",
      "32430 | 32430/46328 = 0.700008634087377 | dialectic | 19:57:12\n",
      "37062 | 37062/46328 = 0.799991365912623 | review | 19:58:35\n",
      "41695 | 41695/46328 = 0.8999956829563115 | sampling | 19:59:58\n",
      "44012 | 44012/46328 = 0.950008634087377 | snatching | 20:00:40\n",
      "44475 | 44475/46328 = 0.960002590226213 | chest | 20:00:48\n",
      "44938 | 44938/46328 = 0.9699965463650492 | gleaned | 20:00:56\n",
      "45401 | 45401/46328 = 0.9799905025038853 | decoded | 20:01:05\n",
      "45865 | 45865/46328 = 0.9900060438611639 | Hasidic | 20:01:13\n",
      "Finish @ 20:01:21\n"
     ]
    }
   ],
   "source": [
    "#14m whole cell [wittgenstein/cpython]\n",
    "def getEntriesMatchingOrthWord(orthWord):\n",
    "    return [entry for entry in lexicon_filtered if entry['Word'] == orthWord]\n",
    "\n",
    "# orthWordToEntries = {orthWord:getEntriesMatchingOrthWord(orthWord) for orthWord in set(orthWords)}\n",
    "\n",
    "orthWordToEntries = dict()\n",
    "\n",
    "orthWordSet = set(orthWords)\n",
    "constructDictWProgressUpdates(getEntriesMatchingOrthWord, orthWordSet, orthWordToEntries)\n",
    "\n",
    "# orthWordSet = set(orthWords)\n",
    "# totalOrthWords = len(orthWordSet)\n",
    "# benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [round(each/100.0 * totalOrthWords) for each in benchmarkPercentages]\n",
    "# for i, orthWord in enumerate(orthWordSet):\n",
    "#     if i in benchmarkIndices:\n",
    "#         print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, totalOrthWords, i/totalOrthWords, orthWord, stamp()))\n",
    "\n",
    "#     orthWordToEntries[orthWord] = getEntriesMatchingOrthWord(orthWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:21.929166Z",
     "start_time": "2019-03-12T03:01:21.923660Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitOrthFreq(orth_word):\n",
    "#     matchingEntries = [entry for entry in slim_lexicon if entry['Word'] == orth_word]\n",
    "    matchingEntries = orthWordToEntries[orth_word]\n",
    "\n",
    "    if matchingEntries == []:\n",
    "        return 0.0\n",
    "    \n",
    "    orthFreq = matchingEntries[0]['SFreq']\n",
    "    assert(all(match['SFreq'] == orthFreq for match in matchingEntries))\n",
    "    \n",
    "    dividedFreq = float(orthFreq) / len(matchingEntries)\n",
    "    return dividedFreq\n",
    "\n",
    "def add_splitOrthFreqAnnotations(entry, inPlace = None):\n",
    "    if inPlace == None:\n",
    "        inPlace = False\n",
    "\n",
    "    splitFreq = splitOrthFreq(entry['Word'])\n",
    "#     print('word: {0}'.format(entry['Word']))\n",
    "    new_entry = dict()\n",
    "    if not inPlace:\n",
    "        new_entry = modify_dict(entry, 'split_SFreq', splitFreq)\n",
    "    else:\n",
    "        new_entry = edit_dict(entry, 'split_SFreq', splitFreq)\n",
    "#     print('new split freq: {0}'.format(splitFreq))\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.028528Z",
     "start_time": "2019-03-12T03:01:21.930348Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Indx', '1'),\n",
       "             ('NPhon', '1'),\n",
       "             ('NSyll', '1'),\n",
       "             ('Nlprob', '6.10491267350873'),\n",
       "             ('Prob', '0.014529081648642661'),\n",
       "             ('SCDcnt', '8382'),\n",
       "             ('SFreq', '20415.27'),\n",
       "             ('StTrn', 'ə0'),\n",
       "             ('UnTrn', 'ə'),\n",
       "             ('Word', 'a'),\n",
       "             ('strBPAV', '0'),\n",
       "             ('strCBPAV', '0'),\n",
       "             ('strCDEN', '114167'),\n",
       "             ('strCLCPOSPAV', '0.10933994'),\n",
       "             ('strCPOSPAV', '0.03349570'),\n",
       "             ('strCTPAV', '0'),\n",
       "             ('strDENS', '21'),\n",
       "             ('strFBPAV', '0'),\n",
       "             ('strFDEN', '145035.32'),\n",
       "             ('strFLCPOSPAV', '0.21988359'),\n",
       "             ('strFPOSPAV', '0.06061931'),\n",
       "             ('strFTPAV', '0'),\n",
       "             ('strLBPAV', '0'),\n",
       "             ('strLCPOSPAV', '0.06666667'),\n",
       "             ('strLDEN', '77.56'),\n",
       "             ('strLLCPOSPAV', '0.07556101'),\n",
       "             ('strLPOSPAV', '0.03338777'),\n",
       "             ('strLTPAV', '0'),\n",
       "             ('strPOSPAV', '0.03588891'),\n",
       "             ('strTPAV', '0'),\n",
       "             ('unsBPAV', '0'),\n",
       "             ('unsCBPAV', '0'),\n",
       "             ('unsCDEN', '136251'),\n",
       "             ('unsCLCPOSPAV', '0.15987477'),\n",
       "             ('unsCPOSPAV', '0.04159586'),\n",
       "             ('unsCTPAV', '0'),\n",
       "             ('unsDENS', '26'),\n",
       "             ('unsFBPAV', '0'),\n",
       "             ('unsFDEN', '150377.45'),\n",
       "             ('unsFLCPOSPAV', '0.22760866'),\n",
       "             ('unsFPOSPAV', '0.07192868'),\n",
       "             ('unsFTPAV', '0'),\n",
       "             ('unsLBPAV', '0'),\n",
       "             ('unsLCPOSPAV', '0.07692308'),\n",
       "             ('unsLDEN', '91.03'),\n",
       "             ('unsLLCPOSPAV', '0.12570126'),\n",
       "             ('unsLPOSPAV', '0.03999447'),\n",
       "             ('unsLTPAV', '0'),\n",
       "             ('unsPOSPAV', '0.04449866'),\n",
       "             ('unsTPAV', '0')])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_filtered)\n",
    "lexicon_filtered[0]\n",
    "add_splitOrthFreqAnnotations(lexicon_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.032226Z",
     "start_time": "2019-03-12T03:01:22.030485Z"
    }
   },
   "outputs": [],
   "source": [
    "# filtered_splitFreqs = [add_splitOrthFreqAnnotations(entry) for entry in lexicon_filtered]\n",
    "# len(filtered_splitFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.446850Z",
     "start_time": "2019-03-12T03:01:22.034043Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitFreqs = list(map(add_splitOrthFreqAnnotations, lexicon_filtered))\n",
    "filtered_splitFreqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.464215Z",
     "start_time": "2019-03-12T03:01:22.448179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941881.0300000808\n"
     ]
    }
   ],
   "source": [
    "sumFreqs = sum([float(entry['split_SFreq']) for entry in filtered_splitFreqs])\n",
    "print(sumFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.470645Z",
     "start_time": "2019-03-12T03:01:22.465258Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow, isclose\n",
    "\n",
    "def add_splitOrthProbAnnotations(entry):\n",
    "    splitFreq = float(entry['split_SFreq'])\n",
    "    \n",
    "    splitProb = splitFreq / sumFreqs\n",
    "    splitNlprob = -1.0 * log2(splitProb)\n",
    "    \n",
    "    new_entry = modify_dict(entry, 'split_Prob', splitProb)\n",
    "    new_entry = modify_dict(new_entry, 'split_Nlprob', splitNlprob)\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.896046Z",
     "start_time": "2019-03-12T03:01:22.471799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635,\n",
       " 'split_Prob': 0.010837499296486653,\n",
       " 'split_Nlprob': 6.527824289979772}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitProbs = list(map(add_splitOrthProbAnnotations, filtered_splitFreqs))\n",
    "filtered_splitProbs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate probability of a phonological word (marginalizing over orthographic wordforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the $n$ phonological wordforms of a given orthographic wordform $o$ with frequency $o_f$ now has frequency $\\frac{o_f}{n}$, but what about different orthographic words that share a phonological wordform? Ultimately we want a probability distribution mapping phonological wordforms to frequencies/probabilities, so we need to map each phonological wordform to a sum of frequencies/probabilities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:01:22.902966Z",
     "start_time": "2019-03-12T03:01:22.897070Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWords = set(phonologicalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.141879Z",
     "start_time": "2019-03-12T03:01:22.904323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start @ 20:01:22\n",
      "496 | 496/49601 = 0.009999798391161469 | s.t.ʌ.d.i.ɪ.ŋ | 20:01:28\n",
      "992 | 992/49601 = 0.019999596782322937 | ə.b.aʊ.n.d.ɪ.d | 20:01:33\n",
      "1488 | 1488/49601 = 0.029999395173484406 | ɑ.p.ɚ.t.u.n.ə.t.i.z | 20:01:38\n",
      "2480 | 2480/49601 = 0.04999899195580734 | l.ɛ.f.t | 20:01:49\n",
      "4960 | 4960/49601 = 0.09999798391161469 | p.ɛ.s.t.ə.s.aɪ.d | 20:02:16\n",
      "9920 | 9920/49601 = 0.19999596782322937 | ɪ.n.s.ɪ.s.t.s | 20:03:10\n",
      "14880 | 14880/49601 = 0.29999395173484406 | ɛ.n.ʃ.ʊ.r.ɪ.ŋ | 20:04:04\n",
      "19840 | 19840/49601 = 0.39999193564645874 | ə.b.l.aɪ.dʒ.ɪ.z | 20:04:59\n",
      "24800 | 24800/49601 = 0.4999899195580734 | r.ɪ.z.ɚ.v.ɪ.s.t.s | 20:05:53\n",
      "29761 | 29761/49601 = 0.6000080643535413 | p.ɑ.r.t.ɛ.n.d | 20:06:46\n",
      "34721 | 34721/49601 = 0.7000060482651559 | ə.n.h.æ.p.ə.l.i | 20:07:40\n",
      "39681 | 39681/49601 = 0.8000040321767706 | d.ɪ.l.ə.t.ɑ.r.i | 20:08:35\n",
      "44641 | 44641/49601 = 0.9000020160883853 | b.ʊ.l.i.z | 20:09:28\n",
      "47121 | 47121/49601 = 0.9500010080441926 | h.ɑ.r.s.m.ə.n | 20:09:55\n",
      "47617 | 47617/49601 = 0.9600008064353541 | h.ɑ.r.s.ʃ.u.z | 20:10:00\n",
      "48113 | 48113/49601 = 0.9700006048265156 | l.ɪ.m.aɪ.n | 20:10:06\n",
      "48609 | 48609/49601 = 0.980000403217677 | m.ɛ.m.oʊ.z | 20:10:11\n",
      "49105 | 49105/49601 = 0.9900002016088385 | ɑ.f.ə.l.i | 20:10:16\n",
      "Finish @ 20:10:22\n"
     ]
    }
   ],
   "source": [
    "#whole cell takes 9m [wittgenstein/cpython]\n",
    "def getEntriesMatchingPhonWord(phonWord):\n",
    "    return [entry for entry in filtered_splitProbs if entry[wordform_field] == phonWord]\n",
    "\n",
    "# phonWordToEntries = {phonWord:getEntriesMatchingPhonWord(phonWord) for phonWord in phonWords}\n",
    "\n",
    "phonWordToEntries = dict()\n",
    "\n",
    "constructDictWProgressUpdates(getEntriesMatchingPhonWord, phonWords, phonWordToEntries)\n",
    "\n",
    "# phonWords\n",
    "# totalPhonWords = len(phonWords)\n",
    "# benchmarkPercentages = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [round(each/100.0 * totalPhonWords) for each in benchmarkPercentages]\n",
    "# for i, phonWord in enumerate(phonWords):\n",
    "#     if i in benchmarkIndices:\n",
    "#         print('{0} | {0}/{1} = {2} | {3} | {4}'.format(i, totalPhonWords, i/totalPhonWords, phonWord, stamp()))\n",
    "\n",
    "#     phonWordToEntries[phonWord] = getEntriesMatchingPhonWord(phonWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.147907Z",
     "start_time": "2019-03-12T03:10:22.143105Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPhonProb(phon_word):\n",
    "#     matchingEntries = [entry[field] for entry in slim_splitProbs if entry[field] == phonrep]\n",
    "    matchingEntries = phonWordToEntries[phon_word]\n",
    "    \n",
    "    if matchingEntries == []:\n",
    "        return 0.0\n",
    "    \n",
    "    prob = sum([entry['split_Prob'] for entry in matchingEntries])\n",
    "    return prob\n",
    "\n",
    "def add_phonProbs(entry, inPlace = None):\n",
    "    if inPlace == None:\n",
    "        inPlace = False\n",
    "    \n",
    "    transcription = entry[wordform_field]\n",
    "    prob = getPhonProb(transcription)\n",
    "    nlprob = -1.0 * log2(prob)\n",
    "    \n",
    "    new_entry = dict()\n",
    "    if not inPlace:\n",
    "        new_entry = modify_dict(entry, 'phon_Prob', prob)\n",
    "        new_entry = modify_dict(new_entry, 'phon_Nlprob', nlprob)\n",
    "    else:\n",
    "        new_entry = edit_dict(entry, 'phon_Prob', prob)\n",
    "        new_entry = edit_dict(new_entry, 'phon_Nlprob', nlprob)\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.647629Z",
     "start_time": "2019-03-12T03:10:22.148883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indx': '1',\n",
       " 'NPhon': '1',\n",
       " 'NSyll': '1',\n",
       " 'Nlprob': '6.10491267350873',\n",
       " 'Prob': '0.014529081648642661',\n",
       " 'SCDcnt': '8382',\n",
       " 'SFreq': '20415.27',\n",
       " 'StTrn': 'ə0',\n",
       " 'UnTrn': 'ə',\n",
       " 'Word': 'a',\n",
       " 'strBPAV': '0',\n",
       " 'strCBPAV': '0',\n",
       " 'strCDEN': '114167',\n",
       " 'strCLCPOSPAV': '0.10933994',\n",
       " 'strCPOSPAV': '0.03349570',\n",
       " 'strCTPAV': '0',\n",
       " 'strDENS': '21',\n",
       " 'strFBPAV': '0',\n",
       " 'strFDEN': '145035.32',\n",
       " 'strFLCPOSPAV': '0.21988359',\n",
       " 'strFPOSPAV': '0.06061931',\n",
       " 'strFTPAV': '0',\n",
       " 'strLBPAV': '0',\n",
       " 'strLCPOSPAV': '0.06666667',\n",
       " 'strLDEN': '77.56',\n",
       " 'strLLCPOSPAV': '0.07556101',\n",
       " 'strLPOSPAV': '0.03338777',\n",
       " 'strLTPAV': '0',\n",
       " 'strPOSPAV': '0.03588891',\n",
       " 'strTPAV': '0',\n",
       " 'unsBPAV': '0',\n",
       " 'unsCBPAV': '0',\n",
       " 'unsCDEN': '136251',\n",
       " 'unsCLCPOSPAV': '0.15987477',\n",
       " 'unsCPOSPAV': '0.04159586',\n",
       " 'unsCTPAV': '0',\n",
       " 'unsDENS': '26',\n",
       " 'unsFBPAV': '0',\n",
       " 'unsFDEN': '150377.45',\n",
       " 'unsFLCPOSPAV': '0.22760866',\n",
       " 'unsFPOSPAV': '0.07192868',\n",
       " 'unsFTPAV': '0',\n",
       " 'unsLBPAV': '0',\n",
       " 'unsLCPOSPAV': '0.07692308',\n",
       " 'unsLDEN': '91.03',\n",
       " 'unsLLCPOSPAV': '0.12570126',\n",
       " 'unsLPOSPAV': '0.03999447',\n",
       " 'unsLTPAV': '0',\n",
       " 'unsPOSPAV': '0.04449866',\n",
       " 'unsTPAV': '0',\n",
       " 'split_SFreq': 10207.635,\n",
       " 'split_Prob': 0.010837499296486653,\n",
       " 'split_Nlprob': 6.527824289979772,\n",
       " 'phon_Prob': 0.010837499296486653,\n",
       " 'phon_Nlprob': 6.527824289979772}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_splitprobs_phonprobs = list(map(add_phonProbs, filtered_splitProbs))\n",
    "filtered_splitprobs_phonprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.678042Z",
     "start_time": "2019-03-12T03:10:22.649534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Indx': '41164',\n",
       "  'NPhon': '5',\n",
       "  'NSyll': '2',\n",
       "  'Nlprob': '19.5269710796252',\n",
       "  'Prob': '1.3237195426009723e-06',\n",
       "  'SCDcnt': '85',\n",
       "  'SFreq': '1.86',\n",
       "  'StTrn': 'r.ɪ1.dʒ.ə0.d',\n",
       "  'UnTrn': 'r.ɪ.dʒ.ə.d',\n",
       "  'Word': 'rigid',\n",
       "  'strBPAV': '0.00216083',\n",
       "  'strCBPAV': '0.00214780',\n",
       "  'strCDEN': '171',\n",
       "  'strCLCPOSPAV': '0.08322913',\n",
       "  'strCPOSPAV': '0.05567915',\n",
       "  'strCTPAV': '0.00004278',\n",
       "  'strDENS': '4',\n",
       "  'strFBPAV': '0.00115820',\n",
       "  'strFDEN': '4',\n",
       "  'strFLCPOSPAV': '0.08932192',\n",
       "  'strFPOSPAV': '0.04696634',\n",
       "  'strFTPAV': '0.00001926',\n",
       "  'strLBPAV': '0.00242575',\n",
       "  'strLCPOSPAV': '0.07578718',\n",
       "  'strLDEN': '1.09',\n",
       "  'strLLCPOSPAV': '0.07965779',\n",
       "  'strLPOSPAV': '0.05789941',\n",
       "  'strLTPAV': '0.00006205',\n",
       "  'strPOSPAV': '0.05668689',\n",
       "  'strTPAV': '0.00005763',\n",
       "  'unsBPAV': '0.00400750',\n",
       "  'unsCBPAV': '0.00400524',\n",
       "  'unsCDEN': '171',\n",
       "  'unsCLCPOSPAV': '0.09365356',\n",
       "  'unsCPOSPAV': '0.06351975',\n",
       "  'unsCTPAV': '0.00008944',\n",
       "  'unsDENS': '4',\n",
       "  'unsFBPAV': '0.00268055',\n",
       "  'unsFDEN': '4',\n",
       "  'unsFLCPOSPAV': '0.10282063',\n",
       "  'unsFPOSPAV': '0.05599098',\n",
       "  'unsFTPAV': '0.00004332',\n",
       "  'unsLBPAV': '0.00437706',\n",
       "  'unsLCPOSPAV': '0.08172226',\n",
       "  'unsLDEN': '1.09',\n",
       "  'unsLLCPOSPAV': '0.08696793',\n",
       "  'unsLPOSPAV': '0.06745590',\n",
       "  'unsLTPAV': '0.00012714',\n",
       "  'unsPOSPAV': '0.06806723',\n",
       "  'unsTPAV': '0.00012880',\n",
       "  'split_SFreq': 0.93,\n",
       "  'split_Prob': 9.873858485077679e-07,\n",
       "  'split_Nlprob': 19.94988269609624,\n",
       "  'phon_Prob': 9.873858485077679e-07,\n",
       "  'phon_Nlprob': 19.94988269609624},\n",
       " {'Indx': '41165',\n",
       "  'NPhon': '5',\n",
       "  'NSyll': '2',\n",
       "  'Nlprob': '19.5269710796252',\n",
       "  'Prob': '1.3237195426009723e-06',\n",
       "  'SCDcnt': '85',\n",
       "  'SFreq': '1.86',\n",
       "  'StTrn': 'r.ɪ1.dʒ.ɪ0.d',\n",
       "  'UnTrn': 'r.ɪ.dʒ.ɪ.d',\n",
       "  'Word': 'rigid',\n",
       "  'strBPAV': '0.00227069',\n",
       "  'strCBPAV': '0.00199256',\n",
       "  'strCDEN': '114',\n",
       "  'strCLCPOSPAV': '0.09081155',\n",
       "  'strCPOSPAV': '0.04768333',\n",
       "  'strCTPAV': '0.00001793',\n",
       "  'strDENS': '3',\n",
       "  'strFBPAV': '0.00232474',\n",
       "  'strFDEN': '2.61',\n",
       "  'strFLCPOSPAV': '0.08851489',\n",
       "  'strFPOSPAV': '0.03718920',\n",
       "  'strFTPAV': '0.00000912',\n",
       "  'strLBPAV': '0.00214621',\n",
       "  'strLCPOSPAV': '0.07950475',\n",
       "  'strLDEN': '0.71',\n",
       "  'strLLCPOSPAV': '0.08022137',\n",
       "  'strLPOSPAV': '0.04534423',\n",
       "  'strLTPAV': '0.00002834',\n",
       "  'strPOSPAV': '0.04302293',\n",
       "  'strTPAV': '0.00003058',\n",
       "  'unsBPAV': '0.00417202',\n",
       "  'unsCBPAV': '0.00365626',\n",
       "  'unsCDEN': '114',\n",
       "  'unsCLCPOSPAV': '0.10311375',\n",
       "  'unsCPOSPAV': '0.05736822',\n",
       "  'unsCTPAV': '0.00007939',\n",
       "  'unsDENS': '3',\n",
       "  'unsFBPAV': '0.00327493',\n",
       "  'unsFDEN': '2.61',\n",
       "  'unsFLCPOSPAV': '0.10352536',\n",
       "  'unsFPOSPAV': '0.04740790',\n",
       "  'unsFTPAV': '0.00003851',\n",
       "  'unsLBPAV': '0.00411429',\n",
       "  'unsLCPOSPAV': '0.08591874',\n",
       "  'unsLDEN': '0.71',\n",
       "  'unsLLCPOSPAV': '0.08827808',\n",
       "  'unsLPOSPAV': '0.05682895',\n",
       "  'unsLTPAV': '0.00012177',\n",
       "  'unsPOSPAV': '0.05703602',\n",
       "  'unsTPAV': '0.00013901',\n",
       "  'split_SFreq': 0.93,\n",
       "  'split_Prob': 9.873858485077679e-07,\n",
       "  'split_Nlprob': 19.94988269609624,\n",
       "  'phon_Prob': 9.873858485077679e-07,\n",
       "  'phon_Nlprob': 19.94988269609624}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findEntrySatisfying(pred, entries):\n",
    "    return [entry for entry in entries if pred(entry)]\n",
    "def findEntryMatchingOrth(orthword, entries):\n",
    "    return findEntrySatisfying(lambda entry: entry['Word'] == orthword, entries)\n",
    "findEntryMatchingOrth('rigid', filtered_splitprobs_phonprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define representation for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.680721Z",
     "start_time": "2019-03-12T03:10:22.679095Z"
    }
   },
   "outputs": [],
   "source": [
    "# # dottedStringToTuple = lambda ds: tuple(ds.split('.'))\n",
    "# # tupleToDottedString = lambda t: '.'.join(t)\n",
    "\n",
    "# # leftEdge = '⋊'\n",
    "# # rightEdge = '⋉'\n",
    "# # edgeSymbols = set([leftEdge, rightEdge])\n",
    "\n",
    "# def padInputSequenceWithBoundaries(inputSeq):\n",
    "#     temp = list(dottedStringToTuple(inputSeq))\n",
    "#     temp = tuple([leftEdge] + temp + [rightEdge])\n",
    "#     return tupleToDottedString(temp)\n",
    "\n",
    "# def trimBoundariesFromSequence(seq):\n",
    "#     temp = list(dottedStringToTuple(seq))\n",
    "#     if temp[0] == leftEdge:\n",
    "#         temp = temp[1:]\n",
    "#     if temp[-1] == rightEdge:\n",
    "#         temp = temp[:-1]\n",
    "#     return tupleToDottedString(tuple(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.685160Z",
     "start_time": "2019-03-12T03:10:22.681509Z"
    }
   },
   "outputs": [],
   "source": [
    "padWordsWithBoundaries = True\n",
    "# padWordsWithBoundaries = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:22.878335Z",
     "start_time": "2019-03-12T03:10:22.686357Z"
    }
   },
   "outputs": [],
   "source": [
    "my_phonWords = set()\n",
    "if padWordsWithBoundaries:\n",
    "    padded_phonWords = set(map(padInputSequenceWithBoundaries, phonWords))\n",
    "    assert(set(map(trimBoundariesFromSequence, padded_phonWords)) == phonWords)\n",
    "    my_phonWords = padded_phonWords\n",
    "else:\n",
    "    my_phonWords = phonWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:23.044421Z",
     "start_time": "2019-03-12T03:10:22.879395Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToProb = {padInputSequenceWithBoundaries(phonWord):getPhonProb(phonWord) for phonWord in phonWords}\n",
    "else:\n",
    "    phonWordToProb = {phonWord:getPhonProb(phonWord) for phonWord in phonWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:23.054744Z",
     "start_time": "2019-03-12T03:10:23.047910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.æ.s.t.⋉'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.942920540612059e-06"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(phonWordToProb.keys())[0]\n",
    "phonWordToProb[ list(phonWordToProb.keys())[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:23.224930Z",
     "start_time": "2019-03-12T03:10:23.060840Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToNlprob = {padInputSequenceWithBoundaries(phonWord):-1.0 * log2(getPhonProb(phonWord)) for phonWord in phonWords}\n",
    "else:\n",
    "    phonWordToNlprob = {phonWord:-1.0 * log2(getPhonProb(phonWord)) for phonWord in phonWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:10:23.233381Z",
     "start_time": "2019-03-12T03:10:23.226350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.æ.s.t.⋉'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18.973341668920227"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(phonWordToNlprob.keys())[0]\n",
    "phonWordToNlprob[ list(phonWordToNlprob.keys())[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export / import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:39.772192Z",
     "start_time": "2019-03-12T03:12:39.763851Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-a6f22bc24076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misNormalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonWordToProb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#FIXME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert isNormalized(phonWordToProb) #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.644808Z",
     "start_time": "2019-03-12T03:13:36.640863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999979"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(phonWordToProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:39.774078Z",
     "start_time": "2019-03-12T03:12:39.758Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.039396Z",
     "start_time": "2019-03-12T03:12:39.876005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'IPhOD-aligned_destressed_pseudocount0.001 f3_Y0Y1_X0X1.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 f6_Y0Y1_X0X1.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p3Y01X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p3Y0X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p3Y1X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p3YX.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p6Y01X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p6Y0X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p6Y1X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 p6YX.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0.001 pYX.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 f3_Y0Y1_X0X1.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 f6_Y0Y1_X0X1.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p3Y01X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p3Y0X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p3Y1X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p3YX.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p6Y01X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p6Y0X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p6Y1X01.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 p6YX.json'\r\n",
      "'IPhOD-aligned_destressed_pseudocount0 pYX.json'\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.046013Z",
     "start_time": "2019-03-12T03:12:40.041653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = my_lexicon_fn[:-4]\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.050854Z",
     "start_time": "2019-03-12T03:12:40.047894Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.056617Z",
     "start_time": "2019-03-12T03:12:40.052864Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.061921Z",
     "start_time": "2019-03-12T03:12:40.058239Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.516651Z",
     "start_time": "2019-03-12T03:12:40.063446Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.940030Z",
     "start_time": "2019-03-12T03:12:40.518356Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_nlprob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToNlprob, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:40.946570Z",
     "start_time": "2019-03-12T03:12:40.942007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.112994Z",
     "start_time": "2019-03-12T03:12:40.948527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *rob.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.120576Z",
     "start_time": "2019-03-12T03:12:41.115226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.285291Z",
     "start_time": "2019-03-12T03:12:41.122301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *IPhOD*rob.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.292975Z",
     "start_time": "2019-03-12T03:12:41.287472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IPhOD2_Words_IPA_prob_caughtCotMerged_schwa'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = my_lexicon_fn[:-4] #defined way up above...\n",
    "my_filename_stem\n",
    "\n",
    "my_fn_suffix_filter = '_' + which\n",
    "\n",
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.298021Z",
     "start_time": "2019-03-12T03:12:41.294786Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.378757Z",
     "start_time": "2019-03-12T03:12:41.303090Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToProb_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.388624Z",
     "start_time": "2019-03-12T03:12:41.383132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.æ.s.t.⋉'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToProb_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.399544Z",
     "start_time": "2019-03-12T03:12:41.390234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942920540612059e-06"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.942920540612059e-06"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb_in[test_k]\n",
    "phonWordToProb[test_k]\n",
    "phonWordToProb_in[test_k] == phonWordToProb[test_k]\n",
    "assert(phonWordToProb_in[test_k] == phonWordToProb[test_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.408411Z",
     "start_time": "2019-03-12T03:12:41.401140Z"
    }
   },
   "outputs": [],
   "source": [
    "def matchAt(key, dA, dB):\n",
    "    return (key in dA and key in dB) and dA[key] == dB[key]\n",
    "\n",
    "def dicts_match(dA, dB):\n",
    "    allKeys = set.union(set(dA.keys()), set(dB.keys))\n",
    "    missingFromB = {k for k in allKeys if k not in set(dB.keys)}\n",
    "    missingFromA = {k for k in allKeys if k not in set(dA.keys)}\n",
    "    missingKeys = set.union(missingFromB, missingFromA)\n",
    "    \n",
    "    sharedKeys = allKeys - missingKeys\n",
    "    differentValues = {k for k in sharedKeys if not dA[key] == dB[key]}\n",
    "    mismatches = set.union(missingKeys, differentValues)\n",
    "    allMatch = mismatches == set()\n",
    "    return allMatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.472326Z",
     "start_time": "2019-03-12T03:12:41.410164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatchingEntries = [entry for entry in my_phonWords if not phonWordToProb_in[entry] == phonWordToProb[entry]]\n",
    "len(mismatchingEntries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.479997Z",
     "start_time": "2019-03-12T03:12:41.473888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942920540612059e-06"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb = phonWordToProb_in\n",
    "phonWordToProb[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.550042Z",
     "start_time": "2019-03-12T03:12:41.481581Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToNlprob_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_nlprob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToNlprob_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.557027Z",
     "start_time": "2019-03-12T03:12:41.551734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.æ.s.t.⋉'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToNlprob_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.565177Z",
     "start_time": "2019-03-12T03:12:41.558586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.973341668920227"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18.973341668920227"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToNlprob_in[test_k]\n",
    "phonWordToNlprob[test_k]\n",
    "phonWordToNlprob_in[test_k] == phonWordToNlprob[test_k]\n",
    "assert(phonWordToNlprob_in[test_k] == phonWordToNlprob[test_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.619844Z",
     "start_time": "2019-03-12T03:12:41.566927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatchingEntries = [entry for entry in my_phonWords if not phonWordToNlprob_in[entry] == phonWordToNlprob[entry]]\n",
    "len(mismatchingEntries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.627831Z",
     "start_time": "2019-03-12T03:12:41.621401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.973341668920227"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToNlprob = phonWordToNlprob_in\n",
    "phonWordToNlprob[test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.792003Z",
     "start_time": "2019-03-12T03:12:41.629577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      "IPhOD2_Words_IPA_prob_caughtCotMerged_schwa_destressed_noBadTriphones_phonWordToProb.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls *rob.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hammond's newdic processing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a json file mapping phonological wordforms to frequencies or probabilities for a word recognition model we need to\n",
    " 1. Remove any entries we don't want or can't use in the word recognition model.\n",
    " 2. Choose a set of phonological wordforms (unstressed or stressed) to map to frequencies/probabilities.\n",
    " 3. Calculate frequencies/probabilities, dealing with the vagaries of homophony (and remembering to normalize ):\n",
    "   - The wordform frequency of each entry is a corpus frequency of the associated orthographic word.\n",
    "   - Some distinct orthographic wordforms share a phonological wordform.\n",
    " 4. Phonological wordforms need to have word edge symbols added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove clutter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.797912Z",
     "start_time": "2019-03-12T03:12:41.793929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'ə'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_newdic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:41.802322Z",
     "start_time": "2019-03-12T03:12:41.799479Z"
    }
   },
   "outputs": [],
   "source": [
    "# def project_dict(the_dict, keys_to_keep):\n",
    "#     new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "#     return new_dict\n",
    "# project_dict({'Name':'Joe','ID':123,'Job':'clerk'},['Job','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.161947Z",
     "start_time": "2019-03-12T03:12:41.803791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'ə', 'Orthography': 'a', 'Frequency': '23178'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim_hammond = list(map(lambda d: project_dict(d,['Orthography','Transcription','Frequency']), hammond_newdic))\n",
    "len(slim_hammond)\n",
    "slim_hammond[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words with triphones that a channel distribution isn't definable for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.169236Z",
     "start_time": "2019-03-12T03:12:42.163486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16389"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illegalDiphones_h = diphoneAnalysis_h['illicit'][which_stress + ' stimuli']\n",
    "len(illegalDiphones_h)\n",
    "illegalTriphones_h = triphoneAnalysis_h['illicit'][which_stress + ' stimuli']\n",
    "len(illegalTriphones_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.179829Z",
     "start_time": "2019-03-12T03:12:42.171043Z"
    }
   },
   "outputs": [],
   "source": [
    "badDiphones = set()\n",
    "badTriphones = set()\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    badDiphones = set()\n",
    "    badTriphones = set()\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    badDiphones = set(illegalDiphones_h)\n",
    "    badTriphones = set(illegalTriphones_h)\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} instead\".format(which_filter))\n",
    "\n",
    "def getNphones_h(hammond_row, n):\n",
    "#     strNFactors = dsToKfactors(n, hammond_row['Transcription'])\n",
    "    unstrNFactors = dsToKfactors(n, hammond_row['Transcription'])\n",
    "#     print(unstrNFactors)\n",
    "    \n",
    "#     uniqueStrNphones = set(strNFactors)\n",
    "#     uniqueUnstrNphones = set(unstrNFactors)\n",
    "    if which_stress == 'destressed':\n",
    "        return unstrNFactors\n",
    "    elif which_stress == 'stressed':\n",
    "        raise Exception('Stressed Hammond transcriptions not calculated yet.')\n",
    "    else:\n",
    "        raise Exception('Bad which_stress arg.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.192585Z",
     "start_time": "2019-03-12T03:12:42.181798Z"
    }
   },
   "outputs": [],
   "source": [
    "def badNphone(nphone, n, badNphones):\n",
    "    if n == 2:\n",
    "        return nphone in badNphones\n",
    "    if n == 3:\n",
    "        return nphone in badNphones\n",
    "    raise Exception('n must be 2 or 3.')\n",
    "\n",
    "def containsBadNphones(row, n, getNphones, badNphones):\n",
    "    phs = getNphones(row, n)\n",
    "    return any([badNphone(ph, n, badNphones) for ph in phs])\n",
    "\n",
    "def rowsWBadNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if containsBadNphones(r, n, getNphones, badNphones)]\n",
    "\n",
    "def observedBadNphones(rows, n, getNphones, badNphones):\n",
    "    return union([getNphones(r, n) for r in rows if containsBadNphones(r, n, getNphones, badNphones)])\n",
    "\n",
    "def onlyRowsWithGoodNphones(rows, n, getNphones, badNphones):\n",
    "    return [r for r in rows if not containsBadNphones(r, n, getNphones, badNphones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.422005Z",
     "start_time": "2019-03-12T03:12:42.194557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03743342892257272"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadTriphs = rowsWBadNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "len(hasBadTriphs)\n",
    "len(slim_hammond)\n",
    "len(hasBadTriphs)/len(slim_hammond) #proportion of the lexicon (by type) that has bad triphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.656004Z",
     "start_time": "2019-03-12T03:12:42.423822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.03763826300696436"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasBadDiphs = rowsWBadNphones(slim_hammond, 2, getNphones_h, badDiphones)\n",
    "len(hasBadDiphs)\n",
    "len(slim_hammond)\n",
    "len(hasBadDiphs)/len(slim_hammond) #proportion of the lexicon (by type) that has bad triphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:42.662368Z",
     "start_time": "2019-03-12T03:12:42.657853Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME see how many words there are like this and what they're like, also check sum of probability of such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.134766Z",
     "start_time": "2019-03-12T03:12:42.664985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1233"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observedBadNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "len(observedBadNphones(slim_hammond, 3, getNphones_h, badTriphones))\n",
    "# observedBadNphones(slim_hammond, 2, getNphones_h, badDiphones)\n",
    "len(observedBadNphones(slim_hammond, 2, getNphones_h, badDiphones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.573740Z",
     "start_time": "2019-03-12T03:12:43.136522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18797"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noBadTriphs_Hammond = onlyRowsWithGoodNphones(slim_hammond, 3, getNphones_h, badTriphones)\n",
    "noBadDiphsOrTriphs_Hammond = onlyRowsWithGoodNphones(noBadTriphs_Hammond, 2, getNphones_h, badDiphones)\n",
    "\n",
    "len(slim_hammond)\n",
    "len(noBadTriphs_Hammond)\n",
    "len(noBadDiphsOrTriphs_Hammond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.584896Z",
     "start_time": "2019-03-12T03:12:43.575641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noBadTriphones'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_filter\n",
    "\n",
    "if which_filter == 'hasBadTriphones':\n",
    "    hammond_filtered = slim_hammond\n",
    "elif which_filter == 'noBadTriphones':\n",
    "    hammond_filtered = noBadDiphsOrTriphs_Hammond\n",
    "else:\n",
    "    raise Exception(\"'which_filter' must be one of 'hasBadTriphones' or 'noBadTriphones'; got {0} intsead\".format(which_filter))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose wordforms for the Hammond lexicon distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we want to export a json file( = a list of dictionaries) mapping each phonological wordform to a number representing a frequency or probability or negative log probability of that phonological wordform. Accordingly, we must pick a phonological wordform representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.599010Z",
     "start_time": "2019-03-12T03:12:43.587667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'destressed'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Transcription'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_stress\n",
    "\n",
    "if which_stress == 'destressed':\n",
    "    wordform_field = 'Transcription'\n",
    "elif which_stress == 'stressed':\n",
    "#     wordform_field = 'StTrn'\n",
    "    raise Exception(\"No stressed transcription is currently supported for Hammond's newdic.\")\n",
    "else:\n",
    "    raise Exception(\"'which_stress' must be one of 'destressed' or 'stressed'; got {0} instead\".format(which_stress))\n",
    "\n",
    "wordform_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.619842Z",
     "start_time": "2019-03-12T03:12:43.600880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18295"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonologicalWords_h = [entry[wordform_field] for entry in hammond_filtered]\n",
    "len(phonologicalWords_h)\n",
    "len(set(phonologicalWords_h))\n",
    "len(phonologicalWords_h) - len(set(phonologicalWords_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate frequencies, probabilities, and informativities of phonological wordforms in Hammond's newdic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We either want a mapping from transcription to probability (by some measure) or from transcription to negative log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up probability of orthographic words in SUBTLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.628404Z",
     "start_time": "2019-03-12T03:12:43.621756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'æ.b.ɚ.ɪ.dʒ.ɪ.n.l',\n",
       " 'Orthography': 'aboriginal',\n",
       " 'Frequency': '1'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entry = hammond_filtered[55]\n",
    "test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.637315Z",
     "start_time": "2019-03-12T03:12:43.630246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthwordToSXF[test_entry['Orthography']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.641880Z",
     "start_time": "2019-03-12T03:12:43.638924Z"
    }
   },
   "outputs": [],
   "source": [
    "def getHammondOrthSFreq(hammond_orthWord):\n",
    "    if hammond_orthWord in orthwordToSXF:\n",
    "        return orthwordToSXF[hammond_orthWord]\n",
    "    return None\n",
    "#     raise Exception('Orthographic word must be in SUBTLEX!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.656223Z",
     "start_time": "2019-03-12T03:12:43.643456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_words = [entry['Orthography'] for entry in hammond_filtered]\n",
    "len(hammond_filtered)\n",
    "len(hammond_words)\n",
    "len(set(hammond_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.661868Z",
     "start_time": "2019-03-12T03:12:43.657931Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_words = set(hammond_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.684278Z",
     "start_time": "2019-03-12T03:12:43.663586Z"
    }
   },
   "outputs": [],
   "source": [
    "# This maps each orthographic word in hammond_filtered to a SUBTLEX frequency taken from IPhOD\n",
    "wordToSFreq = {w:getHammondOrthSFreq(w) for w in hammond_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:43.703597Z",
     "start_time": "2019-03-12T03:12:43.685863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16386"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2407"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definedSFwords = {w:float(wordToSFreq[w]) for w in wordToSFreq if wordToSFreq[w] != None}\n",
    "len(hammond_words)\n",
    "len(definedSFwords)\n",
    "len(hammond_words) - len(definedSFwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.595518Z",
     "start_time": "2019-03-12T03:12:43.705201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16386"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SFdefined_hammond_words = list(definedSFwords.keys())\n",
    "filtered_hammond_SFdefined = [entry for entry in hammond_filtered if entry['Orthography'] in SFdefined_hammond_words]\n",
    "len(hammond_filtered)\n",
    "len(filtered_hammond_SFdefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.601742Z",
     "start_time": "2019-03-12T03:12:47.597113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40454906.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumFreqs = sum(definedSFwords.values())\n",
    "sumFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.613455Z",
     "start_time": "2019-03-12T03:12:47.603252Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow\n",
    "wordToProb = lambda orthword: definedSFwords[orthword] / sumFreqs\n",
    "wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.619006Z",
     "start_time": "2019-03-12T03:12:47.615166Z"
    }
   },
   "outputs": [],
   "source": [
    "entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.625978Z",
     "start_time": "2019-03-12T03:12:47.620669Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_filtered_freqaligned = filtered_hammond_SFdefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-up probability of orthographic words in IPhOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPhOD's frequency estimates are better than whatever's associated with Hammond's newdic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.632996Z",
     "start_time": "2019-03-12T03:12:47.627703Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_entry = hammond_filtered[55]\n",
    "# test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.639046Z",
     "start_time": "2019-03-12T03:12:47.634647Z"
    }
   },
   "outputs": [],
   "source": [
    "# findEntryMatchingOrth(test_entry['Orthography'], lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.644831Z",
     "start_time": "2019-03-12T03:12:47.640816Z"
    }
   },
   "outputs": [],
   "source": [
    "# def getHammondOrthSFreq(hammond_orthWord):\n",
    "#     matchingEntries = findEntryMatchingOrth(hammond_orthWord, lexicon)\n",
    "#     if len(matchingEntries) == 0:\n",
    "#         return None\n",
    "#     matchingSFreq = matchingEntries[0]['SFreq'] #all entries with the same orthography have the same SUBTLEX frequency measure\n",
    "#     return matchingSFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.650004Z",
     "start_time": "2019-03-12T03:12:47.646424Z"
    }
   },
   "outputs": [],
   "source": [
    "# hammond_words = [entry['Orthography'] for entry in hammond_filtered]\n",
    "# len(hammond_filtered)\n",
    "# len(hammond_words)\n",
    "# len(set(hammond_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.657753Z",
     "start_time": "2019-03-12T03:12:47.651552Z"
    }
   },
   "outputs": [],
   "source": [
    "# hammond_words = set(hammond_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.663854Z",
     "start_time": "2019-03-12T03:12:47.659444Z"
    }
   },
   "outputs": [],
   "source": [
    "# This maps each orthographic word in hammond_filtered to a SUBTLEX frequency taken from IPhOD\n",
    "# wordToSFreq = {w:getHammondOrthSFreq(w) for w in hammond_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.669596Z",
     "start_time": "2019-03-12T03:12:47.665488Z"
    }
   },
   "outputs": [],
   "source": [
    "# wordToSFreq2 = dict()\n",
    "# total = len(hammond_words)\n",
    "# benchmarks = [1,2,3,5,10,20,30,40,50,60,70,80,90,95,96,97,98,99,100]\n",
    "# benchmarkIndices = [b/100.0 * total for b in benchmarks]\n",
    "# for i,w in enumerate(hammond_words):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to remove every word that we can't align with IPhOD to have a meaningful probability distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.675129Z",
     "start_time": "2019-03-12T03:12:47.670987Z"
    }
   },
   "outputs": [],
   "source": [
    "# definedSFwords = {w:float(wordToSFreq[w]) for w in wordToSFreq if wordToSFreq[w] != None}\n",
    "# len(hammond_words)\n",
    "# len(definedSFwords)\n",
    "# len(hammond_words) - len(definedSFwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.685786Z",
     "start_time": "2019-03-12T03:12:47.676667Z"
    }
   },
   "outputs": [],
   "source": [
    "# SFdefined_hammond_words = list(definedSFwords.keys())\n",
    "# filtered_hammond_SFdefined = [entry for entry in hammond_filtered if entry['Orthography'] in SFdefined_hammond_words]\n",
    "# len(hammond_filtered)\n",
    "# len(filtered_hammond_SFdefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.693263Z",
     "start_time": "2019-03-12T03:12:47.687461Z"
    }
   },
   "outputs": [],
   "source": [
    "# sumFreqs = sum(definedSFwords.values())\n",
    "# sumFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.700419Z",
     "start_time": "2019-03-12T03:12:47.694742Z"
    }
   },
   "outputs": [],
   "source": [
    "# from math import log, log2, pow\n",
    "# wordToProb = lambda orthword: definedSFwords[orthword] / sumFreqs\n",
    "# wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.706724Z",
     "start_time": "2019-03-12T03:12:47.702035Z"
    }
   },
   "outputs": [],
   "source": [
    "# entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "# entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.712039Z",
     "start_time": "2019-03-12T03:12:47.708204Z"
    }
   },
   "outputs": [],
   "source": [
    "#hammond_filtered_freqaligned = filtered_hammond_SFdefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up COCA unigram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.719379Z",
     "start_time": "2019-03-12T03:12:47.713450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Transcription': 'æ.b.ɚ.ɪ.dʒ.ɪ.n.l',\n",
       " 'Orthography': 'aboriginal',\n",
       " 'Frequency': '1'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entry = hammond_filtered[55]\n",
    "test_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.735765Z",
     "start_time": "2019-03-12T03:12:47.720738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammond_orth_words = [row['Orthography'] for row in hammond_filtered]\n",
    "len(hammond_orth_words)\n",
    "len(set(hammond_orth_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.768909Z",
     "start_time": "2019-03-12T03:12:47.737568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18793"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18180"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hammond_filtered)\n",
    "hammond_filtered_COCA_alignable = [row for row in hammond_filtered if row['Orthography'] in coca_unigram_counts]\n",
    "len(hammond_filtered_COCA_alignable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.816293Z",
     "start_time": "2019-03-12T03:12:47.778197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a&e', 'a&j', 'a&m', 'a&m-commerce', 'a&m-corpus']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(coca_unigram_counts.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.821552Z",
     "start_time": "2019-03-12T03:12:47.818000Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_filtered_freqaligned = hammond_filtered_COCA_alignable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.854080Z",
     "start_time": "2019-03-12T03:12:47.823058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'307,863,432.0'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammandOrthWordToCOCAFreq = Counter({w:coca_unigram_counts[w]\n",
    "                                     for w in hammond_orth_words})\n",
    "sumFreqs = sum(hammandOrthWordToCOCAFreq.values())\n",
    "'{:,}'.format( sumFreqs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.859464Z",
     "start_time": "2019-03-12T03:12:47.855981Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, log2, pow\n",
    "wordToProb = lambda orthword: hammandOrthWordToCOCAFreq[orthword] / sumFreqs\n",
    "wordToNlprob = lambda orthword: -1.0 * log2(wordToProb(orthword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.866156Z",
     "start_time": "2019-03-12T03:12:47.861013Z"
    }
   },
   "outputs": [],
   "source": [
    "entryToProb = lambda entry: wordToProb(entry['Orthography'])\n",
    "entryToNlprob = lambda entry: wordToNlprob(entry['Orthography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map each remaining phonological wordform in Hammond's newdic to a frequency/probability (marginalizing over orthographic wordforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:12:47.880139Z",
     "start_time": "2019-03-12T03:12:47.867892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transcription'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['m.æ.s.t',\n",
       " 'p.ɚ.t.n.eɪ.ʃ.ɪ.s',\n",
       " 'dʒ.ɑ.r.g.ɪ.n',\n",
       " 'p.oʊ.r.t.ə.b.l',\n",
       " 'ɪ.r.r.ɪ.ŋ',\n",
       " 't.r.ɛ.d.l',\n",
       " 'k.ə.n.k.ɚ',\n",
       " 'p.ə.dʒ.æ.m.ə.z',\n",
       " 'k.ə.n.d.ɛ.m.n.ɪ.t.oʊ.r.i',\n",
       " 'ɪ.k.s.tʃ.eɪ.n.dʒ']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform_field\n",
    "\n",
    "phonWords_h = set([entry[wordform_field] for entry in hammond_filtered_freqaligned])\n",
    "len(phonWords_h)\n",
    "list(phonWords_h)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.219009Z",
     "start_time": "2019-03-12T03:12:47.881900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17262"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['ɑ.t',\n",
       " 'l.ɪ.k.ɚ',\n",
       " 'b.ɛ.n.z.i.n',\n",
       " 'z.ɑ.r',\n",
       " 'h.j.u',\n",
       " 'g.ɑ.d',\n",
       " 's.k.ʌ.l',\n",
       " 'k.u',\n",
       " 't.ɑ.r',\n",
       " 'l.i.tʃ']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 401, 3: 36, 4: 2})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entriesMatchingPhonWord(phonWord):\n",
    "    return [entry for entry in hammond_filtered_freqaligned if entry[wordform_field] == phonWord]\n",
    "phonWord_h_To_entries = {phonWord:entriesMatchingPhonWord(phonWord) for phonWord in phonWords_h}\n",
    "\n",
    "unambiguousPhonWords = [w for w in phonWords_h if len(phonWord_h_To_entries[w]) == 1]\n",
    "ambiguousPhonWords = [w for w in phonWords_h if len(phonWord_h_To_entries[w]) > 1]\n",
    "len(phonWords_h)\n",
    "len(unambiguousPhonWords)\n",
    "len(ambiguousPhonWords)\n",
    "list(ambiguousPhonWords)[:10]\n",
    "orthWordCounts = [len(phonWord_h_To_entries[w]) for w in ambiguousPhonWords]\n",
    "Counter(orthWordCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.231123Z",
     "start_time": "2019-03-12T03:13:21.220689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Transcription': 'k.j.u', 'Orthography': 'cue', 'Frequency': '1'},\n",
       " {'Transcription': 'k.j.u', 'Orthography': 'q', 'Frequency': '7'},\n",
       " {'Transcription': 'k.j.u', 'Orthography': 'queue', 'Frequency': '0'}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1.0105129991534688e-05, 1.755973408365044e-05, 2.387422225579555e-06]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.005228630076468e-05"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.005228630076468e-05"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marginalizes over orthographic words with phonological transcription phonWord\n",
    "def phonWord_to_marginalProb(phonWord):\n",
    "    matchingEntries = phonWord_h_To_entries[phonWord]\n",
    "    pPhonWord = sum(map(entryToProb, matchingEntries))\n",
    "    return pPhonWord\n",
    "phonWord_h_To_entries['k.j.u']\n",
    "list(map(entryToProb, phonWord_h_To_entries['k.j.u']))\n",
    "sum(list(map(entryToProb, phonWord_h_To_entries['k.j.u'])))\n",
    "phonWord_to_marginalProb('k.j.u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a minimum probability threshold for inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.337916Z",
     "start_time": "2019-03-12T03:13:21.232566Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb = {padInputSequenceWithBoundaries(w):phonWord_to_marginalProb(w) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.348104Z",
     "start_time": "2019-03-12T03:13:21.339565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('⋊.m.æ.s.t.⋉', 3.1539958925683643e-06)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_h = list(phonWordToProb.items())\n",
    "len(pW_h)\n",
    "pW_h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.368440Z",
     "start_time": "2019-03-12T03:13:21.349843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('⋊.l.ɪ.v.ɚ.w.ɚ.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.g.ɑ.b.l.d.i.g.ʊ.k.⋉', 9.744580512569612e-09),\n",
       " ('⋊.f.l.ə.dʒ.ɪ.ʃ.ɪ.s.⋉', 9.744580512569612e-09),\n",
       " ('⋊.k.aʊ.n.t.ɪ.ŋ.h.aʊ.s.⋉', 9.744580512569612e-09),\n",
       " ('⋊.v.ɚ.g.j.u.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.d.ɪ.s.t.r.ʌ.k.t.ə.b.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ɪ.n.s.aɪ.f.ɚ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.v.ɪ.s.n.ɪ.dʒ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ə.k.l.ɪ.v.ə.t.i.⋉', 9.744580512569612e-09),\n",
       " ('⋊.n.ɛ.g.l.ə.ʒ.eɪ.⋉', 9.744580512569612e-09),\n",
       " ('⋊.s.t.r.æ.ŋ.g.j.ʊ.l.eɪ.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.m.ɛ.g.ə.s.aɪ.k.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.t.ɛ.n.u.ə.t.i.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ɪ.r.ɛ.f.r.ə.g.ə.b.l.⋉', 9.744580512569612e-09),\n",
       " ('⋊.ɛ.f.ɚ.v.ɛ.s.⋉', 9.744580512569612e-09),\n",
       " ('⋊.f.ɑ.n.d.u.⋉', 9.744580512569612e-09),\n",
       " ('⋊.k.w.eɪ.t.⋉', 9.744580512569612e-09),\n",
       " ('⋊.æ.k.s.ɛ.p.t.eɪ.ʃ.ɪ.n.⋉', 9.744580512569612e-09),\n",
       " ('⋊.k.oʊ.m.ɪ.ŋ.z.⋉', 9.744580512569612e-09),\n",
       " ('⋊.tʃ.ɛ.n.t.ɛ.z.ə.m.oʊ.⋉', 9.744580512569612e-09)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowToHi = sorted(pW_h, key=lambda pair: pair[1], reverse=False)\n",
    "lowToHi[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:21.387498Z",
     "start_time": "2019-03-12T03:13:21.370096Z"
    }
   },
   "outputs": [],
   "source": [
    "def trimEdges(phonWord):\n",
    "    return t2ds(ds2t(phonWord)[1:-1])\n",
    "\n",
    "def phonWordToOrthWords_Hammond(phonWord):\n",
    "    w_trimmed = trimEdges(phonWord)\n",
    "    matchingEntries = [entry for entry in hammond_filtered_freqaligned if entry[wordform_field] == w_trimmed]\n",
    "    matchingOrthWords = set([entry['Orthography'] for entry in matchingEntries])\n",
    "    return matchingOrthWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.264414Z",
     "start_time": "2019-03-12T03:13:21.389132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current minimum probability threshold: 1e-08\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 9967\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 17589\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.s.p.ɚ.m.ə.t.ə.z.oʊ.ə.n.⋉ \t {'spermatozoon'} \t 1.299277e-08\n",
      "\t⋊.ə.b.ɪ.z.m.⋉ \t {'abysm'} \t 1.299277e-08\n",
      "\t⋊.h.oʊ.r.ɑ.l.ə.dʒ.i.⋉ \t {'horology'} \t 1.299277e-08\n",
      "\t⋊.p.r.oʊ.θ.ə.l.eɪ.m.i.ə.n.⋉ \t {'prothalamion'} \t 1.299277e-08\n",
      "\t⋊.p.oʊ.l.t.⋉ \t {'poult'} \t 1.299277e-08\n",
      "\t⋊.d.ɛ.m.i.m.ɑ.n.d.eɪ.n.⋉ \t {'demimondaine'} \t 1.299277e-08\n",
      "\t⋊.ə.f.r.aɪ.t.⋉ \t {'affright'} \t 1.299277e-08\n",
      "\t⋊.f.oʊ.r.n.oʊ.⋉ \t {'foreknow'} \t 1.299277e-08\n",
      "\t⋊.ə.dʒ.ʊ.r.⋉ \t {'adjure'} \t 1.299277e-08\n",
      "\t⋊.m.eɪ.k.r.ɑ.n.⋉ \t {'macron'} \t 1.299277e-08\n",
      "\t⋊.w.aɪ.v.⋉ \t {'wive'} \t 1.299277e-08\n",
      "\t⋊.m.ɪ.z.n.⋉ \t {'mizen'} \t 1.299277e-08\n",
      "\t⋊.s.ɛ.l.v.ɪ.dʒ.⋉ \t {'selvedge'} \t 1.299277e-08\n",
      "\t⋊.h.aɪ.d.r.ɪ.s.⋉ \t {'hydrous'} \t 1.299277e-08\n",
      "\t⋊.d.aɪ.ə.p.eɪ.z.n.⋉ \t {'diapason'} \t 1.299277e-08\n",
      "\t⋊.h.æ.n.d.s.p.aɪ.k.⋉ \t {'handspike'} \t 1.299277e-08\n",
      "\t⋊.s.ɪ.ŋ.k.ə.p.eɪ.t.⋉ \t {'syncopate'} \t 1.299277e-08\n",
      "\t⋊.d.i.f.æ.l.k.eɪ.ʃ.ɪ.n.⋉ \t {'defalcation'} \t 1.299277e-08\n",
      "\t⋊.k.ɑ.l.p.oʊ.r.t.ɚ.⋉ \t {'colporteur'} \t 1.299277e-08\n",
      "\t⋊.ʌ.n.h.ɑ.r.n.ɪ.s.⋉ \t {'unharness'} \t 1.299277e-08\n",
      " \n",
      "Current minimum probability threshold: 1e-07\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 9417\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 15662\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.ə.l.ɪ.t.⋉ \t {'alit'} \t 1.006940e-07\n",
      "\t⋊.ɪ.n.t.r.æ.n.s.ɪ.t.ɪ.v.⋉ \t {'intransitive'} \t 1.006940e-07\n",
      "\t⋊.h.j.u.m.ə.n.ɪ.z.eɪ.ʃ.ɪ.n.⋉ \t {'humanization'} \t 1.006940e-07\n",
      "\t⋊.s.ɪ.k.ə.f.ə.n.t.⋉ \t {'sycophant'} \t 1.006940e-07\n",
      "\t⋊.s.ʌ.p.l.i.ə.n.t.⋉ \t {'suppliant'} \t 1.006940e-07\n",
      "\t⋊.b.ʊ.l.r.ʌ.ʃ.⋉ \t {'bulrush'} \t 1.006940e-07\n",
      "\t⋊.m.ə.z.ɚ.k.ə.⋉ \t {'mazurka'} \t 1.006940e-07\n",
      "\t⋊.k.ə.n.d.u.s.⋉ \t {'conduce'} \t 1.006940e-07\n",
      "\t⋊.h.ɚ.n.i.eɪ.ʃ.ɪ.n.⋉ \t {'herniation'} \t 1.006940e-07\n",
      "\t⋊.m.æ.l.ə.d.r.ɔɪ.t.⋉ \t {'maladroit'} \t 1.006940e-07\n",
      "\t⋊.k.æ.v.l.⋉ \t {'cavil'} \t 1.006940e-07\n",
      "\t⋊.ɪ.m.p.æ.s.ɪ.v.ə.t.i.⋉ \t {'impassivity'} \t 1.006940e-07\n",
      "\t⋊.d.aɪ.ə.f.r.æ.g.m.æ.t.ɪ.k.⋉ \t {'diaphragmatic'} \t 1.006940e-07\n",
      "\t⋊.s.æ.d.n.⋉ \t {'sadden'} \t 1.006940e-07\n",
      "\t⋊.l.ɪ.v.ə.b.l.⋉ \t {'liveable'} \t 1.006940e-07\n",
      "\t⋊.h.æ.k.l.⋉ \t {'hackle'} \t 1.006940e-07\n",
      "\t⋊.s.ɪ.s.ə.r.oʊ.n.i.⋉ \t {'cicerone'} \t 1.006940e-07\n",
      "\t⋊.ɪ.n.t.ɑ.k.s.ɪ.k.ɪ.n.t.⋉ \t {'intoxicant'} \t 1.006940e-07\n",
      "\t⋊.k.ə.l.eɪ.ʃ.ɪ.n.⋉ \t {'collation'} \t 1.006940e-07\n",
      "\t⋊.n.ʊ.r.aɪ.t.ɪ.s.⋉ \t {'neuritis'} \t 1.006940e-07\n",
      " \n",
      "Current minimum probability threshold: 5e-07\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 8463\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 12565\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.ə.l.æ.k.r.ə.t.i.⋉ \t {'alacrity'} \t 5.002218e-07\n",
      "\t⋊.p.ɚ.æ.m.ə.d.l.⋉ \t {'pyramidal'} \t 5.002218e-07\n",
      "\t⋊.k.w.aɪ.n.aɪ.n.⋉ \t {'quinine'} \t 5.002218e-07\n",
      "\t⋊.ə.f.ɪ.ʃ.ɪ.s.⋉ \t {'officious'} \t 5.002218e-07\n",
      "\t⋊.k.ə.n.t.r.aɪ.v.ə.n.s.⋉ \t {'contrivance'} \t 5.002218e-07\n",
      "\t⋊.s.ɛ.n.t.r.ə.l.aɪ.z.⋉ \t {'centralize'} \t 5.002218e-07\n",
      "\t⋊.k.æ.v.l.k.eɪ.d.⋉ \t {'cavalcade'} \t 5.002218e-07\n",
      "\t⋊.ɪ.n.k.ɑ.n.t.n.ɪ.n.t.⋉ \t {'incontinent'} \t 5.002218e-07\n",
      "\t⋊.t.æ.k.t.ɪ.ʃ.ɪ.n.⋉ \t {'tactician'} \t 5.002218e-07\n",
      "\t⋊.ɪ.n.f.ɚ.m.ə.t.i.⋉ \t {'infirmity'} \t 5.002218e-07\n",
      "\t⋊.m.ə.r.i.n.oʊ.⋉ \t {'merino'} \t 5.002218e-07\n",
      "\t⋊.d.aɪ.ɚ.ə.s.t.⋉ \t {'diarist'} \t 5.002218e-07\n",
      "\t⋊.k.r.aʊ.t.⋉ \t {'kraut'} \t 5.002218e-07\n",
      "\t⋊.m.oʊ.l.t.⋉ \t {'molt'} \t 5.002218e-07\n",
      "\t⋊.p.ɑ.p.j.ʊ.l.ɚ.aɪ.z.⋉ \t {'popularize'} \t 5.002218e-07\n",
      "\t⋊.m.æ.g.p.aɪ.⋉ \t {'magpie'} \t 5.002218e-07\n",
      "\t⋊.p.ə.t.ɛ.n.tʃ.i.æ.l.ɪ.t.i.⋉ \t {'potentiality'} \t 5.002218e-07\n",
      "\t⋊.æ.p.r.ə.b.eɪ.ʃ.ɪ.n.⋉ \t {'approbation'} \t 5.002218e-07\n",
      "\t⋊.ɪ.n.k.w.ɪ.z.ɪ.t.ɚ.⋉ \t {'inquisitor'} \t 5.002218e-07\n",
      "\t⋊.m.æ.d.h.aʊ.s.⋉ \t {'madhouse'} \t 5.002218e-07\n",
      " \n",
      "Current minimum probability threshold: 1e-06\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 7791\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 10633\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.s.ʊ.p.aɪ.n.⋉ \t {'supine'} \t 1.000444e-06\n",
      "\t⋊.k.l.oʊ.ð.⋉ \t {'clothe'} \t 1.000444e-06\n",
      "\t⋊.r.ɛ.t.r.oʊ.g.r.eɪ.d.⋉ \t {'retrograde'} \t 1.000444e-06\n",
      "\t⋊.t.r.ɪ.p.t.ɪ.k.⋉ \t {'triptych'} \t 1.000444e-06\n",
      "\t⋊.æ.m.ɪ.k.ə.b.l.⋉ \t {'amicable'} \t 1.000444e-06\n",
      "\t⋊.v.ə.v.eɪ.ʃ.ɪ.s.⋉ \t {'vivacious'} \t 1.000444e-06\n",
      "\t⋊.w.ɑ.l.ə.p.⋉ \t {'wallop'} \t 1.000444e-06\n",
      "\t⋊.ɪ.n.t.r.ə.m.j.ʊ.r.l.⋉ \t {'intramural'} \t 1.000444e-06\n",
      "\t⋊.k.ɑ.m.p.oʊ.t.⋉ \t {'compote'} \t 1.000444e-06\n",
      "\t⋊.ə.g.eɪ.p.⋉ \t {'agape'} \t 1.000444e-06\n",
      "\t⋊.g.r.ɪ.s.t.⋉ \t {'grist'} \t 1.000444e-06\n",
      "\t⋊.p.ɪ.k.ɪ.ŋ.z.⋉ \t {'pickings'} \t 1.000444e-06\n",
      "\t⋊.d.ɛ.v.ə.s.t.eɪ.t.⋉ \t {'devastate'} \t 1.000444e-06\n",
      "\t⋊.k.ɑ.n.f.ə.s.k.eɪ.t.⋉ \t {'confiscate'} \t 1.000444e-06\n",
      "\t⋊.r.ɪ.v.oʊ.k.⋉ \t {'revoke'} \t 1.000444e-06\n",
      "\t⋊.ɪ.m.p.r.ɑ.b.ə.b.l.i.⋉ \t {'improbably'} \t 1.000444e-06\n",
      "\t⋊.d.ɪ.m.ə.n.u.ʃ.ɪ.n.⋉ \t {'diminution'} \t 1.000444e-06\n",
      "\t⋊.ʌ.n.ɪ.n.h.ɪ.b.ə.t.ɪ.d.⋉ \t {'uninhibited'} \t 1.003692e-06\n",
      "\t⋊.ə.s.ɪ.dʒ.ʊ.ə.s.l.i.⋉ \t {'assiduously'} \t 1.003692e-06\n",
      "\t⋊.v.ɚ.m.l.⋉ \t {'vermeil'} \t 1.003692e-06\n",
      " \n",
      "Current minimum probability threshold: 1e-05\n",
      "\t|triphones| before vs. after cut-off: 9999 vs. 4883\n",
      "\t|Ws| before vs. after cut-off: 17701 vs. 4221\n",
      "Below are the 20 least probable phonological wordforms with probability above the threshold:\n",
      "\t⋊.s.ɪ.g.ɑ.r.⋉ \t {'cigar'} \t 1.000119e-05\n",
      "\t⋊.g.r.i.d.⋉ \t {'greed'} \t 1.000768e-05\n",
      "\t⋊.p.ɚ.ɪ.m.ə.t.ɚ.⋉ \t {'perimeter'} \t 1.000768e-05\n",
      "\t⋊.tʃ.ɛ.s.⋉ \t {'chess'} \t 1.002393e-05\n",
      "\t⋊.s.t.aɪ.n.⋉ \t {'stein'} \t 1.003042e-05\n",
      "\t⋊.ɪ.n.k.ʌ.m.ɪ.ŋ.⋉ \t {'incoming'} \t 1.003367e-05\n",
      "\t⋊.s.ʌ.b.ɚ.b.⋉ \t {'suburb'} \t 1.003367e-05\n",
      "\t⋊.s.u.ɪ.dʒ.⋉ \t {'sewage'} \t 1.003367e-05\n",
      "\t⋊.ə.s.ɚ.t.⋉ \t {'assert'} \t 1.004017e-05\n",
      "\t⋊.r.ɛ.k.⋉ \t {'wreck'} \t 1.004666e-05\n",
      "\t⋊.n.ɛ.k.l.ə.s.⋉ \t {'necklace'} \t 1.005316e-05\n",
      "\t⋊.d.ɪ.s.k.ɑ.r.d.⋉ \t {'discard', 'discord'} \t 1.006290e-05\n",
      "\t⋊.n.ɔɪ.z.i.⋉ \t {'noisy'} \t 1.006290e-05\n",
      "\t⋊.d.ɪ.f.ɚ.ɛ.n.tʃ.l.⋉ \t {'differential'} \t 1.006940e-05\n",
      "\t⋊.w.ɑ.n.d.ɚ.⋉ \t {'wander'} \t 1.007265e-05\n",
      "\t⋊.f.oʊ.r.θ.k.ʌ.m.ɪ.ŋ.⋉ \t {'forthcoming'} \t 1.009214e-05\n",
      "\t⋊.ə.p.ɑ.r.t.eɪ.t.⋉ \t {'apartheid'} \t 1.010188e-05\n",
      "\t⋊.t.r.ə.m.æ.t.ɪ.k.⋉ \t {'traumatic'} \t 1.010513e-05\n",
      "\t⋊.r.ɑ.t.⋉ \t {'rot', 'wrought'} \t 1.011487e-05\n",
      "\t⋊.æ.m.ə.z.ɑ.n.⋉ \t {'amazon'} \t 1.011487e-05\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def readableEntry(phonWordProbPair):\n",
    "    print('\\t{0} \\t {1} \\t {2:4e}'.format(phonWordProbPair[0], phonWordToOrthWords_Hammond(phonWordProbPair[0]), phonWordProbPair[1]))\n",
    "\n",
    "lowToHi_Ws = [pair[0] for pair in lowToHi]\n",
    "lowToHi_triphones = lexiconTo3factors(lowToHi_Ws)\n",
    "\n",
    "minProbThresholds = [1.0e-08, 1.0e-07, 5.0e-07, 1.0e-06, 1.0e-05]\n",
    "for minProb in minProbThresholds:\n",
    "    lowToHi_aboveThreshold = [each for each in lowToHi if each[1] >= minProb]\n",
    "    lowToHi_aT_triphones = lexiconTo3factors([pair[0] for pair in lowToHi_aboveThreshold])\n",
    "    \n",
    "    print('Current minimum probability threshold: {0}'.format(minProb))\n",
    "    print('\\t|triphones| before vs. after cut-off: {0} vs. {1}'.format(len(lowToHi_triphones), len(lowToHi_aT_triphones)))\n",
    "    print('\\t|Ws| before vs. after cut-off: {0} vs. {1}'.format(len(lowToHi), len(lowToHi_aboveThreshold)))\n",
    "    print('Below are the 20 least probable phonological wordforms with probability above the threshold:')\n",
    "    for entry in lowToHi_aboveThreshold[:20]:\n",
    "#         print(entry)\n",
    "        readableEntry(entry)\n",
    "#     dict(lowToHi_aboveThreshold[:20])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5e-07` seems like a reasonable compromise between not having too many weird words that will skew recognition probabilities and not losing too many wordforms or triphones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.297606Z",
     "start_time": "2019-03-12T03:13:35.266228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990899341367702"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability mass of words lost due to thresholding = 0.0009100658632298364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minProb = 5e-07\n",
    "lowToHi_aboveThreshold = [each for each in lowToHi if each[1] >= minProb]\n",
    "thresholdedLexicon = Counter(dict(lowToHi_aboveThreshold))\n",
    "sumFreqs = sum(thresholdedLexicon.values())\n",
    "sumFreqs\n",
    "print('Probability mass of words lost due to thresholding = {0}'.format(1.0 - sumFreqs))\n",
    "\n",
    "thresholdedLexicon = {k:thresholdedLexicon[k]/sumFreqs for k in thresholdedLexicon}\n",
    "sum(thresholdedLexicon.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.305087Z",
     "start_time": "2019-03-12T03:13:35.299593Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWord_to_marginalProb = lambda w: thresholdedLexicon[padInputSequenceWithBoundaries(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.369366Z",
     "start_time": "2019-03-12T03:13:35.307962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17701"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12565"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12565"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phonWords_h)\n",
    "len(thresholdedLexicon)\n",
    "phonWords_h = {w for w in phonWords_h if padInputSequenceWithBoundaries(w) in thresholdedLexicon}\n",
    "len(phonWords_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define representation for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.373900Z",
     "start_time": "2019-03-12T03:13:35.371516Z"
    }
   },
   "outputs": [],
   "source": [
    "padWordsWithBoundaries = True\n",
    "# padWordsWithBoundaries = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.453285Z",
     "start_time": "2019-03-12T03:13:35.375788Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToProb = {padInputSequenceWithBoundaries(w):phonWord_to_marginalProb(w) for w in phonWords_h}\n",
    "else:\n",
    "    phonWordToProb = {w:phonWord_to_marginalProb(w) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.535785Z",
     "start_time": "2019-03-12T03:13:35.455306Z"
    }
   },
   "outputs": [],
   "source": [
    "if padWordsWithBoundaries:\n",
    "    phonWordToNlProb = {padInputSequenceWithBoundaries(w):-1 * log2(phonWord_to_marginalProb(w)) for w in phonWords_h}\n",
    "else:\n",
    "    phonWordToNlProb = {w:-1 * log2(phonWord_to_marginalProb(w)) for w in phonWords_h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.544867Z",
     "start_time": "2019-03-12T03:13:35.537769Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isNormalized(phonWordToProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.551628Z",
     "start_time": "2019-03-12T03:13:35.547032Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.558378Z",
     "start_time": "2019-03-12T03:13:35.553525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.738866Z",
     "start_time": "2019-03-12T03:13:35.559990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pYX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 pYX.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.744902Z",
     "start_time": "2019-03-12T03:13:35.740751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = hammond_fn[:-4]\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.751942Z",
     "start_time": "2019-03-12T03:13:35.746933Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.758537Z",
     "start_time": "2019-03-12T03:13:35.754476Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.766944Z",
     "start_time": "2019-03-12T03:13:35.760447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.894349Z",
     "start_time": "2019-03-12T03:13:35.768547Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_prob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:35.900348Z",
     "start_time": "2019-03-12T03:13:35.896339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.019658Z",
     "start_time": "2019-03-12T03:13:35.902187Z"
    }
   },
   "outputs": [],
   "source": [
    "with codecs.open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(phonWordToNlProb, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.029417Z",
     "start_time": "2019-03-12T03:13:36.021660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.190831Z",
     "start_time": "2019-03-12T03:13:36.031455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pYX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 pYX.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.198075Z",
     "start_time": "2019-03-12T03:13:36.193013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.358775Z",
     "start_time": "2019-03-12T03:13:36.199903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hammond-aligned_destressed gate3 trials.csv'\r\n",
      "'Hammond-aligned_destressed gate6 trials.csv'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0.001 pYX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f3_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 f6_Y0Y1_X0X1.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p3YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y01X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y0X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6Y1X01.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 p6YX.json'\r\n",
      "'Hammond-aligned_destressed_pseudocount0 pYX.json'\r\n",
      "'Hammond-aligned_destressed response diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed response diphones.txt'\r\n",
      "'Hammond-aligned_destressed response illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed response uniphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_destressed stimuli uniphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based constructible triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphone-based illegal triphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli illegal diphones.txt'\r\n",
      "'Hammond-aligned_stressed stimuli uniphones.txt'\r\n",
      " Hammond_aligned_trials.csv\r\n",
      " Hammond_newdic_IPA_aligned.csv\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n",
      " Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json\r\n",
      " Hammond_newdic_IPA_aligned_destressedTriphones.txt\r\n",
      " Hammond_newdic_IPA.csv\r\n",
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.365456Z",
     "start_time": "2019-03-12T03:13:36.361070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem = hammond_fn[:-4] #'Hammond_newdic_IPA_aligned'\n",
    "my_filename_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.370276Z",
     "start_time": "2019-03-12T03:13:36.367433Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_filter = '_' + which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.376006Z",
     "start_time": "2019-03-12T03:13:36.371841Z"
    }
   },
   "outputs": [],
   "source": [
    "my_fn_suffix_prob = '_phonWordToProb'\n",
    "my_fn_suffix_nlprob = '_phonWordToNlprob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.381456Z",
     "start_time": "2019-03-12T03:13:36.377634Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.388595Z",
     "start_time": "2019-03-12T03:13:36.383014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToProb.json'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.414041Z",
     "start_time": "2019-03-12T03:13:36.390216Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToProb_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter + my_fn_suffix_prob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToProb_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.419123Z",
     "start_time": "2019-03-12T03:13:36.415769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.578872Z",
     "start_time": "2019-03-12T03:13:36.420679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammond_newdic_IPA_aligned_destressed_noBadTriphones_phonWordToNlprob.json\r\n"
     ]
    }
   ],
   "source": [
    "ls Hammond_newdic*prob*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.602078Z",
     "start_time": "2019-03-12T03:13:36.580804Z"
    }
   },
   "outputs": [],
   "source": [
    "phonWordToNlprob_in = None\n",
    "with open(my_filename_stem + my_fn_suffix_filter +  my_fn_suffix_nlprob + '.json', encoding='utf-8') as data_file:\n",
    "   phonWordToNlprob_in = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.608519Z",
     "start_time": "2019-03-12T03:13:36.603769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⋊.m.æ.s.t.⋉'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = list(phonWordToProb_in.keys())[0]\n",
    "test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.617335Z",
     "start_time": "2019-03-12T03:13:36.610100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1568688511444843e-06"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.1568688511444843e-06"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToProb_in[test_k]\n",
    "phonWordToProb[test_k]\n",
    "phonWordToProb_in[test_k] == phonWordToProb[test_k]\n",
    "assert(phonWordToProb_in[test_k] == phonWordToProb[test_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T03:13:36.625921Z",
     "start_time": "2019-03-12T03:13:36.619089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.27307424270357"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18.973341668920227"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonWordToNlprob_in[test_k]\n",
    "phonWordToNlprob[test_k]\n",
    "phonWordToNlprob_in[test_k] == phonWordToNlProb[test_k]\n",
    "assert(phonWordToNlprob_in[test_k] == phonWordToNlProb[test_k])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "134px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
