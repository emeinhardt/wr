{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:05.231640Z",
     "start_time": "2019-05-26T18:25:05.228362Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eric Meinhardt / emeinhardt@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-loading-data\" data-toc-modified-id=\"Imports-/-loading-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Language-model\" data-toc-modified-id=\"Language-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Language model</a></span></li><li><span><a href=\"#Contexts\" data-toc-modified-id=\"Contexts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Contexts</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vocabulary</a></span></li></ul></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li><li><span><a href=\"#Calculate-the-number-of-computations-+-estimate-required-space\" data-toc-modified-id=\"Calculate-the-number-of-computations-+-estimate-required-space-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate the number of computations + estimate required space</a></span></li><li><span><a href=\"#Pick-out-relevant-functions-for-mapping-between-context/word-and-index\" data-toc-modified-id=\"Pick-out-relevant-functions-for-mapping-between-context/word-and-index-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Pick out relevant functions for mapping between context/word and index</a></span></li><li><span><a href=\"#Construct-and-write-distributions\" data-toc-modified-id=\"Construct-and-write-distributions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Construct and write distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Doing-calculations-in-place-/-via-memory-mapped-arrays\" data-toc-modified-id=\"Doing-calculations-in-place-/-via-memory-mapped-arrays-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Doing calculations in-place / via memory mapped arrays</a></span></li><li><span><a href=\"#Doing-calculations-in-memory-and-writing-to-disk\" data-toc-modified-id=\"Doing-calculations-in-memory-and-writing-to-disk-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Doing calculations in memory and writing to disk</a></span></li><li><span><a href=\"#Creating-a-version-that-contains-probabilities-(and-is-normalized)\" data-toc-modified-id=\"Creating-a-version-that-contains-probabilities-(and-is-normalized)-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Creating a version that contains probabilities (and is normalized)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sketch-of-normalization-process\" data-toc-modified-id=\"Sketch-of-normalization-process-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Sketch of normalization process</a></span></li></ul></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a file path $m$ to a `.arpa` file (or, more realistically/practically, a `kenlm` memory mapped version of one) for a language model\n",
    " - a file path $c$ to a set of $n$-gram contexts $C$ (a `.txt` file with one context per line, where a context is sequence of space-separated wordforms)\n",
    " - a file path $v$ to a vocabulary $W$ (a `.txt` file with one wordform per line)\n",
    " - a filepath $o$ for the main output of the notebook\n",
    " \n",
    "this notebook will calculate the distribution $p(W|C)$ as a memory mapped `numpy` array (written to $o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `kenlm`\n",
    " - `numpy`\n",
    " - `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:10.144920Z",
     "start_time": "2019-05-26T18:25:10.142071Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:10.295192Z",
     "start_time": "2019-05-26T18:25:10.292909Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:23.048084Z",
     "start_time": "2019-05-26T18:25:23.044507Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# m = ''\n",
    "m = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_utterances_main_4gram.mmap'\n",
    "\n",
    "# c = ''\n",
    "c = '/home/AD/emeinhar/buckeye-lm' + '/' + 'buckeye_contexts.txt'\n",
    "\n",
    "# v = ''\n",
    "v = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_vocabulary_main.txt'\n",
    "\n",
    "# o = ''\n",
    "o = '/home/AD/emeinhar/wr' + '/' + 'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:23.647787Z",
     "start_time": "2019-05-26T18:25:23.644010Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = path.dirname(o)\n",
    "if not path.exists(output_dir):\n",
    "    print('Making ' + output_dir)\n",
    "    makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:24.770964Z",
     "start_time": "2019-05-26T18:25:24.716126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/buckeye_contexts.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyfile(c, path.join(output_dir, path.basename(c)))\n",
    "copyfile(v, path.join(output_dir, path.basename(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:25.288818Z",
     "start_time": "2019-05-26T18:25:25.285235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:25.697091Z",
     "start_time": "2019-05-26T18:25:25.692461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'boilerplate.py',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " '2a alignment_paths_and_cmds.sh',\n",
       " 'LM_Fisher',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports / loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.596188Z",
     "start_time": "2019-05-26T18:25:28.355770Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.954198Z",
     "start_time": "2019-05-26T18:25:30.598363Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.958430Z",
     "start_time": "2019-05-26T18:25:30.955526Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = 30\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.969981Z",
     "start_time": "2019-05-26T18:25:30.959842Z"
    }
   },
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.986687Z",
     "start_time": "2019-05-26T18:25:30.971321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('a',\n",
       " \"aaron's\",\n",
       " 'ability',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'accommodate')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "with open(c) as file:\n",
    "    for line in file:\n",
    "        contexts.append(line.rstrip())\n",
    "contexts = tuple(contexts)\n",
    "len(contexts)\n",
    "contexts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:30.990735Z",
     "start_time": "2019-05-26T18:25:30.987754Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(contexts)) == len(contexts), \"Contexts must consist of unique strings.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:31.024324Z",
     "start_time": "2019-05-26T18:25:30.991764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(\"'and\",\n",
       " \"'berserkly'\",\n",
       " \"'bout\",\n",
       " \"'burb\",\n",
       " \"'burban\",\n",
       " \"'burbs\",\n",
       " \"'cau\",\n",
       " \"'cause\",\n",
       " \"'cept\",\n",
       " \"'cide\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "with open(v) as file:\n",
    "    for line in file:\n",
    "        vocabulary.append(line.rstrip())\n",
    "vocabulary = tuple(vocabulary)\n",
    "len(vocabulary)\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:31.066004Z",
     "start_time": "2019-05-26T18:25:31.057819Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(vocabulary)) == len(vocabulary), \"Vocabulary must consist of unique wordforms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:32.759094Z",
     "start_time": "2019-05-26T18:25:32.756823Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:32.889986Z",
     "start_time": "2019-05-26T18:25:32.886357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grabbed'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'geraldo'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt = choice(contexts)\n",
    "ctxt\n",
    "\n",
    "wrd = choice(vocabulary)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:33.010539Z",
     "start_time": "2019-05-26T18:25:32.991595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.022207260131836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-11.116127967834473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.684236526489258, 2, False),\n",
       " (-0.21483998000621796, 3, False),\n",
       " (-1.1899677515029907, 4, False),\n",
       " (-4.933162689208984, 2, False),\n",
       " (-0.45743539929389954, 3, False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.47964234650135"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.610599994659424, 1, False),\n",
       " (-1.2509719133377075, 2, False),\n",
       " (-1.429597020149231, 3, False),\n",
       " (-4.933162689208984, 2, False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-10.224331617355347"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"this is a sentence\", eos = False)\n",
    "model.score(\"this is a sentence\")\n",
    "model.score(\"this is a sentence\", eos = True)\n",
    "model.score(\"this is a sentence </s>\", eos=False)\n",
    "model.score(\"this is a sentence </s>\")\n",
    "tuple(model.full_scores(\"this is a sentence\"))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\"))))\n",
    "' '\n",
    "tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:34.574495Z",
     "start_time": "2019-05-26T18:25:34.572348Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log10, log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:34.729433Z",
     "start_time": "2019-05-26T18:25:34.724197Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(word, context, base2=True, surprisal=True):\n",
    "    score_infos = tuple(model.full_scores(context + ' ' + word, eos=False, bos=False))\n",
    "    key_score_log10 = score_infos[-1][0]\n",
    "    if base2:\n",
    "        key_score = key_score_log10 / log10(2)\n",
    "    if surprisal:\n",
    "        key_score = -1.0 * key_score\n",
    "    return key_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:35.116635Z",
     "start_time": "2019-05-26T18:25:35.109236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grabbed'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'geraldo'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.30301092009873"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt\n",
    "wrd\n",
    "score(wrd, ctxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of computations + estimate required space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:36.798189Z",
     "start_time": "2019-05-26T18:25:36.789834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'767,374,560'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'6.13899648 GB'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits_per_cell = 64\n",
    "bytes_per_cell = bits_per_cell / 8\n",
    "\n",
    "len(contexts)\n",
    "len(vocabulary)\n",
    "\"{:,}\".format( len(contexts) * len(vocabulary) )\n",
    "\"{:,} GB\".format( len(contexts) * len(vocabulary) * bytes_per_cell / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.398Z"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.401Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = product(contexts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.404Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = tuple(product(contexts, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.408Z"
    }
   },
   "outputs": [],
   "source": [
    "# from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.411Z"
    }
   },
   "outputs": [],
   "source": [
    "# example_computations = choices(computations, k=10)\n",
    "# example_computations\n",
    "# ex = choice(example_computations)\n",
    "# ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick out relevant functions for mapping between context/word and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:40.127599Z",
     "start_time": "2019-05-26T18:25:40.119220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geraldo'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16024"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'grabbed'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd\n",
    "vocabulary.index(wrd)\n",
    "ctxt\n",
    "contexts.index(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.418Z"
    }
   },
   "outputs": [],
   "source": [
    "# ex\n",
    "# contexts.index(ex[0])\n",
    "# contexts[ contexts.index(ex[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:46.388133Z",
     "start_time": "2019-05-26T18:25:46.384608Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_np(word_idx, context_idx, base2=True, surprisal=True):\n",
    "    return score(vocabulary[word_idx], contexts[context_idx], base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and write distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:48.175442Z",
     "start_time": "2019-05-26T18:25:48.169866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "767374560"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(vocabulary)\n",
    "num_contexts = len(contexts)\n",
    "my_shape = (num_words, num_contexts) #columns are distributions\n",
    "my_shape\n",
    "num_words * num_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:50.117171Z",
     "start_time": "2019-05-26T18:25:50.114341Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_map = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in-place / via memory mapped arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will only begin to make sense if the array is going to be too large to fit in memory or very small; otherwise joblib will use threads to parallelize the computation (because it involves lots of IO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:50.500429Z",
     "start_time": "2019-05-26T18:25:50.496848Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:52.776624Z",
     "start_time": "2019-05-26T18:25:52.774677Z"
    }
   },
   "outputs": [],
   "source": [
    "score_np_vec = np.vectorize(score_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:56.141720Z",
     "start_time": "2019-05-26T18:25:56.138973Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    def define_score(word_idx, context_idx, base2=True, surprisal=True):\n",
    "        hVC[word_idx, context_idx] = score_np(word_idx, context_idx, base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:25:56.793731Z",
     "start_time": "2019-05-26T18:25:56.790966Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Started calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:26:00.983686Z",
     "start_time": "2019-05-26T18:26:00.981290Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    # est 7h on wittgenstein with J=30 and other computations going on in the background\n",
    "    # pretty sure that means it's using threads rather than processes\n",
    "    par(delayed(define_score)(w_idx, ctxt_idx) \n",
    "        for ctxt_idx in range(num_contexts) \n",
    "        for w_idx in range(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:26:02.087570Z",
     "start_time": "2019-05-26T18:26:02.085232Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Ended calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in memory and writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:28:49.606729Z",
     "start_time": "2019-05-26T18:26:07.740888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend MultiprocessingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.1436s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.1436s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=30)]: Done   4 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=30)]: Done  13 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=30)]: Done  24 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=30)]: Done  52 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=30)]: Done 104 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=30)]: Done 156 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=30)]: Done 216 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=30)]: Done 276 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=30)]: Done 344 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=30)]: Done 412 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=30)]: Done 488 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=30)]: Done 564 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=30)]: Done 648 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=30)]: Done 732 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=30)]: Done 824 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=30)]: Done 916 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=30)]: Done 1016 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=30)]: Done 1116 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=30)]: Done 1224 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=30)]: Done 1332 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=30)]: Done 1448 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=30)]: Done 1564 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=30)]: Done 1688 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=30)]: Done 1812 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=30)]: Done 1944 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=30)]: Done 2076 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=30)]: Done 2216 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=30)]: Done 2356 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=30)]: Done 2504 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=30)]: Done 2652 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=30)]: Done 2808 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=30)]: Done 2964 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=30)]: Done 3128 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=30)]: Done 3292 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=30)]: Done 3464 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=30)]: Done 3636 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=30)]: Done 3816 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=30)]: Done 3996 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=30)]: Done 4184 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=30)]: Done 4372 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=30)]: Done 4568 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=30)]: Done 4764 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=30)]: Done 4968 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=30)]: Done 5172 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=30)]: Done 5384 tasks      | elapsed:   49.3s\n",
      "[Parallel(n_jobs=30)]: Done 5596 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=30)]: Done 5816 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=30)]: Done 6036 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=30)]: Done 6264 tasks      | elapsed:   57.2s\n",
      "[Parallel(n_jobs=30)]: Done 6492 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=30)]: Done 6728 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=30)]: Done 6964 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=30)]: Done 7208 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=30)]: Done 7452 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=30)]: Done 7704 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=30)]: Done 7956 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=30)]: Done 8216 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=30)]: Done 8476 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=30)]: Done 8744 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=30)]: Done 9012 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=30)]: Done 9288 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=30)]: Done 9564 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=30)]: Done 9848 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=30)]: Done 10132 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=30)]: Done 10424 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=30)]: Done 10716 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=30)]: Done 11016 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=30)]: Done 11316 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=30)]: Done 11624 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=30)]: Done 11932 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=30)]: Done 12248 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=30)]: Done 12564 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=30)]: Done 12888 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=30)]: Done 13212 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=30)]: Done 13544 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=30)]: Done 13876 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=30)]: Done 14216 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=30)]: Done 14556 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=30)]: Done 14904 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=30)]: Done 15252 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=30)]: Done 15608 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=30)]: Done 15964 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=30)]: Done 16328 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=30)]: Done 16692 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=30)]: Done 17064 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=30)]: Done 17356 out of 17415 | elapsed:  2.7min remaining:    0.5s\n",
      "[Parallel(n_jobs=30)]: Done 17415 out of 17415 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #~30m on wittgenstein\n",
    "#     hVC = np.vstack([score_np_vec(np.array(range(num_words)), c_idx) for c_idx in tqdm(range(num_contexts))]).T\n",
    "    \n",
    "    #takes ~2.75m on wittgenstein with J=30 and other things going on in the background\n",
    "    hVC = np.vstack(par(delayed(score_np_vec)(np.array(range(num_words)), c_idx)\n",
    "                        for c_idx in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:47:20.737081Z",
     "start_time": "2019-05-26T18:46:03.941407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numpy.save(file, arr, allow_pickle=True, fix_imports=True)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype\n",
    "    hVC_on_disk = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    \n",
    "    #takes ~1.25m on wittgenstein\n",
    "    hVC_on_disk[:,:] = hVC\n",
    "    hVC = hVC_on_disk\n",
    "    del hVC_on_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:47:20.741611Z",
     "start_time": "2019-05-26T18:47:20.738828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:47:21.637029Z",
     "start_time": "2019-05-26T18:47:20.742652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:47:22.142053Z",
     "start_time": "2019-05-26T18:47:21.639486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.nbytes / 1e9\n",
    "hVC.shape\n",
    "hVC.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a version that contains probabilities (and is normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just naively convert (-) log-probabilities to probabilities if we're interested in distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:47:36.968276Z",
     "start_time": "2019-05-26T18:47:24.843029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98952166, 0.98384789, 0.86949554, ..., 0.93416071, 0.96380412,\n",
       "       0.86971511])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB! assumes hVC is in base 2 surprisals...\n",
    "dist_norms = np.sum(np.exp2(-1.0 * hVC), axis=0)\n",
    "dist_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:08.166698Z",
     "start_time": "2019-05-26T18:51:08.163696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_context = choice(contexts)\n",
    "random_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:08.649934Z",
     "start_time": "2019-05-26T18:51:08.645942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7121"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts.index(random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:12.639303Z",
     "start_time": "2019-05-26T18:51:09.326285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend MultiprocessingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.0031s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=30)]: Done  12 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  53 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.0299s.) Setting batch_size=1712.\n",
      "[Parallel(n_jobs=30)]: Done 1084 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done 44064 out of 44064 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "hW_rc = np.array(par(delayed(score)(w, random_context) for w in vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:14.226400Z",
     "start_time": "2019-05-26T18:51:14.219762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal( hVC[:,contexts.index(random_context)], hW_rc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:15.379273Z",
     "start_time": "2019-05-26T18:51:15.372598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.81000085e-08, 7.81000085e-08, 3.09774365e-06, 1.09451992e-07,\n",
       "       7.81000085e-08, 7.81000085e-08, 7.81000085e-08, 1.97049012e-04,\n",
       "       3.42848590e-07, 7.81000085e-08])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9849723253583984"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_rc = np.exp2(-1.0 * hW_rc)\n",
    "pW_rc[:10]\n",
    "np.sum(pW_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:21.186556Z",
     "start_time": "2019-05-26T18:51:21.183750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:23.558264Z",
     "start_time": "2019-05-26T18:51:23.452951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𝛆 = 1e-16\n",
      "n = 44064\n",
      "max(λᵢ) = λᵦ = -1.4649360837523755\n",
      "𝚹 = -68.57816236233276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "my_logprobs = -1.0 * hW_rc\n",
    "my_epsilon = 10 ** (-1.0 * 16)\n",
    "print('𝛆 = {0}'.format( my_epsilon ))\n",
    "\n",
    "my_n = my_logprobs.shape[0]\n",
    "print('n = {0}'.format( my_n ))\n",
    "\n",
    "my_max = np.max(my_logprobs)\n",
    "print('max(λᵢ) = λᵦ = {0}'.format( my_max ))\n",
    "\n",
    "my_threshold = np.log2(my_epsilon) - np.log2(my_n)\n",
    "print('𝚹 = {0}'.format( my_threshold ))\n",
    "\n",
    "mask = my_logprobs - my_max >= my_threshold\n",
    "np.sum(mask)\n",
    "to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "my_alphas = np.array([to_alpha(l) for l in my_logprobs])\n",
    "assert np.array_equal(my_alphas, to_alpha_vec(my_logprobs))\n",
    "my_alpha_norm = np.sum(my_alphas)\n",
    "my_probs = my_alphas / my_alpha_norm\n",
    "np.sum(my_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:28.652945Z",
     "start_time": "2019-05-26T18:51:28.640124Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "def normalize_logprobs(logprobs, d=16, axis=0, b=None):\n",
    "# def normalize_logprobs(logprobs, d=16, b=None):\n",
    "#     axis = 0\n",
    "    n = logprobs.shape[axis]\n",
    "    epsilon = 10**(-1.0 * d)\n",
    "    maxlogp = np.max( logprobs[axis] )\n",
    "    if b is None:\n",
    "        threshold = np.log(epsilon) - np.log(n)\n",
    "        to_alpha = lambda logprob: np.exp(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 2:\n",
    "        threshold = np.log2(epsilon) - np.log2(n)\n",
    "        to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 10:\n",
    "        threshold = np.log10(epsilon) - np.log10(n)\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, 10) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    else:\n",
    "        threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, b) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "#     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "    alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "    probs = alpha_is / alpha_norm\n",
    "#     assert np.isclose(np.sum(probs), 1.0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:30.956823Z",
     "start_time": "2019-05-26T18:51:30.950186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.92915765e-08, 7.92915765e-08, 3.14500577e-06, ...,\n",
       "       7.92915765e-08, 1.11121896e-07, 7.92915765e-08])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hW_rc, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:38.650832Z",
     "start_time": "2019-05-26T18:51:38.633826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.46539838e-07, 2.46539838e-07, 1.45738607e-05, ...,\n",
       "       2.46539838e-07, 3.45509264e-07, 2.46539838e-07])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hVC[:,0], b=2)\n",
    "normalize_logprobs(-1.0 * hVC[:,0], b=2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:52:17.167216Z",
     "start_time": "2019-05-26T18:52:17.163170Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    pVC = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:51:50.882074Z",
     "start_time": "2019-05-26T18:51:50.879908Z"
    }
   },
   "outputs": [],
   "source": [
    "# if memory_map:\n",
    "#     for j in range(my_shape[1]):\n",
    "#         pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:53:22.055733Z",
     "start_time": "2019-05-26T18:53:22.052234Z"
    }
   },
   "outputs": [],
   "source": [
    "def normColumn(j):\n",
    "    pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)\n",
    "\n",
    "if memory_map:\n",
    "    # takes 3.4m on wittgenstein with J=30 and other stuff going on in the background\n",
    "    par(delayed(normColumn)(j) for j in range(num_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:56:03.512020Z",
     "start_time": "2019-05-26T18:55:27.468785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend MultiprocessingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.0071s.) Setting batch_size=56.\n",
      "[Parallel(n_jobs=30)]: Batch computation too fast (0.0071s.) Setting batch_size=3160.\n",
      "[Parallel(n_jobs=30)]: Done   2 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=30)]: Done   3 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=30)]: Batch computation too slow (13.4714s.) Setting batch_size=1580.\n",
      "[Parallel(n_jobs=30)]: Done 9540 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=30)]: Done 17415 out of 17415 | elapsed:   31.2s finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #takes ~30s on wittgenstein with J=30 and other stuff going on in the background\n",
    "    pVC = np.vstack(par(delayed(normalize_logprobs)(-1.0 * hVC[:,j])\n",
    "                        for j in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:58:56.424107Z",
     "start_time": "2019-05-26T18:57:41.986082Z"
    }
   },
   "outputs": [],
   "source": [
    "if not memory_map:\n",
    "    #takes ~1.25m on wittgenstein with other stuff going on in the background\n",
    "    pVC_on_disk = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    pVC_on_disk[:,:] = pVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:56:49.023472Z",
     "start_time": "2019-05-26T18:56:48.539328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pVC.shape\n",
    "pVC.dtype\n",
    "pVC.nbytes / 1e9\n",
    "np.sum(pVC, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T18:59:06.650073Z",
     "start_time": "2019-05-26T18:59:06.602659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
