{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.158476Z",
     "start_time": "2019-10-14T21:35:12.151830Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eric Meinhardt / emeinhardt@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-loading-data\" data-toc-modified-id=\"Imports-/-loading-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Language-model\" data-toc-modified-id=\"Language-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Language model</a></span></li><li><span><a href=\"#Contexts\" data-toc-modified-id=\"Contexts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Contexts</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vocabulary</a></span></li></ul></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li><li><span><a href=\"#Calculate-the-number-of-computations-+-estimate-required-space\" data-toc-modified-id=\"Calculate-the-number-of-computations-+-estimate-required-space-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate the number of computations + estimate required space</a></span></li><li><span><a href=\"#Ensure-matrix-metadata-is-standardized\" data-toc-modified-id=\"Ensure-matrix-metadata-is-standardized-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ensure matrix metadata is standardized</a></span></li><li><span><a href=\"#Pick-out-relevant-functions-for-mapping-between-context/word-and-index\" data-toc-modified-id=\"Pick-out-relevant-functions-for-mapping-between-context/word-and-index-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Pick out relevant functions for mapping between context/word and index</a></span></li><li><span><a href=\"#Construct-and-write-distributions\" data-toc-modified-id=\"Construct-and-write-distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Construct and write distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Doing-calculations-in-place-/-via-memory-mapped-arrays\" data-toc-modified-id=\"Doing-calculations-in-place-/-via-memory-mapped-arrays-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Doing calculations in-place / via memory mapped arrays</a></span></li><li><span><a href=\"#Doing-calculations-in-memory-and-writing-to-disk\" data-toc-modified-id=\"Doing-calculations-in-memory-and-writing-to-disk-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Doing calculations in memory and writing to disk</a></span></li><li><span><a href=\"#Creating-a-version-that-contains-probabilities-(and-is-normalized)\" data-toc-modified-id=\"Creating-a-version-that-contains-probabilities-(and-is-normalized)-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Creating a version that contains probabilities (and is normalized)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sketch-of-normalization-process\" data-toc-modified-id=\"Sketch-of-normalization-process-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Sketch of normalization process</a></span></li></ul></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a file path $m$ to a `.arpa` file (or, more realistically/practically, a `kenlm` memory mapped version of one) for a language model\n",
    " - a file path $c$ to a set of $n$-gram contexts $C$ (a `.txt` file with one context per line, where a context is sequence of space-separated wordforms)\n",
    " - a file path $v$ to a vocabulary $W$ (a `.txt` file with one wordform per line)\n",
    " - a filepath $o$ for the main output of the notebook\n",
    " \n",
    "this notebook will calculate the distribution $p(W|C)$ as a memory mapped `numpy` array (written to $o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `kenlm`\n",
    " - `numpy`\n",
    " - `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.264153Z",
     "start_time": "2019-10-14T21:35:12.166935Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.272644Z",
     "start_time": "2019-10-14T21:35:12.268041Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.281962Z",
     "start_time": "2019-10-14T21:35:12.275927Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "m = ''\n",
    "# m = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_utterances_main_4gram.mmap'\n",
    "# m = path.join('LM_Fisher', 'fisher_utterances_main_4gram.mmap')\n",
    "# m = path.join('LM_Fisher', 'LD_Fisher_vocab_add1_unigram_model.arpa')\n",
    "# m = path.join('LM_Fisher', 'LD_Fisher_vocab_add1_unigram_model.pV.json')\n",
    "# m = path.join('LM_Fisher', 'fisher_unigram_counts.tsv')\n",
    "\n",
    "\n",
    "c = ''\n",
    "# c = path.join('C_Buckeye', 'buckeye_contexts_preceding_3_filtered.txt')\n",
    "# c = '/home/AD/emeinhar/buckeye-lm' + '/' + 'buckeye_contexts.txt'\n",
    "\n",
    "v = ''\n",
    "# v = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_vocabulary_main.txt'\n",
    "# v = path.join('LM_Fisher', 'fisher_vocabulary_main.txt')\n",
    "# v = path.join('LTR_Buckeye', 'buckeye_vocabulary_main.txt')\n",
    "\n",
    "o = ''\n",
    "# o = '/home/AD/emeinhar/wr' + '/' + 'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts'\n",
    "# o = 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model'\n",
    "# o = path.join('LD_Fisher_vocab_in_(empty)_(NA)_contexts_1gram_model','LD_Fisher_vocab_in_(empty)_(NA)_contexts_1gram_model')\n",
    "# o = path.join('LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model','LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.289096Z",
     "start_time": "2019-10-14T21:35:12.284933Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = path.dirname(o)\n",
    "if not path.exists(output_dir):\n",
    "    print('Making ' + output_dir)\n",
    "    makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.322160Z",
     "start_time": "2019-10-14T21:35:12.291832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/buckeye_vocabulary_main.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    copyfile(c, path.join(output_dir, path.basename(c)))\n",
    "copyfile(v, path.join(output_dir, path.basename(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.329476Z",
     "start_time": "2019-10-14T21:35:12.324276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.340830Z",
     "start_time": "2019-10-14T21:35:12.331938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MeasBasAnalysis.ipynb',\n",
       " 'probdist.py',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to LTR_NXT_swbd_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres (vec-dev).ipynb',\n",
       " 'LD_Fisher_vocab_in_(empty)_(NA)_contexts_1gram_model',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + dask + tiledb).ipynb',\n",
       " 'Word analysis relation annotation update - NXT_swbd_preceding_contexts_3gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_400_cuda10.json',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'LTR_NXT_swbd_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model.ipynb',\n",
       " 'buckeye_vocab_excluded_by_xlnet.txt',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse tensor calculations + memory issues).ipynb',\n",
       " 'Word analysis relation annotation update - NXT_swbd_following_contexts_2gram_model.ipynb',\n",
       " 'boilerplate.py',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_2000.json',\n",
       " 'buckeye_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_300.json',\n",
       " 'sq.py',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_3gram_model.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - Tarski NXT_2_following.ipynb',\n",
       " 'Analysis Scratch - NXT_swbd - Bigrams.ipynb',\n",
       " 'Tarski param sweep.ipynb',\n",
       " 'Analysis Scratch - NXT_swbd - Trigrams.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_100_cuda10_v2.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_150_cuda10_v2.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_100_cuda10.json',\n",
       " 'nxt_swbd_exploratory_analysis_random_seed.json',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_200_cuda10_v2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_250_cuda10_v2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'Processing Driver Notebook - Wittgenstein Buckeye 2gram following.ipynb',\n",
       " 'Word analysis relation annotation update - NXT_swbd_preceding_contexts_2gram_model.ipynb',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_50.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_150_cuda10.json',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_3gram_model.ipynb',\n",
       " 'XLnet alignment losses.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Sidious.CPU-L).ipynb',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1500.json',\n",
       " 'swbd2003_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_100.json',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_300_cuda10_v2.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1250.json',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       " 'Word analysis relation annotation update - NXT_swbd_following_contexts_3gram_model.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_2gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_50_cuda10.json',\n",
       " 'nxt_swbd_test_data.json.BAK',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Quine.CPU-L).ipynb',\n",
       " 'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       " '__pycache__',\n",
       " 'buckeye_word_analysis_relation_filtered_annotated.json',\n",
       " 'Processing Driver Notebook - Tarski 0 NXT_swbd 3gram preceding.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_500_cuda10_v2.json',\n",
       " 'C_NXT_swbd',\n",
       " 'Analysis Scratch - Buckeye - Bigrams.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_2000_cuda10.json',\n",
       " 'Word analysis relation annotation update - Buckeye_following_contexts_2gram_model.ipynb',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Analysis Scratch - Buckeye - Trigrams.ipynb',\n",
       " 'Processing Driver Notebook - Tarski 0 NXT_swbd 4gram preceding.ipynb',\n",
       " 'Word analysis relation annotation update - Buckeye_preceding_contexts_3gram_model.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_3gram_model.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'memguard.py',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + joblib + tiledb + clean).ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_2500_cuda10.json',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_400_cuda10_v2.json',\n",
       " 'Processing Driver Notebook - Wittgenstein Buckeye Trigram Models.ipynb',\n",
       " 'Word analysis relation annotation update - Buckeye_following_contexts_3gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_750_cuda10_v2.json',\n",
       " 'Calculate segmental wordform and prefix channel matrices - OD.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_250.json',\n",
       " 'Analysis Scratch.ipynb',\n",
       " 'Producing channel distributions - (dev-vec).ipynb',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_3gram_model.ipynb',\n",
       " 'nxt_swbd_test_data.json',\n",
       " 'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_300_cuda10.json',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'buckeye_word_analysis_relation_filtered.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'Processing Driver Notebook - Wittgenstien Buckeye 4-gram preceding.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_3000.json',\n",
       " 'Word analysis relation annotation.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_2gram_model.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1250_cuda10_v2.json',\n",
       " 'LM_Fisher',\n",
       " 'Segmental Posterior Benchmarking.ipynb',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Word analysis relation annotation update - Buckeye_preceding_contexts_2gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_500.json',\n",
       " 'Matrix Metadata Lookup.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_2500.json',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment definition.ipynb',\n",
       " 'Processing Driver Notebook - Tarski 1 NXT_swbd 3gram following.ipynb',\n",
       " 'buckeye_exploratory_analysis_random_seed.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1500_cuda10_v2.json',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_150_cuda10_v2.json',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_100_cuda10_v2.json',\n",
       " 'Dask Examples.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Processing Driver Notebook - Tarski 1 NXT_swbd 4gram following.ipynb',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (old).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Calculate segmental posterior given segmental wordform + context - dev.ipynb',\n",
       " 'LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_150.json',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_4gram_model.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       " 'buckeye_dev_half.json.BAK',\n",
       " 'nxt_swbd_word_analysis_relation_filtered_annotated.json',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Calculate segmental posterior given segmental wordform + context.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_200_cuda10.json',\n",
       " 'buckeye_test_data.json.BAK',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_50_cuda10_v2.json',\n",
       " 'Channel Distribution Analysis.ipynb',\n",
       " 'buckeye_test_data.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_400.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_250_cuda10.json',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'CM_AmE_unaligned_pseudocount0.001',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_4gram_model.ipynb',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_3000_cuda10.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_300_cuda10_v2.json',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_750_cuda10.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1250_cuda10.json',\n",
       " 'Calculate word-prefix relation, Hamming distances, and k-cousin relation.ipynb',\n",
       " 'slice spot checking.ipynb',\n",
       " 'HMMs scratch.ipynb',\n",
       " 'GD_AmE',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_50_cuda10_v2.json',\n",
       " 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_pW_WC_e_calc.log',\n",
       " 'wittgenstein_NXT_swbd_following_5gram_param_sweep_through_batch_size_250_cuda10_v2.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_200_cuda10_v2.json',\n",
       " 'Calculate segmental posterior given segmental wordform + context - Tarski NXT_2_preceding.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1000.json',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'quickstart_sparse',\n",
       " 'Calculate observation distribution given channel models.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_400_cuda10_v2.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1000_cuda10_v2.json',\n",
       " 'Word analysis relation annotation update - NXT_swbd_preceding_contexts_4gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_200.json',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Sidious).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_500_cuda10_v2.json',\n",
       " 'Processing Driver Notebook - Wittgenstien Buckeye 4-gram following.ipynb',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_4gram_model.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'Calculate segmental posterior given segmental wordform + context - OD.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context (dev-Quine).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       " 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       " 'nxt_swbd_dev_half.json',\n",
       " '.git',\n",
       " 'Calculate segmental posterior given segmental wordform + context - LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_4gram_model.ipynb',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres (old).ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Wittgenstein).ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       " 'buckeye_dev_half.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1500_cuda10.json',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'nxt_swbd_dev_half.json.BAK',\n",
       " 'C_Buckeye',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_750.json',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context-BAK.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Kotoba.GPU).ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " '.pW_V.npz',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0',\n",
       " 'nxt_swbd_word_analysis_relation_filtered.json',\n",
       " 'Word analysis relation annotation update - NXT_swbd_following_contexts_4gram_model.ipynb',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_500_cuda10.json',\n",
       " 'tarski_NXT_swbd_following_5gram_param_sweep_through_batch_size_1000_cuda10.json',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Quine).ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports / loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.628002Z",
     "start_time": "2019-10-14T21:35:12.344501Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.911229Z",
     "start_time": "2019-10-14T21:35:12.631181Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote, exportMatrixMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.931487Z",
     "start_time": "2019-10-14T21:35:12.913847Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.937815Z",
     "start_time": "2019-10-14T21:35:12.934228Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    model = kenlm.LanguageModel(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.949428Z",
     "start_time": "2019-10-14T21:35:12.940478Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    contexts = []\n",
    "    with open(c) as file:\n",
    "        for line in file:\n",
    "            contexts.append(line.rstrip())\n",
    "    contexts = tuple(contexts)\n",
    "    len(contexts)\n",
    "    contexts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.957219Z",
     "start_time": "2019-10-14T21:35:12.952771Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    assert len(set(contexts)) == len(contexts), \"Contexts must consist of unique strings.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.982044Z",
     "start_time": "2019-10-14T21:35:12.960333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(\"'em\",\n",
       " 'a',\n",
       " \"a's\",\n",
       " \"aaron's\",\n",
       " 'abandoned',\n",
       " 'abercrombie',\n",
       " 'abhorrent',\n",
       " 'abide',\n",
       " 'ability',\n",
       " 'able')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "with open(v) as file:\n",
    "    for line in file:\n",
    "        vocabulary.append(line.rstrip())\n",
    "vocabulary = tuple(vocabulary)\n",
    "len(vocabulary)\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.990024Z",
     "start_time": "2019-10-14T21:35:12.984546Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(vocabulary)) == len(vocabulary), \"Vocabulary must consist of unique wordforms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:12.995803Z",
     "start_time": "2019-10-14T21:35:12.992358Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.003752Z",
     "start_time": "2019-10-14T21:35:12.998098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taboo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c!= '':\n",
    "    ctxt = choice(contexts)\n",
    "    ctxt\n",
    "\n",
    "wrd = choice(vocabulary)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.016739Z",
     "start_time": "2019-10-14T21:35:13.006051Z"
    }
   },
   "outputs": [],
   "source": [
    "if c!= '':\n",
    "    model.score(\"'and\")\n",
    "    model.score(\"'and </s>\", eos = False)\n",
    "    model.score(\"'and\", eos = False)\n",
    "    model.score(\"this is a sentence\", eos = False)\n",
    "    model.score(\"this is a sentence\")\n",
    "    model.score(\"this is a sentence\", eos = True)\n",
    "    model.score(\"this is a sentence </s>\", eos=False)\n",
    "    model.score(\"this is a sentence </s>\")\n",
    "    tuple(model.full_scores(\"this is a sentence\"))\n",
    "    sum(map(lambda triple: triple[0],\n",
    "            tuple(model.full_scores(\"this is a sentence\"))))\n",
    "    ' '\n",
    "    tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))\n",
    "    sum(map(lambda triple: triple[0],\n",
    "            tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.022387Z",
     "start_time": "2019-10-14T21:35:13.019234Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.032379Z",
     "start_time": "2019-10-14T21:35:13.024573Z"
    }
   },
   "outputs": [],
   "source": [
    "def cli_run(command_string, juststdout=True, autosplit=True, stdout_redirect=None):\n",
    "    if autosplit:\n",
    "        split_string = command_string.split(' ') #will choke on e.g. any filepath or command with spaces\n",
    "    else:\n",
    "        split_string = command_string\n",
    "    if stdout_redirect is None:\n",
    "        if not juststdout:\n",
    "            return subprocess.run(split_string, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        else:\n",
    "            return subprocess.run(split_string, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.decode('UTF-8')\n",
    "    else:\n",
    "        subprocess.run(split_string, stdout=stdout_redirect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.040314Z",
     "start_time": "2019-10-14T21:35:13.034696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.065051Z",
     "start_time": "2019-10-14T21:35:13.042700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['echo', 'foo'], returncode=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(subprocess.run(\"echo foo\".split(' '), stdout=subprocess.PIPE).stdout.decode('UTF-8'))\n",
    "\n",
    "with open(\"foo.txt\", 'w') as f:\n",
    "    subprocess.run(\"echo foo\".split(' '), stdout=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.307631Z",
     "start_time": "2019-10-14T21:35:13.071262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls foo.txt\n",
    "%rm foo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.329476Z",
     "start_time": "2019-10-14T21:35:13.313597Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"foo.txt\", 'w') as f:\n",
    "    cli_run(\"echo foo\", stdout_redirect = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.573753Z",
     "start_time": "2019-10-14T21:35:13.332984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls foo.txt\n",
    "%rm foo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.583641Z",
     "start_time": "2019-10-14T21:35:13.578583Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(cli_run(\"ls -l\", True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.591967Z",
     "start_time": "2019-10-14T21:35:13.586335Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(cli_run(\"echo 'foo'\", True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.604783Z",
     "start_time": "2019-10-14T21:35:13.595552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_unigram_counts.tsv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye/buckeye_vocabulary_main.txt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.610719Z",
     "start_time": "2019-10-14T21:35:13.607524Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     srilm_unigram_scored_fn = v + \".srilm_unigram_scored\"\n",
    "#     srilm_unigram_scored_fn\n",
    "#     with open(srilm_unigram_scored_fn, 'w') as f:\n",
    "#         cli_run(f\"ngram -order 1 -lm {m} -ppl {v} -map-unk '<rem>' -unk -debug 1 -no-eos\", stdout_redirect=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.617205Z",
     "start_time": "2019-10-14T21:35:13.613562Z"
    }
   },
   "outputs": [],
   "source": [
    "# from boilerplate import importSeqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.624492Z",
     "start_time": "2019-10-14T21:35:13.620241Z"
    }
   },
   "outputs": [],
   "source": [
    "from funcy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.640772Z",
     "start_time": "2019-10-14T21:35:13.627423Z"
    }
   },
   "outputs": [],
   "source": [
    "from probdist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:35:13.648955Z",
     "start_time": "2019-10-14T21:35:13.643553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_unigram_counts.tsv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:36:20.406185Z",
     "start_time": "2019-10-14T21:36:02.491229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num_counts = 12887502\n",
      "|LM_vocab| = len(LM_vocab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"'and\", \"'berserkly'\", \"'bout\", \"'burb\", \"'burban\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('retrial', 1),\n",
       " ('adaptability', 1),\n",
       " ('troubles', 31),\n",
       " ('backs', 58),\n",
       " ('olandry', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|OOV items in LTR_Buckeye/buckeye_vocabulary_main.txt| = 506\n",
      "|OOV items| / |vocabulary in LTR_Buckeye/buckeye_vocabulary_main.txt| = 0.06326581645411353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aderal', \"adriatico's\", 'aeronautical', 'ag', 'agitation']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pseudocount file for OOV items with pseudocount = 1 @ filepath LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model.OOV_pseudocounts\n",
      "Creating merged count file for LM input @ LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model.smoothed_counts\n",
      "Creating .arpa file @ LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model_smoothed.arpa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.429792"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hV_np norm = 0.9999998269055321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/emeinhar/anaconda3/envs/jax-gpu2/lib/python3.7/site-packages/ipykernel_launcher.py:124: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"'and\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.467263500220385e-08"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\tLD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model.pV.npy\n",
      " to \n",
      "\tLD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model.pV.npy_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if c == '' and 'counts' in m:\n",
    "    #load vast majority of unigram counts \n",
    "    LM_word_count_s = importSeqs(m, lambda lines: lmap(lambda line: line.split('\\t'),\n",
    "                                                       lines))\n",
    "    LM_word_count = lmap(lambda pair: (pair[0], int(pair[1])), \n",
    "                         LM_word_count_s)\n",
    "    total_num_counts = sum(lmap(second, LM_word_count))\n",
    "    print(f\"total_num_counts = {total_num_counts}\")\n",
    "    LM_word_freqs = Counter(dict(LM_word_count))\n",
    "    \n",
    "    #figure out what the LM vocab is\n",
    "    LM_vocab = set(LM_word_freqs.keys())\n",
    "    print(f\"|LM_vocab| = len(LM_vocab)\")\n",
    "    LM_vocab_t = tuple(sorted(LM_vocab))\n",
    "    LM_vocab_t[:5]\n",
    "    \n",
    "    LM_word_count[:5]\n",
    "    \n",
    "    #figure out how many OOV items there are in the vocabulary you want to apply the LM to\n",
    "    OOV_items = [v for v in vocabulary if v not in LM_vocab]\n",
    "    print(f\"|OOV items in {v}| = {len(OOV_items)}\")\n",
    "    print(f\"|OOV items| / |vocabulary in {v}| = {len(OOV_items) / len(vocabulary)}\")\n",
    "    OOV_items[:5]\n",
    "    \n",
    "    #Create smoothed counts in the form of a count file\n",
    "    smoothing_count = 1\n",
    "    pseudocount_lines = [(v, smoothing_count)\n",
    "                          for v in sorted(OOV_items)]\n",
    "    pseudocount_lines_str = [f\"{l[0]}\\t{str(l[1])}\"\n",
    "                             for l in pseudocount_lines]\n",
    "    pseudocount_fp = o + '.OOV_pseudocounts'\n",
    "#     pseudocount_fp\n",
    "    print(f\"Creating pseudocount file for OOV items with pseudocount = {smoothing_count} @ filepath {pseudocount_fp}\")\n",
    "    exportSeqs(pseudocount_fp, pseudocount_lines_str)\n",
    "    \n",
    "    #combine the pseudocounts with the main set of counts\n",
    "    combined_lm_counts_fp = o + '.smoothed_counts'\n",
    "    print(f\"Creating merged count file for LM input @ {combined_lm_counts_fp}\")\n",
    "    with open(combined_lm_counts_fp, 'w') as f:\n",
    "        cli_run(f\"cat {pseudocount_fp} {m}\", stdout_redirect = f)\n",
    "     \n",
    "    #create a language model from this\n",
    "    combined_lm_fp = o + '_smoothed.arpa'\n",
    "    print(f\"Creating .arpa file @ {combined_lm_fp}\")\n",
    "    with open(combined_lm_fp, 'w') as f:\n",
    "        cli_run(f\"ngram-count -order 1 -read {combined_lm_counts_fp} -addsmooth 0 -lm {combined_lm_fp}\", stdout_redirect = f)\n",
    "    \n",
    "    #turn it into a tsv by trimming arpa crap from the top and bottom\n",
    "    combined_lm_fp_logpV_fp = o + '.log10pV'\n",
    "#     with open(combined_lm_fp, 'r') as arpa_file:\n",
    "    with open(combined_lm_fp_logpV_fp, 'w') as f:\n",
    "        cli_run([\"sed\", '1,5d;$d;/^$/d', combined_lm_fp], autosplit=False, stdout_redirect = f)\n",
    "#         cli_run(f\"sed '1,5d;$d;/^$/d' {combined_lm_fp}\", stdout_redirect = f)\n",
    "    \n",
    "    #swap the columns\n",
    "    swapped_cols_fp = combined_lm_fp_logpV_fp + '_swapped_cols'\n",
    "    with open(swapped_cols_fp, 'w') as f:\n",
    "        cli_run([\"awk\", ' { print $2 \"\\t\" $1 } ', combined_lm_fp_logpV_fp], autosplit=False, stdout_redirect = f)\n",
    "\n",
    "    #load the swapped column file as a list of dictionaries\n",
    "    log10_probs = loadTSV_as_dictlist(swapped_cols_fp, fieldnames=['Orthographic_Wordform', 'log10_prob'])\n",
    "    modeled_Vs = lpluck('Orthographic_Wordform', log10_probs)\n",
    "    modeled_Vs_t = tuple(sorted(modeled_Vs))\n",
    "    assert len(modeled_Vs) > 0, f\"|modeled_Vs| = 0. Something is wrong...\"\n",
    "    \n",
    "    #...and create a dictionary mapping into log10 probabilities...\n",
    "    pV_log10 = dict()\n",
    "    for each_row in log10_probs:\n",
    "        my_w = each_row['Orthographic_Wordform']\n",
    "        try:\n",
    "            pw = float(each_row['log10_prob'])\n",
    "        except ValueError:\n",
    "            print(my_w)\n",
    "            print(each_row['log10_prob'])\n",
    "            raise\n",
    "        pV_log10[my_w] = pw\n",
    "        \n",
    "    pV_log10[\"'bout\"]\n",
    "        \n",
    "    #...and surprisals\n",
    "    def to_log2_surprisal(log10_score):\n",
    "        return -1.0 * (log10_score / np.log10(2))\n",
    "\n",
    "    hV = walk_values(to_log2_surprisal, pV_log10)\n",
    "    \n",
    "    hV_np = np.array([hV[v] for v in modeled_Vs_t])\n",
    "    dist_norm = np.sum(np.exp2(-1.0 * hV_np), axis=0)\n",
    "    print(f'hV_np norm = {dist_norm}')\n",
    "    \n",
    "    # from https://stats.stackexchange.com/a/66621\n",
    "    def normalize_logprobs(logprobs, d=16, axis=0, b=None):\n",
    "    # def normalize_logprobs(logprobs, d=16, b=None):\n",
    "    #     axis = 0\n",
    "        n = logprobs.shape[axis]\n",
    "        epsilon = 10**(-1.0 * d)\n",
    "        maxlogp = np.max( logprobs[axis] )\n",
    "        if b is None:\n",
    "            threshold = np.log(epsilon) - np.log(n)\n",
    "            to_alpha = lambda logprob: np.exp(logprob - maxlogp) if (logprob - maxlogp) >= threshold else 0.0\n",
    "            to_alpha_vec = lambda logprobs: np.exp(logprobs - maxlogp) * (logprobs - maxlogp >= threshold)\n",
    "        elif b == 2:\n",
    "            threshold = np.log2(epsilon) - np.log2(n)\n",
    "            to_alpha = lambda logprob: np.exp2(logprob - maxlogp) if (logprob - maxlogp) >= threshold else 0.0\n",
    "            to_alpha_vec = lambda logprobs: np.exp2(logprobs - maxlogp) * (logprobs - maxlogp >= threshold)\n",
    "        elif b == 10:\n",
    "            threshold = np.log10(epsilon) - np.log10(n)\n",
    "            to_alpha = lambda logprob: np.power(logprob - maxlogp, 10) if (logprob - maxlogp) >= threshold else 0.0\n",
    "            to_alpha_vec = lambda logprobs: np.power(logprobs - maxlogp, 10) * (logprobs - maxlogp >= threshold)\n",
    "        else:\n",
    "            threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "            to_alpha = lambda logprob: np.power(logprob - maxlogp, b) if (logprob - maxlogp) >= threshold else 0.0\n",
    "            to_alpha_vec = lambda logprobs: np.power(logprobs - maxlogp, 10) * (logprobs - maxlogp >= threshold)\n",
    "        alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "    #     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "        alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "        probs = alpha_is / alpha_norm\n",
    "    #     assert np.isclose(np.sum(probs), 1.0)\n",
    "        return probs\n",
    "    \n",
    "    pV_np = normalize_logprobs(-1.0 * hV_np, b=2)\n",
    "    pV_np.sum()\n",
    "    is_a_distribution_np(pV_np)\n",
    "    \n",
    "    hV_np_new = -1.0 * np.log2(pV_np)\n",
    "    np.sum(np.exp2(-1.0 * hV_np_new), axis=0)\n",
    "    \n",
    "    pV = NPdistToDist(pV_np, outcomeToIndexMap = {v:modeled_Vs_t.index(v) for v in modeled_Vs_t})\n",
    "    list(pV.keys())[0]\n",
    "    pV[list(pV.keys())[0]]\n",
    "    \n",
    "    exportProbDist(o + '.pV.json', pV)\n",
    "    np.save(o + '.pV.npy', pV_np)\n",
    "    \n",
    "    pV_dim_md = {'V':{'from fp':f'fisher_utterances_main.txt, {v}',\n",
    "                   'changes':'extracted vocabulary and then sorted alphabetically in both corpora, combined them',\n",
    "                   'size':len(pV)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.pV.npy'+'_metadata.json',\n",
    "                         o+'.pV.npy',\n",
    "                         pV_np,\n",
    "                         pV_dim_md,\n",
    "                         'n/a',\n",
    "                         'Producing contextual distributions',\n",
    "                         dict())\n",
    "    \n",
    "    np.save(o + '.hV.npy', hV_np_new)\n",
    "    \n",
    "    \n",
    "    #create the object and file that downstream notebooks are looking for\n",
    "    pVC = np.expand_dims(pV_np, 1)\n",
    "    pVC.shape\n",
    "    \n",
    "    pVC_on_disk = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=pVC.shape)\n",
    "    pVC_on_disk[:,:] = pVC\n",
    "    \n",
    "    pVC_dim_md = {'V':{'from fp':f'fisher_utterances_main.txt, {v}',\n",
    "                    'changes':'extracted vocabulary and then sorted alphabetically in both corpora, combined them',\n",
    "                    'size':len(pV)},\n",
    "                  'C':{'from fp':\"None. Dimension exists only for downstream compatibility.\",\n",
    "                       'changes':'',\n",
    "                       'size':0}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.pV_C'+'_metadata.json',\n",
    "                         o+'.pV_C',\n",
    "                         pVC,\n",
    "                         pVC_dim_md,\n",
    "                         'n/a',\n",
    "                         'Producing contextual distributions',\n",
    "                         dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:36:20.414874Z",
     "start_time": "2019-10-14T21:36:20.409392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model/LD_Buckeye_vocab_in_(empty)_(NA)_contexts_1gram_model'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T20:33:44.124360Z",
     "start_time": "2019-10-14T20:33:44.116542Z"
    }
   },
   "outputs": [],
   "source": [
    "if c == '' and 'pV' in m:\n",
    "    pV_model = importProbDist(m)\n",
    "    norm(pV_model)\n",
    "    assert isNormalized(pV_model)\n",
    "    pV_model = ProbDist(pV_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:34:46.792118Z",
     "start_time": "2019-10-14T19:34:46.714157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007727179799032135"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.007727179799032135"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.007727179799032135"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.3497831977576897e-06"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c == '' and 'pV' in m:\n",
    "    def score(word, surprisal=False):\n",
    "        if word not in pV:\n",
    "            return pV_model['<rem>']\n",
    "        if not surprisal:\n",
    "            return pV_model[word]\n",
    "        return -1.0 * np.log2(pV_model[word])\n",
    "    \n",
    "    hV_model = walk_values(lambda w: score(w, True),\n",
    "                           dict(pV_model))\n",
    "    \n",
    "    score('froob')\n",
    "    score('asdf')\n",
    "    score('<rem>')\n",
    "    score('banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:55:54.351193Z",
     "start_time": "2019-10-14T19:55:54.308394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'and\",\n",
       " \"'berserkly'\",\n",
       " \"'bout\",\n",
       " \"'burb\",\n",
       " \"'burban\",\n",
       " \"'burbs\",\n",
       " \"'cau\",\n",
       " \"'cause\",\n",
       " \"'cept\",\n",
       " \"'cide\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.997957609040984"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c == '' and 'pV' in m:\n",
    "    v_model_by_prob = sorted(list(pV_model.keys()),\n",
    "                             key=lambda v_model:pV_model[v])\n",
    "    v_model_by_prob[:10]\n",
    "    v_model_by_prob.index('<rem>')\n",
    "    1 - (v_model_by_prob.index('<rem>') / len(v_model_by_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:37:43.586120Z",
     "start_time": "2019-10-14T19:37:43.577181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7998"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T20:02:05.327785Z",
     "start_time": "2019-10-14T20:02:05.241332Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk prob = 0.007727179799032135\n",
      "divided unk prob = 1.5271106322197896e-05\n",
      "# words with less probability than divided unk prob = 41754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9475332455861663"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(W | p(w) < p_divided_unk) = 0.046478355811345934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sampled',\n",
       " 'shale',\n",
       " 'corinthians',\n",
       " 'inalienable',\n",
       " 'micron',\n",
       " 'sunbury',\n",
       " 'carwash',\n",
       " 'tvs',\n",
       " 'prescriptive',\n",
       " 'derringer',\n",
       " 'surmounted',\n",
       " 'murfield',\n",
       " 'blisters',\n",
       " 'tangibles',\n",
       " 'gestapo',\n",
       " 'residencies',\n",
       " 'unix',\n",
       " 'amazement',\n",
       " 'lemonjello',\n",
       " 'unemployable',\n",
       " 'partitioned',\n",
       " 'racists',\n",
       " 'ellison',\n",
       " 'locates',\n",
       " 'fedora',\n",
       " 'pokedex',\n",
       " 'entrenched',\n",
       " 'beatitudes',\n",
       " 'egregious',\n",
       " 'edventure',\n",
       " \"regular's\",\n",
       " 'khazakstan',\n",
       " 'potheads',\n",
       " 'schnook',\n",
       " 'conforms',\n",
       " 'tallow',\n",
       " 'promissory',\n",
       " 'hatfield',\n",
       " 'poories',\n",
       " 'nullified',\n",
       " 'jamilia',\n",
       " 'agler',\n",
       " 'agitation',\n",
       " 'recitation',\n",
       " 'apostolic',\n",
       " 'meaningfulness',\n",
       " 'suburbanite',\n",
       " 'fizzed',\n",
       " 'mindboggling',\n",
       " 'wetlands',\n",
       " 'tradeoff',\n",
       " 'pataskala',\n",
       " 'c.a.h.s.',\n",
       " 'hetero',\n",
       " 'hemispheres',\n",
       " 'pipefitters',\n",
       " 'permeate',\n",
       " 'anorism',\n",
       " \"u's\",\n",
       " \"freshman's\",\n",
       " 'ephraim',\n",
       " 'miha',\n",
       " \"columbus's\",\n",
       " 'redirected',\n",
       " 'departing',\n",
       " 'beechwold',\n",
       " 'expanses',\n",
       " 'jukebox',\n",
       " 'check-up',\n",
       " 'correctiveness',\n",
       " 'chested',\n",
       " 'reteaching',\n",
       " 'arranges',\n",
       " 'mcconnells',\n",
       " 'sobers',\n",
       " 'unruined',\n",
       " 'hafta',\n",
       " 'vestibule',\n",
       " 'ynkow',\n",
       " 'bainby',\n",
       " 'dejected',\n",
       " 'silkscreening',\n",
       " 'wadsworth',\n",
       " 'uncomplicated',\n",
       " 'autoworkers',\n",
       " \"shopper's\",\n",
       " 'jocks',\n",
       " 'modules',\n",
       " 'morse',\n",
       " 'im-ing',\n",
       " 'twig',\n",
       " 'vietcong',\n",
       " 'zollinger',\n",
       " \"peer's\",\n",
       " 'um-hmm',\n",
       " 'shadowed',\n",
       " 'hilliards',\n",
       " 'misapprehensions',\n",
       " 'subwoofers',\n",
       " 'cesareans']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c == '' and 'pV' in m:\n",
    "    unks = {v for v in vocabulary if v not in pV_model}\n",
    "    len(unks)\n",
    "    \n",
    "    unk_prob = pV_model['<rem>']\n",
    "    divided_unk_prob = unk_prob / len(unks)\n",
    "    print(f\"unk prob = {unk_prob}\")\n",
    "    print(f\"divided unk prob = {divided_unk_prob}\")\n",
    "    \n",
    "    model_probs = sorted(list(pV_model.values()))\n",
    "    probs_gt_unk = lfilter(lambda prob: prob >= unk_prob,\n",
    "                           model_probs)\n",
    "    probs_lt_unk = lfilter(lambda prob: prob < unk_prob,\n",
    "                           model_probs)\n",
    "    probs_gt_divunk = lfilter(lambda prob: prob >= divided_unk_prob,\n",
    "                              model_probs)\n",
    "    probs_lt_divunk = lfilter(lambda prob: prob < divided_unk_prob,\n",
    "                              model_probs)\n",
    "    print(f\"# words with less probability than divided unk prob = {len(probs_lt_divunk)}\")\n",
    "    len(probs_lt_divunk) / len(list(pV.keys()))\n",
    "    print(f\"p(W | p(w) < p_divided_unk) = {sum(probs_lt_divunk)}\")\n",
    "    \n",
    "    list(unks)[:100]\n",
    "#     pV = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:26:56.055693Z",
     "start_time": "2019-10-14T19:26:56.050628Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     scored_lines = importSeqs(srilm_unigram_scored_fn, tuple)\n",
    "#     chunked_lines = lmap(partial(take, 3), lpartition(4, 4, scored_lines))\n",
    "    \n",
    "#     def parse_chunk(chunk):\n",
    "#         orth_word = chunk[0]\n",
    "#         score_info = chunk[2]\n",
    "#         logprob = score_info.split(' ')[3]\n",
    "#         return {orth_word:float(logprob)}\n",
    "    \n",
    "#     parsed_chunks = lmap(parse_chunk, chunked_lines)\n",
    "#     pV_log10 = join(parsed_chunks)\n",
    "# #     np.sum(10 ** np.array(list(pV_log10.values())))\n",
    "#     hV_log2 = walk_values(lambda log10_prob: -1.0 * (log10_prob / np.log10(2)),\n",
    "#                           pV_log10)\n",
    "#     my_norm = np.sum(np.exp2(-1.0 * np.array(list(hV_log2.values()))), axis=0)\n",
    "#     my_norm\n",
    "#     assert np.isclose(my_norm, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:26:57.645196Z",
     "start_time": "2019-10-14T19:26:57.640849Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     scored_lines[:7]\n",
    "#     scored_lines[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:26:59.402009Z",
     "start_time": "2019-10-14T19:26:59.397853Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     chunked_lines[:2]\n",
    "#     chunked_lines[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:27:00.613125Z",
     "start_time": "2019-10-14T19:27:00.608878Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     parsed_chunks[:2]\n",
    "#     parsed_chunks[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:27:02.255609Z",
     "start_time": "2019-10-14T19:27:02.251477Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     pV_log10[\"'and\"]\n",
    "#     10 ** pV_log10[\"'and\"]\n",
    "#     -1.0 * np.log2( 10 ** pV_log10[\"'and\"] )\n",
    "#     -1.0 * (pV_log10[\"'and\"] / np.log10(2))\n",
    "#     np.exp2( (pV_log10[\"'and\"] / np.log10(2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:27:04.282693Z",
     "start_time": "2019-10-14T19:27:04.278859Z"
    }
   },
   "outputs": [],
   "source": [
    "# if c == '':\n",
    "#     pV_log10[\"'and\"]\n",
    "#     hV_log2[\"'and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:33:11.312459Z",
     "start_time": "2019-10-14T19:33:11.306820Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log10, log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:33:13.233706Z",
     "start_time": "2019-10-14T19:33:13.222199Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    def score(word, context, base2=True, surprisal=True):\n",
    "        score_infos = tuple(model.full_scores(context + ' ' + word, eos=False, bos=False))\n",
    "        key_score_log10 = score_infos[-1][0]\n",
    "        if base2:\n",
    "            key_score = key_score_log10 / log10(2)\n",
    "        if surprisal:\n",
    "            key_score = -1.0 * key_score\n",
    "        return key_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:33:13.967643Z",
     "start_time": "2019-10-14T19:33:13.961642Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    ctxt\n",
    "    wrd\n",
    "    score(wrd, ctxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of computations + estimate required space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T19:33:26.847818Z",
     "start_time": "2019-10-14T19:33:26.838989Z"
    }
   },
   "outputs": [],
   "source": [
    "if c != '':\n",
    "    bits_per_cell = 64\n",
    "    bytes_per_cell = bits_per_cell / 8\n",
    "\n",
    "    len(contexts)\n",
    "    len(vocabulary)\n",
    "    \"{:,}\".format( len(contexts) * len(vocabulary) )\n",
    "    \"{:,} GB\".format( len(contexts) * len(vocabulary) * bytes_per_cell / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.265082Z",
     "start_time": "2019-07-17T22:20:49.262209Z"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.273762Z",
     "start_time": "2019-07-17T22:20:49.267588Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = product(contexts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.281630Z",
     "start_time": "2019-07-17T22:20:49.276246Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = tuple(product(contexts, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.289676Z",
     "start_time": "2019-07-17T22:20:49.284197Z"
    }
   },
   "outputs": [],
   "source": [
    "# from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.296511Z",
     "start_time": "2019-07-17T22:20:49.292308Z"
    }
   },
   "outputs": [],
   "source": [
    "# example_computations = choices(computations, k=10)\n",
    "# example_computations\n",
    "# ex = choice(example_computations)\n",
    "# ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure matrix metadata is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.307508Z",
     "start_time": "2019-07-17T22:20:49.299010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)\n",
    "if c != '':\n",
    "    type(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.325223Z",
     "start_time": "2019-07-17T22:20:49.309820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary) == sorted(list(vocabulary))\n",
    "if c != '':\n",
    "    list(contexts) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.337868Z",
     "start_time": "2019-07-17T22:20:49.327828Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_sorted = tuple(sorted(list(vocabulary)))\n",
    "if c != '':\n",
    "    contexts_sorted = tuple(sorted(list(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.351001Z",
     "start_time": "2019-07-17T22:20:49.340025Z"
    }
   },
   "outputs": [],
   "source": [
    "assert list(vocabulary_sorted) == sorted(list(vocabulary))\n",
    "if c != '':\n",
    "    assert list(contexts_sorted) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick out relevant functions for mapping between context/word and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.367421Z",
     "start_time": "2019-07-17T22:20:49.353312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'confirmations'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8063"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'yknow she was'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16831"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd\n",
    "vocabulary_sorted.index(wrd)\n",
    "if c != '':\n",
    "    ctxt\n",
    "    contexts_sorted.index(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.372960Z",
     "start_time": "2019-07-17T22:20:49.370029Z"
    }
   },
   "outputs": [],
   "source": [
    "# ex\n",
    "# contexts.index(ex[0])\n",
    "# contexts[ contexts.index(ex[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.383618Z",
     "start_time": "2019-07-17T22:20:49.375668Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_np(word_idx, context_idx, base2=True, surprisal=True):\n",
    "    return score(vocabulary_sorted[word_idx], contexts_sorted[context_idx], base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:44:41.769832Z",
     "start_time": "2019-09-05T15:44:40.537844Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'choice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de3e7f533368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrandom_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mrandom_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# contexts_sorted.index('a couple of')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'choice' is not defined"
     ]
    }
   ],
   "source": [
    "def hW_C_lookup(w=None,c=None):\n",
    "    Ws_t = vocabulary_sorted\n",
    "    Cs_t = contexts_sorted\n",
    "    if w is None and c is None:\n",
    "        raise Exception('Must specify at least one of a context string or orthographic wordform string')\n",
    "    if w is None:\n",
    "#         my_pW_c = pW_C[:,Cs_t.index(c)]\n",
    "#         my_pW_c_as_dict = dict(zip(Cs_t, my_pW_c))\n",
    "        my_hW_c_as_dict = {w:score(w,c) for w in vocabulary_sorted}\n",
    "        return my_hW_c_as_dict\n",
    "    if w is not None and c is not None:\n",
    "        my_hw_c = score(w, c)\n",
    "        return my_hw_c\n",
    "    if c is None:\n",
    "#         my_pw_C = pW_C[Ws_t.index(w), :]\n",
    "#         my_pw_C_as_dict = dict(zip(Cs_t, my_pw_C))\n",
    "        my_hw_C_as_dict = {c:score(w,c) for c in contexts_sorted}\n",
    "        return my_hw_C_as_dict\n",
    "    \n",
    "if c != '':\n",
    "    random_context = choice(contexts_sorted)\n",
    "    random_context\n",
    "    # contexts_sorted.index('a couple of')\n",
    "    # my_surp_dist = hW_C_lookup(c='a couple of')\n",
    "    my_surp_dist = hW_C_lookup(c=random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from probdist import *\n",
    "\n",
    "# def pW_C_lookup(w=None,c=None):\n",
    "#     Ws_t = vocabulary_sorted\n",
    "#     Cs_t = contexts_sorted\n",
    "#     if w is None and c is None:\n",
    "#         raise Exception('Must specify at least one of a context string or orthographic wordform string')\n",
    "#     if w is None:\n",
    "# #         my_pW_c = pW_C[:,Cs_t.index(c)]\n",
    "# #         my_pW_c_as_dict = dict(zip(Cs_t, my_pW_c))\n",
    "#         my_pW_c_as_dict = {w:score(w,c, surprisal=False) for w in vocabulary_sorted}\n",
    "#         assert isNormalized(my_pW_c_as_dict), f\"norm = {norm(my_pW_c_as_dict)}\"\n",
    "#         return ProbDist(my_pW_c_as_dict)\n",
    "#     if w is not None and c is not None:\n",
    "#         my_pw_c = score(w, c, surprisal=False)\n",
    "#         return my_pw_c\n",
    "#     if c is None:\n",
    "# #         my_pw_C = pW_C[Ws_t.index(w), :]\n",
    "# #         my_pw_C_as_dict = dict(zip(Cs_t, my_pw_C))\n",
    "#         my_pw_C_as_dict = {c:score(w,c, surprisal=False) for c in contexts_sorted}\n",
    "#         return my_pw_C_as_dict\n",
    "    \n",
    "    \n",
    "# contexts_sorted.index('a couple of')\n",
    "# my_dist = pW_C_lookup(c='a couple of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and write distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:07.496706Z",
     "start_time": "2019-07-17T22:23:07.484042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "767374560"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(vocabulary_sorted)\n",
    "if c != '':\n",
    "    num_contexts = len(contexts_sorted)\n",
    "    my_shape = (num_words, num_contexts) #columns are distributions\n",
    "    my_shape\n",
    "    num_words * num_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:08.006592Z",
     "start_time": "2019-07-17T22:23:08.002230Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_map = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in-place / via memory mapped arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will only begin to make sense if the array is going to be too large to fit in memory or very small; otherwise joblib will use threads to parallelize the computation (because it involves lots of IO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:09.683214Z",
     "start_time": "2019-07-17T22:23:09.672335Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:09.936416Z",
     "start_time": "2019-07-17T22:23:09.931608Z"
    }
   },
   "outputs": [],
   "source": [
    "score_np_vec = np.vectorize(score_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.191649Z",
     "start_time": "2019-07-17T22:23:10.184446Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    def define_score(word_idx, context_idx, base2=True, surprisal=True):\n",
    "        hVC[word_idx, context_idx] = score_np(word_idx, context_idx, base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.531798Z",
     "start_time": "2019-07-17T22:23:10.526941Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Started calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.837237Z",
     "start_time": "2019-07-17T22:23:10.830457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    # est 7h on wittgenstein with J=30 and other computations going on in the background\n",
    "    # pretty sure that means it's using threads rather than processes\n",
    "    par(delayed(define_score)(w_idx, ctxt_idx) \n",
    "        for ctxt_idx in range(num_contexts) \n",
    "        for w_idx in range(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:11.411337Z",
     "start_time": "2019-07-17T22:23:11.406884Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Ended calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:12.594231Z",
     "start_time": "2019-07-17T22:23:12.589543Z"
    }
   },
   "outputs": [],
   "source": [
    "# no need for testing ordering because of the way define_score is defined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:13.876898Z",
     "start_time": "2019-07-17T22:23:13.867806Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC_dim_md = {'C':{'from fp':c,\n",
    "                       'changes':'sorted alphabetically',\n",
    "                       'size':len(contexts_sorted)},\n",
    "                  'V':{'from fp':v,\n",
    "                       'changes':'none - already sorted',\n",
    "                       'size':len(vocabulary_sorted)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                         o+'.hV_C',\n",
    "                         hVC,\n",
    "                         hVC_dim_md,\n",
    "                         'Step 2b',\n",
    "                         'Producing contextual distributions',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in memory and writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:23.675339Z",
     "start_time": "2019-07-17T22:23:25.682619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 365 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 441 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 648 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 693 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 738 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 785 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 881 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 930 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1032 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1085 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1138 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1305 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1421 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1480 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1541 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1602 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1665 tasks      | elapsed:   47.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1728 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1793 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1925 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1992 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2061 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2130 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2201 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2272 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2345 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2493 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2645 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2801 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2961 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3042 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3125 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3208 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3293 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3378 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3465 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3552 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3641 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3730 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3821 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3912 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4005 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4193 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4288 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4385 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4482 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4581 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4680 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4781 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4882 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4985 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5088 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5298 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5405 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5512 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5621 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5730 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5841 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5952 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6065 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6178 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6293 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6408 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6525 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6642 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6761 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6880 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7001 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7122 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7245 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7368 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7493 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7618 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7745 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7872 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8001 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8130 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8261 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8392 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8525 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8658 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8793 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8928 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9065 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9202 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9341 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9480 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9621 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9762 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9905 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10048 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10193 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10338 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10485 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10632 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10781 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10930 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11081 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11232 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11385 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11538 tasks      | elapsed:  3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 11693 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11848 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 12005 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12162 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12321 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12480 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12641 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12802 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12965 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 13128 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 13293 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 13458 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13625 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13792 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13961 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14130 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14301 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14472 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14645 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14818 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14993 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 15168 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15345 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15522 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15701 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15880 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16061 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16242 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16425 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16608 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16793 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 16978 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map and c != '':\n",
    "    #~30m on wittgenstein\n",
    "#     hVC = np.vstack([score_np_vec(np.array(range(num_words)), c_idx) for c_idx in tqdm(range(num_contexts))]).T\n",
    "    \n",
    "    #takes ~2.75m on wittgenstein with J=30 and other things going on in the background\n",
    "    #takes 5m on sidious with J=-1 and a full load going on in the background\n",
    "    hVC = np.vstack(par(delayed(score_np_vec)(np.array(range(num_words)), c_idx)\n",
    "                        for c_idx in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for reasons of caution and paranoia, we check that the ordering on contexts and wordforms is preserved by picking a thousand random context-wordform pairs, doing the calculations manually and checking that the calculation matches what's at the corresponding location in `hVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:23.702494Z",
     "start_time": "2019-07-17T22:28:23.680494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    hVC[0,0]\n",
    "    score_np(0,0) #word idx, context idx\n",
    "\n",
    "    hVC[2,3]\n",
    "    score_np(2,3)\n",
    "\n",
    "    hVC[3,2]\n",
    "    score_np(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:24.034500Z",
     "start_time": "2019-07-17T22:28:23.706359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choices\n",
    "if c != '':\n",
    "    N_test_pairs = 10000\n",
    "    random_context_indices = choices(range(num_contexts), k=N_test_pairs)\n",
    "    random_orthWord_indices = choices(range(num_words), k=N_test_pairs)\n",
    "\n",
    "    random_index_pairs = tuple(zip(random_orthWord_indices,\n",
    "                                   random_context_indices))\n",
    "\n",
    "    tests = [hVC[i,j] == score_np(i,j) for i,j in random_index_pairs]\n",
    "    all(tests)\n",
    "    assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.337706Z",
     "start_time": "2019-07-11T19:12:11.438259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not memory_map and c != '':\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype\n",
    "    hVC_on_disk = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    \n",
    "    #takes ~1.25m on wittgenstein\n",
    "    hVC_on_disk[:,:] = hVC\n",
    "    hVC = hVC_on_disk\n",
    "    del hVC_on_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.356969Z",
     "start_time": "2019-07-11T19:13:52.344908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.402238Z",
     "start_time": "2019-07-11T19:13:52.361822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.421622Z",
     "start_time": "2019-07-11T19:13:52.406656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.shape\n",
    "    hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:45.228741Z",
     "start_time": "2019-07-11T19:35:45.219379Z"
    }
   },
   "outputs": [],
   "source": [
    "# def exportMatrixMetadata(md_fp, matrix_fp, matrix, dim_md, step_name, nb_name, other_md):\n",
    "#     md = {'matrix fp':matrix_fp,\n",
    "#           'matrix shape':matrix.shape,\n",
    "#           'Produced in step':step_name,\n",
    "#           'Produced in notebook':nb_name}\n",
    "#     md.update(dim_md)\n",
    "#     md.update(other_md)\n",
    "#     exportDict(md_fp, md)\n",
    "#     print(f'Wrote metadata for \\n\\t{matrix_fp}\\n to \\n\\t{md_fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:33:11.492027Z",
     "start_time": "2019-07-11T19:33:11.484423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:47.103542Z",
     "start_time": "2019-07-11T19:35:47.089939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if c != '':\n",
    "    hVC_dim_md = {'C':{'from fp':c,\n",
    "                       'changes':'sorted alphabetically',\n",
    "                       'size':len(contexts_sorted)},\n",
    "                  'V':{'from fp':v,\n",
    "                       'changes':'none - already sorted',\n",
    "                       'size':len(vocabulary_sorted)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                         o+'.hV_C',\n",
    "                         hVC,\n",
    "                         hVC_dim_md,\n",
    "                         'Step 2b',\n",
    "                         'Producing contextual distributions',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:36:01.113482Z",
     "start_time": "2019-07-11T19:36:00.649990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a version that contains probabilities (and is normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just naively convert (-) log-probabilities to probabilities if we're interested in distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.630144Z",
     "start_time": "2019-07-17T22:28:24.038007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98952166,  0.92990437,  0.97399201, ...,  0.96380412,\n",
       "        0.9685743 ,  0.86971511])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    # NB! assumes hVC is in base 2 surprisals...\n",
    "    dist_norms = np.sum(np.exp2(-1.0 * hVC), axis=0)\n",
    "    dist_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.641274Z",
     "start_time": "2019-07-17T22:29:06.633434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not to'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    random_context = choice(contexts_sorted)\n",
    "    random_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.685445Z",
     "start_time": "2019-07-17T22:29:06.644253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4108"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    contexts.index(random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:32.369598Z",
     "start_time": "2019-07-17T22:29:06.688482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0191s.) Setting batch_size=20.\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0252s.) Setting batch_size=318.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1120s.) Setting batch_size=1134.\n",
      "[Parallel(n_jobs=-1)]: Done 3342 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 7476 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 44064 out of 44064 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "if c != '':\n",
    "    hW_rc = np.array(par(delayed(score)(w, random_context) for w in vocabulary_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:32.391183Z",
     "start_time": "2019-07-17T22:39:32.377569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    np.array_equal( hVC[:,contexts_sorted.index(random_context)], hW_rc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.464736Z",
     "start_time": "2019-07-17T22:39:32.393878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18327779e-08,   4.18327779e-08,   1.65924722e-06,\n",
       "         5.86258692e-08,   4.18327779e-08,   4.18327779e-08,\n",
       "         4.18327779e-08,   1.79446150e-03,   1.83640299e-07,\n",
       "         4.18327779e-08])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.95894399281575127"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    pW_rc = np.exp2(-1.0 * hW_rc)\n",
    "    pW_rc[:10]\n",
    "    np.sum(pW_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.482962Z",
     "start_time": "2019-07-17T22:39:33.476186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    hVC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.709075Z",
     "start_time": "2019-07-17T22:39:33.486345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = 1e-16\n",
      "n = 44064\n",
      "max() =  = -3.6741742649792664\n",
      " = -68.57816236233276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    # from https://stats.stackexchange.com/a/66621\n",
    "    my_logprobs = -1.0 * hW_rc\n",
    "    my_epsilon = 10 ** (-1.0 * 16)\n",
    "    print(' = {0}'.format( my_epsilon ))\n",
    "\n",
    "    my_n = my_logprobs.shape[0]\n",
    "    print('n = {0}'.format( my_n ))\n",
    "\n",
    "    my_max = np.max(my_logprobs)\n",
    "    print('max() =  = {0}'.format( my_max ))\n",
    "\n",
    "    my_threshold = np.log2(my_epsilon) - np.log2(my_n)\n",
    "    print(' = {0}'.format( my_threshold ))\n",
    "\n",
    "    mask = my_logprobs - my_max >= my_threshold\n",
    "    np.sum(mask)\n",
    "    to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "    to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    my_alphas = np.array([to_alpha(l) for l in my_logprobs])\n",
    "    assert np.array_equal(my_alphas, to_alpha_vec(my_logprobs))\n",
    "    my_alpha_norm = np.sum(my_alphas)\n",
    "    my_probs = my_alphas / my_alpha_norm\n",
    "    np.sum(my_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.729828Z",
     "start_time": "2019-07-17T22:39:33.711270Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "def normalize_logprobs(logprobs, d=16, axis=0, b=None):\n",
    "# def normalize_logprobs(logprobs, d=16, b=None):\n",
    "#     axis = 0\n",
    "    n = logprobs.shape[axis]\n",
    "    epsilon = 10**(-1.0 * d)\n",
    "    maxlogp = np.max( logprobs[axis] )\n",
    "    if b is None:\n",
    "        threshold = np.log(epsilon) - np.log(n)\n",
    "        to_alpha = lambda logprob: np.exp(logprob - maxlogp) if (logprob - maxlogp) >= threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp(logprobs - maxlogp) * (logprobs - maxlogp >= threshold)\n",
    "    elif b == 2:\n",
    "        threshold = np.log2(epsilon) - np.log2(n)\n",
    "        to_alpha = lambda logprob: np.exp2(logprob - maxlogp) if (logprob - maxlogp) >= threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp2(logprobs - maxlogp) * (logprobs - maxlogp >= threshold)\n",
    "    elif b == 10:\n",
    "        threshold = np.log10(epsilon) - np.log10(n)\n",
    "        to_alpha = lambda logprob: np.power(logprob - maxlogp, 10) if (logprob - maxlogp) >= threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - maxlogp, 10) * (logprobs - maxlogp >= threshold)\n",
    "    else:\n",
    "        threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "        to_alpha = lambda logprob: np.power(logprob - maxlogp, b) if (logprob - maxlogp) >= threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - maxlogp, 10) * (logprobs - maxlogp >= threshold)\n",
    "    alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "#     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "    alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "    probs = alpha_is / alpha_norm\n",
    "#     assert np.isclose(np.sum(probs), 1.0)\n",
    "    return probs\n",
    "\n",
    "# from https://stats.stackexchange.com/a/66621\n",
    "def normalize_logprobs_buggy(logprobs, d=16, axis=0, b=None):\n",
    "# def normalize_logprobs(logprobs, d=16, b=None):\n",
    "#     axis = 0\n",
    "    n = logprobs.shape[axis]\n",
    "    epsilon = 10**(-1.0 * d)\n",
    "    maxlogp = np.max( logprobs[axis] )\n",
    "    if b is None:\n",
    "        threshold = np.log(epsilon) - np.log(n)\n",
    "        to_alpha = lambda logprob: np.exp(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 2:\n",
    "        threshold = np.log2(epsilon) - np.log2(n)\n",
    "        to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 10:\n",
    "        threshold = np.log10(epsilon) - np.log10(n)\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, 10) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    else:\n",
    "        threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, b) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "#     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "    alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "    probs = alpha_is / alpha_norm\n",
    "#     assert np.isclose(np.sum(probs), 1.0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.745598Z",
     "start_time": "2019-07-17T22:39:33.732006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.36237968e-08,   4.36237968e-08,   1.73028585e-06, ...,\n",
       "         4.36237968e-08,   6.11358636e-08,   4.36237968e-08])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    normalize_logprobs(-1.0 * hW_rc, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.762722Z",
     "start_time": "2019-07-17T22:39:33.748149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.46539838e-07,   2.46539838e-07,   1.45738607e-05, ...,\n",
       "         2.46539838e-07,   3.45509264e-07,   2.46539838e-07])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    normalize_logprobs(-1.0 * hVC[:,0], b=2)\n",
    "    normalize_logprobs(-1.0 * hVC[:,0], b=2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:19.225819Z",
     "start_time": "2019-07-11T19:40:19.219327Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map and c != '':\n",
    "    pVC = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:21.093501Z",
     "start_time": "2019-07-11T19:40:21.088617Z"
    }
   },
   "outputs": [],
   "source": [
    "# if memory_map:\n",
    "#     for j in range(my_shape[1]):\n",
    "#         pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.771155Z",
     "start_time": "2019-07-17T22:39:33.765473Z"
    }
   },
   "outputs": [],
   "source": [
    "def normColumn(j):\n",
    "    pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)\n",
    "\n",
    "if memory_map and c != '':\n",
    "    # takes 3.4m on wittgenstein with J=30 and other stuff going on in the background\n",
    "    par(delayed(normColumn)(j) for j in range(num_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:32.707485Z",
     "start_time": "2019-07-17T22:39:33.773436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0541s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 362 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 578 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 692 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1082 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1220 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1370 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1520 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1682 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1844 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2018 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2192 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2378 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2564 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2762 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2960 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3170 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3380 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3602 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3824 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4058 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4292 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4538 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4784 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 5042 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 5300 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5570 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 5840 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6122 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6404 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6698 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 6992 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 7298 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7604 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 8240 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 8570 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 8900 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9242 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9584 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 9938 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 10292 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 10658 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11024 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11402 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11780 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 12170 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 12560 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 12962 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 13364 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 13778 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 14192 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 14618 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 15044 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 15482 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done 15920 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 16370 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 16820 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed:   42.7s finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map and c != '':\n",
    "    #takes ~30s on wittgenstein with J=30 and other stuff going on in the background\n",
    "    pVC = np.vstack(par(delayed(normalize_logprobs)(-1.0 * hVC[:,j])\n",
    "                        for j in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:52.652832Z",
     "start_time": "2019-07-17T22:41:32.716656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:19<00:00, 251.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    N_test_indices = 5000\n",
    "    random_context_indices = choices(range(num_contexts), k=N_test_indices)\n",
    "\n",
    "    tests = [np.allclose(pVC[:,j], normalize_logprobs(-1.0 * hVC[:,j])) for j in tqdm(random_context_indices)]\n",
    "\n",
    "    all(tests)\n",
    "    assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:23.177940Z",
     "start_time": "2019-07-11T19:45:50.694876Z"
    }
   },
   "outputs": [],
   "source": [
    "if not memory_map and c != '':\n",
    "    #takes ~1.25m on wittgenstein with other stuff going on in the background\n",
    "    pVC_on_disk = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    pVC_on_disk[:,:] = pVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:58.318296Z",
     "start_time": "2019-07-17T22:41:52.656350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if c != '':\n",
    "    pVC.shape\n",
    "    pVC.dtype\n",
    "    pVC.nbytes / 1e9\n",
    "    np.sum(pVC, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:26.273807Z",
     "start_time": "2019-07-11T19:47:26.266946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:44.265249Z",
     "start_time": "2019-07-11T19:48:44.250158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if c != '':\n",
    "    pVC_dim_md = {'C':{'from fp':c,\n",
    "                       'changes':'sorted alphabetically',\n",
    "                       'size':len(contexts_sorted)},\n",
    "                  'V':{'from fp':v,\n",
    "                       'changes':'none - already sorted',\n",
    "                       'size':len(vocabulary_sorted)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.pV_C'+'_metadata.json',\n",
    "                         o+'.pV_C',\n",
    "                         pVC,\n",
    "                         pVC_dim_md,\n",
    "                         'Step 2b',\n",
    "                         'Producing contextual distributions',\n",
    "                         {'Comment':'Non-trivially normalized version of hVC with nearly the same name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:56.126618Z",
     "start_time": "2019-07-11T19:48:55.383173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    },\r\n",
      "    \"Comment\": \"Non-trivially normalized version of hVC with nearly the same name\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
