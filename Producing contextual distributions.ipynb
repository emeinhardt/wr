{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:51:49.069648Z",
     "start_time": "2019-07-11T18:51:49.061926Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eric Meinhardt / emeinhardt@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-loading-data\" data-toc-modified-id=\"Imports-/-loading-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Language-model\" data-toc-modified-id=\"Language-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Language model</a></span></li><li><span><a href=\"#Contexts\" data-toc-modified-id=\"Contexts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Contexts</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vocabulary</a></span></li></ul></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li><li><span><a href=\"#Calculate-the-number-of-computations-+-estimate-required-space\" data-toc-modified-id=\"Calculate-the-number-of-computations-+-estimate-required-space-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate the number of computations + estimate required space</a></span></li><li><span><a href=\"#Ensure-matrix-metadata-is-standardized\" data-toc-modified-id=\"Ensure-matrix-metadata-is-standardized-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ensure matrix metadata is standardized</a></span></li><li><span><a href=\"#Pick-out-relevant-functions-for-mapping-between-context/word-and-index\" data-toc-modified-id=\"Pick-out-relevant-functions-for-mapping-between-context/word-and-index-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Pick out relevant functions for mapping between context/word and index</a></span></li><li><span><a href=\"#Construct-and-write-distributions\" data-toc-modified-id=\"Construct-and-write-distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Construct and write distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Doing-calculations-in-place-/-via-memory-mapped-arrays\" data-toc-modified-id=\"Doing-calculations-in-place-/-via-memory-mapped-arrays-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Doing calculations in-place / via memory mapped arrays</a></span></li><li><span><a href=\"#Doing-calculations-in-memory-and-writing-to-disk\" data-toc-modified-id=\"Doing-calculations-in-memory-and-writing-to-disk-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Doing calculations in memory and writing to disk</a></span></li><li><span><a href=\"#Creating-a-version-that-contains-probabilities-(and-is-normalized)\" data-toc-modified-id=\"Creating-a-version-that-contains-probabilities-(and-is-normalized)-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Creating a version that contains probabilities (and is normalized)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sketch-of-normalization-process\" data-toc-modified-id=\"Sketch-of-normalization-process-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Sketch of normalization process</a></span></li></ul></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a file path $m$ to a `.arpa` file (or, more realistically/practically, a `kenlm` memory mapped version of one) for a language model\n",
    " - a file path $c$ to a set of $n$-gram contexts $C$ (a `.txt` file with one context per line, where a context is sequence of space-separated wordforms)\n",
    " - a file path $v$ to a vocabulary $W$ (a `.txt` file with one wordform per line)\n",
    " - a filepath $o$ for the main output of the notebook\n",
    " \n",
    "this notebook will calculate the distribution $p(W|C)$ as a memory mapped `numpy` array (written to $o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `kenlm`\n",
    " - `numpy`\n",
    " - `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:51:52.467088Z",
     "start_time": "2019-07-11T18:51:52.460363Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:51:52.590457Z",
     "start_time": "2019-07-11T18:51:52.584921Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:51:54.583933Z",
     "start_time": "2019-07-11T18:51:54.576063Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "m = ''\n",
    "# m = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_utterances_main_4gram.mmap'\n",
    "\n",
    "c = ''\n",
    "# c = '/home/AD/emeinhar/buckeye-lm' + '/' + 'buckeye_contexts.txt'\n",
    "\n",
    "v = ''\n",
    "# v = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_vocabulary_main.txt'\n",
    "\n",
    "o = ''\n",
    "# o = '/home/AD/emeinhar/wr' + '/' + 'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:25.800823Z",
     "start_time": "2019-07-11T18:54:25.792480Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = path.dirname(o)\n",
    "if not path.exists(output_dir):\n",
    "    print('Making ' + output_dir)\n",
    "    makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:52:02.909848Z",
     "start_time": "2019-07-11T18:52:02.685602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/buckeye_contexts.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyfile(c, path.join(output_dir, path.basename(c)))\n",
    "copyfile(v, path.join(output_dir, path.basename(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:52:08.597623Z",
     "start_time": "2019-07-11T18:52:08.588934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:52:09.108204Z",
     "start_time": "2019-07-11T18:52:09.094769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + dask + tiledb).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse tensor calculations + memory issues).ipynb',\n",
       " 'boilerplate.py',\n",
       " 'buckeye_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'swbd2003_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'LM_Fisher',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Matrix Metadata Lookup.ipynb',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Dask Examples.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Calculate segmental posterior given segmental wordform + context.ipynb',\n",
       " 'Channel Distribution Analysis.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'HMMs scratch.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
       " 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " '.pW_V.npz',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports / loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:06.968015Z",
     "start_time": "2019-07-11T18:54:06.608254Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:08.152179Z",
     "start_time": "2019-07-11T18:54:07.953048Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote, exportMatrixMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:13.941599Z",
     "start_time": "2019-07-11T18:54:13.905970Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:19.091195Z",
     "start_time": "2019-07-11T18:54:17.053885Z"
    }
   },
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:32.691848Z",
     "start_time": "2019-07-11T18:54:32.640682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('a',\n",
       " \"aaron's\",\n",
       " 'ability',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'accommodate')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "with open(c) as file:\n",
    "    for line in file:\n",
    "        contexts.append(line.rstrip())\n",
    "contexts = tuple(contexts)\n",
    "len(contexts)\n",
    "contexts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:37.775512Z",
     "start_time": "2019-07-11T18:54:37.765804Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(contexts)) == len(contexts), \"Contexts must consist of unique strings.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:39.191377Z",
     "start_time": "2019-07-11T18:54:39.091498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(\"'and\",\n",
       " \"'berserkly'\",\n",
       " \"'bout\",\n",
       " \"'burb\",\n",
       " \"'burban\",\n",
       " \"'burbs\",\n",
       " \"'cau\",\n",
       " \"'cause\",\n",
       " \"'cept\",\n",
       " \"'cide\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "with open(v) as file:\n",
    "    for line in file:\n",
    "        vocabulary.append(line.rstrip())\n",
    "vocabulary = tuple(vocabulary)\n",
    "len(vocabulary)\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:40.422828Z",
     "start_time": "2019-07-11T18:54:40.406218Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(vocabulary)) == len(vocabulary), \"Vocabulary must consist of unique wordforms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:46.181342Z",
     "start_time": "2019-07-11T18:54:46.175724Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:46.726437Z",
     "start_time": "2019-07-11T18:54:46.714368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there <rem>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sustaining'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt = choice(contexts)\n",
    "ctxt\n",
    "\n",
    "wrd = choice(vocabulary)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:49.635639Z",
     "start_time": "2019-07-11T18:54:49.577117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.022207260131836"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-11.116127967834473"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.684236526489258, 2, False),\n",
       " (-0.21483998000621796, 3, False),\n",
       " (-1.1899677515029907, 4, False),\n",
       " (-4.933162689208984, 2, False),\n",
       " (-0.45743539929389954, 3, False))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.47964234650135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.610599994659424, 1, False),\n",
       " (-1.2509719133377075, 2, False),\n",
       " (-1.429597020149231, 3, False),\n",
       " (-4.933162689208984, 2, False))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-10.224331617355347"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"this is a sentence\", eos = False)\n",
    "model.score(\"this is a sentence\")\n",
    "model.score(\"this is a sentence\", eos = True)\n",
    "model.score(\"this is a sentence </s>\", eos=False)\n",
    "model.score(\"this is a sentence </s>\")\n",
    "tuple(model.full_scores(\"this is a sentence\"))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\"))))\n",
    "' '\n",
    "tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:50.861863Z",
     "start_time": "2019-07-11T18:54:50.856086Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log10, log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:51.282263Z",
     "start_time": "2019-07-11T18:54:51.272493Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(word, context, base2=True, surprisal=True):\n",
    "    score_infos = tuple(model.full_scores(context + ' ' + word, eos=False, bos=False))\n",
    "    key_score_log10 = score_infos[-1][0]\n",
    "    if base2:\n",
    "        key_score = key_score_log10 / log10(2)\n",
    "    if surprisal:\n",
    "        key_score = -1.0 * key_score\n",
    "    return key_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:51.701038Z",
     "start_time": "2019-07-11T18:54:51.687141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there <rem>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sustaining'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17.7518716448236"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt\n",
    "wrd\n",
    "score(wrd, ctxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of computations + estimate required space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:54:55.112777Z",
     "start_time": "2019-07-11T18:54:55.090338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'767,374,560'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'6.13899648 GB'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits_per_cell = 64\n",
    "bytes_per_cell = bits_per_cell / 8\n",
    "\n",
    "len(contexts)\n",
    "len(vocabulary)\n",
    "\"{:,}\".format( len(contexts) * len(vocabulary) )\n",
    "\"{:,} GB\".format( len(contexts) * len(vocabulary) * bytes_per_cell / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.398Z"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.401Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = product(contexts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.404Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = tuple(product(contexts, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.408Z"
    }
   },
   "outputs": [],
   "source": [
    "# from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.411Z"
    }
   },
   "outputs": [],
   "source": [
    "# example_computations = choices(computations, k=10)\n",
    "# example_computations\n",
    "# ex = choice(example_computations)\n",
    "# ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure matrix metadata is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:55:46.711462Z",
     "start_time": "2019-07-11T18:55:46.697709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)\n",
    "type(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:55:58.139872Z",
     "start_time": "2019-07-11T18:55:58.116280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary) == sorted(list(vocabulary))\n",
    "list(contexts) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:57:11.475661Z",
     "start_time": "2019-07-11T18:57:11.459674Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_sorted = tuple(sorted(list(vocabulary)))\n",
    "contexts_sorted = tuple(sorted(list(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:57:25.125649Z",
     "start_time": "2019-07-11T18:57:25.107841Z"
    }
   },
   "outputs": [],
   "source": [
    "assert list(vocabulary_sorted) == sorted(list(vocabulary))\n",
    "assert list(contexts_sorted) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick out relevant functions for mapping between context/word and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:57:47.931076Z",
     "start_time": "2019-07-11T18:57:47.907002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sustaining'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "38367"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'there <rem>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12319"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd\n",
    "vocabulary_sorted.index(wrd)\n",
    "ctxt\n",
    "contexts_sorted.index(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T22:41:22.418Z"
    }
   },
   "outputs": [],
   "source": [
    "# ex\n",
    "# contexts.index(ex[0])\n",
    "# contexts[ contexts.index(ex[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:12.367533Z",
     "start_time": "2019-07-11T18:58:12.360263Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_np(word_idx, context_idx, base2=True, surprisal=True):\n",
    "    return score(vocabulary_sorted[word_idx], contexts_sorted[context_idx], base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and write distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:19.402806Z",
     "start_time": "2019-07-11T18:58:19.386313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "767374560"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(vocabulary_sorted)\n",
    "num_contexts = len(contexts_sorted)\n",
    "my_shape = (num_words, num_contexts) #columns are distributions\n",
    "my_shape\n",
    "num_words * num_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:21.879212Z",
     "start_time": "2019-07-11T18:58:21.873729Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_map = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in-place / via memory mapped arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will only begin to make sense if the array is going to be too large to fit in memory or very small; otherwise joblib will use threads to parallelize the computation (because it involves lots of IO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:34.491059Z",
     "start_time": "2019-07-11T18:58:34.483917Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:35.050269Z",
     "start_time": "2019-07-11T18:58:35.044700Z"
    }
   },
   "outputs": [],
   "source": [
    "score_np_vec = np.vectorize(score_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:39.276668Z",
     "start_time": "2019-07-11T18:58:39.269186Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    def define_score(word_idx, context_idx, base2=True, surprisal=True):\n",
    "        hVC[word_idx, context_idx] = score_np(word_idx, context_idx, base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:40.465492Z",
     "start_time": "2019-07-11T18:58:40.460149Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Started calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:44.402488Z",
     "start_time": "2019-07-11T18:58:44.395409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    # est 7h on wittgenstein with J=30 and other computations going on in the background\n",
    "    # pretty sure that means it's using threads rather than processes\n",
    "    par(delayed(define_score)(w_idx, ctxt_idx) \n",
    "        for ctxt_idx in range(num_contexts) \n",
    "        for w_idx in range(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T18:58:44.917953Z",
     "start_time": "2019-07-11T18:58:44.912611Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Ended calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for testing ordering because of the way define_score is defined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC_dim_md = {'C':{'from fp':c,\n",
    "                       'changes':'sorted alphabetically',\n",
    "                       'size':len(contexts_sorted)},\n",
    "                  'V':{'from fp':v,\n",
    "                       'changes':'none - already sorted',\n",
    "                       'size':len(vocabulary_sorted)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                         o+'.hV_C',\n",
    "                         hVC,\n",
    "                         hVC_dim_md,\n",
    "                         'Step 2b',\n",
    "                         'Producing contextual distributions',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in memory and writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:11:10.155780Z",
     "start_time": "2019-07-11T18:59:34.967714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 317 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 373 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 433 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 497 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 530 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 565 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-1)]: Done 637 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 674 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 713 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 793 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 834 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done 877 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 920 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 965 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1010 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1057 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1153 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1202 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1253 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1357 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1410 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1465 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1520 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1577 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1634 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1693 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1752 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1813 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1874 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1937 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2065 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2130 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2197 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2264 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2333 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2402 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2473 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2544 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2617 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2690 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2765 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2840 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2917 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2994 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3073 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3152 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3233 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3314 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3397 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3480 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3565 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3650 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3737 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3824 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3913 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4002 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4093 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4277 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4370 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4465 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4560 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4657 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4754 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4853 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4952 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5053 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5154 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5257 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5360 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5465 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5570 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5677 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5784 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5893 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6002 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6113 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6224 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6337 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6450 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6565 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6680 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6797 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6914 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7033 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7152 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7273 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7394 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7517 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7640 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7765 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7890 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8017 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8144 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8273 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8402 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8533 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8664 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 8797 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8930 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9065 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9200 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9337 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9474 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9613 tasks      | elapsed:  6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 9752 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9893 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10034 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10177 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10320 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10465 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10610 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10757 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10904 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11053 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11202 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11353 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11504 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11657 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 11810 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11965 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12120 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12277 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 12434 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12593 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 12752 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 12913 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 13074 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 13237 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 13400 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13565 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 13730 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13897 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14064 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14233 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14402 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14573 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14744 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14917 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15090 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 15265 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15440 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 15617 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15794 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15973 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16152 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16333 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 16514 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 16697 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 16880 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 17065 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 17250 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed: 11.5min finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #~30m on wittgenstein\n",
    "#     hVC = np.vstack([score_np_vec(np.array(range(num_words)), c_idx) for c_idx in tqdm(range(num_contexts))]).T\n",
    "    \n",
    "    #takes ~2.75m on wittgenstein with J=30 and other things going on in the background\n",
    "    hVC = np.vstack(par(delayed(score_np_vec)(np.array(range(num_words)), c_idx)\n",
    "                        for c_idx in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for reasons of caution and paranoia, we check that the ordering on contexts and wordforms is preserved by picking a thousand random context-wordform pairs, doing the calculations manually and checking that the calculation matches what's at the corresponding location in `hVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:11:43.351870Z",
     "start_time": "2019-07-11T19:11:43.324811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC[0,0]\n",
    "score_np(0,0) #word idx, context idx\n",
    "\n",
    "hVC[2,3]\n",
    "score_np(2,3)\n",
    "\n",
    "hVC[3,2]\n",
    "score_np(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:11:47.089062Z",
     "start_time": "2019-07-11T19:11:46.857248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choices\n",
    "\n",
    "N_test_pairs = 10000\n",
    "random_context_indices = choices(range(num_contexts), k=N_test_pairs)\n",
    "random_orthWord_indices = choices(range(num_words), k=N_test_pairs)\n",
    "\n",
    "random_index_pairs = tuple(zip(random_orthWord_indices,\n",
    "                               random_context_indices))\n",
    "\n",
    "tests = [hVC[i,j] == score_np(i,j) for i,j in random_index_pairs]\n",
    "all(tests)\n",
    "assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.337706Z",
     "start_time": "2019-07-11T19:12:11.438259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype\n",
    "    hVC_on_disk = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    \n",
    "    #takes ~1.25m on wittgenstein\n",
    "    hVC_on_disk[:,:] = hVC\n",
    "    hVC = hVC_on_disk\n",
    "    del hVC_on_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.356969Z",
     "start_time": "2019-07-11T19:13:52.344908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.402238Z",
     "start_time": "2019-07-11T19:13:52.361822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.421622Z",
     "start_time": "2019-07-11T19:13:52.406656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.nbytes / 1e9\n",
    "hVC.shape\n",
    "hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:45.228741Z",
     "start_time": "2019-07-11T19:35:45.219379Z"
    }
   },
   "outputs": [],
   "source": [
    "# def exportMatrixMetadata(md_fp, matrix_fp, matrix, dim_md, step_name, nb_name, other_md):\n",
    "#     md = {'matrix fp':matrix_fp,\n",
    "#           'matrix shape':matrix.shape,\n",
    "#           'Produced in step':step_name,\n",
    "#           'Produced in notebook':nb_name}\n",
    "#     md.update(dim_md)\n",
    "#     md.update(other_md)\n",
    "#     exportDict(md_fp, md)\n",
    "#     print(f'Wrote metadata for \\n\\t{matrix_fp}\\n to \\n\\t{md_fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:33:11.492027Z",
     "start_time": "2019-07-11T19:33:11.484423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:47.103542Z",
     "start_time": "2019-07-11T19:35:47.089939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "hVC_dim_md = {'C':{'from fp':c,\n",
    "                   'changes':'sorted alphabetically',\n",
    "                   'size':len(contexts_sorted)},\n",
    "              'V':{'from fp':v,\n",
    "                   'changes':'none - already sorted',\n",
    "                   'size':len(vocabulary_sorted)}}\n",
    "# other_md = {'Produced in step':'Step 2b',\n",
    "#             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                     o+'.hV_C',\n",
    "                     hVC,\n",
    "                     hVC_dim_md,\n",
    "                     'Step 2b',\n",
    "                     'Producing contextual distributions',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:36:01.113482Z",
     "start_time": "2019-07-11T19:36:00.649990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a version that contains probabilities (and is normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just naively convert (-) log-probabilities to probabilities if we're interested in distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:38:42.268581Z",
     "start_time": "2019-07-11T19:38:12.031329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98952166,  0.92990437,  0.97399201, ...,  0.96380412,\n",
       "        0.9685743 ,  0.86971511])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB! assumes hVC is in base 2 surprisals...\n",
    "dist_norms = np.sum(np.exp2(-1.0 * hVC), axis=0)\n",
    "dist_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:38:52.487321Z",
     "start_time": "2019-07-11T19:38:52.477554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i did'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_context = choice(contexts_sorted)\n",
    "random_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:38:54.297667Z",
     "start_time": "2019-07-11T19:38:54.285731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2993"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts.index(random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:13.629788Z",
     "start_time": "2019-07-11T19:39:09.680760Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0117s.) Setting batch_size=34.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0252s.) Setting batch_size=538.\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 626 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1808s.) Setting batch_size=1190.\n",
      "[Parallel(n_jobs=-1)]: Done 44064 out of 44064 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "hW_rc = np.array(par(delayed(score)(w, random_context) for w in vocabulary_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:17.207094Z",
     "start_time": "2019-07-11T19:39:17.068135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal( hVC[:,contexts_sorted.index(random_context)], hW_rc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:21.408773Z",
     "start_time": "2019-07-11T19:39:21.392227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.08164937e-08,   6.08164937e-08,   2.41221365e-06,\n",
       "         8.52301967e-08,   6.08164937e-08,   6.08164937e-08,\n",
       "         6.08164937e-08,   1.48785937e-03,   2.66976272e-07,\n",
       "         6.08164937e-08])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.95962677681213815"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_rc = np.exp2(-1.0 * hW_rc)\n",
    "pW_rc[:10]\n",
    "np.sum(pW_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:23.531176Z",
     "start_time": "2019-07-11T19:39:23.523165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:43.465085Z",
     "start_time": "2019-07-11T19:39:43.182262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = 1e-16\n",
      "n = 44064\n",
      "max() =  = -4.113823025793098\n",
      " = -68.57816236233276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.99999999999999989"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "my_logprobs = -1.0 * hW_rc\n",
    "my_epsilon = 10 ** (-1.0 * 16)\n",
    "print(' = {0}'.format( my_epsilon ))\n",
    "\n",
    "my_n = my_logprobs.shape[0]\n",
    "print('n = {0}'.format( my_n ))\n",
    "\n",
    "my_max = np.max(my_logprobs)\n",
    "print('max() =  = {0}'.format( my_max ))\n",
    "\n",
    "my_threshold = np.log2(my_epsilon) - np.log2(my_n)\n",
    "print(' = {0}'.format( my_threshold ))\n",
    "\n",
    "mask = my_logprobs - my_max >= my_threshold\n",
    "np.sum(mask)\n",
    "to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "my_alphas = np.array([to_alpha(l) for l in my_logprobs])\n",
    "assert np.array_equal(my_alphas, to_alpha_vec(my_logprobs))\n",
    "my_alpha_norm = np.sum(my_alphas)\n",
    "my_probs = my_alphas / my_alpha_norm\n",
    "np.sum(my_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:56.799305Z",
     "start_time": "2019-07-11T19:39:56.746512Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "def normalize_logprobs(logprobs, d=16, axis=0, b=None):\n",
    "# def normalize_logprobs(logprobs, d=16, b=None):\n",
    "#     axis = 0\n",
    "    n = logprobs.shape[axis]\n",
    "    epsilon = 10**(-1.0 * d)\n",
    "    maxlogp = np.max( logprobs[axis] )\n",
    "    if b is None:\n",
    "        threshold = np.log(epsilon) - np.log(n)\n",
    "        to_alpha = lambda logprob: np.exp(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 2:\n",
    "        threshold = np.log2(epsilon) - np.log2(n)\n",
    "        to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 10:\n",
    "        threshold = np.log10(epsilon) - np.log10(n)\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, 10) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    else:\n",
    "        threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, b) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "#     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "    alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "    probs = alpha_is / alpha_norm\n",
    "#     assert np.isclose(np.sum(probs), 1.0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:39:58.207758Z",
     "start_time": "2019-07-11T19:39:58.195017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.33751529e-08,   6.33751529e-08,   2.51369982e-06, ...,\n",
       "         6.33751529e-08,   8.88159842e-08,   6.33751529e-08])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hW_rc, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:00.838606Z",
     "start_time": "2019-07-11T19:40:00.674291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.46539838e-07,   2.46539838e-07,   1.45738607e-05, ...,\n",
       "         2.46539838e-07,   3.45509264e-07,   2.46539838e-07])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hVC[:,0], b=2)\n",
    "normalize_logprobs(-1.0 * hVC[:,0], b=2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:19.225819Z",
     "start_time": "2019-07-11T19:40:19.219327Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    pVC = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:21.093501Z",
     "start_time": "2019-07-11T19:40:21.088617Z"
    }
   },
   "outputs": [],
   "source": [
    "# if memory_map:\n",
    "#     for j in range(my_shape[1]):\n",
    "#         pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:27.773140Z",
     "start_time": "2019-07-11T19:40:27.764752Z"
    }
   },
   "outputs": [],
   "source": [
    "def normColumn(j):\n",
    "    pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)\n",
    "\n",
    "if memory_map:\n",
    "    # takes 3.4m on wittgenstein with J=30 and other stuff going on in the background\n",
    "    par(delayed(normColumn)(j) for j in range(num_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:42:34.263435Z",
     "start_time": "2019-07-11T19:40:44.615676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0191s.) Setting batch_size=20.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 548 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 728 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 948 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1168 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1428 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1688 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1988 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2288 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2628 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2968 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3348 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3728 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4148 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4568 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 5028 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 5488 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 5988 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6488 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 7028 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done 7568 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 8148 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 8728 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9348 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 9968 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 10628 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11288 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11988 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12688 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 13428 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14168 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14948 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15728 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16548 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 17368 out of 17415 | elapsed:  1.7min remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #takes ~30s on wittgenstein with J=30 and other stuff going on in the background\n",
    "    pVC = np.vstack(par(delayed(normalize_logprobs)(-1.0 * hVC[:,j])\n",
    "                        for j in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:45:44.054084Z",
     "start_time": "2019-07-11T19:45:15.776258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:28<00:00, 177.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test_indices = 5000\n",
    "random_context_indices = choices(range(num_contexts), k=N_test_indices)\n",
    "\n",
    "tests = [np.allclose(pVC[:,j], normalize_logprobs(-1.0 * hVC[:,j])) for j in tqdm(random_context_indices)]\n",
    "\n",
    "all(tests)\n",
    "assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:23.177940Z",
     "start_time": "2019-07-11T19:45:50.694876Z"
    }
   },
   "outputs": [],
   "source": [
    "if not memory_map:\n",
    "    #takes ~1.25m on wittgenstein with other stuff going on in the background\n",
    "    pVC_on_disk = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    pVC_on_disk[:,:] = pVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:26.263220Z",
     "start_time": "2019-07-11T19:47:23.183530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pVC.shape\n",
    "pVC.dtype\n",
    "pVC.nbytes / 1e9\n",
    "np.sum(pVC, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:26.273807Z",
     "start_time": "2019-07-11T19:47:26.266946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:44.265249Z",
     "start_time": "2019-07-11T19:48:44.250158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "pVC_dim_md = {'C':{'from fp':c,\n",
    "                   'changes':'sorted alphabetically',\n",
    "                   'size':len(contexts_sorted)},\n",
    "              'V':{'from fp':v,\n",
    "                   'changes':'none - already sorted',\n",
    "                   'size':len(vocabulary_sorted)}}\n",
    "# other_md = {'Produced in step':'Step 2b',\n",
    "#             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "exportMatrixMetadata(o+'.pV_C'+'_metadata.json',\n",
    "                     o+'.pV_C',\n",
    "                     pVC,\n",
    "                     pVC_dim_md,\n",
    "                     'Step 2b',\n",
    "                     'Producing contextual distributions',\n",
    "                     {'Comment':'Non-trivially normalized version of hVC with nearly the same name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:56.126618Z",
     "start_time": "2019-07-11T19:48:55.383173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    },\r\n",
      "    \"Comment\": \"Non-trivially normalized version of hVC with nearly the same name\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
