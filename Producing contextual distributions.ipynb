{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.281108Z",
     "start_time": "2019-07-17T22:20:45.273615Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eric Meinhardt / emeinhardt@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-loading-data\" data-toc-modified-id=\"Imports-/-loading-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Language-model\" data-toc-modified-id=\"Language-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Language model</a></span></li><li><span><a href=\"#Contexts\" data-toc-modified-id=\"Contexts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Contexts</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vocabulary</a></span></li></ul></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li><li><span><a href=\"#Calculate-the-number-of-computations-+-estimate-required-space\" data-toc-modified-id=\"Calculate-the-number-of-computations-+-estimate-required-space-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate the number of computations + estimate required space</a></span></li><li><span><a href=\"#Ensure-matrix-metadata-is-standardized\" data-toc-modified-id=\"Ensure-matrix-metadata-is-standardized-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ensure matrix metadata is standardized</a></span></li><li><span><a href=\"#Pick-out-relevant-functions-for-mapping-between-context/word-and-index\" data-toc-modified-id=\"Pick-out-relevant-functions-for-mapping-between-context/word-and-index-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Pick out relevant functions for mapping between context/word and index</a></span></li><li><span><a href=\"#Construct-and-write-distributions\" data-toc-modified-id=\"Construct-and-write-distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Construct and write distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Doing-calculations-in-place-/-via-memory-mapped-arrays\" data-toc-modified-id=\"Doing-calculations-in-place-/-via-memory-mapped-arrays-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Doing calculations in-place / via memory mapped arrays</a></span></li><li><span><a href=\"#Doing-calculations-in-memory-and-writing-to-disk\" data-toc-modified-id=\"Doing-calculations-in-memory-and-writing-to-disk-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Doing calculations in memory and writing to disk</a></span></li><li><span><a href=\"#Creating-a-version-that-contains-probabilities-(and-is-normalized)\" data-toc-modified-id=\"Creating-a-version-that-contains-probabilities-(and-is-normalized)-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Creating a version that contains probabilities (and is normalized)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sketch-of-normalization-process\" data-toc-modified-id=\"Sketch-of-normalization-process-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Sketch of normalization process</a></span></li></ul></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a file path $m$ to a `.arpa` file (or, more realistically/practically, a `kenlm` memory mapped version of one) for a language model\n",
    " - a file path $c$ to a set of $n$-gram contexts $C$ (a `.txt` file with one context per line, where a context is sequence of space-separated wordforms)\n",
    " - a file path $v$ to a vocabulary $W$ (a `.txt` file with one wordform per line)\n",
    " - a filepath $o$ for the main output of the notebook\n",
    " \n",
    "this notebook will calculate the distribution $p(W|C)$ as a memory mapped `numpy` array (written to $o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `kenlm`\n",
    " - `numpy`\n",
    " - `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.331169Z",
     "start_time": "2019-07-17T22:20:45.289924Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.340264Z",
     "start_time": "2019-07-17T22:20:45.335515Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.350169Z",
     "start_time": "2019-07-17T22:20:45.344068Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "m = ''\n",
    "# m = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_utterances_main_4gram.mmap'\n",
    "\n",
    "c = ''\n",
    "# c = '/home/AD/emeinhar/buckeye-lm' + '/' + 'buckeye_contexts.txt'\n",
    "\n",
    "v = ''\n",
    "# v = '/home/AD/emeinhar/fisher-lm' + '/' + 'fisher_vocabulary_main.txt'\n",
    "\n",
    "o = ''\n",
    "# o = '/home/AD/emeinhar/wr' + '/' + 'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.364464Z",
     "start_time": "2019-07-17T22:20:45.353394Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = path.dirname(o)\n",
    "if not path.exists(output_dir):\n",
    "    print('Making ' + output_dir)\n",
    "    makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.690969Z",
     "start_time": "2019-07-17T22:20:45.367210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/buckeye_contexts.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyfile(c, path.join(output_dir, path.basename(c)))\n",
    "copyfile(v, path.join(output_dir, path.basename(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.700498Z",
     "start_time": "2019-07-17T22:20:45.694233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:45.747782Z",
     "start_time": "2019-07-17T22:20:45.703355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + dask + tiledb).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse tensor calculations + memory issues).ipynb',\n",
       " 'boilerplate.py',\n",
       " 'buckeye_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'swbd2003_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + joblib + tiledb + clean).ipynb',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'LM_Fisher',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Matrix Metadata Lookup.ipynb',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Dask Examples.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (old).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Calculate segmental posterior given segmental wordform + context.ipynb',\n",
       " 'Channel Distribution Analysis.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'HMMs scratch.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
       " 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " '.pW_V.npz',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports / loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:46.561681Z",
     "start_time": "2019-07-17T22:20:45.752615Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:46.784352Z",
     "start_time": "2019-07-17T22:20:46.565483Z"
    }
   },
   "outputs": [],
   "source": [
    "from boilerplate import stamp, stampedNote, exportMatrixMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:46.813180Z",
     "start_time": "2019-07-17T22:20:46.788983Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:48.970124Z",
     "start_time": "2019-07-17T22:20:46.816129Z"
    }
   },
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.013158Z",
     "start_time": "2019-07-17T22:20:48.975049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('a',\n",
       " \"aaron's\",\n",
       " 'ability',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'accommodate')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "with open(c) as file:\n",
    "    for line in file:\n",
    "        contexts.append(line.rstrip())\n",
    "contexts = tuple(contexts)\n",
    "len(contexts)\n",
    "contexts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.020922Z",
     "start_time": "2019-07-17T22:20:49.015299Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(contexts)) == len(contexts), \"Contexts must consist of unique strings.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.090844Z",
     "start_time": "2019-07-17T22:20:49.023338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(\"'and\",\n",
       " \"'berserkly'\",\n",
       " \"'bout\",\n",
       " \"'burb\",\n",
       " \"'burban\",\n",
       " \"'burbs\",\n",
       " \"'cau\",\n",
       " \"'cause\",\n",
       " \"'cept\",\n",
       " \"'cide\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "with open(v) as file:\n",
    "    for line in file:\n",
    "        vocabulary.append(line.rstrip())\n",
    "vocabulary = tuple(vocabulary)\n",
    "len(vocabulary)\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.104011Z",
     "start_time": "2019-07-17T22:20:49.093466Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(set(vocabulary)) == len(vocabulary), \"Vocabulary must consist of unique wordforms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.113165Z",
     "start_time": "2019-07-17T22:20:49.106274Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.165010Z",
     "start_time": "2019-07-17T22:20:49.116302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yknow she was'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'confirmations'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt = choice(contexts)\n",
    "ctxt\n",
    "\n",
    "wrd = choice(vocabulary)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.207967Z",
     "start_time": "2019-07-17T22:20:49.168577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.022207260131836"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.479642868041992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-11.116127967834473"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.684236526489258, 2, False),\n",
       " (-0.21483998000621796, 3, False),\n",
       " (-1.1899677515029907, 4, False),\n",
       " (-4.933162689208984, 2, False),\n",
       " (-0.45743539929389954, 3, False))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.47964234650135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.610599994659424, 1, False),\n",
       " (-1.2509719133377075, 2, False),\n",
       " (-1.429597020149231, 3, False),\n",
       " (-4.933162689208984, 2, False))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-10.224331617355347"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"this is a sentence\", eos = False)\n",
    "model.score(\"this is a sentence\")\n",
    "model.score(\"this is a sentence\", eos = True)\n",
    "model.score(\"this is a sentence </s>\", eos=False)\n",
    "model.score(\"this is a sentence </s>\")\n",
    "tuple(model.full_scores(\"this is a sentence\"))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\"))))\n",
    "' '\n",
    "tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(model.full_scores(\"this is a sentence\", eos=False, bos=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.213444Z",
     "start_time": "2019-07-17T22:20:49.210359Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log10, log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.225663Z",
     "start_time": "2019-07-17T22:20:49.215718Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(word, context, base2=True, surprisal=True):\n",
    "    score_infos = tuple(model.full_scores(context + ' ' + word, eos=False, bos=False))\n",
    "    key_score_log10 = score_infos[-1][0]\n",
    "    if base2:\n",
    "        key_score = key_score_log10 / log10(2)\n",
    "    if surprisal:\n",
    "        key_score = -1.0 * key_score\n",
    "    return key_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.240257Z",
     "start_time": "2019-07-17T22:20:49.228310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yknow she was'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'confirmations'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23.48230500497572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxt\n",
    "wrd\n",
    "score(wrd, ctxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of computations + estimate required space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.256580Z",
     "start_time": "2019-07-17T22:20:49.243087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'767,374,560'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'6.13899648 GB'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits_per_cell = 64\n",
    "bytes_per_cell = bits_per_cell / 8\n",
    "\n",
    "len(contexts)\n",
    "len(vocabulary)\n",
    "\"{:,}\".format( len(contexts) * len(vocabulary) )\n",
    "\"{:,} GB\".format( len(contexts) * len(vocabulary) * bytes_per_cell / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.265082Z",
     "start_time": "2019-07-17T22:20:49.262209Z"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.273762Z",
     "start_time": "2019-07-17T22:20:49.267588Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = product(contexts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.281630Z",
     "start_time": "2019-07-17T22:20:49.276246Z"
    }
   },
   "outputs": [],
   "source": [
    "# computations = tuple(product(contexts, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.289676Z",
     "start_time": "2019-07-17T22:20:49.284197Z"
    }
   },
   "outputs": [],
   "source": [
    "# from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.296511Z",
     "start_time": "2019-07-17T22:20:49.292308Z"
    }
   },
   "outputs": [],
   "source": [
    "# example_computations = choices(computations, k=10)\n",
    "# example_computations\n",
    "# ex = choice(example_computations)\n",
    "# ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure matrix metadata is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.307508Z",
     "start_time": "2019-07-17T22:20:49.299010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)\n",
    "type(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.325223Z",
     "start_time": "2019-07-17T22:20:49.309820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary) == sorted(list(vocabulary))\n",
    "list(contexts) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.337868Z",
     "start_time": "2019-07-17T22:20:49.327828Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_sorted = tuple(sorted(list(vocabulary)))\n",
    "contexts_sorted = tuple(sorted(list(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.351001Z",
     "start_time": "2019-07-17T22:20:49.340025Z"
    }
   },
   "outputs": [],
   "source": [
    "assert list(vocabulary_sorted) == sorted(list(vocabulary))\n",
    "assert list(contexts_sorted) == sorted(list(contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick out relevant functions for mapping between context/word and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.367421Z",
     "start_time": "2019-07-17T22:20:49.353312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'confirmations'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8063"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'yknow she was'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16831"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd\n",
    "vocabulary_sorted.index(wrd)\n",
    "ctxt\n",
    "contexts_sorted.index(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.372960Z",
     "start_time": "2019-07-17T22:20:49.370029Z"
    }
   },
   "outputs": [],
   "source": [
    "# ex\n",
    "# contexts.index(ex[0])\n",
    "# contexts[ contexts.index(ex[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:20:49.383618Z",
     "start_time": "2019-07-17T22:20:49.375668Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_np(word_idx, context_idx, base2=True, surprisal=True):\n",
    "    return score(vocabulary_sorted[word_idx], contexts_sorted[context_idx], base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:44:41.769832Z",
     "start_time": "2019-09-05T15:44:40.537844Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'choice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de3e7f533368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrandom_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mrandom_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# contexts_sorted.index('a couple of')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'choice' is not defined"
     ]
    }
   ],
   "source": [
    "def hW_C_lookup(w=None,c=None):\n",
    "    Ws_t = vocabulary_sorted\n",
    "    Cs_t = contexts_sorted\n",
    "    if w is None and c is None:\n",
    "        raise Exception('Must specify at least one of a context string or orthographic wordform string')\n",
    "    if w is None:\n",
    "#         my_pW_c = pW_C[:,Cs_t.index(c)]\n",
    "#         my_pW_c_as_dict = dict(zip(Cs_t, my_pW_c))\n",
    "        my_hW_c_as_dict = {w:score(w,c) for w in vocabulary_sorted}\n",
    "        return my_hW_c_as_dict\n",
    "    if w is not None and c is not None:\n",
    "        my_hw_c = score(w, c)\n",
    "        return my_hw_c\n",
    "    if c is None:\n",
    "#         my_pw_C = pW_C[Ws_t.index(w), :]\n",
    "#         my_pw_C_as_dict = dict(zip(Cs_t, my_pw_C))\n",
    "        my_hw_C_as_dict = {c:score(w,c) for c in contexts_sorted}\n",
    "        return my_hw_C_as_dict\n",
    "    \n",
    "\n",
    "random_context = choice(contexts_sorted)\n",
    "random_context\n",
    "# contexts_sorted.index('a couple of')\n",
    "# my_surp_dist = hW_C_lookup(c='a couple of')\n",
    "my_surp_dist = hW_C_lookup(c=random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from probdist import *\n",
    "\n",
    "# def pW_C_lookup(w=None,c=None):\n",
    "#     Ws_t = vocabulary_sorted\n",
    "#     Cs_t = contexts_sorted\n",
    "#     if w is None and c is None:\n",
    "#         raise Exception('Must specify at least one of a context string or orthographic wordform string')\n",
    "#     if w is None:\n",
    "# #         my_pW_c = pW_C[:,Cs_t.index(c)]\n",
    "# #         my_pW_c_as_dict = dict(zip(Cs_t, my_pW_c))\n",
    "#         my_pW_c_as_dict = {w:score(w,c, surprisal=False) for w in vocabulary_sorted}\n",
    "#         assert isNormalized(my_pW_c_as_dict), f\"norm = {norm(my_pW_c_as_dict)}\"\n",
    "#         return ProbDist(my_pW_c_as_dict)\n",
    "#     if w is not None and c is not None:\n",
    "#         my_pw_c = score(w, c, surprisal=False)\n",
    "#         return my_pw_c\n",
    "#     if c is None:\n",
    "# #         my_pw_C = pW_C[Ws_t.index(w), :]\n",
    "# #         my_pw_C_as_dict = dict(zip(Cs_t, my_pw_C))\n",
    "#         my_pw_C_as_dict = {c:score(w,c, surprisal=False) for c in contexts_sorted}\n",
    "#         return my_pw_C_as_dict\n",
    "    \n",
    "    \n",
    "# contexts_sorted.index('a couple of')\n",
    "# my_dist = pW_C_lookup(c='a couple of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and write distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:07.496706Z",
     "start_time": "2019-07-17T22:23:07.484042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "767374560"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(vocabulary_sorted)\n",
    "num_contexts = len(contexts_sorted)\n",
    "my_shape = (num_words, num_contexts) #columns are distributions\n",
    "my_shape\n",
    "num_words * num_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:08.006592Z",
     "start_time": "2019-07-17T22:23:08.002230Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_map = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in-place / via memory mapped arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will only begin to make sense if the array is going to be too large to fit in memory or very small; otherwise joblib will use threads to parallelize the computation (because it involves lots of IO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:09.683214Z",
     "start_time": "2019-07-17T22:23:09.672335Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:09.936416Z",
     "start_time": "2019-07-17T22:23:09.931608Z"
    }
   },
   "outputs": [],
   "source": [
    "score_np_vec = np.vectorize(score_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.191649Z",
     "start_time": "2019-07-17T22:23:10.184446Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    def define_score(word_idx, context_idx, base2=True, surprisal=True):\n",
    "        hVC[word_idx, context_idx] = score_np(word_idx, context_idx, base2=base2, surprisal=surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.531798Z",
     "start_time": "2019-07-17T22:23:10.526941Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Started calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:10.837237Z",
     "start_time": "2019-07-17T22:23:10.830457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    # est 7h on wittgenstein with J=30 and other computations going on in the background\n",
    "    # pretty sure that means it's using threads rather than processes\n",
    "    par(delayed(define_score)(w_idx, ctxt_idx) \n",
    "        for ctxt_idx in range(num_contexts) \n",
    "        for w_idx in range(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:11.411337Z",
     "start_time": "2019-07-17T22:23:11.406884Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    stampedNote(\"Ended calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:12.594231Z",
     "start_time": "2019-07-17T22:23:12.589543Z"
    }
   },
   "outputs": [],
   "source": [
    "# no need for testing ordering because of the way define_score is defined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:23:13.876898Z",
     "start_time": "2019-07-17T22:23:13.867806Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    hVC_dim_md = {'C':{'from fp':c,\n",
    "                       'changes':'sorted alphabetically',\n",
    "                       'size':len(contexts_sorted)},\n",
    "                  'V':{'from fp':v,\n",
    "                       'changes':'none - already sorted',\n",
    "                       'size':len(vocabulary_sorted)}}\n",
    "    # other_md = {'Produced in step':'Step 2b',\n",
    "    #             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "    exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                         o+'.hV_C',\n",
    "                         hVC,\n",
    "                         hVC_dim_md,\n",
    "                         'Step 2b',\n",
    "                         'Producing contextual distributions',\n",
    "                         {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing calculations in memory and writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:23.675339Z",
     "start_time": "2019-07-17T22:23:25.682619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 365 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 441 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 648 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 693 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 738 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 785 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 881 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 930 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1032 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1085 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1138 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1305 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1421 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1480 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1541 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1602 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1665 tasks      | elapsed:   47.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1728 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1793 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1925 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1992 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2061 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2130 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2201 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2272 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2345 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2493 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2645 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2801 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2961 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3042 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3125 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3208 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3293 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3378 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3465 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3552 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3641 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3730 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3821 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3912 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4005 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4193 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4288 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4385 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4482 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4581 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4680 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4781 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4882 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4985 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5088 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5193 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5298 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5405 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5512 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5621 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5730 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5841 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5952 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6065 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6178 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6293 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6408 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6525 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6642 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6761 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6880 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7001 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7122 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7245 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7368 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7493 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7618 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7745 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7872 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8001 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8130 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8261 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8392 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8525 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8658 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8793 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8928 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9065 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9202 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9341 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9480 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9621 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9762 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9905 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10048 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10193 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10338 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10485 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10632 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10781 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10930 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11081 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11232 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11385 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11538 tasks      | elapsed:  3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 11693 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11848 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 12005 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12162 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12321 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12480 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12641 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12802 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12965 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 13128 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 13293 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 13458 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13625 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13792 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13961 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14130 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14301 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14472 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14645 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14818 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14993 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 15168 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15345 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15522 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15701 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 15880 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16061 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16242 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16425 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16608 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16793 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 16978 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #~30m on wittgenstein\n",
    "#     hVC = np.vstack([score_np_vec(np.array(range(num_words)), c_idx) for c_idx in tqdm(range(num_contexts))]).T\n",
    "    \n",
    "    #takes ~2.75m on wittgenstein with J=30 and other things going on in the background\n",
    "    #takes 5m on sidious with J=-1 and a full load going on in the background\n",
    "    hVC = np.vstack(par(delayed(score_np_vec)(np.array(range(num_words)), c_idx)\n",
    "                        for c_idx in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for reasons of caution and paranoia, we check that the ordering on contexts and wordforms is preserved by picking a thousand random context-wordform pairs, doing the calculations manually and checking that the calculation matches what's at the corresponding location in `hVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:23.702494Z",
     "start_time": "2019-07-17T22:28:23.680494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21.966872680419925"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16.888473711424368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.400280759634796"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC[0,0]\n",
    "score_np(0,0) #word idx, context idx\n",
    "\n",
    "hVC[2,3]\n",
    "score_np(2,3)\n",
    "\n",
    "hVC[3,2]\n",
    "score_np(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:28:24.034500Z",
     "start_time": "2019-07-17T22:28:23.706359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choices\n",
    "\n",
    "N_test_pairs = 10000\n",
    "random_context_indices = choices(range(num_contexts), k=N_test_pairs)\n",
    "random_orthWord_indices = choices(range(num_words), k=N_test_pairs)\n",
    "\n",
    "random_index_pairs = tuple(zip(random_orthWord_indices,\n",
    "                               random_context_indices))\n",
    "\n",
    "tests = [hVC[i,j] == score_np(i,j) for i,j in random_index_pairs]\n",
    "all(tests)\n",
    "assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.337706Z",
     "start_time": "2019-07-11T19:12:11.438259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    hVC.nbytes / 1e9\n",
    "    hVC.dtype\n",
    "    hVC_on_disk = np.memmap(o + '.hV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    \n",
    "    #takes ~1.25m on wittgenstein\n",
    "    hVC_on_disk[:,:] = hVC\n",
    "    hVC = hVC_on_disk\n",
    "    del hVC_on_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.356969Z",
     "start_time": "2019-07-11T19:13:52.344908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.402238Z",
     "start_time": "2019-07-11T19:13:52.361822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:13:52.421622Z",
     "start_time": "2019-07-11T19:13:52.406656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.nbytes / 1e9\n",
    "hVC.shape\n",
    "hVC.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:45.228741Z",
     "start_time": "2019-07-11T19:35:45.219379Z"
    }
   },
   "outputs": [],
   "source": [
    "# def exportMatrixMetadata(md_fp, matrix_fp, matrix, dim_md, step_name, nb_name, other_md):\n",
    "#     md = {'matrix fp':matrix_fp,\n",
    "#           'matrix shape':matrix.shape,\n",
    "#           'Produced in step':step_name,\n",
    "#           'Produced in notebook':nb_name}\n",
    "#     md.update(dim_md)\n",
    "#     md.update(other_md)\n",
    "#     exportDict(md_fp, md)\n",
    "#     print(f'Wrote metadata for \\n\\t{matrix_fp}\\n to \\n\\t{md_fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:33:11.492027Z",
     "start_time": "2019-07-11T19:33:11.484423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:35:47.103542Z",
     "start_time": "2019-07-11T19:35:47.089939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "hVC_dim_md = {'C':{'from fp':c,\n",
    "                   'changes':'sorted alphabetically',\n",
    "                   'size':len(contexts_sorted)},\n",
    "              'V':{'from fp':v,\n",
    "                   'changes':'none - already sorted',\n",
    "                   'size':len(vocabulary_sorted)}}\n",
    "# other_md = {'Produced in step':'Step 2b',\n",
    "#             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "exportMatrixMetadata(o+'.hV_C'+'_metadata.json',\n",
    "                     o+'.hV_C',\n",
    "                     hVC,\n",
    "                     hVC_dim_md,\n",
    "                     'Step 2b',\n",
    "                     'Producing contextual distributions',\n",
    "                     {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:36:01.113482Z",
     "start_time": "2019-07-11T19:36:00.649990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a version that contains probabilities (and is normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just naively convert (-) log-probabilities to probabilities if we're interested in distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.630144Z",
     "start_time": "2019-07-17T22:28:24.038007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98952166,  0.92990437,  0.97399201, ...,  0.96380412,\n",
       "        0.9685743 ,  0.86971511])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB! assumes hVC is in base 2 surprisals...\n",
    "dist_norms = np.sum(np.exp2(-1.0 * hVC), axis=0)\n",
    "dist_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.641274Z",
     "start_time": "2019-07-17T22:29:06.633434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not to'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_context = choice(contexts_sorted)\n",
    "random_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:29:06.685445Z",
     "start_time": "2019-07-17T22:29:06.644253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4108"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts.index(random_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:32.369598Z",
     "start_time": "2019-07-17T22:29:06.688482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0191s.) Setting batch_size=20.\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0252s.) Setting batch_size=318.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1120s.) Setting batch_size=1134.\n",
      "[Parallel(n_jobs=-1)]: Done 3342 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 7476 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 44064 out of 44064 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "hW_rc = np.array(par(delayed(score)(w, random_context) for w in vocabulary_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:32.391183Z",
     "start_time": "2019-07-17T22:39:32.377569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal( hVC[:,contexts_sorted.index(random_context)], hW_rc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.464736Z",
     "start_time": "2019-07-17T22:39:32.393878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18327779e-08,   4.18327779e-08,   1.65924722e-06,\n",
       "         5.86258692e-08,   4.18327779e-08,   4.18327779e-08,\n",
       "         4.18327779e-08,   1.79446150e-03,   1.83640299e-07,\n",
       "         4.18327779e-08])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.95894399281575127"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_rc = np.exp2(-1.0 * hW_rc)\n",
    "pW_rc[:10]\n",
    "np.sum(pW_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.482962Z",
     "start_time": "2019-07-17T22:39:33.476186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hVC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.709075Z",
     "start_time": "2019-07-17T22:39:33.486345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = 1e-16\n",
      "n = 44064\n",
      "max() =  = -3.6741742649792664\n",
      " = -68.57816236233276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44064"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "my_logprobs = -1.0 * hW_rc\n",
    "my_epsilon = 10 ** (-1.0 * 16)\n",
    "print(' = {0}'.format( my_epsilon ))\n",
    "\n",
    "my_n = my_logprobs.shape[0]\n",
    "print('n = {0}'.format( my_n ))\n",
    "\n",
    "my_max = np.max(my_logprobs)\n",
    "print('max() =  = {0}'.format( my_max ))\n",
    "\n",
    "my_threshold = np.log2(my_epsilon) - np.log2(my_n)\n",
    "print(' = {0}'.format( my_threshold ))\n",
    "\n",
    "mask = my_logprobs - my_max >= my_threshold\n",
    "np.sum(mask)\n",
    "to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "my_alphas = np.array([to_alpha(l) for l in my_logprobs])\n",
    "assert np.array_equal(my_alphas, to_alpha_vec(my_logprobs))\n",
    "my_alpha_norm = np.sum(my_alphas)\n",
    "my_probs = my_alphas / my_alpha_norm\n",
    "np.sum(my_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.729828Z",
     "start_time": "2019-07-17T22:39:33.711270Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://stats.stackexchange.com/a/66621\n",
    "def normalize_logprobs(logprobs, d=16, axis=0, b=None):\n",
    "# def normalize_logprobs(logprobs, d=16, b=None):\n",
    "#     axis = 0\n",
    "    n = logprobs.shape[axis]\n",
    "    epsilon = 10**(-1.0 * d)\n",
    "    maxlogp = np.max( logprobs[axis] )\n",
    "    if b is None:\n",
    "        threshold = np.log(epsilon) - np.log(n)\n",
    "        to_alpha = lambda logprob: np.exp(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 2:\n",
    "        threshold = np.log2(epsilon) - np.log2(n)\n",
    "        to_alpha = lambda logprob: np.exp2(logprob - my_max) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.exp2(logprobs - my_max) * (logprobs - my_max >= my_threshold)\n",
    "    elif b == 10:\n",
    "        threshold = np.log10(epsilon) - np.log10(n)\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, 10) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    else:\n",
    "        threshold = (np.log(epsilon) / np.log(b)) - (np.log(n) / np.log(b))\n",
    "        to_alpha = lambda logprob: np.power(logprob - my_max, b) if (logprob - my_max) >= my_threshold else 0.0\n",
    "        to_alpha_vec = lambda logprobs: np.power(logprobs - my_max, 10) * (logprobs - my_max >= my_threshold)\n",
    "    alpha_is = np.apply_along_axis(to_alpha_vec, axis=axis, arr=logprobs)\n",
    "#     alpha_is = np.array([to_alpha(l) for l in logprobs])\n",
    "    alpha_norm = np.sum(alpha_is, axis=axis)\n",
    "    probs = alpha_is / alpha_norm\n",
    "#     assert np.isclose(np.sum(probs), 1.0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.745598Z",
     "start_time": "2019-07-17T22:39:33.732006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.36237968e-08,   4.36237968e-08,   1.73028585e-06, ...,\n",
       "         4.36237968e-08,   6.11358636e-08,   4.36237968e-08])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hW_rc, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.762722Z",
     "start_time": "2019-07-17T22:39:33.748149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.46539838e-07,   2.46539838e-07,   1.45738607e-05, ...,\n",
       "         2.46539838e-07,   3.45509264e-07,   2.46539838e-07])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_logprobs(-1.0 * hVC[:,0], b=2)\n",
    "normalize_logprobs(-1.0 * hVC[:,0], b=2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:19.225819Z",
     "start_time": "2019-07-11T19:40:19.219327Z"
    }
   },
   "outputs": [],
   "source": [
    "if memory_map:\n",
    "    pVC = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:40:21.093501Z",
     "start_time": "2019-07-11T19:40:21.088617Z"
    }
   },
   "outputs": [],
   "source": [
    "# if memory_map:\n",
    "#     for j in range(my_shape[1]):\n",
    "#         pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:39:33.771155Z",
     "start_time": "2019-07-17T22:39:33.765473Z"
    }
   },
   "outputs": [],
   "source": [
    "def normColumn(j):\n",
    "    pVC[:,j] = normalize_logprobs(-1.0 * hVC[:,j], b=2)\n",
    "\n",
    "if memory_map:\n",
    "    # takes 3.4m on wittgenstein with J=30 and other stuff going on in the background\n",
    "    par(delayed(normColumn)(j) for j in range(num_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:32.707485Z",
     "start_time": "2019-07-17T22:39:33.773436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 160 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0541s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 362 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 578 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 692 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1082 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1220 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1370 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1520 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1682 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1844 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2018 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2192 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2378 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2564 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2762 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2960 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3170 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3380 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3602 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3824 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4058 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4292 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4538 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4784 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 5042 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 5300 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5570 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 5840 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6122 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6404 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6698 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 6992 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 7298 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7604 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 8240 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 8570 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 8900 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9242 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9584 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 9938 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 10292 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 10658 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11024 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11402 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11780 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 12170 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 12560 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 12962 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 13364 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 13778 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 14192 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 14618 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 15044 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 15482 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done 15920 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 16370 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 16820 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 17415 out of 17415 | elapsed:   42.7s finished\n"
     ]
    }
   ],
   "source": [
    "if not memory_map:\n",
    "    #takes ~30s on wittgenstein with J=30 and other stuff going on in the background\n",
    "    pVC = np.vstack(par(delayed(normalize_logprobs)(-1.0 * hVC[:,j])\n",
    "                        for j in range(num_contexts))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:52.652832Z",
     "start_time": "2019-07-17T22:41:32.716656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:19<00:00, 251.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test_indices = 5000\n",
    "random_context_indices = choices(range(num_contexts), k=N_test_indices)\n",
    "\n",
    "tests = [np.allclose(pVC[:,j], normalize_logprobs(-1.0 * hVC[:,j])) for j in tqdm(random_context_indices)]\n",
    "\n",
    "all(tests)\n",
    "assert all(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:23.177940Z",
     "start_time": "2019-07-11T19:45:50.694876Z"
    }
   },
   "outputs": [],
   "source": [
    "if not memory_map:\n",
    "    #takes ~1.25m on wittgenstein with other stuff going on in the background\n",
    "    pVC_on_disk = np.memmap(o + '.pV_C', dtype='float64', mode='w+', shape=my_shape)\n",
    "    pVC_on_disk[:,:] = pVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T22:41:58.318296Z",
     "start_time": "2019-07-17T22:41:52.656350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44064, 17415)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.13899648"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pVC.shape\n",
    "pVC.dtype\n",
    "pVC.nbytes / 1e9\n",
    "np.sum(pVC, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:47:26.273807Z",
     "start_time": "2019-07-11T19:47:26.266946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       " 'buckeye_contexts.txt',\n",
       " 'LM_filtered_buckeye_contexts.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       " 'Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C_metadata.json',\n",
       " 'Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\n",
       " 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb',\n",
       " 'LD_fisher_vocab_in_buckeye_contexts.hV_C',\n",
       " 'fisher_vocabulary_main.txt',\n",
       " 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:44.265249Z",
     "start_time": "2019-07-11T19:48:44.250158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metadata for \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\n",
      " to \n",
      "\t/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json\n"
     ]
    }
   ],
   "source": [
    "pVC_dim_md = {'C':{'from fp':c,\n",
    "                   'changes':'sorted alphabetically',\n",
    "                   'size':len(contexts_sorted)},\n",
    "              'V':{'from fp':v,\n",
    "                   'changes':'none - already sorted',\n",
    "                   'size':len(vocabulary_sorted)}}\n",
    "# other_md = {'Produced in step':'Step 2b',\n",
    "#             'Base notebook name':'Producing contextual distributions'}\n",
    "\n",
    "exportMatrixMetadata(o+'.pV_C'+'_metadata.json',\n",
    "                     o+'.pV_C',\n",
    "                     pVC,\n",
    "                     pVC_dim_md,\n",
    "                     'Step 2b',\n",
    "                     'Producing contextual distributions',\n",
    "                     {'Comment':'Non-trivially normalized version of hVC with nearly the same name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T19:48:56.126618Z",
     "start_time": "2019-07-11T19:48:55.383173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"matrix fp\": \"/home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C\",\r\n",
      "    \"matrix shape\": [\r\n",
      "        44064,\r\n",
      "        17415\r\n",
      "    ],\r\n",
      "    \"Produced in step\": \"Step 2b\",\r\n",
      "    \"Produced in notebook\": \"Producing contextual distributions\",\r\n",
      "    \"C\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/buckeye-lm/buckeye_contexts.txt\",\r\n",
      "        \"changes\": \"sorted alphabetically\",\r\n",
      "        \"size\": 17415\r\n",
      "    },\r\n",
      "    \"V\": {\r\n",
      "        \"from fp\": \"/home/AD/emeinhar/fisher-lm/fisher_vocabulary_main.txt\",\r\n",
      "        \"changes\": \"none - already sorted\",\r\n",
      "        \"size\": 44064\r\n",
      "    },\r\n",
      "    \"Comment\": \"Non-trivially normalized version of hVC with nearly the same name\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/AD/emeinhar/wr/LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C_metadata.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
