{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:25.303756Z",
     "start_time": "2019-05-15T23:13:25.300248Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Precursor-steps\" data-toc-modified-id=\"Precursor-steps-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Precursor steps</a></span></li><li><span><a href=\"#Step-1:-Make-transcribed-lexicons\" data-toc-modified-id=\"Step-1:-Make-transcribed-lexicons-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 1: Make transcribed lexicons</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-transcribed-lexicons\" data-toc-modified-id=\"Check-for-transcribed-lexicons-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Check for transcribed lexicons</a></span></li><li><span><a href=\"#How-are-transcribed-lexicon-relations-made?\" data-toc-modified-id=\"How-are-transcribed-lexicon-relations-made?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>How are transcribed lexicon relations made?</a></span></li></ul></li><li><span><a href=\"#Step-2:-Segment-inventory-alignment\" data-toc-modified-id=\"Step-2:-Segment-inventory-alignment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 2: Segment inventory alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-2a:-Define-inventory-alignment-projections\" data-toc-modified-id=\"Step-2a:-Define-inventory-alignment-projections-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Step 2a: Define inventory alignment projections</a></span></li><li><span><a href=\"#Step-2b:-Apply-inventory-alignment-projections\" data-toc-modified-id=\"Step-2b:-Apply-inventory-alignment-projections-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Step 2b: Apply inventory alignment projections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-projection-definitions\" data-toc-modified-id=\"Check-for-projection-definitions-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Check for projection definitions</a></span></li><li><span><a href=\"#How-are-inventory-alignment-projections-actually-applied?\" data-toc-modified-id=\"How-are-inventory-alignment-projections-actually-applied?-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>How are inventory alignment projections actually applied?</a></span></li><li><span><a href=\"#Apply-projection-definitions\" data-toc-modified-id=\"Apply-projection-definitions-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Apply projection definitions</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-3:-Generating-channel-and-lexicon-distributions\" data-toc-modified-id=\"Step-3:-Generating-channel-and-lexicon-distributions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 3: Generating channel and lexicon distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-3a:-Generating-channel-distributions-and-associated-metadata\" data-toc-modified-id=\"Step-3a:-Generating-channel-distributions-and-associated-metadata-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 3a: Generating channel distributions and associated metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Metadata</a></span></li><li><span><a href=\"#Channel-distributions\" data-toc-modified-id=\"Channel-distributions-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Channel distributions</a></span></li></ul></li><li><span><a href=\"#Step-3b:-Filtering-and-alignment-('vocabulary-normalization')\" data-toc-modified-id=\"Step-3b:-Filtering-and-alignment-('vocabulary-normalization')-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 3b: Filtering and alignment ('vocabulary normalization')</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution\" data-toc-modified-id=\"Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Filter transcription lexicons to only include words that can be modeled by a given channel distribution</a></span></li><li><span><a href=\"#Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary\" data-toc-modified-id=\"Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Filter transcription lexicons to only include words that are in a language model's vocabulary</a></span></li><li><span><a href=\"#Filter-the-conditioning-events-of-channel-distributions-to-only-include-$n$-phones-contained-in-a-transcription-lexicon's-segmental-wordform-list\" data-toc-modified-id=\"Filter-the-conditioning-events-of-channel-distributions-to-only-include-$n$-phones-contained-in-a-transcription-lexicon's-segmental-wordform-list-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Filter the conditioning events of channel distributions to only include $n$-phones contained in a transcription lexicon's segmental wordform list</a></span></li></ul></li><li><span><a href=\"#Step-3c:-Define-conditional-distributions-on-segmental-wordforms-given-an-orthographic-wordform\" data-toc-modified-id=\"Step-3c:-Define-conditional-distributions-on-segmental-wordforms-given-an-orthographic-wordform-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step 3c: Define conditional distributions on segmental wordforms given an orthographic wordform</a></span></li><li><span><a href=\"#Step-3d:-Define-a-distribution-over-the-transcribed-vocabulary-of-a-language-model-for-each-n-gram-context-in-a-set-of-(possibly-empty)-n-gram-contexts\" data-toc-modified-id=\"Step-3d:-Define-a-distribution-over-the-transcribed-vocabulary-of-a-language-model-for-each-n-gram-context-in-a-set-of-(possibly-empty)-n-gram-contexts-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Step 3d: Define a distribution over the transcribed vocabulary of a language model for each n-gram context in a set of (possibly empty) n-gram contexts</a></span></li></ul></li><li><span><a href=\"#Step-4:-Generating-posterior-distributions\" data-toc-modified-id=\"Step-4:-Generating-posterior-distributions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 4: Generating posterior distributions</a></span></li><li><span><a href=\"#Step-5:-Generating-analysis-measures\" data-toc-modified-id=\"Step-5:-Generating-analysis-measures-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Step 5: Generating analysis measures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processing pipeline from \n",
    " - gating data\n",
    " - transcribed lexicon\n",
    " - a language model and (possibly empty) n-gram contexts\n",
    "\n",
    "to \n",
    " - channel distribution\n",
    " - lexicon distribution(s) (distributions over wordforms)\n",
    " - expected posterior distribution over intended wordform given what has been produced of what was intended.\n",
    " \n",
    " \n",
    "It describes what happens at each step, checks some pre- and post-conditions, describes what you, the user must do (if anything), and scripts some commands to automatically do the necessary processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.501970Z",
     "start_time": "2019-05-15T23:13:27.406224Z"
    }
   },
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.506480Z",
     "start_time": "2019-05-15T23:13:31.504162Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.513067Z",
     "start_time": "2019-05-15T23:13:31.507952Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.518708Z",
     "start_time": "2019-05-15T23:13:31.515026Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.531242Z",
     "start_time": "2019-05-15T23:13:31.520286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd()\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:31.541498Z",
     "start_time": "2019-05-15T23:13:31.532602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boilerplate.py',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " '__pycache__',\n",
       " '1 initial directory setup.txt',\n",
       " '2a alignment_paths_and_cmds.sh',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'string_utils.py',\n",
       " 'LTR_CMU_stressed',\n",
       " '.git',\n",
       " 'LTR_newdic_destressed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_contents_0 = listdir()\n",
    "repo_contents_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precursor steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - I assume all relevant transcriptions have been converted to Unicode IPA characters and that segment sequences have `.` as a separator. For each data source used here, the IPA alignment step is documented in a GitHub repository elsewhere. (While I don't *think* any script here depends on use of Unicode IPA symbols, I haven't - and won't - test that idea, and it is *absolutely required* that contiguous sequences of segments be separated by `.` in data files.)\n",
    " - Where language models and n-gram contexts (drawn from speech corpora) are referenced, each of these is assumed to have come from as is from other GitHub repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Make transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each transcribed lexicon `LEXNAME` should be in a folder (e.g. `LTR_LEXNAME`) containing a file `LTR_LEXNAME.tsv`. For documentation purposes, the source file and a notebook documenting the production of the `.tsv` file should, if practicable be included in the folder as well.\n",
    "   - A transcribed lexicon `LTR_....tsv` file should have two columns: `Orthographic_Wordform` and `Transcription`.\n",
    "   - NB: The `LTR_` prefix on transcribed lexicon data files and containing folders is simply a convention for organization and readability, but is not required or expected by any file or script in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions in the code below will only succeed if step 1 is complete for all transcribed lexicons listed for checking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:36:56.189475Z",
     "start_time": "2019-05-17T18:36:56.180829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2184.15it/s]\n"
     ]
    }
   ],
   "source": [
    "newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "# nxt_swbd_ltr_folder = \n",
    "\n",
    "LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder)\n",
    "LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder)\n",
    "\n",
    "for dirname in tqdm(LTR_folders_to_process):\n",
    "    assert path.exists(dirname), 'Transcribed lexicon directory {0} not found in repo directory'.format(dirname)\n",
    "    fname = path.join(dirname, dirname + '.tsv')\n",
    "    assert path.exists(fname), 'Transcribed lexicon {0} not found in repo directory'.format(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are transcribed lexicon relations made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the `Making a Transcribed Lexicon relation - <LEXNAME>.ipynb` notebook in each `LTR_LEXNAME` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:37:06.328171Z",
     "start_time": "2019-05-17T18:37:06.321852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - newdic_destressed.ipynb',\n",
       " 'newdic_IPA.tsv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Making a Transcribed Lexicon Relation - CMU_destressed.ipynb',\n",
       " 'LTR_CMU_destressed.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'cmudict-0.7b_IPA_destressed.tsv']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_Buckeye.tsv',\n",
       " 'buckeye_words_analysis_relation.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye.ipynb']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname in LTR_folders_to_process:\n",
    "    listdir(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Segment inventory alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Define inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment inventory of any given transcribed lexicon and the segment inventory of the gating data often do not line up. For the gating data to be usefully applied to a given lexicon of transcriptions, the strings in the lexicon must contain only segments found in the gating data stimuli inventory.\n",
    "\n",
    "To ensure this happens, the notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` \n",
    " - takes as inputs \n",
    "     - a transcribed lexicon file path and a gating data file path\n",
    "     - a lexicon projection file path and a gating data projection file path\n",
    " - identifies the inventories of each and what symbols are relatively unique to the lexicon and the gating data\n",
    " - produces \n",
    "   - *a Jupyter notebook* for **you to open and finish by defining two projection functions** (i.e. Python dictionaries) to be applied to strings in the transcribed lexicon and to the gating data (one function for each). When you finish doing this (and set an export flag in the notebook to True and run the remainder of the notebook), this notebook will produce\n",
    "     - two *.json files storing these projections* according to the previously provided output file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:36.598195Z",
     "start_time": "2019-05-15T23:13:36.293044Z"
    }
   },
   "outputs": [],
   "source": [
    "%rm -rf *_aligned_w_*\n",
    "%rm -rf *\" alignment definition\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will only succeed if the American English gating data of Warner, McQueen, and Cutler (2014) is contained in the repo directory with a particular directory and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:37.680994Z",
     "start_time": "2019-05-15T23:13:37.676541Z"
    }
   },
   "outputs": [],
   "source": [
    "gating_data_folder = 'GD_AmE'\n",
    "gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "gating_data_fp = path.join(gating_data_folder, gating_data_fn)\n",
    "\n",
    "assert path.exists(gating_data_folder), 'AmE gating data folder {0} not found in repo directory'.format(gating_data_folder)\n",
    "assert path.exists(gating_data_fp), 'AmE gating data {0} not found in repo directory'.format(gating_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third cell below will create a notebook for alignment projection definitions for each of the transcribed lexicons from the previous step and the AmE gating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:39.011414Z",
     "start_time": "2019-05-15T23:13:39.006660Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeExtension(fp):\n",
    "    dir_name = path.dirname(fp)\n",
    "    file_name = path.basename(fp)\n",
    "    ext = file_name.split('.')[-1]\n",
    "    rest = '.'.join( file_name.split('.')[:-1] )\n",
    "    return path.join(dir_name, rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:13:40.675969Z",
     "start_time": "2019-05-15T23:13:40.622393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.22it/s]\n"
     ]
    }
   ],
   "source": [
    "arg_bundles = []\n",
    "for LTR_dirname in tqdm(LTR_folders_to_process):\n",
    "    LTR_fn = LTR_dirname + '.tsv'\n",
    "    LTR_fp = path.join(LTR_dirname, LTR_fn)\n",
    "    \n",
    "    nb_output_name = 'GD_AmE-diphones - ' + LTR_dirname + ' alignment definition' + '.ipynb'\n",
    "    my_g = gating_data_fp\n",
    "    my_l = LTR_fp\n",
    "    my_s = 'destressed'\n",
    "    \n",
    "    gd_alignment_dn = 'GD_AmE_' + my_s + '_' + 'aligned_w_' + LTR_dirname\n",
    "    gd_alignment_fn = 'alignment_of_' + removeExtension(gating_data_fn) + '_w_' + LTR_dirname + '.json'\n",
    "    gd_alignment_fp = path.join(gd_alignment_dn, gd_alignment_fn)\n",
    "    if not path.exists(gd_alignment_dn):\n",
    "        makedirs(gd_alignment_dn)\n",
    "    my_gp = gd_alignment_fp\n",
    "    \n",
    "    ltr_alignment_dn = LTR_dirname + '_aligned_w_' + 'GD_AmE_' + my_s\n",
    "    ltr_alignment_fn = 'alignment_of_' + LTR_dirname + '_w_' + removeExtension(gating_data_fn) + '.json'\n",
    "    ltr_alignment_fp = path.join(ltr_alignment_dn, ltr_alignment_fn)\n",
    "    if not path.exists(ltr_alignment_dn):\n",
    "        makedirs(ltr_alignment_dn)\n",
    "    my_lp = ltr_alignment_fp\n",
    "    \n",
    "    \n",
    "    my_arg_bundle = OrderedDict({\n",
    "        'LTR_dirname':LTR_dirname,\n",
    "        'LTR_fn':LTR_fn,\n",
    "        'LTR_fp':LTR_fp,\n",
    "        'gd_alignment_dn':gd_alignment_dn,\n",
    "        'gd_alignment_fn':gd_alignment_fn,\n",
    "        'gd_alignment_fp':gd_alignment_fp,\n",
    "        'ltr_alignment_dn':ltr_alignment_dn,\n",
    "        'ltr_alignment_fn':ltr_alignment_fn,\n",
    "        'ltr_alignment_fp':ltr_alignment_fp,\n",
    "        'align_def_nb_output_name':nb_output_name,\n",
    "        'my_g':my_g,\n",
    "        'my_l':my_l,\n",
    "        'my_s':my_s,\n",
    "        'my_gp':my_gp,\n",
    "        'my_lp':my_lp,\n",
    "    })\n",
    "    arg_bundles.append(my_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:14:19.170873Z",
     "start_time": "2019-05-15T23:13:44.879864Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 34.50it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 34.57it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 35.97it/s]\u001b[A\n",
      " 25%|██▌       | 16/64 [00:00<00:01, 35.00it/s]\u001b[A\n",
      " 30%|██▉       | 19/64 [00:00<00:01, 28.60it/s]\u001b[A\n",
      " 34%|███▍      | 22/64 [00:00<00:01, 28.85it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:01<00:02, 16.84it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 19.64it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 21.38it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:04<00:09,  3.00it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:06,  3.97it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:07,  3.34it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:05<00:05,  3.97it/s]\u001b[A\n",
      " 69%|██████▉   | 44/64 [00:06<00:03,  5.00it/s]\u001b[A\n",
      " 72%|███████▏  | 46/64 [00:06<00:03,  5.74it/s]\u001b[A\n",
      " 77%|███████▋  | 49/64 [00:06<00:02,  7.48it/s]\u001b[A\n",
      " 81%|████████▏ | 52/64 [00:06<00:01,  9.43it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:01,  9.51it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:06<00:00, 11.87it/s]\u001b[A\n",
      " 92%|█████████▏| 59/64 [00:06<00:00, 13.22it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:07<00:00, 11.43it/s]\u001b[A\n",
      " 98%|█████████▊| 63/64 [00:07<00:00, 12.75it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:10<00:20, 10.05s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/64 [00:00<00:03, 15.89it/s]\u001b[A\n",
      "  9%|▉         | 6/64 [00:00<00:03, 19.31it/s]\u001b[A\n",
      " 16%|█▌        | 10/64 [00:00<00:02, 20.61it/s]\u001b[A\n",
      " 20%|██        | 13/64 [00:00<00:02, 19.16it/s]\u001b[A\n",
      " 23%|██▎       | 15/64 [00:00<00:03, 16.08it/s]\u001b[A\n",
      " 28%|██▊       | 18/64 [00:00<00:02, 18.52it/s]\u001b[A\n",
      " 33%|███▎      | 21/64 [00:00<00:02, 20.58it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:02<00:04,  8.20it/s]\u001b[A\n",
      " 42%|████▏     | 27/64 [00:02<00:04,  8.16it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:02<00:03,  9.68it/s]\u001b[A\n",
      " 48%|████▊     | 31/64 [00:02<00:03, 10.56it/s]\u001b[A\n",
      " 52%|█████▏    | 33/64 [00:02<00:02, 11.93it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:05<00:15,  1.90it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:06<00:12,  2.08it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:07<00:10,  2.28it/s]\u001b[A\n",
      " 64%|██████▍   | 41/64 [00:08<00:11,  2.08it/s]\u001b[A\n",
      " 69%|██████▉   | 44/64 [00:08<00:06,  2.88it/s]\u001b[A\n",
      " 73%|███████▎  | 47/64 [00:08<00:04,  3.94it/s]\u001b[A\n",
      " 77%|███████▋  | 49/64 [00:08<00:03,  4.95it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:08<00:02,  6.08it/s]\u001b[A\n",
      " 83%|████████▎ | 53/64 [00:08<00:01,  7.04it/s]\u001b[A\n",
      " 86%|████████▌ | 55/64 [00:09<00:01,  7.60it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:09<00:00,  8.18it/s]\u001b[A\n",
      " 92%|█████████▏| 59/64 [00:09<00:00,  9.25it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:09<00:00, 10.46it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:23<00:10, 10.93s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▍         | 3/64 [00:00<00:06,  9.04it/s]\u001b[A\n",
      "  9%|▉         | 6/64 [00:00<00:05, 10.24it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:05, 10.74it/s]\u001b[A\n",
      " 17%|█▋        | 11/64 [00:00<00:03, 13.28it/s]\u001b[A\n",
      " 22%|██▏       | 14/64 [00:00<00:03, 14.66it/s]\u001b[A\n",
      " 25%|██▌       | 16/64 [00:01<00:04, 11.93it/s]\u001b[A\n",
      " 31%|███▏      | 20/64 [00:01<00:03, 13.88it/s]\u001b[A\n",
      " 38%|███▊      | 24/64 [00:01<00:02, 16.87it/s]\u001b[A\n",
      " 42%|████▏     | 27/64 [00:01<00:02, 16.52it/s]\u001b[A\n",
      " 47%|████▋     | 30/64 [00:01<00:01, 17.18it/s]\u001b[A\n",
      " 53%|█████▎    | 34/64 [00:02<00:01, 17.38it/s]\u001b[A\n",
      " 56%|█████▋    | 36/64 [00:04<00:13,  2.15it/s]\u001b[A\n",
      " 59%|█████▉    | 38/64 [00:06<00:13,  1.99it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:07<00:12,  1.97it/s]\u001b[A\n",
      " 64%|██████▍   | 41/64 [00:07<00:12,  1.84it/s]\u001b[A\n",
      " 69%|██████▉   | 44/64 [00:07<00:07,  2.56it/s]\u001b[A\n",
      " 73%|███████▎  | 47/64 [00:07<00:04,  3.52it/s]\u001b[A\n",
      " 78%|███████▊  | 50/64 [00:08<00:02,  4.67it/s]\u001b[A\n",
      " 83%|████████▎ | 53/64 [00:08<00:01,  6.23it/s]\u001b[A\n",
      " 88%|████████▊ | 56/64 [00:08<00:01,  7.63it/s]\u001b[A\n",
      " 91%|█████████ | 58/64 [00:08<00:00,  9.15it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:08<00:00, 11.30it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:34<00:00, 11.02s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for arg_bundle in tqdm(arg_bundles):\n",
    "    nb = pm.execute_notebook(\n",
    "        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "        arg_bundle['align_def_nb_output_name'],\n",
    "        parameters=dict(g = arg_bundle['my_g'], \n",
    "                        l = arg_bundle['my_l'], \n",
    "                        s = arg_bundle['my_s'], \n",
    "                        gp = arg_bundle['my_gp'], \n",
    "                        lp = arg_bundle['my_lp'])\n",
    "    )\n",
    "#     pm.execute_notebook(\n",
    "#        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "#        nb_output_name,\n",
    "#        parameters=dict(g = my_g, l = my_l, s = my_s, gp = my_gp, lp = my_lp)\n",
    "#     )\n",
    "    print(\"Finished creating alignment definition notebook {0}.\\nOpen and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\\n\".format(arg_bundle['align_def_nb_output_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Apply inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:16:15.802032Z",
     "start_time": "2019-05-15T23:16:15.639432Z"
    }
   },
   "outputs": [],
   "source": [
    "%rm -rf *\" alignment application \"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will succeed if you have run each of the previously produced notebooks correctly and produced a projection mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:16:17.224421Z",
     "start_time": "2019-05-15T23:16:17.215874Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1466.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for arg_bundle in tqdm(arg_bundles):\n",
    "    args = arg_bundle\n",
    "    assert path.exists(args['gd_alignment_fp']), 'Gating data alignment projection mapping not found:\\n\\t{0}'.format(args['gd_alignment_fp'])\n",
    "    assert path.exists(args['ltr_alignment_fp']), 'Transcribed lexicon data alignment projection mapping not found:\\n\\t{0}'.format(args['ltr_alignment_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are inventory alignment projections actually applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Align transcriptions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below applies each pair of alignment projections to each matched pair of gating data and transcribed lexicon choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:22:32.458974Z",
     "start_time": "2019-05-17T18:21:43.890277Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tLTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_newdic_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tLTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:03<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_CMU_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tLTR_Buckeye/LTR_Buckeye.tsv\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb\n",
      "100%|██████████| 64/64 [00:05<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_Buckeye/LTR_Buckeye.tsv\n",
      "Result saved to\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for arg_bundle in arg_bundles:\n",
    "    args = arg_bundle\n",
    "    LTR_fn = args['LTR_fn']\n",
    "    \n",
    "    my_pg = args['my_gp']\n",
    "    my_g = args['my_g']\n",
    "    my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    args['my_og'] = my_og\n",
    "    print(\"Creating notebook '{0}' w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}\".format(args['align_apply_gd_nb_output_name'], my_pg, my_g, my_og))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_gd_nb_output_name'],\n",
    "        parameters=dict(p = my_pg,\n",
    "                        g = my_g,\n",
    "                        o = my_og)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tg = {1}\\nResult saved to\\n\\t{2}'.format(my_pg, my_g, my_og))\n",
    "    print(' ')\n",
    "    \n",
    "    my_pl = args['my_lp']\n",
    "    my_l = args['my_l']\n",
    "    my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    print('Creating notebook {0} w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}'.format(args['align_apply_ltr_nb_output_name'], my_pg, my_l, my_ol))\n",
    "    args['my_ol'] = my_ol\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_ltr_nb_output_name'],\n",
    "        parameters=dict(p = my_pl,\n",
    "                        l = my_l,\n",
    "                        o = my_ol)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tl = {1}\\nResult saved to\\n\\t{2}'.format(my_pl, my_l, my_ol))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generating channel and lexicon distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. **Channel distribution**\n",
    "\n",
    " A. Given gating data, we can define uniphone and triphone channel distributions.\n",
    " \n",
    "II. **Lexicon distribution**\n",
    "\n",
    "A. **Alignment of the vocabulary with the channel model and with a language model, alignment of the channel model with a lexicon**\n",
    " 1. Given a transcribed lexicon relation and information about the conditioning events and sample space of a channel model, we can project a transcribed lexicon relation down to just what can be modeled using the channel model.\n",
    " 2. Given the vocabulary of an (orthographic) $n$-gram model, we can project a transcribed lexicon relation down to just what can be aligned with the vocabulary of the $n$-gram model. (I.e. if the $n$-gram model vocabulary is a unary relation on orthographic wordforms, this calculates the *natural join* of the $n$-gram model vocabulary and the transcribed lexicon relation.)\n",
    " 3. Given a channel model and a transcribed lexicon relation that the channel model will be applied to, we can remove $n$-phones from the channel model that do not appear in the transcribed lexicon relation.\n",
    "\n",
    "B. **Distribution construction**\n",
    " 1. Given a transcribed lexicon relation, we can define a family of conditional distributions on segmental wordforms given a choice of orthographic wordform: e.g. if orthographic wordform $w$ can surface as all and only the segmental sequences $s_0$ and $s_1$, then we construct a uniform distribution over $s_0$ and $s_1$ given $w$.\n",
    " 2. Given a (unigram) distribution over orthographic wordforms and orthographic->segmental wordform conditional distributions, we can define a distribution over segmental wordforms.\n",
    " 3. Given an $n$-gram model, the vocabulary of the model, a set of (orthographic) $n$-gram contexts $C$, and a target vocabulary of orthographic wordforms of interest (i.e. the ones we have transcriptions for), we can calculate a distribution over the target vocabulary for each $n$-gram context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:22:57.480985Z",
     "start_time": "2019-05-17T18:22:57.475281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('LTR_dirname', 'LTR_Buckeye'),\n",
       "             ('LTR_fn', 'LTR_Buckeye.tsv'),\n",
       "             ('LTR_fp', 'LTR_Buckeye/LTR_Buckeye.tsv'),\n",
       "             ('gd_alignment_dn', 'GD_AmE_destressed_aligned_w_LTR_Buckeye'),\n",
       "             ('gd_alignment_fn',\n",
       "              'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json'),\n",
       "             ('gd_alignment_fp',\n",
       "              'GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json'),\n",
       "             ('ltr_alignment_dn', 'LTR_Buckeye_aligned_w_GD_AmE_destressed'),\n",
       "             ('ltr_alignment_fn',\n",
       "              'alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json'),\n",
       "             ('ltr_alignment_fp',\n",
       "              'LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json'),\n",
       "             ('align_def_nb_output_name',\n",
       "              'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'),\n",
       "             ('my_g', 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv'),\n",
       "             ('my_l', 'LTR_Buckeye/LTR_Buckeye.tsv'),\n",
       "             ('my_s', 'destressed'),\n",
       "             ('my_gp',\n",
       "              'GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json'),\n",
       "             ('my_lp',\n",
       "              'LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json'),\n",
       "             ('align_apply_gd_nb_output_name',\n",
       "              'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'),\n",
       "             ('align_apply_ltr_nb_output_name',\n",
       "              'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'),\n",
       "             ('my_og',\n",
       "              'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'),\n",
       "             ('my_ol',\n",
       "              'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv')])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Generating channel distributions and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:52:36.500476Z",
     "start_time": "2019-05-17T17:52:36.384472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "%ls -d GD_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:54:38.571735Z",
     "start_time": "2019-05-17T17:54:38.566761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_folders = ('GD_AmE', ) + tuple(map(lambda ab: ab['gd_alignment_dn'], arg_bundles))\n",
    "gating_data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:29:27.396528Z",
     "start_time": "2019-05-17T18:29:27.391715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_fps = ('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',) + \\\n",
    "                  tuple(map(lambda ab: ab['my_og'], arg_bundles))\n",
    "gating_data_fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (for downstream convenience) we identify the $n$-phones (not) contained in and (not) constructible from each of the versions of the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** This is done by calling the notebook `Run n-phone analysis of gating data.ipynb`, passing it **the filepath to a gating data `.csv`/`.tsv` file** and **a path to an output directory** for the dozen or so files the notebook will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:33:31.710140Z",
     "start_time": "2019-05-17T18:31:33.647422Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE/GD_AmE n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'GD_AmE n-phone analysis.ipynb']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed stimuli uniphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response illegal diphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'stressed stimuli uniphones.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb',\n",
       " 'stressed stimuli illegal diphones.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gating_data_fp in gating_data_fps:\n",
    "    gd_dir = path.dirname(gating_data_fp)\n",
    "    nb = pm.execute_notebook(\n",
    "        'Run n-phone analysis of gating data.ipynb',\n",
    "#         args['align_apply_ltr_nb_output_name'],\n",
    "        path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "        parameters=dict(g = gating_data_fp,\n",
    "                        o = gd_dir)\n",
    "    )\n",
    "    listdir(gd_dir)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the notebook `Producing channel distributions.ipynb` will create `.json` files defining (among other things) a uniphone and triphone channel distribution. It requires the following arguments to specify information about what kind of channel model to build and where to put it:\n",
    " - **a filepath** to a gating data file\n",
    " - **a directory** containing metadata indicating possible/impossible $n$-phones\n",
    " - **a string argument** (\"stressed\" or \"destressed\") indicating whether the distribution will be over a segment inventory with or without stress information\n",
    " - **a real valued**  smoothing parameter (a pseudocount to add to every channel outcome)\n",
    " - **an output directory** to write the channel model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T23:55:18.549309Z",
     "start_time": "2019-05-19T23:55:18.546257Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudocounts = (0, 0.01, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:24:37.033094Z",
     "start_time": "2019-05-20T05:24:37.028533Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_arg_bundles = []\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    metadata_dir = path.dirname(gating_data_fp)\n",
    "    s = \"destressed\"\n",
    "    channel_model_dir_stem = 'CM' + metadata_dir[2:]\n",
    "    \n",
    "    for pc in pseudocounts:\n",
    "        channel_model_dir_suffix = '_pseudocount' + str(pc)\n",
    "        if metadata_dir == 'GD_AmE':\n",
    "            channel_model_dir = channel_model_dir_stem + '_' + s + '_unaligned' + '_' + channel_model_dir_suffix\n",
    "        else:\n",
    "            channel_model_dir = channel_model_dir_stem + channel_model_dir_suffix\n",
    "        nb_output_name = 'Producing channel distributions from ' + metadata_dir + ', pc={0}'.format(pc) + '.ipynb'\n",
    "        new_arg_bundle = {'gating_data_fp':gating_data_fp,\n",
    "                          'metadata_dir':metadata_dir,\n",
    "                          's':s,\n",
    "                          'c':pc,\n",
    "                          'cm_dir':channel_model_dir,\n",
    "                          'nb_output_name':nb_output_name}\n",
    "        cm_arg_bundles.append(new_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:24:29.831869Z",
     "start_time": "2019-05-20T05:24:29.828510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gating_data_fp': 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'metadata_dir': 'GD_AmE',\n",
       " 's': 'destressed',\n",
       " 'c': 0,\n",
       " 'cm_dir': 'CM_AmE_pseudocount0',\n",
       " 'nb_output_name': 'Producing channel distributions from GD_AmE, pc=0.ipynb'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arg_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-20T05:24:39.597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_unaligned_destressed_pseudocount0/Producing channel distributions from GD_AmE, pc=0.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/189 [00:00<00:19,  9.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 3/189 [00:00<00:18, 10.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 5/189 [00:00<00:18, 10.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 7/189 [00:00<00:17, 10.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 8/189 [00:00<00:17, 10.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 9/189 [00:00<00:18,  9.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 11/189 [00:01<00:16, 10.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 13/189 [00:01<00:15, 11.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 15/189 [00:01<00:15, 10.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 17/189 [00:01<00:15, 11.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 19/189 [00:02<00:27,  6.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 20/189 [00:02<00:24,  6.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 21/189 [00:02<00:24,  6.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 23/189 [00:02<00:22,  7.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 25/189 [00:02<00:20,  8.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 26/189 [00:02<00:20,  8.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 27/189 [00:03<00:20,  8.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 29/189 [00:03<00:18,  8.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 30/189 [00:03<00:17,  9.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 32/189 [00:03<00:16,  9.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 34/189 [00:03<00:15,  9.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 36/189 [00:03<00:14, 10.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 38/189 [00:06<01:16,  1.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 39/189 [00:07<01:00,  2.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 41/189 [00:07<00:46,  3.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 42/189 [00:07<00:38,  3.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 43/189 [00:07<00:31,  4.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 44/189 [00:07<00:26,  5.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 45/189 [00:07<00:22,  6.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▍       | 47/189 [00:07<00:19,  7.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 48/189 [00:07<00:17,  7.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 50/189 [00:08<00:16,  8.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 52/189 [00:08<00:15,  9.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 54/189 [00:08<00:13,  9.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 56/189 [00:09<00:33,  4.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 57/189 [00:09<00:27,  4.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 59/189 [00:10<00:23,  5.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 60/189 [00:10<00:22,  5.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 62/189 [00:10<00:18,  6.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 63/189 [00:10<00:17,  7.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 64/189 [00:10<00:16,  7.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 66/189 [00:10<00:14,  8.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 68/189 [00:10<00:13,  9.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 70/189 [00:11<00:12,  9.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 72/189 [00:11<00:12,  9.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 74/189 [00:11<00:11,  9.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 76/189 [00:11<00:11,  9.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 77/189 [00:11<00:11,  9.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 79/189 [00:12<00:10, 10.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 81/189 [00:17<01:32,  1.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 83/189 [00:17<01:06,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 85/189 [00:17<00:48,  2.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 86/189 [00:17<00:38,  2.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 88/189 [00:18<00:29,  3.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 89/189 [00:18<00:24,  4.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 91/189 [00:18<00:18,  5.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 93/189 [00:18<00:17,  5.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 94/189 [00:18<00:15,  6.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 96/189 [00:18<00:13,  7.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████▏    | 97/189 [00:19<00:12,  7.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 98/189 [00:19<00:11,  8.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 99/189 [00:19<00:10,  8.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 100/189 [00:19<00:10,  8.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 101/189 [00:19<00:10,  8.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 102/189 [00:22<01:27,  1.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 103/189 [00:22<01:03,  1.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 105/189 [00:25<01:22,  1.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 106/189 [00:25<00:59,  1.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 108/189 [00:26<00:43,  1.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 110/189 [00:26<00:31,  2.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 111/189 [00:26<00:24,  3.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 112/189 [00:40<05:52,  4.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████▉    | 113/189 [00:42<04:35,  3.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 114/189 [00:42<03:14,  2.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 115/189 [00:42<02:16,  1.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 117/189 [01:11<06:47,  5.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 118/189 [01:40<15:01, 12.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 119/189 [01:41<10:24,  8.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 120/189 [01:41<07:13,  6.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 121/189 [01:41<05:01,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▍   | 122/189 [01:41<03:30,  3.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 123/189 [01:41<02:36,  2.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 125/189 [01:42<01:48,  1.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 127/189 [01:42<01:14,  1.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 128/189 [01:42<00:53,  1.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 129/189 [01:42<00:38,  1.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 131/189 [01:42<00:28,  2.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 132/189 [01:42<00:21,  2.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 133/189 [01:43<00:17,  3.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 135/189 [01:43<00:13,  4.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 136/189 [01:43<00:11,  4.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 137/189 [01:43<00:09,  5.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 138/189 [01:57<03:34,  4.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▎  | 139/189 [01:57<02:28,  2.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 141/189 [01:57<01:41,  2.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 142/189 [01:57<01:11,  1.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 144/189 [01:57<00:49,  1.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 146/189 [01:57<00:34,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 147/189 [01:58<00:24,  1.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 148/189 [01:58<00:18,  2.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 149/189 [01:58<00:14,  2.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 150/189 [01:58<00:11,  3.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 152/189 [01:58<00:08,  4.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 153/189 [01:58<00:07,  4.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 154/189 [01:58<00:06,  5.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 155/189 [01:59<00:05,  6.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 157/189 [01:59<00:04,  6.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 158/189 [01:59<00:04,  7.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 159/189 [01:59<00:04,  7.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 160/189 [01:59<00:04,  7.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 161/189 [01:59<00:03,  7.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 163/189 [02:00<00:03,  7.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 164/189 [02:00<00:03,  7.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 165/189 [02:00<00:03,  7.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 166/189 [02:00<00:03,  7.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 167/189 [02:00<00:02,  7.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 168/189 [02:00<00:02,  7.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 170/189 [02:00<00:02,  8.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 171/189 [02:00<00:02,  8.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 172/189 [02:58<04:56, 17.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 173/189 [02:59<03:16, 12.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 174/189 [02:59<02:09,  8.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 175/189 [02:59<01:25,  6.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 176/189 [02:59<00:57,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▎| 177/189 [03:00<00:37,  3.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 178/189 [03:00<00:24,  2.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▍| 179/189 [03:00<00:16,  1.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 180/189 [03:00<00:10,  1.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 181/189 [03:00<00:06,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▋| 182/189 [03:00<00:04,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 183/189 [03:00<00:02,  2.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 184/189 [03:00<00:01,  2.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 185/189 [03:01<00:01,  3.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 186/189 [03:01<00:00,  3.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 187/189 [03:01<00:00,  4.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 188/189 [03:01<00:00,  5.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 189/189 [03:01<00:00,  5.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_unaligned_destressed_pseudocount0.01/Producing channel distributions from GD_AmE, pc=0.01.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/189 [00:00<00:25,  7.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 2/189 [00:00<00:25,  7.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 4/189 [00:00<00:23,  7.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 5/189 [00:00<00:23,  7.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 6/189 [00:00<00:22,  8.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 8/189 [00:00<00:20,  8.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 10/189 [00:01<00:20,  8.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 12/189 [00:01<00:18,  9.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 13/189 [00:01<00:19,  9.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 14/189 [00:01<00:23,  7.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 16/189 [00:01<00:20,  8.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 18/189 [00:01<00:18,  9.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 19/189 [00:02<00:57,  2.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 20/189 [00:02<00:44,  3.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 22/189 [00:03<00:34,  4.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 24/189 [00:03<00:29,  5.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 25/189 [00:03<00:25,  6.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 27/189 [00:03<00:26,  6.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 28/189 [00:03<00:23,  6.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 30/189 [00:04<00:19,  8.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 32/189 [00:04<00:21,  7.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 33/189 [00:04<00:20,  7.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 34/189 [00:04<00:20,  7.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 36/189 [00:04<00:19,  8.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 38/189 [00:07<01:21,  1.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 39/189 [00:07<01:02,  2.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 41/189 [00:08<00:47,  3.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 43/189 [00:08<00:37,  3.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 44/189 [00:08<00:40,  3.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 45/189 [00:08<00:35,  4.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▍       | 47/189 [00:09<00:28,  4.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 48/189 [00:09<00:24,  5.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 49/189 [00:09<00:23,  5.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 50/189 [00:09<00:20,  6.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 51/189 [00:09<00:22,  6.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 52/189 [00:09<00:21,  6.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 54/189 [00:09<00:19,  6.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 55/189 [00:10<00:19,  6.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 56/189 [00:11<01:03,  2.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 58/189 [00:11<00:47,  2.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 59/189 [00:11<00:37,  3.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 61/189 [00:11<00:29,  4.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 63/189 [00:12<00:23,  5.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 64/189 [00:12<00:22,  5.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 66/189 [00:12<00:19,  6.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 67/189 [00:12<00:21,  5.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 69/189 [00:12<00:17,  6.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 70/189 [00:12<00:16,  7.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 71/189 [00:13<00:15,  7.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 72/189 [00:13<00:14,  8.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▊      | 73/189 [00:13<00:13,  8.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 74/189 [00:13<00:15,  7.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 76/189 [00:13<00:13,  8.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 77/189 [00:13<00:15,  7.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 78/189 [00:13<00:15,  7.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 79/189 [00:14<00:14,  7.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 80/189 [00:16<01:43,  1.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 81/189 [00:19<02:46,  1.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 82/189 [00:20<02:02,  1.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 83/189 [00:20<01:29,  1.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 85/189 [00:20<01:04,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 86/189 [00:20<00:47,  2.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 87/189 [00:20<00:36,  2.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 88/189 [00:20<00:29,  3.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 89/189 [00:20<00:26,  3.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 90/189 [00:21<00:21,  4.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▊     | 92/189 [00:21<00:17,  5.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 93/189 [00:36<07:44,  4.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 94/189 [00:37<05:27,  3.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 95/189 [00:38<04:28,  2.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 96/189 [00:38<03:09,  2.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████▏    | 97/189 [00:38<02:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 98/189 [00:38<01:36,  1.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 99/189 [00:39<01:10,  1.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 100/189 [00:39<00:52,  1.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 101/189 [00:39<00:40,  2.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 102/189 [01:04<11:30,  7.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 103/189 [01:04<08:01,  5.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 104/189 [01:05<05:36,  3.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 105/189 [01:30<14:37, 10.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 106/189 [01:30<10:10,  7.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 107/189 [01:30<07:04,  5.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 108/189 [01:31<05:01,  3.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 109/189 [01:31<03:31,  2.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 110/189 [01:31<02:28,  1.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 111/189 [01:31<01:45,  1.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 112/189 [01:49<07:56,  6.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████▉    | 113/189 [01:51<06:20,  5.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 114/189 [01:51<04:26,  3.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 115/189 [01:51<03:06,  2.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████▏   | 116/189 [01:51<02:11,  1.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for ab in cm_arg_bundles:\n",
    "    if not path.exists(ab['cm_dir']):\n",
    "        mkdir(ab['cm_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing channel distributions.ipynb',\n",
    "    path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "    parameters=dict(g = ab['gating_data_fp'],\n",
    "                    m = ab['metadata_dir'],\n",
    "                    s = ab['s'],\n",
    "                    c = ab['c'],\n",
    "                    o = ab['cm_dir'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filtering and alignment ('vocabulary normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter transcription lexicons to only include words that can be modeled by a given channel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter transcription lexicons to only include words that are in a language model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the conditioning events of channel distributions to only include $n$-phones contained in a transcription lexicon's segmental wordform list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Define conditional distributions on segmental wordforms given an orthographic wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: Define a distribution over the transcribed vocabulary of a language model for each n-gram context in a set of (possibly empty) n-gram contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generating posterior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Generating analysis measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
