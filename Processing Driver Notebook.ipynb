{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:50.729282Z",
     "start_time": "2019-05-26T22:32:50.726540Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Todo</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Step-0:-Check-for-foundational-files\" data-toc-modified-id=\"Step-0:-Check-for-foundational-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 0: Check for foundational files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-0a:-Check-for-gating-data\" data-toc-modified-id=\"Step-0a:-Check-for-gating-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 0a: Check for gating data</a></span></li><li><span><a href=\"#Step-0b:-Check-for-transcribed-lexicons\" data-toc-modified-id=\"Step-0b:-Check-for-transcribed-lexicons-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 0b: Check for transcribed lexicons</a></span></li><li><span><a href=\"#Step-0c:-Check-for-n-gram-contexts\" data-toc-modified-id=\"Step-0c:-Check-for-n-gram-contexts-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Step 0c: Check for n-gram contexts</a></span></li><li><span><a href=\"#Step-0d:-Check-for-language-model(s)\" data-toc-modified-id=\"Step-0d:-Check-for-language-model(s)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Step 0d: Check for language model(s)</a></span></li></ul></li><li><span><a href=\"#Step-1:-Segment-inventory-alignment\" data-toc-modified-id=\"Step-1:-Segment-inventory-alignment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 1: Segment inventory alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1a:-Define-inventory-alignment-projections\" data-toc-modified-id=\"Step-1a:-Define-inventory-alignment-projections-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Step 1a: Define inventory alignment projections</a></span></li><li><span><a href=\"#Step-1b:-Apply-inventory-alignment-projections\" data-toc-modified-id=\"Step-1b:-Apply-inventory-alignment-projections-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Step 1b: Apply inventory alignment projections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-projection-definitions\" data-toc-modified-id=\"Check-for-projection-definitions-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Check for projection definitions</a></span></li><li><span><a href=\"#How-are-inventory-alignment-projections-actually-applied?\" data-toc-modified-id=\"How-are-inventory-alignment-projections-actually-applied?-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>How are inventory alignment projections actually applied?</a></span></li><li><span><a href=\"#Apply-projection-definitions\" data-toc-modified-id=\"Apply-projection-definitions-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Apply projection definitions</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions\" data-toc-modified-id=\"Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 2: Generating channel and (orthographic) lexicon distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-2a:-Generating-channel-distributions-and-associated-metadata\" data-toc-modified-id=\"Step-2a:-Generating-channel-distributions-and-associated-metadata-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Step 2a: Generating channel distributions and associated metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Metadata</a></span></li><li><span><a href=\"#Channel-distributions\" data-toc-modified-id=\"Channel-distributions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Channel distributions</a></span></li></ul></li><li><span><a href=\"#Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)\" data-toc-modified-id=\"Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)</a></span></li></ul></li><li><span><a href=\"#Step-3:-Creating-combinable-models\" data-toc-modified-id=\"Step-3:-Creating-combinable-models-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 3: Creating combinable models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution\" data-toc-modified-id=\"Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution</a></span></li><li><span><a href=\"#Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary\" data-toc-modified-id=\"Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary</a></span></li><li><span><a href=\"#Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list\" data-toc-modified-id=\"Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list</a></span></li><li><span><a href=\"#Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms\" data-toc-modified-id=\"Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms</a></span></li><li><span><a href=\"#Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform\" data-toc-modified-id=\"Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform</a></span></li></ul></li><li><span><a href=\"#Step-4:-Pre-calculate-remaining-forward-model-component(s)\" data-toc-modified-id=\"Step-4:-Pre-calculate-remaining-forward-model-component(s)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 4: Pre-calculate remaining forward model component(s)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-4a:-Define-incremental-channel-distributions-on-segmental-wordforms,-organized-by-length\" data-toc-modified-id=\"Step-4a:-Define-incremental-channel-distributions-on-segmental-wordforms,-organized-by-length-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Step 4a: Define incremental channel distributions on segmental wordforms, organized by length</a></span></li></ul></li><li><span><a href=\"#Step-5:-Calculate-posterior-distributions\" data-toc-modified-id=\"Step-5:-Calculate-posterior-distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Step 5: Calculate posterior distributions</a></span></li><li><span><a href=\"#Step-6:-Generating-analysis-measures\" data-toc-modified-id=\"Step-6:-Generating-analysis-measures-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Step 6: Generating analysis measures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processing pipeline from \n",
    " - gating data\n",
    " - transcribed lexicon\n",
    " - a language model and (possibly empty) n-gram contexts\n",
    "\n",
    "to \n",
    " - channel distribution\n",
    " - lexicon distribution(s) (distributions over wordforms)\n",
    " - expected posterior distribution over intended wordform given what has been produced of what was intended.\n",
    " \n",
    " \n",
    "It describes what happens at each step, checks some pre- and post-conditions, describes what you, the user must do (if anything), and scripts some commands to automatically do the necessary processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "0. **Extensibility**: Step 3b (filtering transcribed lexicons against language model vocabularies) uses output filenames that won't scale if any transcription lexicon is used with more than one language model. (This also affects 3c.)\n",
    "1. **Portability/Reproducibility**: For every file that this repository depends on that *isn't* tracked by the repository (e.g. processed versions of swbd2003, Buckeye, etc.), there should be *something* (e.g. a cell in this script) that lets the user identify where those files are located, and then said something copies them to wherever this repository is expecting to find them.\n",
    "2. **Portability/Reproducibility**: Check for and remove absolute paths in this and other files.\n",
    "2. **Documentation**: \n",
    "   1. Fix motivation cell immediately above this one.\n",
    "   2. Math-y documentation in channel distribution and posterior distribution notebooks probably needs to be updated / at least have notation overhauled.\n",
    "   3. Go through notebooks used here and make sure `Overview` cells are accurate.\n",
    "   4. Go through notebooks used here and make sure Papermill-related `Usage` cells are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.016407Z",
     "start_time": "2019-05-26T22:32:52.925746Z"
    }
   },
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.020193Z",
     "start_time": "2019-05-26T22:32:54.017826Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.026588Z",
     "start_time": "2019-05-26T22:32:54.021420Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.032632Z",
     "start_time": "2019-05-26T22:32:54.028433Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.046570Z",
     "start_time": "2019-05-26T22:32:54.033975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd()\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:54.110708Z",
     "start_time": "2019-05-26T22:32:54.106526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'boilerplate.py',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " '2a alignment_paths_and_cmds.sh',\n",
       " 'LM_Fisher',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Filter channel model by transcribed lexicon relation.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_contents_0 = listdir()\n",
    "repo_contents_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Check for foundational files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    " - I assume all relevant transcriptions have been converted to Unicode IPA characters and that segment sequences have `.` as a separator. For each data source used here, the IPA alignment step is documented in a GitHub repository elsewhere. (While I don't *think* any script here depends on use of Unicode IPA symbols, I haven't - and won't - test that idea, and it is *absolutely required* that contiguous sequences of segments be separated by `.` in data files.)\n",
    " - Where language models and n-gram contexts (drawn from speech corpora) are referenced, each of these is assumed to have come from as is from other GitHub repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four steps below verify that foundational files assumed to be present by downstream notebooks are, in fact, present in the repository directory. If for some reason those files are not present, the processing pipeline will be aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0a: Check for gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:55.740946Z",
     "start_time": "2019-05-26T22:32:55.737753Z"
    }
   },
   "outputs": [],
   "source": [
    "AmE_gating_data_dir = 'GD_AmE'\n",
    "AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "assert path.exists(AmE_gating_data_dir), 'Gating data directory {0} does not exist.'.format(AmE_gating_data_dir)\n",
    "assert path.exists(AmE_GD_fp), 'Gating data file {0} does not exist.'.format(AmE_GD_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed gating data used here come from \n",
    " - https://github.com/emeinhardt/wmc2014-ipa\n",
    " \n",
    "See those repositories for information on how they were produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0b: Check for transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each transcribed lexicon `LEXNAME` should be in a folder (e.g. `LTR_LEXNAME`) containing a file `LTR_LEXNAME.tsv`. For documentation purposes, the source file and a notebook documenting the production of the `.tsv` file should, if practicable be included in the folder as well.\n",
    "   - A transcribed lexicon `LTR_....tsv` file should have two columns: `Orthographic_Wordform` and `Transcription`.\n",
    "   - NB: The `LTR_` prefix on transcribed lexicon data files and containing folders is simply a convention for organization and readability, but is not required or expected by any file or script in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions in the code below will only succeed if step 1 is complete for all transcribed lexicons listed for checking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:56.926950Z",
     "start_time": "2019-05-26T22:32:56.919420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3432.33it/s]\n"
     ]
    }
   ],
   "source": [
    "newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "# nxt_swbd_ltr_folder = \n",
    "\n",
    "LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder)\n",
    "LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder)\n",
    "\n",
    "for dirname in tqdm(LTR_folders_to_process):\n",
    "    assert path.exists(dirname), 'Transcribed lexicon directory {0} not found in repo directory'.format(dirname)\n",
    "    fname = path.join(dirname, dirname + '.tsv')\n",
    "    assert path.exists(fname), 'Transcribed lexicon {0} not found in repo directory'.format(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are transcribed lexicon relations made?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each was created by processing a transcription source (a lexical database, an annotated corpus, etc.). The processing step is described in other repositories:\n",
    " - https://github.com/emeinhardt/newdic-nettalk-ipa\n",
    " - https://github.com/emeinhardt/cmu-ipa\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    "\n",
    "Given the processed outputs of these repositories, the `Making a Transcribed Lexicon Relation - <LEXNAME>.ipynb` notebook in each `LTR_LEXNAME` folder describes how the homogeneous `.tsv` files downstream steps are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:58.059675Z",
     "start_time": "2019-05-26T22:32:58.052742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - newdic_destressed.ipynb',\n",
       " 'newdic_IPA.tsv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Making a Transcribed Lexicon Relation - CMU_destressed.ipynb',\n",
       " 'LTR_CMU_destressed.pW_V.json',\n",
       " 'LTR_CMU_destressed.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'cmudict-0.7b_IPA_destressed.tsv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_Buckeye.tsv',\n",
       " 'buckeye_words_analysis_relation.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye.ipynb']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname in LTR_folders_to_process:\n",
    "    listdir(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0c: Check for n-gram contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:32:59.356617Z",
     "start_time": "2019-05-26T22:32:59.352602Z"
    }
   },
   "outputs": [],
   "source": [
    "buckeye_contexts = 'buckeye_contexts.txt'\n",
    "swbd2003_contexts = 'swbd2003_contexts.txt'\n",
    "\n",
    "contexts = (buckeye_contexts, swbd2003_contexts)\n",
    "\n",
    "for c_fn in contexts:\n",
    "    assert path.exists(c_fn), \"N-gram contexts file {0} does not exist.\".format(c_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram context files are taken from\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    " \n",
    "See those repositories for more information on how the contexts were extracted. (*NB*: The context files are not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0d: Check for language model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:00.505983Z",
     "start_time": "2019-05-26T22:33:00.502172Z"
    }
   },
   "outputs": [],
   "source": [
    "fisher_lm_dir = 'LM_Fisher'\n",
    "fisher_lm_fn = 'fisher_utterances_main_4gram.mmap'\n",
    "fisher_lm_fp = path.join(fisher_lm_dir, fisher_lm_fn)\n",
    "\n",
    "fisher_lm_vocab_fn = 'fisher_vocabulary_main.txt'\n",
    "fisher_lm_vocab_fp = path.join(fisher_lm_dir, fisher_lm_vocab_fn)\n",
    "\n",
    "assert path.exists(fisher_lm_fp), 'Language model {0} not found'.format(fisher_lm_fp)\n",
    "assert path.exists(fisher_lm_vocab_fp), 'Language model vocabulary {0} not found'.format(fisher_lm_vocab_fp)\n",
    "\n",
    "fisher_lm_fps = {'lm':fisher_lm_fp, \n",
    "                 'vocab':fisher_lm_vocab_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (memory mapped) 4-gram language model is copied from the output of this repository:\n",
    " - https://github.com/emeinhardt/fisher-lm\n",
    " \n",
    "See that repository for more information. (*NB* Again, the language model file is not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment inventory alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Define inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment inventory of any given transcribed lexicon and the segment inventory of the gating data often do not line up. For the gating data to be usefully applied to a given lexicon of transcriptions, the strings in the (segmental) lexicon must contain only segments found in the gating data stimuli inventory.\n",
    "\n",
    "To ensure this happens, the notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` \n",
    " - takes as inputs \n",
    "     - a transcribed lexicon file path and a gating data file path\n",
    "     - a lexicon projection file path and a gating data projection file path\n",
    " - identifies the inventories of each and what symbols are relatively unique to the lexicon and the gating data\n",
    " - produces \n",
    "   - *a Jupyter notebook* for **you to open and finish by defining two projection functions** (i.e. Python dictionaries) to be applied to strings in the transcribed lexicon and to the gating data (one function for each). When you finish doing this (and set an export flag in the notebook to True and run the remainder of the notebook), this notebook will produce\n",
    "     - two *.json files storing these projections* according to the previously provided output file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:21:18.706053Z",
     "start_time": "2019-05-26T22:21:15.861870Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf LTR*_aligned_w_*\n",
    "# %rm -rf *\" alignment definition\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will only succeed if the American English gating data of Warner, McQueen, and Cutler (2014) is contained in the repo directory with a particular directory and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:06.065464Z",
     "start_time": "2019-05-26T22:33:06.062390Z"
    }
   },
   "outputs": [],
   "source": [
    "gating_data_folder = 'GD_AmE'\n",
    "gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "gating_data_fp = path.join(gating_data_folder, gating_data_fn)\n",
    "\n",
    "assert path.exists(gating_data_folder), 'AmE gating data folder {0} not found in repo directory'.format(gating_data_folder)\n",
    "assert path.exists(gating_data_fp), 'AmE gating data {0} not found in repo directory'.format(gating_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third cell below will create a notebook for alignment projection definitions for each of the transcribed lexicons from the previous step and the AmE gating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:07.653650Z",
     "start_time": "2019-05-26T22:33:07.650001Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeExtension(fp):\n",
    "    dir_name = path.dirname(fp)\n",
    "    file_name = path.basename(fp)\n",
    "    ext = file_name.split('.')[-1]\n",
    "    rest = '.'.join( file_name.split('.')[:-1] )\n",
    "    return path.join(dir_name, rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:08.260173Z",
     "start_time": "2019-05-26T22:33:08.249817Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment_arg_bundles = []\n",
    "for LTR_dirname in LTR_folders_to_process:\n",
    "    LTR_fn = LTR_dirname + '.tsv'\n",
    "    LTR_fp = path.join(LTR_dirname, LTR_fn)\n",
    "    \n",
    "    nb_output_name = 'GD_AmE-diphones - ' + LTR_dirname + ' alignment definition' + '.ipynb'\n",
    "    my_g = gating_data_fp\n",
    "    my_l = LTR_fp\n",
    "    my_s = 'destressed'\n",
    "    \n",
    "    gd_alignment_dn = 'GD_AmE_' + my_s + '_' + 'aligned_w_' + LTR_dirname\n",
    "    gd_alignment_fn = 'alignment_of_' + removeExtension(gating_data_fn) + '_w_' + LTR_dirname + '.json'\n",
    "    gd_alignment_fp = path.join(gd_alignment_dn, gd_alignment_fn)\n",
    "    if not path.exists(gd_alignment_dn):\n",
    "        makedirs(gd_alignment_dn)\n",
    "    my_gp = gd_alignment_fp\n",
    "    \n",
    "    ltr_alignment_dn = LTR_dirname + '_aligned_w_' + 'GD_AmE_' + my_s\n",
    "    ltr_alignment_fn = 'alignment_of_' + LTR_dirname + '_w_' + removeExtension(gating_data_fn) + '.json'\n",
    "    ltr_alignment_fp = path.join(ltr_alignment_dn, ltr_alignment_fn)\n",
    "    if not path.exists(ltr_alignment_dn):\n",
    "        makedirs(ltr_alignment_dn)\n",
    "    my_lp = ltr_alignment_fp\n",
    "    \n",
    "    \n",
    "    my_arg_bundle = OrderedDict({\n",
    "        'LTR_dirname':LTR_dirname,\n",
    "        'LTR_fn':LTR_fn,\n",
    "        'LTR_fp':LTR_fp,\n",
    "        'gd_alignment_dn':gd_alignment_dn,\n",
    "        'gd_alignment_fn':gd_alignment_fn,\n",
    "        'gd_alignment_fp':gd_alignment_fp,\n",
    "        'ltr_alignment_dn':ltr_alignment_dn,\n",
    "        'ltr_alignment_fn':ltr_alignment_fn,\n",
    "        'ltr_alignment_fp':ltr_alignment_fp,\n",
    "        'align_def_nb_output_name':nb_output_name,\n",
    "        'my_g':my_g,\n",
    "        'my_l':my_l,\n",
    "        'my_s':my_s,\n",
    "        'my_gp':my_gp,\n",
    "        'my_lp':my_lp,\n",
    "    })\n",
    "    alignment_arg_bundles.append(my_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:22:15.871314Z",
     "start_time": "2019-05-26T22:21:47.889541Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 34.52it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 36.00it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 36.35it/s]\u001b[A\n",
      " 23%|██▎       | 15/64 [00:00<00:01, 27.91it/s]\u001b[A\n",
      " 28%|██▊       | 18/64 [00:00<00:01, 26.93it/s]\u001b[A\n",
      " 33%|███▎      | 21/64 [00:00<00:01, 27.73it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:00<00:01, 21.98it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 24.43it/s]\u001b[A\n",
      " 52%|█████▏    | 33/64 [00:01<00:01, 26.13it/s]\u001b[A\n",
      " 56%|█████▋    | 36/64 [00:04<00:09,  2.84it/s]\u001b[A\n",
      " 59%|█████▉    | 38/64 [00:04<00:06,  3.78it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:08,  2.68it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:05<00:06,  3.59it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:03,  4.86it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:06<00:02,  6.42it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:06<00:01,  8.36it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:00, 10.52it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:06<00:00, 13.00it/s]\u001b[A\n",
      " 94%|█████████▍| 60/64 [00:06<00:00, 15.48it/s]\u001b[A\n",
      " 98%|█████████▊| 63/64 [00:06<00:00, 17.80it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:09<00:18,  9.06s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 31.90it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 32.13it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 32.78it/s]\u001b[A\n",
      " 25%|██▌       | 16/64 [00:00<00:01, 32.91it/s]\u001b[A\n",
      " 30%|██▉       | 19/64 [00:00<00:01, 31.89it/s]\u001b[A\n",
      " 36%|███▌      | 23/64 [00:00<00:01, 32.07it/s]\u001b[A\n",
      " 41%|████      | 26/64 [00:00<00:01, 22.80it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 23.72it/s]\u001b[A\n",
      " 52%|█████▏    | 33/64 [00:01<00:01, 25.34it/s]\u001b[A\n",
      " 56%|█████▋    | 36/64 [00:04<00:09,  3.02it/s]\u001b[A\n",
      " 59%|█████▉    | 38/64 [00:04<00:08,  2.92it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:09,  2.58it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:06<00:08,  2.68it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:05,  3.64it/s]\u001b[A\n",
      " 73%|███████▎  | 47/64 [00:06<00:03,  4.82it/s]\u001b[A\n",
      " 78%|███████▊  | 50/64 [00:06<00:02,  6.29it/s]\u001b[A\n",
      " 83%|████████▎ | 53/64 [00:07<00:01,  8.13it/s]\u001b[A\n",
      " 88%|████████▊ | 56/64 [00:07<00:00, 10.36it/s]\u001b[A\n",
      " 92%|█████████▏| 59/64 [00:07<00:00, 12.59it/s]\u001b[A\n",
      " 97%|█████████▋| 62/64 [00:07<00:00, 14.96it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:19<00:09,  9.35s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 34.08it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 35.22it/s]\u001b[A\n",
      " 20%|██        | 13/64 [00:00<00:01, 36.44it/s]\u001b[A\n",
      " 27%|██▋       | 17/64 [00:00<00:01, 35.42it/s]\u001b[A\n",
      " 31%|███▏      | 20/64 [00:00<00:01, 33.41it/s]\u001b[A\n",
      " 38%|███▊      | 24/64 [00:00<00:01, 33.17it/s]\u001b[A\n",
      " 42%|████▏     | 27/64 [00:00<00:01, 23.21it/s]\u001b[A\n",
      " 47%|████▋     | 30/64 [00:01<00:01, 24.05it/s]\u001b[A\n",
      " 52%|█████▏    | 33/64 [00:01<00:01, 24.90it/s]\u001b[A\n",
      " 56%|█████▋    | 36/64 [00:04<00:09,  2.91it/s]\u001b[A\n",
      " 61%|██████    | 39/64 [00:04<00:06,  3.96it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:05<00:05,  3.69it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:05<00:03,  4.94it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:05<00:02,  6.52it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:05<00:01,  8.51it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:05<00:00, 10.66it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:05<00:00, 13.09it/s]\u001b[A\n",
      " 94%|█████████▍| 60/64 [00:06<00:00, 15.62it/s]\u001b[A\n",
      " 98%|█████████▊| 63/64 [00:06<00:00, 17.58it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:27<00:00,  9.21s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for arg_bundle in tqdm(alignment_arg_bundles):\n",
    "    nb = pm.execute_notebook(\n",
    "        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "        arg_bundle['align_def_nb_output_name'],\n",
    "        parameters=dict(g = arg_bundle['my_g'], \n",
    "                        l = arg_bundle['my_l'], \n",
    "                        s = arg_bundle['my_s'], \n",
    "                        gp = arg_bundle['my_gp'], \n",
    "                        lp = arg_bundle['my_lp'])\n",
    "    )\n",
    "#     pm.execute_notebook(\n",
    "#        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "#        nb_output_name,\n",
    "#        parameters=dict(g = my_g, l = my_l, s = my_s, gp = my_gp, lp = my_lp)\n",
    "#     )\n",
    "    print(\"Finished creating alignment definition notebook '{0}'.\\nOpen and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\\n\".format(arg_bundle['align_def_nb_output_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Apply inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T01:26:12.419332Z",
     "start_time": "2019-05-26T01:26:12.301398Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf *\" alignment application \"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will succeed if you have run each of the previously produced notebooks correctly and produced a projection mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:14.959320Z",
     "start_time": "2019-05-26T22:33:14.953509Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    assert path.exists(args['gd_alignment_fp']), 'Gating data alignment projection mapping not found:\\n\\t{0}'.format(args['gd_alignment_fp'])\n",
    "    assert path.exists(args['ltr_alignment_fp']), 'Transcribed lexicon data alignment projection mapping not found:\\n\\t{0}'.format(args['ltr_alignment_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are inventory alignment projections actually applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Align transcriptions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below applies each pair of alignment projections to each matched pair of gating data and transcribed lexicon choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:18.539266Z",
     "start_time": "2019-05-26T22:33:18.532007Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    LTR_fn = args['LTR_fn']\n",
    "    \n",
    "#     my_pg = args['my_gp']\n",
    "#     my_g = args['my_g']\n",
    "    my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    args['my_og'] = my_og\n",
    "    \n",
    "#     my_pl = args['my_lp']\n",
    "#     my_l = args['my_l']\n",
    "    my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    args['my_ol'] = my_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:30:50.127854Z",
     "start_time": "2019-05-26T22:30:05.708300Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tLTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_newdic_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tLTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:03<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_CMU_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tLTR_Buckeye/LTR_Buckeye.tsv\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_Buckeye/LTR_Buckeye.tsv\n",
      "Result saved to\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "#     LTR_fn = args['LTR_fn']\n",
    "    \n",
    "    my_pg = args['my_gp']\n",
    "    my_g = args['my_g']\n",
    "#     my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "#     my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "#     args['my_og'] = my_og\n",
    "    my_og = args['my_og']\n",
    "    print(\"Creating notebook '{0}' w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}\".format(args['align_apply_gd_nb_output_name'], my_pg, my_g, my_og))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_gd_nb_output_name'],\n",
    "        parameters=dict(p = my_pg,\n",
    "                        g = my_g,\n",
    "                        o = my_og)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tg = {1}\\nResult saved to\\n\\t{2}'.format(my_pg, my_g, my_og))\n",
    "    print(' ')\n",
    "    \n",
    "    my_pl = args['my_lp']\n",
    "    my_l = args['my_l']\n",
    "#     my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "#     my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "#     args['my_ol'] = my_ol\n",
    "    my_ol = args['my_ol']\n",
    "    print('Creating notebook {0} w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}'.format(args['align_apply_ltr_nb_output_name'], my_pg, my_l, my_ol))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_ltr_nb_output_name'],\n",
    "        parameters=dict(p = my_pl,\n",
    "                        l = my_l,\n",
    "                        o = my_ol)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tl = {1}\\nResult saved to\\n\\t{2}'.format(my_pl, my_l, my_ol))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generating channel and (orthographic) lexicon distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Generating channel distributions and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:29.400706Z",
     "start_time": "2019-05-26T22:33:29.283992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "%ls -d GD_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:34.123048Z",
     "start_time": "2019-05-26T22:33:34.119722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_folders = ('GD_AmE', ) + tuple(map(lambda ab: ab['gd_alignment_dn'], alignment_arg_bundles))\n",
    "gating_data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:38.336324Z",
     "start_time": "2019-05-26T22:33:38.331738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_fps = ('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',) + \\\n",
    "                  tuple(map(lambda ab: ab['my_og'], alignment_arg_bundles))\n",
    "gating_data_fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (for downstream convenience) we identify the $n$-phones (not) contained in and (not) constructible from each of the versions of the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** This is done by calling the notebook `Run n-phone analysis of gating data.ipynb`, passing it **the filepath to a gating data `.csv`/`.tsv` file** and **a path to an output directory** for the dozen or so files the notebook will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T17:18:09.969847Z",
     "start_time": "2019-05-27T17:16:04.270457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE/GD_AmE n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'GD_AmE n-phone analysis.ipynb']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphones.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gating_data_fp in gating_data_fps:\n",
    "    gd_dir = path.dirname(gating_data_fp)\n",
    "    nb = pm.execute_notebook(\n",
    "        'Run n-phone analysis of gating data.ipynb',\n",
    "#         args['align_apply_ltr_nb_output_name'],\n",
    "        path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "        parameters=dict(g = gating_data_fp,\n",
    "                        o = gd_dir)\n",
    "    )\n",
    "    listdir(gd_dir)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the notebook `Producing channel distributions.ipynb` will create `.json` files defining (among other things) a uniphone and triphone channel distribution. It requires the following arguments to specify information about what kind of channel model to build and where to put it:\n",
    " - **a filepath** to a gating data file\n",
    " - **a directory** containing metadata indicating possible/impossible $n$-phones\n",
    " - **a string argument** (\"stressed\" or \"destressed\") indicating whether the distribution will be over a segment inventory with or without stress information\n",
    " - **a real valued**  smoothing parameter (a pseudocount to add to every channel outcome)\n",
    " - **an output directory** to write the channel model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:53.950718Z",
     "start_time": "2019-05-26T22:33:53.947905Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudocounts = (0, 0.01, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:54.219873Z",
     "start_time": "2019-05-26T22:33:54.215269Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_arg_bundles = []\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    metadata_dir = path.dirname(gating_data_fp)\n",
    "    s = \"destressed\"\n",
    "    channel_model_dir_stem = 'CM' + metadata_dir[2:]\n",
    "    \n",
    "    for pc in pseudocounts:\n",
    "        channel_model_dir_suffix = '_pseudocount' + str(pc)\n",
    "        if metadata_dir == 'GD_AmE':\n",
    "            channel_model_dir = channel_model_dir_stem + '_' + s + '_unaligned' + channel_model_dir_suffix\n",
    "        else:\n",
    "            channel_model_dir = channel_model_dir_stem + channel_model_dir_suffix\n",
    "        nb_output_name = 'Producing channel distributions from ' + metadata_dir + ', pc={0}'.format(pc) + '.ipynb'\n",
    "        new_arg_bundle = {'gating_data_fp':gating_data_fp,\n",
    "                          'metadata_dir':metadata_dir,\n",
    "                          's':s,\n",
    "                          'c':pc,\n",
    "                          'cm_dir':channel_model_dir,\n",
    "                          'nb_output_name':nb_output_name}\n",
    "        cm_arg_bundles.append(new_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:33:55.823277Z",
     "start_time": "2019-05-26T22:33:55.819115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gating_data_fp': 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'metadata_dir': 'GD_AmE',\n",
       " 's': 'destressed',\n",
       " 'c': 0,\n",
       " 'cm_dir': 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'nb_output_name': 'Producing channel distributions from GD_AmE, pc=0.ipynb'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arg_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:09.934163Z",
     "start_time": "2019-05-27T17:18:09.972176Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0/Producing channel distributions from GD_AmE, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.01/Producing channel distributions from GD_AmE, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.05/Producing channel distributions from GD_AmE, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [08:01<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.1/Producing channel distributions from GD_AmE, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:48<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:56<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:50<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:52<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:59<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:53<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:49<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:57<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:55<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#takes ~100m on wittgenstein\n",
    "for ab in cm_arg_bundles:\n",
    "    if not path.exists(ab['cm_dir']):\n",
    "        mkdir(ab['cm_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing channel distributions.ipynb',\n",
    "    path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "    parameters=dict(g = ab['gating_data_fp'],\n",
    "                    m = ab['metadata_dir'],\n",
    "                    s = ab['s'],\n",
    "                    c = ab['c'],\n",
    "                    o = ab['cm_dir'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are channel distributions created?**\n",
    "\n",
    "Take a look at `Producing channel distributions.ipynb`. Besides removing stress information, there are some mathematically non-trivial details that go into defining both uniphone and triphone channel distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a language model $m$ (defined by a `.arpa` file or `kenlm` memory-mapped analogue) \n",
    " - a choice of n-gram contexts $C$ (a `.txt` file with one context per line)\n",
    " - a vocabulary $V$ (a `.txt` file with one word per line)\n",
    " - a (partial) output filepath $o$ / output filepath prefix $o$\n",
    " \n",
    "`Producing contextual distributions.ipynb` will write a serialized/memory-mapped `numpy` array to $o$.hV_C that defines $-log_2( p(V|C) )$ - slightly transformed output from `kenlm`. It will also write $p(V|C)$ to $o$.pV_C, and copy both $V$ and $C$ to the base directory specified by $o$. (In both cases, each column is associated with the distribution $p(V|c)$ for some $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:01.957387Z",
     "start_time": "2019-05-26T22:34:01.953866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('buckeye_contexts.txt', 'swbd2003_contexts.txt')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts\n",
    "fisher_lm_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:03.619584Z",
     "start_time": "2019-05-26T22:34:03.613167Z"
    }
   },
   "outputs": [],
   "source": [
    "LDs = [{'LD_dir':'LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':buckeye_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_Buckeye_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')},\n",
    "       {'LD_dir':'LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_swbd2003_contexts' + '/' + 'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':swbd2003_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_swbd2003_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')}\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T20:21:42.664184Z",
     "start_time": "2019-05-26T19:00:16.251722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/86 [00:00<00:03, 24.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 6/86 [00:00<00:03, 26.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 9/86 [00:00<00:02, 26.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 12/86 [00:00<00:02, 26.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 15/86 [00:00<00:02, 26.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 18/86 [00:00<00:02, 25.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 21/86 [00:00<00:03, 18.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 24/86 [00:01<00:03, 19.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 27/86 [00:01<00:02, 20.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 30/86 [00:01<00:02, 19.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/86 [00:01<00:02, 21.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 36/86 [00:01<00:02, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 39/86 [00:01<00:02, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 42/86 [00:01<00:02, 21.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/86 [00:02<00:01, 21.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 48/86 [00:02<00:01, 20.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 51/86 [00:02<00:01, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 54/86 [00:02<00:01, 22.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▋   | 57/86 [00:02<00:01, 21.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:02<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:21<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 61/86 [02:47<20:35, 49.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 62/86 [04:03<22:59, 57.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 63/86 [04:04<15:28, 40.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 64/86 [04:04<10:24, 28.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 65/86 [04:05<07:00, 20.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 66/86 [04:05<04:42, 14.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 67/86 [04:05<03:08,  9.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 68/86 [04:18<03:14, 10.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 69/86 [04:18<02:10,  7.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 70/86 [04:18<01:26,  5.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 71/86 [04:19<00:59,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 72/86 [04:26<01:08,  4.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 73/86 [04:26<00:45,  3.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 74/86 [04:26<00:29,  2.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 75/86 [04:27<00:21,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 76/86 [04:28<00:15,  1.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 77/86 [04:28<00:10,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 79/86 [04:28<00:05,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 81/86 [04:28<00:02,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 82/86 [04:28<00:01,  2.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 83/86 [05:05<00:34, 11.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 84/86 [06:19<01:00, 30.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 85/86 [06:20<00:21, 21.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 86/86 [06:20<00:00, 15.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/87 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/87 [00:00<00:03, 24.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 7/87 [00:00<00:03, 26.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 10/87 [00:00<00:02, 26.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 13/87 [00:00<00:02, 26.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 16/87 [00:00<00:03, 21.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 19/87 [00:00<00:03, 22.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 22/87 [00:01<00:03, 17.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 25/87 [00:01<00:03, 19.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 28/87 [00:01<00:03, 17.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 30/87 [00:01<00:03, 17.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/87 [00:01<00:02, 18.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 36/87 [00:01<00:02, 19.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 39/87 [00:01<00:02, 21.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 42/87 [00:02<00:02, 21.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/87 [00:02<00:01, 22.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 48/87 [00:02<00:01, 22.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 51/87 [00:02<00:01, 22.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 54/87 [00:02<00:01, 22.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 57/87 [00:02<00:01, 22.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:02<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:13<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 61/87 [16:04<2:05:03, 288.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 62/87 [38:25<4:11:48, 604.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 63/87 [38:26<2:49:14, 423.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▎  | 64/87 [38:26<1:53:33, 296.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 65/87 [38:26<1:16:02, 207.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 66/87 [38:26<50:49, 145.23s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 67/87 [38:26<33:53, 101.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 68/87 [42:50<47:35, 150.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 69/87 [42:58<32:19, 107.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 70/87 [43:05<21:58, 77.56s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 71/87 [44:09<19:35, 73.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 72/87 [45:28<18:44, 74.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 73/87 [50:38<33:55, 145.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 74/87 [50:38<22:05, 101.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 75/87 [50:38<14:16, 71.40s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 76/87 [50:43<09:26, 51.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▊ | 77/87 [50:43<06:00, 36.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 78/87 [50:43<03:47, 25.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 79/87 [50:44<02:21, 17.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 80/87 [50:44<01:28, 12.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 81/87 [50:44<00:53,  8.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 82/87 [50:45<00:31,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 83/87 [50:45<00:17,  4.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 84/87 [55:08<04:06, 82.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 85/87 [1:13:43<13:04, 392.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 86/87 [1:13:51<04:36, 276.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 87/87 [1:14:01<00:00, 196.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ld in LDs:\n",
    "    if not path.exists(ld['LD_dir']):\n",
    "        mkdir(ld['LD_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing contextual distributions.ipynb',\n",
    "#     'Producing ' + ld['LD_dir'].replace('_', ' ')[3:] + ' contextual distributions.ipynb',\n",
    "    ld['nb_fp'],\n",
    "    parameters=dict(m = ld['m'],\n",
    "                    v = ld['v'],\n",
    "                    c = ld['c'],\n",
    "                    o = ld['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating combinable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic problem:**\n",
    "\n",
    "1. **Channel model + transcribed lexicon relation**: Even after the gating data and a transcribed lexicon relation are defined over the same inventory, \n",
    " - the lexicon may contain triphones that are not in the stimuli triphones of a triphone channel model.\n",
    " - the channel distribution will contain triphones that cannot be found in the transcribed lexicon relation. (While the other steps here are strictly necessary, this is simply a practical step for making downstream computation faster.)\n",
    "2. **Language model + transcribed lexicon relation**: The orthographic vocabulary of a transcribed lexicon relation may contain wordforms not in an n-gram model's vocabulary. (We *don't* want to use the out-of-vocabulary estimate for those wordforms.)\n",
    "3.  **Contextual distributions + transcribed lexicon relation**: The contextual distributions from Step 3b above are defined over the *language model's* orthographic vocabulary, which will likely include wordforms that are not in the transcribed lexicon relation. We want to create modified forms of these distributions where we condition on the choice of an orthographic wordform that is in the transcribed lexicon relation.\n",
    "\n",
    "Once we have\n",
    " - a version $l'$ of the transcribed lexicon relation $l$ trimmed with respect to both the triphones of the channel model $c$ and the (orthographic) vocabulary of a language model $m$\n",
    " - a version $d'$ of the contextual distributions over $m$'s vocabulary (with respect to some set of n-gram contexts) $d$ trimmed to only define distributions over $l'$\n",
    " - a version $c'$ of the channel model $c$ trimmed with respect to a transcribed lexicon relation $l'$\n",
    " - a probability distribution over segmental wordforms given an orthographic wordform\n",
    " \n",
    "we will be able to combine everything together to calculate confusability of wordforms in corpus contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather relevant LTRs and their associated CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:17.892406Z",
     "start_time": "2019-05-26T22:34:17.888149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs = list(map(lambda ab: {'LTR_fp':ab['my_ol'],\n",
    "                                    'GD_fp':ab['my_og']},\n",
    "                        alignment_arg_bundles))\n",
    "list(map(lambda d: d['LTR_fp'],\n",
    "         aligned_LTRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:21.305181Z",
     "start_time": "2019-05-26T22:34:21.300434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CM_dirs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "#                                 'GD_fp':cab['gating_data_fp']},\n",
    "#                    filter(lambda cab: cab['c'] != 0, cm_arg_bundles)))\n",
    "# CM_dirs\n",
    "# CM_dirs[5]\n",
    "# # listdir(CM_dirs[0][])\n",
    "\n",
    "aligned_CMs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "                                    'GD_fp':cab['gating_data_fp']},\n",
    "                       filter(lambda cab: cab['c'] != 0 and 'aligned' in cab['gating_data_fp'], \n",
    "                              cm_arg_bundles)))\n",
    "aligned_CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:22.776606Z",
     "start_time": "2019-05-26T22:34:22.772112Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aligned_CMs(ltr_bundle):\n",
    "    matches = [cm_bundle for cm_bundle in aligned_CMs if cm_bundle['GD_fp'] == ltr_bundle['GD_fp']]\n",
    "    return list(map(lambda d: d['CM_fp'],\n",
    "                    matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:24.145659Z",
     "start_time": "2019-05-26T22:34:24.141157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs[0]['LTR_fp']\n",
    "\n",
    "#NB: all of these will have the same set of stimuli triphones\n",
    "#    ...which is all we care about here\n",
    "get_aligned_CMs(aligned_LTRs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:26.683185Z",
     "start_time": "2019-05-26T22:34:26.678323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs_and_CM = [{'LTR_fp':ltr['LTR_fp'],\n",
    "                        'matching_CMs':get_aligned_CMs(ltr)}\n",
    "                       for ltr in aligned_LTRs]\n",
    "aligned_LTRs_and_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:33.239012Z",
     "start_time": "2019-05-26T22:34:33.232479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "nb = LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    each['c'] = each['matching_CMs'][0]\n",
    "    each['l'] = each['LTR_fp']\n",
    "    o_dir = path.dirname(each['LTR_fp'])\n",
    "    o_fn = path.basename(each['LTR_fp']).split('w_')[0] + 'CM_filtered' + '.tsv'\n",
    "    each['o'] = path.join(o_dir, o_fn)\n",
    "    \n",
    "    nb_fn = 'Filter ' + o_fn.split('_aligned')[0] + ' against channel model.ipynb'\n",
    "    each['nb_fp'] = path.join(o_dir, nb_fn)\n",
    "    \n",
    "    print('c = {0}\\nl = {1}\\no = {2}\\nnb = {3}'.format(each['c'], each['l'], each['o'], each['nb_fp']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:27.279456Z",
     "start_time": "2019-05-27T19:06:09.936034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      "100%|██████████| 26/26 [00:02<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      "100%|██████████| 26/26 [00:04<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      "100%|██████████| 26/26 [00:01<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    if not path.exists(path.dirname(each['o'])):\n",
    "        mkdir(path.dirname(each['o']))\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter transcription lexicon by channel model.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** For this step, we will use shell commands provided by the `csvkit` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:42.295490Z",
     "start_time": "2019-05-26T22:34:42.292113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_fps\n",
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:34:45.575669Z",
     "start_time": "2019-05-26T22:34:45.570103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_fps = list(map(lambda pair: pair['LTR_fp'],\n",
    "                   aligned_LTRs))\n",
    "LTR_fps\n",
    "\n",
    "LTR_CM_filtered = list(map(lambda d: d['o'],\n",
    "                           aligned_LTRs_and_CM))\n",
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lexicon entries have unmodelable triphones? (We'll next check how many such lexicon entries aren't in the language model vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:27.888602Z",
     "start_time": "2019-05-27T19:06:27.282078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "133855 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "7999 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:28.264386Z",
     "start_time": "2019-05-27T19:06:27.890901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using `csvjoin` to do an inner join of the LM vocabulary with each transcribed lexicon relation:\n",
    "```\n",
    "csvjoin -t LTR_LEXNAME.tsv LM_VOCAB.tsv -c Orthographic_Wordform | csvformat -T > LTR_LEXNAME_filtered.tsv\n",
    "```\n",
    "Performs an inner join of the transcribed lexicon relation in `LTR_LEXNAME.tsv` with the language model vocabulary in `LM_VOCAB.tsv` along the `Orthographic_Wordform` column and writes the result to `LTR_LEXNAME_filtered.tsv`. `LTR_LEXNAME_filtered.tsv` will only contain those rows of `LTR_LEXNAME.tsv` where the `Orthographic_Wordform` value is contained in `LM_VOCAB.tsv`.\n",
    "\n",
    "To do this, we first need to label the vocabulary file with the right column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:28.588210Z",
     "start_time": "2019-05-27T19:06:28.266716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tOrthographic_Wordform\tTranscription\n",
      "     2\ta\tə\n",
      "     3\taardvark\tɑ.ɹ.d.v.ɑ.ɹ.k\n",
      "     4\taback\tə.b.æ.k\n",
      "     5\tabacus\tæ.b.ə.k.ə.s\n",
      "     6\tabaft\tə.b.æ.f.t\n",
      "     7\tabalone\tæ.b.ə.l.oʊ.n.i\n",
      "     8\tabandon\tə.b.æ.n.d.ɪ.n\n",
      "     9\tabase\tə.b.eɪ.s\n",
      "    10\tabash\tə.b.æ.ʃ\n",
      "cat: write error: Broken pipe\n",
      "     1\t'and\n",
      "     2\t'berserkly'\n",
      "     3\t'bout\n",
      "     4\t'burb\n",
      "     5\t'burban\n",
      "     6\t'burbs\n",
      "     7\t'cau\n",
      "     8\t'cause\n",
      "     9\t'cept\n",
      "    10\t'cide\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat -n LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv | head -10\n",
    "!cat -n LM_Fisher/fisher_vocabulary_main.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:28.863749Z",
     "start_time": "2019-05-27T19:06:28.590562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tOrthographic_Wordform\r\n",
      "     2\t'and\r\n",
      "     3\t'berserkly'\r\n",
      "     4\t'bout\r\n",
      "     5\t'burb\r\n",
      "     6\t'burban\r\n",
      "     7\t'burbs\r\n",
      "     8\t'cau\r\n",
      "     9\t'cause\r\n",
      "    10\t'cept\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Orthographic_Wordform\\n$(cat LM_Fisher/fisher_vocabulary_main.txt)\" > LM_Fisher/fisher_vocabulary_main.tsv\n",
    "!cat -n LM_Fisher/fisher_vocabulary_main.tsv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit less than half** of the triphone channel model-modelable `newdic` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:34.782355Z",
     "start_time": "2019-05-27T19:06:28.866057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "9416 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -t LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv LM_Fisher/fisher_vocabulary_main.tsv -c Orthographic_Wordform | csvformat -T > LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv\n",
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About three quarters** of the triphone channel model-modelable `CMU_destressed` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:41.556044Z",
     "start_time": "2019-05-27T19:06:34.785983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "33120 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -t LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv LM_Fisher/fisher_vocabulary_main.tsv -c Orthographic_Wordform | csvformat -T > LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Less than 10%** of the triphone channel model-modelable `Buckeye` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:46.720785Z",
     "start_time": "2019-05-27T19:06:41.558811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "6574 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!csvjoin -t LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv LM_Fisher/fisher_vocabulary_main.tsv -c Orthographic_Wordform | csvformat -T > LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting the filtered transcribed lexicon relations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:44:42.948940Z",
     "start_time": "2019-05-27T19:44:42.944554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered = list(map(lambda fp: fp[:-4] + '_LM_filtered.tsv',\n",
    "                                       LTR_CM_filtered))\n",
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:46:18.061069Z",
     "start_time": "2019-05-27T19:46:18.058861Z"
    }
   },
   "outputs": [],
   "source": [
    "#what channel models might you want to use with what lexicons?\n",
    "# there are 3x3 triphone CMs that are aligned with one of 3 LTRs and have one of 3 relevant pseudocount levels\n",
    "# For each of the 3 LTRs aligned with the gating data, there's exactly 1 `LTR...CM_filtered_LM_filtered.tsv` file\n",
    "# ∴ there are 3x3 channel models to trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T20:01:44.839401Z",
     "start_time": "2019-05-27T20:01:44.817142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    LTR_dir = path.dirname(each['LTR_fp'])\n",
    "    LTR_trimmed_fn = path.basename(each['LTR_fp']).split('_w_')[0] + '_CM_filtered_LM_filtered.tsv'\n",
    "    each['LTR_trimmed_fp'] = path.join(LTR_dir, LTR_trimmed_fn)\n",
    "    each['matching_trimmed_CMs'] = [path.join(path.dirname(fp),\n",
    "                                              LTR_trimmed_fn[:-4] + '_' + path.basename(fp))\n",
    "                                    for fp in each['matching_CMs']]\n",
    "    \n",
    "#     each['LTR_fp']\n",
    "    each['LTR_trimmed_fp']\n",
    "    each['matching_CMs']\n",
    "    each['matching_trimmed_CMs']\n",
    "#     each['trimmed LTR_fp']\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T20:08:37.527786Z",
     "start_time": "2019-05-27T20:08:37.498665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_LTR_CM_triples = []\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    assert len(each['matching_CMs']) == len(each['matching_trimmed_CMs'])\n",
    "    \n",
    "    for i in range(len(each['matching_CMs'])):\n",
    "        args = dict()\n",
    "        args['l'] = each['LTR_trimmed_fp']\n",
    "        args['c'] = each['matching_CMs'][i]\n",
    "        args['o'] = each['matching_trimmed_CMs'][i]\n",
    "        args['nb_fp'] = f\"Filter {path.dirname(args['c'])} against {path.basename(args['l'])[:-4]}.ipynb\"\n",
    "        args\n",
    "        trimmed_LTR_CM_triples.append(args)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T20:11:13.571235Z",
     "start_time": "2019-05-27T20:10:01.712050Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in trimmed_LTR_CM_triples:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Creating output path '{output_dir}'\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter channel model by transcription lexicon.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:55:44.067936Z",
     "start_time": "2019-05-26T23:55:44.064484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:55:44.797698Z",
     "start_time": "2019-05-26T23:55:44.792476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles = []\n",
    "for ltr_fp in LTR_CM_filtered_LM_filtered:\n",
    "    LTR_n = path.basename( ltr_fp )[:-4]\n",
    "    LTR_dir = path.dirname( ltr_fp )\n",
    "    pW_V_fp_bundles.append({'LTR_fp':ltr_fp,\n",
    "                          'pW_V_fp':ltr_fp[:-4] + '.pW_V',\n",
    "                          'nb_fp':path.join(LTR_dir,'Define pW_V given {0}.ipynb'.format(LTR_n))})\n",
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T00:30:28.456665Z",
     "start_time": "2019-05-27T00:01:36.521Z"
    }
   },
   "outputs": [],
   "source": [
    "for each in pW_V_fp_bundles:\n",
    "    pW_V_fp_output_dir = path.dirname(each['pW_V_fp'])\n",
    "    if not path.exists(pW_V_fp_output_dir):\n",
    "        mkdir(pW_V_fp_output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['LTR_fp'],\n",
    "                    o = each['pW_V_fp'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pre-calculate remaining forward model component(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Define incremental channel distributions on segmental wordforms, organized by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate posterior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generating analysis measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
