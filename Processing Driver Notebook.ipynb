{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:36.431245Z",
     "start_time": "2019-09-12T23:13:36.428051Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-we-want-to-calculate\" data-toc-modified-id=\"What-we-want-to-calculate-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What we <em>want</em> to calculate</a></span></li><li><span><a href=\"#What-we-can-calculate\" data-toc-modified-id=\"What-we-can-calculate-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>What we <em>can</em> calculate</a></span></li><li><span><a href=\"#The-structure-of-calculations\" data-toc-modified-id=\"The-structure-of-calculations-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>The structure of calculations</a></span></li><li><span><a href=\"#The-structure-of-the-pipeline\" data-toc-modified-id=\"The-structure-of-the-pipeline-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>The structure of the pipeline</a></span></li></ul></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Requirements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Python-environment\" data-toc-modified-id=\"Python-environment-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Python environment</a></span></li><li><span><a href=\"#Hardware-/-runtime-expectations\" data-toc-modified-id=\"Hardware-/-runtime-expectations-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Hardware / runtime expectations</a></span></li></ul></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Todo</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Permitted-steps-/-control-flow\" data-toc-modified-id=\"Permitted-steps-/-control-flow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Permitted steps / control flow</a></span></li><li><span><a href=\"#Step-0:-Import/check-for-foundational-files\" data-toc-modified-id=\"Step-0:-Import/check-for-foundational-files-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 0: Import/check for foundational files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-existing-data-and-creating-directories\" data-toc-modified-id=\"Importing-existing-data-and-creating-directories-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Importing existing data and creating directories</a></span></li><li><span><a href=\"#Step-0a:-Check-for-gating-data\" data-toc-modified-id=\"Step-0a:-Check-for-gating-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Step 0a: Check for gating data</a></span></li><li><span><a href=\"#Step-0b:-Check-for-transcribed-lexicons\" data-toc-modified-id=\"Step-0b:-Check-for-transcribed-lexicons-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Step 0b: Check for transcribed lexicons</a></span></li><li><span><a href=\"#Step-0c:-Check-for-n-gram-contexts\" data-toc-modified-id=\"Step-0c:-Check-for-n-gram-contexts-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Step 0c: Check for n-gram contexts</a></span></li><li><span><a href=\"#Step-0d:-Check-for-language-model(s)\" data-toc-modified-id=\"Step-0d:-Check-for-language-model(s)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Step 0d: Check for language model(s)</a></span></li></ul></li><li><span><a href=\"#Step-1:-Segment-inventory-alignment\" data-toc-modified-id=\"Step-1:-Segment-inventory-alignment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 1: Segment inventory alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1a:-Define-inventory-alignment-projections\" data-toc-modified-id=\"Step-1a:-Define-inventory-alignment-projections-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Step 1a: Define inventory alignment projections</a></span></li><li><span><a href=\"#Step-1b:-Apply-inventory-alignment-projections\" data-toc-modified-id=\"Step-1b:-Apply-inventory-alignment-projections-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Step 1b: Apply inventory alignment projections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-projection-definitions\" data-toc-modified-id=\"Check-for-projection-definitions-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Check for projection definitions</a></span></li><li><span><a href=\"#How-are-inventory-alignment-projections-actually-applied?\" data-toc-modified-id=\"How-are-inventory-alignment-projections-actually-applied?-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>How are inventory alignment projections actually applied?</a></span></li><li><span><a href=\"#Apply-projection-definitions\" data-toc-modified-id=\"Apply-projection-definitions-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Apply projection definitions</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions\" data-toc-modified-id=\"Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 2: Generating channel and (orthographic) lexicon distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-2a:-Generating-channel-distributions-and-associated-metadata\" data-toc-modified-id=\"Step-2a:-Generating-channel-distributions-and-associated-metadata-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 2a: Generating channel distributions and associated metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Metadata</a></span></li><li><span><a href=\"#Channel-distributions\" data-toc-modified-id=\"Channel-distributions-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Channel distributions</a></span></li></ul></li><li><span><a href=\"#Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)\" data-toc-modified-id=\"Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)</a></span></li></ul></li><li><span><a href=\"#Step-3:-Creating-combinable-models\" data-toc-modified-id=\"Step-3:-Creating-combinable-models-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 3: Creating combinable models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution\" data-toc-modified-id=\"Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution</a></span></li><li><span><a href=\"#Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary\" data-toc-modified-id=\"Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary</a></span></li><li><span><a href=\"#Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-$k$-factors-contained-in-elements-of-a-transcription-lexicon's-segmental-wordforms\" data-toc-modified-id=\"Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-$k$-factors-contained-in-elements-of-a-transcription-lexicon's-segmental-wordforms-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Step 3c: Filter the conditioning events of channel distributions to only include $k$-factors contained in elements of a transcription lexicon's segmental wordforms</a></span></li><li><span><a href=\"#Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms\" data-toc-modified-id=\"Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms</a></span></li><li><span><a href=\"#Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform\" data-toc-modified-id=\"Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform</a></span></li></ul></li><li><span><a href=\"#Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data\" data-toc-modified-id=\"Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Step 4: Pre-calculate remaining forward model components and meta-data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model\" data-toc-modified-id=\"Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Step 4a: Generate triphone lexicon distributions for every triphone channel model</a></span></li><li><span><a href=\"#Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon\" data-toc-modified-id=\"Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon</a></span></li><li><span><a href=\"#Step-4c:-Calculate-the-marginal-probability-$p(W|C)$-of-each-segmental-wordform-$w$-given-$n$-gram-contexts-$C$\" data-toc-modified-id=\"Step-4c:-Calculate-the-marginal-probability-$p(W|C)$-of-each-segmental-wordform-$w$-given-$n$-gram-contexts-$C$-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Step 4c: Calculate the marginal probability $p(W|C)$ of each segmental wordform $w$ given $n$-gram contexts $C$</a></span></li><li><span><a href=\"#Step-4d:-Define-observation-distributions\" data-toc-modified-id=\"Step-4d:-Define-observation-distributions-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Step 4d: Define observation distributions</a></span></li><li><span><a href=\"#Step-4e:-Define-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)\" data-toc-modified-id=\"Step-4e:-Define-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Step 4e: Define channel distributions on a set of segmental wordforms(+prefixes)</a></span></li></ul></li><li><span><a href=\"#Step-5:-Calculate-posterior-probabilities\" data-toc-modified-id=\"Step-5:-Calculate-posterior-probabilities-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Step 5: Calculate posterior probabilities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-5a:-Calculate-$p(V|W,-C)$\" data-toc-modified-id=\"Step-5a:-Calculate-$p(V|W,-C)$-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Step 5a: Calculate $p(V|W, C)$</a></span></li><li><span><a href=\"#Step-5b:-Calculate-$p(\\hat{X}_0^f|X_0^f,-C)$\" data-toc-modified-id=\"Step-5b:-Calculate-$p(\\hat{X}_0^f|X_0^f,-C)$-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Step 5b: Calculate $p(\\hat{X}_0^f|X_0^f, C)$</a></span></li><li><span><a href=\"#Step-5c:-Calculate-$p(\\hat{V}-=-v^*|-V-=-v^*,-C)$\" data-toc-modified-id=\"Step-5c:-Calculate-$p(\\hat{V}-=-v^*|-V-=-v^*,-C)$-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Step 5c: Calculate $p(\\hat{V} = v^*| V = v^*, C)$</a></span></li></ul></li><li><span><a href=\"#Step-6:-Generating-analysis-measures\" data-toc-modified-id=\"Step-6:-Generating-analysis-measures-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Step 6: Generating analysis measures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processing pipeline from \n",
    " - gating data\n",
    " - transcribed lexicon\n",
    " - a language model and (possibly empty) n-gram contexts\n",
    "\n",
    "to \n",
    " - channel distribution\n",
    " - lexicon distribution(s) (distributions over wordforms)\n",
    " - expected posterior distribution over intended wordform given what has been produced of what was intended.\n",
    " \n",
    " \n",
    "It describes what happens at each step, checks some pre- and post-conditions, describes what you, the user must do (if anything), and scripts some commands to automatically do the necessary processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "There are about one-and-a-half calculations that this processing pipeline enables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    " - $C = \\{c_0, c_1 ... c_{n-1}\\}$ denote a set of $n$-gram contexts (sequences of $\\leq (n-1)$ orthographic wordforms) drawn from a speech corpus (e.g. Buckeye or Switchboard).\n",
    " - $V = \\{v_0, v_1 ... v_V\\}$ denote a set of orthographic wordforms (a 'vocabulary') drawn from a speech corpus (the same corpus $C$ is drawn from).\n",
    " - $W = \\{w_0, w_1 ... w_W\\}$ denote a set of segmental wordforms ('transcribed wordforms'). Each element $w \\in W$ consists of a sequence $x_0 x_1 ... x_f = x_0^f$ of segments ('phonemes') drawn from a set of symbols $\\Sigma_x$.\n",
    " \n",
    "(All sets are finite, unless otherwise noted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - A language model describes $p(V|C)$.\n",
    " - A lexical database or speech corpus describes a relation $r$ between orthographic wordforms $V$ and segmental wordforms $W$, where $(v,w) \\in r$ iff $v$ can be produced as $w$.\n",
    " - Let $p(W|V)$ be defined as follows: $p(w|v) = \\frac{1}{r_{v}}$, where $r_v = |\\{w' | (v, w') \\in r\\}|$\n",
    " - Given that the $i$th segment that a speaker intended to produce is $x_i$, a triphone channel model describes $p(Y_i|x_{i-1}, x_i; x_{i+1})$, a distribution over received/perceived segments $\\Sigma_y$. This is estimated from diphone gating data. Note that it doesn't permit modeling insertions or deletions.\n",
    " - $p(Y_0^f|W) = p(Y_0^f|X_0^f)$ is a (channel) distribution over perceived (segmental) wordforms given an intended (segmental) wordform. It is completely defined by a choice of $p(W|V)$ and a choice of $p(Y_i|x_{i-1}, x_i; x_{i+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *forward model*, then defines a distribution $p(Y_0^f, X_0^f, V|C) = p(Y_0^f|X_0^f)p(X_0^f|V)p(V|C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we *want* to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in calculating the *expected posterior of a listener* over what orthographic wordform the speaker intended given what the speaker actually intended (taking the expectation over the speaker's actual intended segmental wordform, the perceived segmental wordform, and the listener's estimate of the speaker's intended segmental wordform):\n",
    "\n",
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, y_0^f, x_0^{'f}} p(\\hat{V} = v^*, \\hat{X}_0^f = x_0^{'f}, y_0^f, X_0^f = x_0^{*f}|V = v^*, c)$$\n",
    "\n",
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, y_0^f, x_0^{'f}} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) p(Y_0^f = y_0^f | X_0^f = x_0^{*f})p(X_0^f = x_0^{*f}|V = v^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "1. $p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) = \\frac{p(y_0^f|x_0^{'f})p(x_0^{'f}|c)}{p(y_0^f | c)}$\n",
    "2. $p(x_0^{'f}|c) = \\sum\\limits_{v'} p(x_0^{'f}|v')p(v'|c)$\n",
    "3. $p(y_0^f| c) = \\sum\\limits_{v', x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|v')p(v'|c) = \\sum\\limits_{x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|c)$\n",
    "4. $p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) = \\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we *can* calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Unfortunately, because of the enormous size of $Y_0^f$, exact marginalization over all $y_0^f$ is *not* feasible.\n",
    " 2. Fortunately, because, for any given $x_0^{*f}$, the fraction of $Y_0^f$ with non-negligible mass in $p(Y_0^f|x_0^{*f})$ is small, we can construct a Monte Carlo estimate of \n",
    " $$p(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) = \\sum\\limits_{y_0^f} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c) p(y_0^f|x_0^{*f})$$\n",
    " as\n",
    " $$\\hat{p}(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) = \\frac{1}{n} \\sum\\limits_{y_0^f \\in S} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c)$$\n",
    " where $S = $ a set of $n$ samples from $p(Y_0^f|x_0^{*f})$. In practice an $n \\approx 50$ seems to result in estimates that are within $10^{-6}$ of the true estimate. \n",
    " \n",
    " 3. Unfortunately, even with this approximation $p(\\hat{X}_0^f|X_0^{*f}, c)$ has about $10^8 - 10^9$ entries: too many to feasibly calculate exactly, especially across *all choices of $c$* as well.\n",
    " 4. However, because of the relative dispersion of wordforms, the relatively low overall rate of channel noise, and the information provided by sentential context, the fraction of $\\hat{X}_0^f$ assigned non-negligible mass in $p(\\hat{X}_0^f|x_0^{*f}, c)$ for any given $x_0^{*f}$ is relatively small, and largely concentrated on $x_0^{'f}$ that are within $k$ edits (substitutions) of $x_0^{*f}$ for small $k$. Accordingly, we can get an arbitrarily good approximation $\\hat{p}^{k}$ of $p(\\hat{X}_0^f|x_0^{*f}, c)$ by \n",
    "  - choosing an arbitrarily small threshold $\\epsilon$\n",
    "  - calculating\n",
    "$$p^k = \\{p(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) | x_0^{'f} \\text{ is within } k \\text{ edits of }x_0^{*f}\\}$$ for progressively increasing $k$, stopping when $1 - \\sum p_i^k \\leq \\epsilon$ and assigning $0$ probability to all $x_0^{'f}$ not associated with $p^k$.\n",
    "\n",
    "Note that this means that if we choose the same $\\epsilon$ for all $(x_0^{*f}, c)$ pairs, some segmental wordforms $x_0^f$ will have approximations involving higher or lower $k$ values than others, but that all will have distributions that are at least as accurate as some same minimum (determined by $\\epsilon$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, x_0^{'f}} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) p(\\hat{X}_0^f = x_0^{'f}| X_0^f = x_0^{*f}, c) p(X_0^f = x_0^{*f}|V = v^*)$$\n",
    "\n",
    "so $$p(\\hat{V} = v^*|V = v^*, c) \\approx \\sum\\limits_{x_0^{*f}, x_0^{'f}. d(x_0^{*f}, x_0^{'f}) \\leq k} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) \\hat{p}(\\hat{X}_0^f = x_0^{'f}| X_0^f = x_0^{*f}, c) p(X_0^f = x_0^{*f}|V = v^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have the following distributions pre-computed (or as close to that as possible) as `numpy` arrays\n",
    " 1. $p(W|V) = p(X_0^f|V)$\n",
    " 2. $\\hat{p}(\\hat{X}_0^f|X_0^f, C)$\n",
    " 3. $p(Y_0^f|W)$\n",
    " 4. $p(W|C)$\n",
    " 5. $p(V|W, C)$\n",
    " \n",
    "so that we can efficiently calculate $p(\\hat{V} = v^*| V = v^*, c)$ for all pairs of $(v^*, c)$ and\n",
    "where \n",
    " - $p(\\hat{V} = v^*| V = v^*, c)$\n",
    " - $\\hat{p}(\\hat{X}_0^f|X_0^f, C)$, and \n",
    " - $p(V|W,C)$ \n",
    " \n",
    "involve decreasing amounts of non-trivial computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of the pipeline\n",
    "\n",
    "Given foundational files:\n",
    " - a language model $m$ over some orthographic vocabulary $V_m$\n",
    " - $n$-gram contexts $C$ taken from one or more speech corpora\n",
    " - a target orthographic vocabulary $V$ taken from each of the speech corpora that contexts are taken from\n",
    " - diphone gating data\n",
    " - one or more transcription relations $r$\n",
    " \n",
    "the first step in the processing pipeline involves aligning segmental inventories of gating data and transcription relations. Once that's been done, we can define triphone channel distributions on transcription-relation aligned gating data that can be applied to gating-data aligned transcribed wordforms. We also need to define $p(V_m|C)$ for each choice of $C$.\n",
    "\n",
    "Next, we create mutually combinable versions of transcription relations, channel distributions, and distributions over orthographic vocabularies - e.g.\n",
    " - we need to remove words from each $r$ that contain triphones that can't be modeled by the aligned triphone channel distribution\n",
    " - we need to remove words from each $r$ that aren't in the language model vocabulary $V_m$\n",
    " - $p(V_m|C)$ needs to be projected down to remove orthographic words that aren't in transcription relations and context sets $C$ need to be scrubbed of contexts containing orthographic wordforms not in the language model\n",
    "\n",
    "With all the atomic components of the model(/each possible combined model) constructed, we then pre-calculate remaining components of the forward model(s). Finally, we calculate components of the posterior. (The separation of this last step from *everything previous* facilitates parallel computation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Python environment\n",
    "This repository was developed using Python 3.6 with the following package requirements (upstream dependencies not included):\n",
    " - **Jupyter/notebook related packages**: `jupyter` `jupyter_contrib_nbextensions` `papermill`\n",
    "    - Notable notebook extensions used include `ExecuteTime` and `Table of Contents (2)`.\n",
    " - **Numeric computing and data processing**: `scipy` `numpy` `pandas` `tqdm` `joblib` `numba` `sparse` `pytorch` `torchvision`\n",
    " - **Language modeling**: `kenlm`, `arpa`\n",
    " - **Plotting**: `matplotlib` `plotnine`\n",
    " - **Misc**: `funcy` `more_itertools`\n",
    "\n",
    "See `dev_environment.sh` for `conda` and `pip` commands to create an environment that has all of these packages. (Not all are available on conda, and not all of those that are available on conda are available from the main channel.)\n",
    "\n",
    "\n",
    "### Hardware / runtime expectations\n",
    "\n",
    "The last machine this repository was developed on (`wittgenstein`) has 32 cores. `joblib` is used extensively to parallelize data processing. On this machine, using `joblib` and often using nearly all of those cores:\n",
    "1. Step 1 takes about a minute.\n",
    "2. Step 2 takes about 3 hours.\n",
    "3. Step 3 takes about 15-20 minutes.\n",
    "4. Step 4 takes about 4.5 hours.\n",
    "\n",
    "To run all of these scripts comfortably and without modification, you will need about 128GB of memory and ≈1 TB of hard disk space. Little or no attempt has been made to compress or avoid unnecessary file outputs except insofar as it creates representations that lead to usefully smaller memory loads or facilitate matrix-based compuations. Some unused or older versions of scripts in this repository were developed on machines with 180-190GB of RAM and use somewhere between 140 and 160GB of RAM on the largest inputs (invariably something related to the CMU dictionary - the largest lexicon by far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:36.611938Z",
     "start_time": "2019-09-12T23:13:36.469128Z"
    }
   },
   "outputs": [],
   "source": [
    "import watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:36.629058Z",
     "start_time": "2019-09-12T23:13:36.613706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2019-09-12T16:13:36-07:00\n",
      "\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-62-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 32\n",
      "interpreter: 64bit\n",
      "host name  : wittgenstein\n"
     ]
    }
   ],
   "source": [
    "%watermark -ihmuv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:36.770731Z",
     "start_time": "2019-09-12T23:13:36.631331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\r\n",
      "CPU op-mode(s):      32-bit, 64-bit\r\n",
      "Byte Order:          Little Endian\r\n",
      "CPU(s):              32\r\n",
      "On-line CPU(s) list: 0-31\r\n",
      "Thread(s) per core:  2\r\n",
      "Core(s) per socket:  16\r\n",
      "Socket(s):           1\r\n",
      "NUMA node(s):        1\r\n",
      "Vendor ID:           AuthenticAMD\r\n",
      "CPU family:          23\r\n",
      "Model:               8\r\n",
      "Model name:          AMD Ryzen Threadripper 2950X 16-Core Processor\r\n",
      "Stepping:            2\r\n",
      "CPU MHz:             1895.432\r\n",
      "CPU max MHz:         3500.0000\r\n",
      "CPU min MHz:         2200.0000\r\n",
      "BogoMIPS:            6986.04\r\n",
      "Virtualization:      AMD-V\r\n",
      "L1d cache:           32K\r\n",
      "L1i cache:           64K\r\n",
      "L2 cache:            512K\r\n",
      "L3 cache:            8192K\r\n",
      "NUMA node0 CPU(s):   0-31\r\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:36.886179Z",
     "start_time": "2019-09-12T23:13:36.772892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           125G        1.0G        6.8G        105M        118G        123G\r\n",
      "Swap:          2.0G        104M        1.9G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:37.003210Z",
     "start_time": "2019-09-12T23:13:36.887965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version:        430.40\r\n",
      "srcversion:     B9960A7E9AD113F15C3277E\r\n"
     ]
    }
   ],
   "source": [
    "!modinfo nvidia | grep version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "0. **Extensibility**: Step 3b (filtering transcribed lexicons against language model vocabularies) uses output filenames that won't scale if any transcription lexicon is used with more than one language model. (This also affects 3c.)\n",
    "1. **Extensibility/Modularity/Maintainability**: Every notebook that depends on the behavior or output of another notebook being a certain way should have that assumption flagged at the top of the notebook. \n",
    "   - **Most common example**: one of notebook $B$'s arguments is a *directory path* $d$, where it expects to find a specific set of files with certain fixed filenames (or with filenames that are derived in some way from one of $B$'s arguments - often $d$); this assumption is predicated on the behavior of some notebook $A$ earlier in the pipeline producing exactly some set of files in a common directory all with certain filenames.\n",
    "2. **Portability/Reproducibility**: For every file that this repository depends on that *isn't* tracked by the repository (e.g. processed versions of swbd2003, Buckeye, etc.), there should be *something* (e.g. a cell in this script) that lets the user identify where those files are located, and then said something copies them to wherever this repository is expecting to find them.\n",
    "3. **Portability/Reproducibility**: Check for and remove absolute paths in this and other files.\n",
    "4. **Portability/Reproducibility**: For platform independence (read: supporting windows users, I guess?) use `path.join` instead of manually choosing directory slashes...\n",
    "5. **Documentation**: \n",
    "   1. Make sure motivation section is up to date.\n",
    "   2. Math-y documentation in channel distribution and posterior distribution notebooks probably needs to be updated / at least have notation overhauled.\n",
    "   3. Go through notebooks used here and make sure `Overview` cells are accurate.\n",
    "   4. Go through notebooks used here and make sure Papermill-related `Usage` cells are filled in / accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:44.953639Z",
     "start_time": "2019-09-12T23:13:37.005001Z"
    }
   },
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:44.958169Z",
     "start_time": "2019-09-12T23:13:44.955312Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:44.964459Z",
     "start_time": "2019-09-12T23:13:44.961085Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:44.970311Z",
     "start_time": "2019-09-12T23:13:44.965799Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:44.976327Z",
     "start_time": "2019-09-12T23:13:44.971474Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.715709Z",
     "start_time": "2019-09-12T23:13:44.977777Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from boilerplate import ensure_dir_exists, union, endNote, startNote, stampedNote, stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.721476Z",
     "start_time": "2019-09-12T23:13:45.717466Z"
    }
   },
   "outputs": [],
   "source": [
    "def progress_report(nb_fp, arg_dict):\n",
    "    startNote()\n",
    "    output_dir = path.dirname(nb_fp)\n",
    "    nb_fn = path.basename(nb_fp)\n",
    "    print(f\"Running notebook:\\n\\t{nb_fn}\")\n",
    "    print(f\"Output directory:\\n\\t{output_dir}\")\n",
    "    print(\"Arguments:\")\n",
    "    print(json.dumps(arg_dict, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.728569Z",
     "start_time": "2019-09-12T23:13:45.722904Z"
    }
   },
   "outputs": [],
   "source": [
    "from funcy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.742979Z",
     "start_time": "2019-09-12T23:13:45.729985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd()\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.750761Z",
     "start_time": "2019-09-12T23:13:45.744779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MeasBasAnalysis.ipynb',\n",
       " 'probdist.py',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to LTR_NXT_swbd_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres (vec-dev).ipynb',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + dask + tiledb).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'LTR_NXT_swbd_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       " 'buckeye_vocab_excluded_by_xlnet.txt',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse tensor calculations + memory issues).ipynb',\n",
       " 'boilerplate.py',\n",
       " 'buckeye_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'sq.py',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'XLnet alignment losses.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Sidious.CPU-L).ipynb',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'swbd2003_contexts_filtered_against_fisher_vocabulary_main.txt',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Quine.CPU-L).ipynb',\n",
       " 'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       " 'C_NXT_swbd',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'memguard.py',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (sparse + joblib + tiledb + clean).ipynb',\n",
       " 'Calculate segmental wordform and prefix channel matrices - OD.ipynb',\n",
       " 'Producing channel distributions - (dev-vec).ipynb',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       " 'LM_Fisher',\n",
       " 'Segmental Posterior Benchmarking.ipynb',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Matrix Metadata Lookup.ipynb',\n",
       " 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment definition.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Dask Examples.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       " 'Calculate orthographic posterior given segmental wordform + context (old).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Calculate segmental posterior given segmental wordform + context - dev.ipynb',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Calculate segmental posterior given segmental wordform + context.ipynb',\n",
       " 'Channel Distribution Analysis.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'CM_AmE_unaligned_pseudocount0.001',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01',\n",
       " 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'Calculate word-prefix relation, Hamming distances, and k-cousin relation.ipynb',\n",
       " 'slice spot checking.ipynb',\n",
       " 'HMMs scratch.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'Calculate observation distribution given channel models.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Sidious).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'Calculate segmental posterior given segmental wordform + context (dev-Quine).ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       " 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres (old).ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Wittgenstein).ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'C_Buckeye',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context-BAK.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context - (Benchmark.Kotoba.GPU).ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " '.pW_V.npz',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'Calculate segmental posterior given segmental wordform + context (Quine).ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_contents = listdir()\n",
    "repo_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permitted steps / control flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate quick (re)running of just the steps you want to (re)do and to avoid tediously (re)running steps you do not want to calculate, this list here controls what calculation steps will actually be done. (Combine with 'Run All' to facilitate quick calculation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.755224Z",
     "start_time": "2019-09-12T23:13:45.752412Z"
    }
   },
   "outputs": [],
   "source": [
    "permittedSteps = [\n",
    "#     '0',\n",
    "#     '1a',\n",
    "#     '1b',\n",
    "#     '2ai',\n",
    "#     '2aii',\n",
    "#     '2b',\n",
    "#     '3a',\n",
    "#     '3b',\n",
    "#     '3c',\n",
    "#     '3d',\n",
    "#     '3e',\n",
    "#     '4a',\n",
    "#     '4b',\n",
    "#     '4c',\n",
    "#     '4d',\n",
    "#     '4e',\n",
    "    '5a',\n",
    "#     '5b',\n",
    "#     '5c'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the flag below is `False`, then (in selected places where I've added support in this notebook) the driver notebook will check for and not overwrite existing output notebooks (evidence of a previous and assumed-to-be-successful run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.760781Z",
     "start_time": "2019-09-12T23:13:45.756638Z"
    }
   },
   "outputs": [],
   "source": [
    "overwrite = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the flag below is `True`, then this machine will skip (*only* in Step 4e) processing 'trim' inputs. (See parameters in the Step 3e notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.767140Z",
     "start_time": "2019-09-12T23:13:45.762231Z"
    }
   },
   "outputs": [],
   "source": [
    "skip_trim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import/check for foundational files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    " - I assume all relevant transcriptions have been converted to Unicode IPA characters and that segment sequences have `.` as a separator. For each data source used here, the IPA alignment step is documented in a GitHub repository elsewhere. (While I don't *think* any script here depends on use of Unicode IPA symbols, I haven't - and won't - test that idea, and it is *absolutely required* that contiguous sequences of segments be separated by `.` in data files.)\n",
    " - Where language models and n-gram contexts (drawn from speech corpora) are referenced, each of these is assumed to have come from as is from other GitHub repositories/from executing the notebooks in those repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing existing data and creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.773141Z",
     "start_time": "2019-09-12T23:13:45.768456Z"
    }
   },
   "outputs": [],
   "source": [
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.778715Z",
     "start_time": "2019-09-12T23:13:45.774540Z"
    }
   },
   "outputs": [],
   "source": [
    "# AmE_gating_data_dir = 'GD_AmE'\n",
    "# AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "# AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "\n",
    "# ensure_dir_exists(AmE_gating_data_dir)\n",
    "\n",
    "# if not path.exists(AmE_GD_fp):\n",
    "#     'Gating data file {0} does not exist. Attempting to copy from repository in sister folder.'.format(AmE_GD_fp)\n",
    "    \n",
    "#     wmc2014ipa_repo_dir = '../wmc2014-ipa'\n",
    "\n",
    "#     assert path.exists(wmc2014ipa_repo_dir), \"Cannot move gating data file from repository for you because it cannot be found.\\n For automatic placement, you must download the wmc2014-ipa repository (from https://github.com/emeinhardt/wmc2014-ipa) and place it in the same folder as 'wr'\"\n",
    "\n",
    "#     copy(path.join(wmc2014ipa_repo_dir, AmE_gating_data_fn),\n",
    "#          path.join(repo_dir, AmE_gating_data_dir, AmE_gating_data_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.785007Z",
     "start_time": "2019-09-12T23:13:45.780100Z"
    }
   },
   "outputs": [],
   "source": [
    "# newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "# cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "# cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "# buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "# nxt_swbd_ltr_folder = 'LTR_NXT_swbd_destressed'\n",
    "\n",
    "# LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder, nxt_swbd_ltr_folder)\n",
    "# LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder, nxt_swbd_ltr_folder)\n",
    "\n",
    "# LTR_folders_to_repo_dir = {\n",
    "#     newdic_destressed_ltr_folder:'../newdic-nettalk-ipa',\n",
    "#     cmu_destressed_ltr_folder:'../cmu-ipa',\n",
    "#     cmu_stressed_ltr_folder:'../cmu-ipa',\n",
    "#     buckeye_ltr_folder:'../buckeye-lm',\n",
    "#     nxt_swbd_ltr_folder:'../switchboard-lm'\n",
    "# }\n",
    "\n",
    "# for LTR_dir in LTR_folders_to_process:\n",
    "#     ensure_dir_exists(LTR_dir)\n",
    "#     LTR_fp = path.join(LTR_dir, LTR_dir + '.tsv')\n",
    "    \n",
    "#     if not path.exists(LTR_fp):\n",
    "#         'Transcribed lexicon relation {0} does not exist. Attempting to copy from repository in sister folder.'.format(LTR_fp)\n",
    "    \n",
    "#     LTR_repo_dir = LTR_folders_to_repo_dir[LTR_dir]\n",
    "#     assert path.exists(LTR_repo_dir), \"Cannot move transcribed lexicon relation .tsv from repository for you because it cannot be found.\\n For automatic placement, you must download the {0} repository (from https://github.com/emeinhardt/{0}) and place it in the same folder as 'wr'\".format(LTR_repo_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four steps below verify that foundational files assumed to be present by downstream notebooks are, in fact, present in the repository directory. If for some reason those files are not present, the processing pipeline will be aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0a: Check for gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.806748Z",
     "start_time": "2019-09-12T23:13:45.786355Z"
    }
   },
   "outputs": [],
   "source": [
    "AmE_gating_data_dir = 'GD_AmE'\n",
    "AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "\n",
    "ensure_dir_exists(AmE_gating_data_dir)\n",
    "\n",
    "if not path.exists(AmE_GD_fp):\n",
    "    'Gating data file {0} does not exist. Attempting to copy from repository in sister folder.'.format(AmE_GD_fp)\n",
    "    \n",
    "    wmc2014ipa_repo_dir = '../wmc2014-ipa'\n",
    "\n",
    "    assert path.exists(wmc2014ipa_repo_dir), \"Cannot move gating data file from repository for you because it cannot be found.\\n For automatic placement, you must download the wmc2014-ipa repository (from https://github.com/emeinhardt/wmc2014-ipa) and place it in the same folder as 'wr'\"\n",
    "\n",
    "    copy(path.join(wmc2014ipa_repo_dir, AmE_gating_data_fn),\n",
    "         path.join(repo_dir, AmE_gating_data_dir, AmE_gating_data_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.815014Z",
     "start_time": "2019-09-12T23:13:45.811633Z"
    }
   },
   "outputs": [],
   "source": [
    "# AmE_gating_data_dir = 'GD_AmE'\n",
    "# AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "# AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "assert path.exists(AmE_gating_data_dir), 'Gating data directory {0} does not exist.'.format(AmE_gating_data_dir)\n",
    "assert path.exists(AmE_GD_fp), 'Gating data file {0} does not exist.'.format(AmE_GD_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed gating data used here come from \n",
    " - https://github.com/emeinhardt/wmc2014-ipa\n",
    " \n",
    "See those repositories for information on how they were produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0b: Check for transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each transcribed lexicon `LEXNAME` should be in a folder (e.g. `LTR_LEXNAME`) containing a file `LTR_LEXNAME.tsv`. For documentation purposes, the source file and a notebook documenting the production of the `.tsv` file should, if practicable be included in the folder as well.\n",
    "   - A transcribed lexicon `LTR_....tsv` file should have two columns: `Orthographic_Wordform` and `Transcription`.\n",
    "   - NB: The `LTR_` prefix on transcribed lexicon data files and containing folders is simply a convention for organization and readability, but is not required or expected by any file or script in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions in the code below will only succeed if step 1 is complete for all transcribed lexicons listed for checking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.839641Z",
     "start_time": "2019-09-12T23:13:45.816743Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 314.90it/s]\n"
     ]
    }
   ],
   "source": [
    "newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "nxt_swbd_ltr_folder = 'LTR_NXT_swbd_destressed'\n",
    "\n",
    "LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder, nxt_swbd_ltr_folder)\n",
    "LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder, nxt_swbd_ltr_folder)\n",
    "\n",
    "# LTR_folders_to_repo_dir = {\n",
    "#     newdic_destressed_ltr_folder:'../newdic-nettalk-ipa',\n",
    "#     cmu_destressed_ltr_folder:'../cmu-ipa',\n",
    "#     cmu_stressed_ltr_folder:'../cmu-ipa',\n",
    "#     buckeye_ltr_folder:'../buckeye-lm',\n",
    "#     nxt_swbd_ltr_folder:'../switchboard-lm'\n",
    "# }\n",
    "\n",
    "# for LTR_dir in LTR_folders_to_process:\n",
    "#     ensure_dir_exists(LTR_dir)\n",
    "#     LTR_fp = path.join(LTR_dir, LTR_dir + '.tsv')\n",
    "    \n",
    "#     if not path.exists(LTR_fp):\n",
    "#         'Transcribed lexicon relation {0} does not exist.'.format(LTR_fp)\n",
    "    \n",
    "#         if LTR_dir in {buckeye_ltr_folder, nxt_swbd_ltr_folder}:\n",
    "#             print('Attempting to copy transcribed lexicon relation .tsv from repository folder...')\n",
    "            \n",
    "#             LTR_repo_dir = LTR_folders_to_repo_dir[LTR_dir]\n",
    "            \n",
    "#             assert path.exists(LTR_repo_dir), \"Cannot move transcribed lexicon relation .tsv from repository for you because it cannot be found.\\n For automatic placement, you must download the {0} repository (from https://github.com/emeinhardt/{0}) and place it in the same folder as 'wr'\".format(LTR_repo_dir)\n",
    "            \n",
    "#             copy(path.join(LTR_repo_dir, FIXME), \n",
    "#                  path.join(repo_dir, LTR_dir, LTR_dir + '.tsv'))\n",
    "    \n",
    "    \n",
    "\n",
    "for dirname in tqdm(LTR_folders_to_process):\n",
    "    assert path.exists(dirname), 'Transcribed lexicon directory {0} not found in repo directory'.format(dirname)\n",
    "    fname = path.join(dirname, dirname + '.tsv')\n",
    "    assert path.exists(fname), 'Transcribed lexicon {0} not found in repo directory'.format(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are transcribed lexicon relations made?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each was created by processing a transcription source (a lexical database, an annotated corpus, etc.). The processing step is described in other repositories:\n",
    " - https://github.com/emeinhardt/newdic-nettalk-ipa\n",
    " - https://github.com/emeinhardt/cmu-ipa\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    "\n",
    "Given the processed outputs of these repositories, the `Making a Transcribed Lexicon Relation - <LEXNAME>.ipynb` notebook in each `LTR_LEXNAME` folder describes how the homogeneous `.tsv` files downstream steps are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.848923Z",
     "start_time": "2019-09-12T23:13:45.841358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - newdic_destressed.ipynb',\n",
       " 'newdic_IPA.tsv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_CMU_destressed.pW_V.npz_metadata.json',\n",
       " 'Making a Transcribed Lexicon Relation - CMU_destressed.ipynb',\n",
       " 'LTR_CMU_destressed.pW_V.json',\n",
       " 'LTR_CMU_destressed.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_CMU_destressed.pW_V.npz',\n",
       " 'cmudict-0.7b_IPA_destressed.tsv',\n",
       " 'LTR_CMU_destressed_Orthographic_Wordforms.txt',\n",
       " 'LTR_CMU_destressed_Transcriptions.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_Buckeye.tsv',\n",
       " 'buckeye_orthography_phonemic_transcription_relation.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye-old.ipynb',\n",
       " 'buckeye_words_analysis_relation.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye.ipynb']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['nxt_swbd_orthography_transcription_relation.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - NXT_swbd.ipynb',\n",
       " 'LTR_NXT_swbd_destressed.tsv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname in LTR_folders_to_process:\n",
    "    listdir(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0c: Check for n-gram contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.852419Z",
     "start_time": "2019-09-12T23:13:45.850227Z"
    }
   },
   "outputs": [],
   "source": [
    "from funcy import str_join, walk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.888940Z",
     "start_time": "2019-09-12T23:13:45.853782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Buckeye': ('buckeye_contexts_following_1_filtered.txt',\n",
       "  'buckeye_contexts_following_2_filtered.txt',\n",
       "  'buckeye_contexts_following_3_filtered.txt',\n",
       "  'buckeye_contexts_following_4_filtered.txt',\n",
       "  'buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'buckeye_contexts_bidirectional_filtered.json'),\n",
       " 'NXT_swbd': ('nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_bidirectional_filtered.json')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_fns = {\n",
    "    #swbd2003 wordforms need to be POS tagged and \n",
    "    # contexts need to be organized by size + correctly constructed and\n",
    "    # exclusion criteria need to be applied\n",
    "#     'swbd2003':FIXME,\n",
    "    'Buckeye':{'preceding':{l:str_join('_', ['buckeye', 'contexts', 'preceding', str(l), 'filtered']) + '.txt' \n",
    "                            for l in (1,2,3,4)},\n",
    "               'following':{l:str_join('_', ['buckeye', 'contexts', 'following', str(l), 'filtered']) + '.txt' \n",
    "                            for l in (1,2,3,4)},\n",
    "               'bidirectional':'buckeye_contexts_bidirectional_filtered.json'},\n",
    "    'NXT_swbd':{'preceding':{l:str_join('_', ['nxt_swbd', 'contexts', 'preceding', str(l), 'filtered']) + '.txt' \n",
    "                            for l in (1,2,3,4)},\n",
    "                'following':{l:str_join('_', ['nxt_swbd', 'contexts', 'following', str(l), 'filtered']) + '.txt' \n",
    "                            for l in (1,2,3,4)},\n",
    "                'bidirectional':'nxt_swbd_contexts_bidirectional_filtered.json'}\n",
    "}\n",
    "\n",
    "context_dirs = tuple([str_join('_', ('C', corpus_name)) \n",
    "                      for corpus_name in context_fns])\n",
    "\n",
    "for context_dir in context_dirs:\n",
    "    ensure_dir_exists(context_dir)\n",
    "\n",
    "context_dir_to_repo_dir = {\n",
    "#     swbd2003_ltr_folder:'../switchboard-lm',\n",
    "    'Buckeye':'../buckeye-lm',\n",
    "    'NXT_swbd':'../switchboard-lm'\n",
    "}\n",
    "\n",
    "corpus_to_contexts = {\n",
    "    'Buckeye':list(sorted({context_fns['Buckeye'][direction][l] \n",
    "                           for direction in ('preceding', 'following') for l in (1,2,3,4)})) + ['buckeye_contexts_bidirectional_filtered.json'],\n",
    "    'NXT_swbd':list(sorted({context_fns['NXT_swbd'][direction][l] \n",
    "                           for direction in ('preceding', 'following') for l in (1,2,3,4)})) + ['nxt_swbd_contexts_bidirectional_filtered.json'],\n",
    "}\n",
    "corpus_to_contexts = walk_values(tuple, corpus_to_contexts)\n",
    "corpus_to_contexts\n",
    "\n",
    "for context_dir in context_dirs:\n",
    "    context_name = context_dir[2:]\n",
    "    for direction in context_fns[context_name]:\n",
    "        if direction == 'bidirectional':\n",
    "            context_fn = context_fns[context_name][direction]\n",
    "            \n",
    "            if not path.exists(path.join(context_dir, context_fn)):\n",
    "                print('{0} not found in {1}.'.format(context_fn, context_dir))\n",
    "                repo_dir = context_dir_to_repo_dir[context_name]\n",
    "                print('Attempting to copy context file {0} from {1}.'.format(context_fn, repo_dir))\n",
    "                \n",
    "                copy(path.join(repo_dir, context_fn),\n",
    "                     path.join(context_dir, context_fn))\n",
    "        else:\n",
    "            for l in context_fns[context_name][direction]:\n",
    "                context_fn = context_fns[context_name][direction][l]\n",
    "                \n",
    "                if not path.exists(path.join(context_dir, context_fn)):\n",
    "                    print('{0} not found in {1}.'.format(context_fn, context_dir))\n",
    "                    repo_dir = context_dir_to_repo_dir[context_name]\n",
    "                    print('Attempting to copy context file {0} from {1}.'.format(context_fn, repo_dir))\n",
    "                \n",
    "                    copy(path.join(repo_dir, context_fn),\n",
    "                         path.join(context_dir, context_fn))\n",
    "                \n",
    "\n",
    "# buckeye_contexts = 'buckeye_contexts.txt'\n",
    "# swbd2003_contexts = 'swbd2003_contexts.txt'\n",
    "\n",
    "# contexts = (buckeye_contexts, swbd2003_contexts)\n",
    "\n",
    "# for c_fn in contexts:\n",
    "#     assert path.exists(c_fn), \"N-gram contexts file {0} does not exist.\".format(c_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram context files are taken from\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    " \n",
    "See those repositories for more information on how the contexts were extracted. (*NB*: Like the transcription lexicons, the context files are not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and (various) Switchboard corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0d: Check for language model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.904046Z",
     "start_time": "2019-09-12T23:13:45.890695Z"
    }
   },
   "outputs": [],
   "source": [
    "fisher_lm_dir = 'LM_Fisher'\n",
    "\n",
    "ensure_dir_exists(fisher_lm_dir)\n",
    "\n",
    "LM_fn_stem = 'fisher_utterances_main'\n",
    "LM_fns = {\n",
    "    '':{l:LM_fn_stem + '_' + str(l) + 'gram.mmap'\n",
    "        for l in (2,3,4,5)},\n",
    "    'rev':{l:LM_fn_stem + '_' + 'rev' + '_' + str(l) + 'gram.mmap'\n",
    "           for l in (2,3,4,5)}\n",
    "}\n",
    "\n",
    "fisher_lm_repo_dir = '../fisher-lm'\n",
    "\n",
    "for direction in LM_fns:\n",
    "    for l in LM_fns[direction]:\n",
    "        lm_fn = LM_fns[direction][l]\n",
    "        \n",
    "        if not path.exists(path.join(fisher_lm_dir, lm_fn)):\n",
    "            print('{0} not found in {1}'.format(lm_fn, fisher_lm_dir))\n",
    "            print('Attempting to copy from repository directory...')\n",
    "            copy(path.join(fisher_lm_repo_dir, lm_fn),\n",
    "                 path.join(fisher_lm_dir, lm_fn))\n",
    "            \n",
    "\n",
    "# fisher_lm_fn = 'fisher_utterances_main_4gram.mmap'\n",
    "# fisher_lm_fp = path.join(fisher_lm_dir, fisher_lm_fn)\n",
    "\n",
    "fisher_lm_vocab_fn = 'fisher_vocabulary_main.txt'\n",
    "if not path.exists(path.join(fisher_lm_dir, fisher_lm_vocab_fn)):\n",
    "    print('{0} not found in {1}'.format(fisher_lm_vocab_fn, fisher_lm_dir))\n",
    "    print('Attempting to copy from repository directory...')\n",
    "    copy(path.join(fisher_lm_repo_dir, fisher_lm_vocab_fn),\n",
    "         path.join(fisher_lm_dir, fisher_lm_vocab_fn))\n",
    "\n",
    "# fisher_lm_vocab_fp = path.join(fisher_lm_dir, fisher_lm_vocab_fn)\n",
    "\n",
    "# assert path.exists(fisher_lm_fp), 'Language model {0} not found'.format(fisher_lm_fp)\n",
    "# assert path.exists(fisher_lm_vocab_fp), 'Language model vocabulary {0} not found'.format(fisher_lm_vocab_fp)\n",
    "\n",
    "# fisher_lm_fps = {'lm':fisher_lm_fp, \n",
    "#                  'vocab':fisher_lm_vocab_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (memory mapped) n-gram language models are copied from the output of this repository:\n",
    " - https://github.com/emeinhardt/fisher-lm\n",
    " \n",
    "See that repository for more information. (*NB* Again, the language model file is not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment inventory alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Define inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment inventory of any given transcribed lexicon and the segment inventory of the gating data often do not line up. For the gating data to be usefully applied to a given lexicon of transcriptions, the strings in the (segmental) lexicon must contain only segments found in the gating data stimuli inventory.\n",
    "\n",
    "To ensure this happens, the notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` \n",
    " - takes as inputs \n",
    "     - a transcribed lexicon file path and a gating data file path\n",
    "     - a lexicon projection file path and a gating data projection file path\n",
    " - identifies the inventories of each and what symbols are relatively unique to the lexicon and the gating data\n",
    " - produces \n",
    "   - *a Jupyter notebook* for **you to open and finish by defining two projection functions** (i.e. Python dictionaries) to be applied to strings in the transcribed lexicon and to the gating data (one function for each). When you finish doing this (and set an export flag in the notebook to True and run the remainder of the notebook), this notebook will produce\n",
    "     - two *.json files storing these projections* according to the previously provided output file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.907950Z",
     "start_time": "2019-09-12T23:13:45.905966Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf LTR*_aligned_w_*\n",
    "# %rm -rf *\" alignment definition\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will only succeed if the American English gating data of Warner, McQueen, and Cutler (2014) is contained in the repo directory with a particular directory and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.915781Z",
     "start_time": "2019-09-12T23:13:45.909379Z"
    }
   },
   "outputs": [],
   "source": [
    "gating_data_folder = 'GD_AmE'\n",
    "gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "gating_data_fp = path.join(gating_data_folder, gating_data_fn)\n",
    "\n",
    "assert path.exists(gating_data_folder), 'AmE gating data folder {0} not found in repo directory'.format(gating_data_folder)\n",
    "assert path.exists(gating_data_fp), 'AmE gating data {0} not found in repo directory'.format(gating_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third cell below will create a notebook for alignment projection definitions for each of the transcribed lexicons from the previous step and the AmE gating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.923519Z",
     "start_time": "2019-09-12T23:13:45.917160Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME replace usage with path.splitext\n",
    "def removeExtension(fp):\n",
    "    dir_name = path.dirname(fp)\n",
    "    file_name = path.basename(fp)\n",
    "    ext = file_name.split('.')[-1]\n",
    "    rest = '.'.join( file_name.split('.')[:-1] )\n",
    "    return path.join(dir_name, rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.934002Z",
     "start_time": "2019-09-12T23:13:45.925003Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment_arg_bundles = []\n",
    "for LTR_dirname in LTR_folders_to_process:\n",
    "    LTR_fn = LTR_dirname + '.tsv'\n",
    "    LTR_fp = path.join(LTR_dirname, LTR_fn)\n",
    "    \n",
    "    nb_output_name = 'GD_AmE-diphones - ' + LTR_dirname + ' alignment definition' + '.ipynb'\n",
    "    my_g = gating_data_fp\n",
    "    my_l = LTR_fp\n",
    "    my_s = 'destressed'\n",
    "    \n",
    "    gd_alignment_dn = 'GD_AmE_' + my_s + '_' + 'aligned_w_' + LTR_dirname\n",
    "    gd_alignment_fn = 'alignment_of_' + removeExtension(gating_data_fn) + '_w_' + LTR_dirname + '.json'\n",
    "    gd_alignment_fp = path.join(gd_alignment_dn, gd_alignment_fn)\n",
    "    if not path.exists(gd_alignment_dn):\n",
    "        makedirs(gd_alignment_dn)\n",
    "    my_gp = gd_alignment_fp\n",
    "    \n",
    "    ltr_alignment_dn = LTR_dirname + '_aligned_w_' + 'GD_AmE_' + my_s\n",
    "    ltr_alignment_fn = 'alignment_of_' + LTR_dirname + '_w_' + removeExtension(gating_data_fn) + '.json'\n",
    "    ltr_alignment_fp = path.join(ltr_alignment_dn, ltr_alignment_fn)\n",
    "    if not path.exists(ltr_alignment_dn):\n",
    "        makedirs(ltr_alignment_dn)\n",
    "    my_lp = ltr_alignment_fp\n",
    "    \n",
    "    \n",
    "    my_arg_bundle = OrderedDict({\n",
    "        'LTR_dirname':LTR_dirname,\n",
    "        'LTR_fn':LTR_fn,\n",
    "        'LTR_fp':LTR_fp,\n",
    "        'gd_alignment_dn':gd_alignment_dn,\n",
    "        'gd_alignment_fn':gd_alignment_fn,\n",
    "        'gd_alignment_fp':gd_alignment_fp,\n",
    "        'ltr_alignment_dn':ltr_alignment_dn,\n",
    "        'ltr_alignment_fn':ltr_alignment_fn,\n",
    "        'ltr_alignment_fp':ltr_alignment_fp,\n",
    "        'align_def_nb_output_name':nb_output_name,\n",
    "        'my_g':my_g,\n",
    "        'my_l':my_l,\n",
    "        'my_s':my_s,\n",
    "        'my_gp':my_gp,\n",
    "        'my_lp':my_lp,\n",
    "    })\n",
    "    alignment_arg_bundles.append(my_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.941425Z",
     "start_time": "2019-09-12T23:13:45.935703Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '1a' in permittedSteps:\n",
    "    # takes ~30s on wittgenstein\n",
    "    for arg_bundle in tqdm(alignment_arg_bundles):\n",
    "        nb = pm.execute_notebook(\n",
    "            'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "            arg_bundle['align_def_nb_output_name'],\n",
    "            parameters=dict(g = arg_bundle['my_g'], \n",
    "                            l = arg_bundle['my_l'], \n",
    "                            s = arg_bundle['my_s'], \n",
    "                            gp = arg_bundle['my_gp'], \n",
    "                            lp = arg_bundle['my_lp'])\n",
    "        )\n",
    "    #     pm.execute_notebook(\n",
    "    #        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "    #        nb_output_name,\n",
    "    #        parameters=dict(g = my_g, l = my_l, s = my_s, gp = my_gp, lp = my_lp)\n",
    "    #     )\n",
    "        print(\"Finished creating alignment definition notebook '{0}'.\\nOpen and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\\n\".format(arg_bundle['align_def_nb_output_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Apply inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:45.947529Z",
     "start_time": "2019-09-12T23:13:45.942847Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf *\" alignment application \"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will succeed if you have run each of the previously produced notebooks correctly and produced a projection mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.000390Z",
     "start_time": "2019-09-12T23:13:45.948543Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    assert path.exists(args['gd_alignment_fp']), 'Gating data alignment projection mapping not found:\\n\\t{0}'.format(args['gd_alignment_fp'])\n",
    "    assert path.exists(args['ltr_alignment_fp']), 'Transcribed lexicon data alignment projection mapping not found:\\n\\t{0}'.format(args['ltr_alignment_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are inventory alignment projections actually applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Align transcriptions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below applies each pair of alignment projections to each matched pair of gating data and transcribed lexicon choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.008528Z",
     "start_time": "2019-09-12T23:13:46.002115Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    LTR_fn = args['LTR_fn']\n",
    "    \n",
    "#     my_pg = args['my_gp']\n",
    "#     my_g = args['my_g']\n",
    "    my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    args['my_og'] = my_og\n",
    "    \n",
    "#     my_pl = args['my_lp']\n",
    "#     my_l = args['my_l']\n",
    "    my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    args['my_ol'] = my_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.021342Z",
     "start_time": "2019-09-12T23:13:46.009951Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '1b' in permittedSteps:\n",
    "    # takes ~45s on wittgenstein\n",
    "    for arg_bundle in alignment_arg_bundles:\n",
    "        args = arg_bundle\n",
    "    #     LTR_fn = args['LTR_fn']\n",
    "        startNote()\n",
    "        my_pg = args['my_gp']\n",
    "        my_g = args['my_g']\n",
    "    #     my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    #     my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    #     args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    #     args['my_og'] = my_og\n",
    "        my_og = args['my_og']\n",
    "        print(\"Creating notebook '{0}' w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}\".format(args['align_apply_gd_nb_output_name'], my_pg, my_g, my_og))\n",
    "        nb = pm.execute_notebook(\n",
    "            'Align transcriptions.ipynb',\n",
    "            args['align_apply_gd_nb_output_name'],\n",
    "            parameters=dict(p = my_pg,\n",
    "                            g = my_g,\n",
    "                            o = my_og)\n",
    "        )\n",
    "        print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tg = {1}\\nResult saved to\\n\\t{2}'.format(my_pg, my_g, my_og))\n",
    "        print(' ')\n",
    "\n",
    "        my_pl = args['my_lp']\n",
    "        my_l = args['my_l']\n",
    "    #     my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    #     my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    #     args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    #     args['my_ol'] = my_ol\n",
    "        my_ol = args['my_ol']\n",
    "        print('Creating notebook {0} w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}'.format(args['align_apply_ltr_nb_output_name'], my_pg, my_l, my_ol))\n",
    "        nb = pm.execute_notebook(\n",
    "            'Align transcriptions.ipynb',\n",
    "            args['align_apply_ltr_nb_output_name'],\n",
    "            parameters=dict(p = my_pl,\n",
    "                            l = my_l,\n",
    "                            o = my_ol)\n",
    "        )\n",
    "        print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tl = {1}\\nResult saved to\\n\\t{2}'.format(my_pl, my_l, my_ol))\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generating channel and (orthographic) lexicon distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Generating channel distributions and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.139005Z",
     "start_time": "2019-09-12T23:13:46.022595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment application to LTR_NXT_swbd_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_NXT_swbd_destressed alignment definition.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "%ls -d GD_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.145970Z",
     "start_time": "2019-09-12T23:13:46.141013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_folders = ('GD_AmE', ) + tuple(map(lambda ab: ab['gd_alignment_dn'], alignment_arg_bundles))\n",
    "gating_data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.153233Z",
     "start_time": "2019-09-12T23:13:46.147486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed/GD_AmE-diphones_aligned_w_LTR_NXT_swbd_destressed.tsv')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_fps = ('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',) + \\\n",
    "                  tuple(map(lambda ab: ab['my_og'], alignment_arg_bundles))\n",
    "gating_data_fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (for downstream convenience) we identify the $n$-phones (not) contained in and (not) constructible from each of the versions of the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** This is done by calling the notebook `Run n-phone analysis of gating data.ipynb`, passing it **the filepath to a gating data `.csv`/`.tsv` file** and **a path to an output directory** for the dozen or so files the notebook will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.159668Z",
     "start_time": "2019-09-12T23:13:46.154775Z"
    }
   },
   "outputs": [],
   "source": [
    "if '2ai' in permittedSteps:\n",
    "    # takes ~2m on wittgenstein\n",
    "    for gating_data_fp in gating_data_fps:\n",
    "        gd_dir = path.dirname(gating_data_fp)\n",
    "\n",
    "        progress_report(path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "                        dict(g = gating_data_fp,\n",
    "                            o = gd_dir))\n",
    "        nb = pm.execute_notebook(\n",
    "            'Run n-phone analysis of gating data.ipynb',\n",
    "    #         args['align_apply_ltr_nb_output_name'],\n",
    "            path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "            parameters=dict(g = gating_data_fp,\n",
    "                            o = gd_dir)\n",
    "        )\n",
    "        listdir(gd_dir)\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the notebook `Producing channel distributions.ipynb` will create `.json` files defining (among other things) a uniphone and triphone channel distribution. It requires the following arguments to specify information about what kind of channel model to build and where to put it:\n",
    " - **a filepath** to a gating data file\n",
    " - **a directory** containing metadata indicating possible/impossible $n$-phones\n",
    " - **a string argument** (\"stressed\" or \"destressed\") indicating whether the distribution will be over a segment inventory with or without stress information\n",
    " - **a real valued**  smoothing parameter (a pseudocount to add to every channel outcome)\n",
    " - **an output directory** to write the channel model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.166325Z",
     "start_time": "2019-09-12T23:13:46.161085Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudocounts = (0, 0.01, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.176462Z",
     "start_time": "2019-09-12T23:13:46.167792Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_arg_bundles = []\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    metadata_dir = path.dirname(gating_data_fp)\n",
    "    s = \"destressed\"\n",
    "    channel_model_dir_stem = 'CM' + metadata_dir[2:]\n",
    "    \n",
    "    for pc in pseudocounts:\n",
    "        channel_model_dir_suffix = '_pseudocount' + str(pc)\n",
    "        if metadata_dir == 'GD_AmE':\n",
    "            channel_model_dir = channel_model_dir_stem + '_' + s + '_unaligned' + channel_model_dir_suffix\n",
    "        else:\n",
    "            channel_model_dir = channel_model_dir_stem + channel_model_dir_suffix\n",
    "        nb_output_name = 'Producing channel distributions from ' + metadata_dir + ', pc={0}'.format(pc) + '.ipynb'\n",
    "        new_arg_bundle = {'gating_data_fp':gating_data_fp,\n",
    "                          'metadata_dir':metadata_dir,\n",
    "                          's':s,\n",
    "                          'c':pc,\n",
    "                          'cm_dir':channel_model_dir,\n",
    "                          'nb_output_name':nb_output_name,\n",
    "                          'nb_fp':path.join(channel_model_dir, nb_output_name)}\n",
    "        cm_arg_bundles.append(new_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.183718Z",
     "start_time": "2019-09-12T23:13:46.177830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gating_data_fp': 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'metadata_dir': 'GD_AmE',\n",
       " 's': 'destressed',\n",
       " 'c': 0,\n",
       " 'cm_dir': 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'nb_output_name': 'Producing channel distributions from GD_AmE, pc=0.ipynb',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0/Producing channel distributions from GD_AmE, pc=0.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arg_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.192260Z",
     "start_time": "2019-09-12T23:13:46.185129Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '2aii' in permittedSteps:\n",
    "    # takes ~110m on wittgenstein\n",
    "    for ab in cm_arg_bundles:\n",
    "        ensure_dir_exists(ab['cm_dir'])\n",
    "\n",
    "        progress_report(ab['nb_fp'],\n",
    "#                         path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "                        dict(g = ab['gating_data_fp'],\n",
    "                             m = ab['metadata_dir'],\n",
    "                             s = ab['s'],\n",
    "                             c = ab['c'],\n",
    "                             o = ab['cm_dir']))\n",
    "\n",
    "        if not overwrite and path.exists(ab['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(ab['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        nb = pm.execute_notebook(\n",
    "        'Producing channel distributions.ipynb',\n",
    "        ab['nb_fp'],\n",
    "#         path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "        parameters=dict(g = ab['gating_data_fp'],\n",
    "                        m = ab['metadata_dir'],\n",
    "                        s = ab['s'],\n",
    "                        c = ab['c'],\n",
    "                        o = ab['cm_dir'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are channel distributions created?**\n",
    "\n",
    "Take a look at `Producing channel distributions.ipynb`. Besides removing stress information, there are some mathematically non-trivial details that go into defining both uniphone and triphone channel distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a language model $m$ (defined by a `.arpa` file or `kenlm` memory-mapped analogue) \n",
    " - a choice of n-gram contexts $C$ (a `.txt` file with one context per line)\n",
    " - a vocabulary $V$ (a `.txt` file with one word per line)\n",
    " - a (partial) output filepath $o$ / output filepath prefix $o$\n",
    " \n",
    "`Producing contextual distributions.ipynb` will write a serialized/memory-mapped `numpy` array to $o$.hV_C that defines $-log_2( p(V|C) )$ - slightly transformed output from `kenlm`. It will also write $p(V|C)$ to $o$.pV_C, and copy both $V$ and $C$ to the base directory specified by $o$. (In both cases, each column is associated with the distribution $p(V|c)$ for some $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.201713Z",
     "start_time": "2019-09-12T23:13:46.193411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Buckeye': {'preceding': {1: 'buckeye_contexts_preceding_1_filtered.txt',\n",
       "   2: 'buckeye_contexts_preceding_2_filtered.txt',\n",
       "   3: 'buckeye_contexts_preceding_3_filtered.txt',\n",
       "   4: 'buckeye_contexts_preceding_4_filtered.txt'},\n",
       "  'following': {1: 'buckeye_contexts_following_1_filtered.txt',\n",
       "   2: 'buckeye_contexts_following_2_filtered.txt',\n",
       "   3: 'buckeye_contexts_following_3_filtered.txt',\n",
       "   4: 'buckeye_contexts_following_4_filtered.txt'},\n",
       "  'bidirectional': 'buckeye_contexts_bidirectional_filtered.json'},\n",
       " 'NXT_swbd': {'preceding': {1: 'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "   2: 'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "   3: 'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "   4: 'nxt_swbd_contexts_preceding_4_filtered.txt'},\n",
       "  'following': {1: 'nxt_swbd_contexts_following_1_filtered.txt',\n",
       "   2: 'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "   3: 'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "   4: 'nxt_swbd_contexts_following_4_filtered.txt'},\n",
       "  'bidirectional': 'nxt_swbd_contexts_bidirectional_filtered.json'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('C_Buckeye', 'C_NXT_swbd')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Buckeye': ('buckeye_contexts_following_1_filtered.txt',\n",
       "  'buckeye_contexts_following_2_filtered.txt',\n",
       "  'buckeye_contexts_following_3_filtered.txt',\n",
       "  'buckeye_contexts_following_4_filtered.txt',\n",
       "  'buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'buckeye_contexts_bidirectional_filtered.json'),\n",
       " 'NXT_swbd': ('nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_bidirectional_filtered.json')}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Buckeye': '../buckeye-lm', 'NXT_swbd': '../switchboard-lm'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_fns\n",
    "context_dirs\n",
    "corpus_to_contexts\n",
    "context_dir_to_repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.212629Z",
     "start_time": "2019-09-12T23:13:46.203307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Buckeye': {'preceding': {1: 'buckeye_contexts_preceding_1_filtered.txt',\n",
       "   2: 'buckeye_contexts_preceding_2_filtered.txt',\n",
       "   3: 'buckeye_contexts_preceding_3_filtered.txt',\n",
       "   4: 'buckeye_contexts_preceding_4_filtered.txt'},\n",
       "  'following': {1: 'buckeye_contexts_following_1_filtered.txt',\n",
       "   2: 'buckeye_contexts_following_2_filtered.txt',\n",
       "   3: 'buckeye_contexts_following_3_filtered.txt',\n",
       "   4: 'buckeye_contexts_following_4_filtered.txt'},\n",
       "  'bidirectional': 'buckeye_contexts_bidirectional_filtered.json'},\n",
       " 'NXT_swbd': {'preceding': {1: 'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "   2: 'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "   3: 'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "   4: 'nxt_swbd_contexts_preceding_4_filtered.txt'},\n",
       "  'following': {1: 'nxt_swbd_contexts_following_1_filtered.txt',\n",
       "   2: 'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "   3: 'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "   4: 'nxt_swbd_contexts_following_4_filtered.txt'},\n",
       "  'bidirectional': 'nxt_swbd_contexts_bidirectional_filtered.json'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('C_Buckeye', 'C_NXT_swbd')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Buckeye': ('buckeye_contexts_following_1_filtered.txt',\n",
       "  'buckeye_contexts_following_2_filtered.txt',\n",
       "  'buckeye_contexts_following_3_filtered.txt',\n",
       "  'buckeye_contexts_following_4_filtered.txt',\n",
       "  'buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'buckeye_contexts_bidirectional_filtered.json'),\n",
       " 'NXT_swbd': ('nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nxt_swbd_contexts_bidirectional_filtered.json')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Buckeye': '../buckeye-lm', 'NXT_swbd': '../switchboard-lm'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'': {2: 'fisher_utterances_main_2gram.mmap',\n",
       "  3: 'fisher_utterances_main_3gram.mmap',\n",
       "  4: 'fisher_utterances_main_4gram.mmap',\n",
       "  5: 'fisher_utterances_main_5gram.mmap'},\n",
       " 'rev': {2: 'fisher_utterances_main_rev_2gram.mmap',\n",
       "  3: 'fisher_utterances_main_rev_3gram.mmap',\n",
       "  4: 'fisher_utterances_main_rev_4gram.mmap',\n",
       "  5: 'fisher_utterances_main_rev_5gram.mmap'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_fns\n",
    "context_dirs\n",
    "corpus_to_contexts\n",
    "context_dir_to_repo_dir\n",
    "\n",
    "# fisher_lm_dir = 'LM_Fisher'\n",
    "\n",
    "LM_fns\n",
    "# LM_fn_stem = 'fisher_utterances_main'\n",
    "# LM_fns = {\n",
    "#     '':{l:LM_fn_stem + '_' + str(l) + 'gram.mmap'\n",
    "#         for l in (2,3,4,5)},\n",
    "#     'rev':{l:LM_fn_stem + '_' + 'rev' + '_' + str(l) + 'gram.mmap'\n",
    "#            for l in (2,3,4,5)}\n",
    "# }\n",
    "\n",
    "\n",
    "# fisher_lm_vocab_fn = 'fisher_vocabulary_main.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.227384Z",
     "start_time": "2019-09-12T23:13:46.214149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_1_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Producing Fisher vocab in Buckeye following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_2_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Producing Fisher vocab in Buckeye following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_3_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Producing Fisher vocab in Buckeye following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_4_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Producing Fisher vocab in Buckeye following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Producing Fisher vocab in Buckeye preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Producing Fisher vocab in Buckeye preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Producing Fisher vocab in Buckeye preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Producing Fisher vocab in Buckeye preceding contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Producing Fisher vocab in NXT swbd following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Producing Fisher vocab in NXT swbd following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Producing Fisher vocab in NXT swbd following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Producing Fisher vocab in NXT swbd following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Producing Fisher vocab in NXT swbd preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Producing Fisher vocab in NXT swbd preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Producing Fisher vocab in NXT swbd preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Producing Fisher vocab in NXT swbd preceding contexts 5gram model contextual distributions.ipynb'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocab_fn = fisher_lm_vocab_fn\n",
    "LD_bundles = []\n",
    "\n",
    "for corpus_name in corpus_to_contexts:\n",
    "    for context_fn in corpus_to_contexts[corpus_name]:\n",
    "        if 'bidirectional' in context_fn:\n",
    "            continue #not supported for now\n",
    "        elif 'preceding' in context_fn:\n",
    "            context_direction = 'preceding'\n",
    "            lm_direction = ''\n",
    "        else:\n",
    "            context_direction = 'following'\n",
    "            lm_direction = 'rev'\n",
    "        \n",
    "        context_size = context_fn[-14]\n",
    "        order = int(context_size) + 1\n",
    "        \n",
    "        LD_id = str_join('_', ['LD','Fisher','vocab','in',\n",
    "                               corpus_name, context_direction, 'contexts',\n",
    "                               str(order) + 'gram', 'model'])\n",
    "        \n",
    "        new_bundle = {\n",
    "            'corpus':corpus_name,\n",
    "            'context_fn':context_fn,\n",
    "            'context_fp':path.join('C_' + corpus_name, context_fn),\n",
    "            'lm_fn':LM_fns[lm_direction][order],\n",
    "            'lm_fp':path.join(fisher_lm_dir, LM_fns[lm_direction][order]),\n",
    "            'LD_dir':LD_id,\n",
    "            'o_fn_stem':LD_id,\n",
    "            'o':path.join(LD_id, LD_id),\n",
    "            'm':path.join(fisher_lm_dir, LM_fns[lm_direction][order]),\n",
    "            'v':path.join(fisher_lm_dir, my_vocab_fn),\n",
    "            'c':path.join('C_' + corpus_name, context_fn),\n",
    "            'nb_fp':path.join(LD_id, \n",
    "                              'Producing ' + LD_id.replace('_', ' ')[3:] + ' contextual distributions.ipynb')\n",
    "        }\n",
    "        \n",
    "        LD_bundles.append(new_bundle)\n",
    "LD_bundles\n",
    "LDs = LD_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.232896Z",
     "start_time": "2019-09-12T23:13:46.228547Z"
    }
   },
   "outputs": [],
   "source": [
    "# lmap(partial(omit, keys=('v', 'nb_fp', 'corpus', 'context_fp', 'lm_fp', 'o_fn_stem')),\n",
    "#      LDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.238423Z",
     "start_time": "2019-09-12T23:13:46.233930Z"
    }
   },
   "outputs": [],
   "source": [
    "# contexts\n",
    "# fisher_lm_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.244804Z",
     "start_time": "2019-09-12T23:13:46.239907Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDs = [{'LD_dir':'LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "#         'o_fn_stem':'LD_fisher_vocab_in_buckeye_contexts',\n",
    "#         'o':'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts',\n",
    "#         'm':fisher_lm_fps['lm'],\n",
    "#         'v':fisher_lm_fps['vocab'],\n",
    "#         'c':buckeye_contexts,\n",
    "#         'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts', \n",
    "#                           'Producing ' + 'LD_Fisher_vocab_in_Buckeye_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')},\n",
    "# #        {'LD_dir':'LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "# #         'o_fn_stem':'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "# #         'o':'LD_Fisher_vocab_in_swbd2003_contexts' + '/' + 'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "# #         'm':fisher_lm_fps['lm'],\n",
    "# #         'v':fisher_lm_fps['vocab'],\n",
    "# #         'c':swbd2003_contexts,\n",
    "# #         'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts', \n",
    "# #                           'Producing ' + 'LD_Fisher_vocab_in_swbd2003_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')},\n",
    "#        {'LD_dir':'LD_Fisher_vocab_in_NXT_swbd_contexts',\n",
    "#         'o_fn_stem':'LD_Fisher_vocab_in_nxt_swbd_contexts',\n",
    "#         'o':'LD_Fisher_vocab_in_NXT_swbd_contexts' + '/' + 'LD_Fisher_vocab_in_nxt_swbd_contexts',\n",
    "#         'm':FIXME,\n",
    "#         'v':,\n",
    "#         'c':,\n",
    "#         'nb_fp':path.join('LD_Fisher_vocab_in_NXT_swbd_contexts',\n",
    "#                           'Producing ' + 'LD_Fisher_vocab_in_NXT_swbd_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')}\n",
    "#       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.253860Z",
     "start_time": "2019-09-12T23:13:46.246293Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '2b' in permittedSteps:\n",
    "    # used to take ~80m on wittgenstein\n",
    "    \n",
    "    # timing data\n",
    "    # corpus = Buckeye\n",
    "    #     preceding contexts:\n",
    "    #         n = 2\n",
    "    #             pitts/1.3m\n",
    "    #         n = 3\n",
    "    #             montague/7.5m\n",
    "    #         n = 4\n",
    "    #             pitts/12m\n",
    "    #         n = 5\n",
    "    #             sidious/16m\n",
    "    #     following contexts:\n",
    "    #         n = 2r\n",
    "    #             x\n",
    "    #         n = 3r\n",
    "    #             x\n",
    "    #         n = 4r\n",
    "    #             x\n",
    "    #         n = 5r\n",
    "    #             wittgenstein/14m\n",
    "    # corpus = NXT_swbd\n",
    "    #     preceding contexts:\n",
    "    #         n = 2\n",
    "    #             sidious/2.66m\n",
    "    #         n = 3\n",
    "    #             sidious/13.5m\n",
    "    #         n = 4\n",
    "    #             x\n",
    "    #         n = 5\n",
    "    #             wittgenstein/34m\n",
    "    #     following contexts:\n",
    "    #         n = 2r\n",
    "    #             montague/3m\n",
    "    #         n = 3r\n",
    "    #             montague/16m\n",
    "    #         n = 4r\n",
    "    #             wittgenstein/28.5m\n",
    "    #         n = 5r\n",
    "    #             wittgenstein/32m\n",
    "    for ld in LDs:\n",
    "#         output_dir = path.dirname(ld['LD_dir'])\n",
    "        output_dir = ld['LD_dir']\n",
    "        ensure_dir_exists(output_dir)\n",
    "        \n",
    "        progress_report(ld['nb_fp'],\n",
    "                        dict(m = ld['m'],\n",
    "                        v = ld['v'],\n",
    "                        c = ld['c'],\n",
    "                        o = ld['o']))\n",
    "        \n",
    "        if not overwrite and path.exists(ld['nb_fp']):\n",
    "            print('{0} already exists. Skipping...'.format(ld['nb_fp']))\n",
    "            endNote()\n",
    "            continue\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Producing contextual distributions.ipynb',\n",
    "    #     'Producing ' + ld['LD_dir'].replace('_', ' ')[3:] + ' contextual distributions.ipynb',\n",
    "        ld['nb_fp'],\n",
    "        parameters=dict(m = ld['m'],\n",
    "                        v = ld['v'],\n",
    "                        c = ld['c'],\n",
    "                        o = ld['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.260806Z",
     "start_time": "2019-09-12T23:13:46.255242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating combinable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic problem:**\n",
    "\n",
    "1. **Channel model + transcribed lexicon relation**: Even after the gating data and a transcribed lexicon relation are defined over the same inventory, \n",
    " - the lexicon may contain triphones or diphones that are not in the stimuli triphones/diphones of a channel model.\n",
    " - channel distributions will contain triphones/diphones that cannot be found in the transcribed lexicon relation. (While the other steps here are strictly necessary, this is simply a practical step for making downstream computation faster.)\n",
    "2. **Language model + transcribed lexicon relation**: The orthographic vocabulary of a transcribed lexicon relation may contain wordforms not in an n-gram model's vocabulary. (We *don't* want to use the out-of-vocabulary estimate for those wordforms.)\n",
    "3.  **Contextual distributions + transcribed lexicon relation**: The contextual distributions from Step 3b above are defined over the *language model's* orthographic vocabulary, which will likely include wordforms that are not in the transcribed lexicon relation. We want to create modified forms of these distributions where we condition on the choice of an orthographic wordform that is in the transcribed lexicon relation.\n",
    "\n",
    "Once we have\n",
    " - a version $l'$ of the transcribed lexicon relation $l$ trimmed with respect to both the triphones of the channel model $c$ and the (orthographic) vocabulary of a language model $m$\n",
    " - a version $d'$ of the contextual distributions over $m$'s vocabulary (with respect to some set of n-gram contexts) $d$ trimmed to only define distributions over $l'$\n",
    " - a version $c'$ of the channel model $c$ trimmed with respect to a transcribed lexicon relation $l'$\n",
    " - a probability distribution over segmental wordforms given an orthographic wordform\n",
    " \n",
    "we will be able to combine everything together to calculate confusability of wordforms in corpus contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.266068Z",
     "start_time": "2019-09-12T23:13:46.261873Z"
    }
   },
   "outputs": [],
   "source": [
    "#gather relevant LTRs and their associated CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.272819Z",
     "start_time": "2019-09-12T23:13:46.267455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs = lmap(lambda ab: {'LTR_fp':ab['my_ol'],\n",
    "                                'GD_fp':ab['my_og']},\n",
    "                    alignment_arg_bundles)\n",
    "lmap(lambda d: d['LTR_fp'],\n",
    "     aligned_LTRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.280823Z",
     "start_time": "2019-09-12T23:13:46.274357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed/GD_AmE-diphones_aligned_w_LTR_NXT_swbd_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed/GD_AmE-diphones_aligned_w_LTR_NXT_swbd_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed/GD_AmE-diphones_aligned_w_LTR_NXT_swbd_destressed.tsv'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CM_dirs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "#                                 'GD_fp':cab['gating_data_fp']},\n",
    "#                    filter(lambda cab: cab['c'] != 0, cm_arg_bundles)))\n",
    "# CM_dirs\n",
    "# CM_dirs[5]\n",
    "# # listdir(CM_dirs[0][])\n",
    "\n",
    "aligned_CMs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "                                    'GD_fp':cab['gating_data_fp']},\n",
    "                       filter(lambda cab: cab['c'] != 0 and 'aligned' in cab['gating_data_fp'], \n",
    "                              cm_arg_bundles)))\n",
    "aligned_CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.286508Z",
     "start_time": "2019-09-12T23:13:46.282323Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aligned_CMs(ltr_bundle):\n",
    "    matches = [cm_bundle for cm_bundle in aligned_CMs if cm_bundle['GD_fp'] == ltr_bundle['GD_fp']]\n",
    "    return list(map(lambda d: d['CM_fp'],\n",
    "                    matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.293944Z",
     "start_time": "2019-09-12T23:13:46.287878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs[0]['LTR_fp']\n",
    "\n",
    "#NB: all of these will have the same set of stimuli triphones\n",
    "#    ...which is all we care about here\n",
    "get_aligned_CMs(aligned_LTRs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.299900Z",
     "start_time": "2019-09-12T23:13:46.295303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json']}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs_and_CM = [{'LTR_fp':ltr['LTR_fp'],\n",
    "                        'matching_CMs':get_aligned_CMs(ltr)}\n",
    "                       for ltr in aligned_LTRs]\n",
    "aligned_LTRs_and_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.308044Z",
     "start_time": "2019-09-12T23:13:46.301257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "nb = LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Filter LTR_NXT_swbd_destressed against channel model.ipynb\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    each['c'] = each['matching_CMs'][0]\n",
    "    each['l'] = each['LTR_fp']\n",
    "    o_dir = path.dirname(each['LTR_fp'])\n",
    "    o_fn = path.basename(each['LTR_fp']).split('w_')[0] + 'CM_filtered' + '.tsv'\n",
    "    each['o'] = path.join(o_dir, o_fn)\n",
    "    \n",
    "    nb_fn = 'Filter ' + o_fn.split('_aligned')[0] + ' against channel model.ipynb'\n",
    "    each['nb_fp'] = path.join(o_dir, nb_fn)\n",
    "    \n",
    "    print('c = {0}\\nl = {1}\\no = {2}\\nnb = {3}'.format(each['c'], each['l'], each['o'], each['nb_fp']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.315936Z",
     "start_time": "2019-09-12T23:13:46.309169Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '3a' in permittedSteps:\n",
    "    # takes about 30s on wittgenstein\n",
    "    for each in aligned_LTRs_and_CM:\n",
    "        output_dir = path.dirname(each['o'])\n",
    "        ensure_dir_exists(output_dir)\n",
    "\n",
    "        progress_report(each['nb_fp'],\n",
    "                        dict(l = each['l'],\n",
    "                             c = each['c'],\n",
    "                             o = each['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Filter transcription lexicon by channel model.ipynb',\n",
    "        each['nb_fp'],\n",
    "        parameters=dict(l = each['l'],\n",
    "                        c = each['c'],\n",
    "                        o = each['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3a**: `LTR_..._aligned_CM_filtered....tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.321809Z",
     "start_time": "2019-09-12T23:13:46.317358Z"
    }
   },
   "outputs": [],
   "source": [
    "# fisher_lm_fps\n",
    "# fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:46.331646Z",
     "start_time": "2019-09-12T23:13:46.323158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_fps = list(map(lambda pair: pair['LTR_fp'],\n",
    "                   aligned_LTRs))\n",
    "LTR_fps\n",
    "\n",
    "LTR_CM_filtered = list(map(lambda d: d['o'],\n",
    "                           aligned_LTRs_and_CM))\n",
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lexicon entries have unmodelable triphones? (We'll next check how many such lexicon entries aren't in the language model vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.141295Z",
     "start_time": "2019-09-12T23:13:46.333021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "133855 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "7999 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "15814 LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_w_GD_AmE-diphones.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.637405Z",
     "start_time": "2019-09-12T23:13:47.143346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "15318 LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for newdic:\n",
    " - $19529 - 17079 = 2450$ word types lost due to unmodelable triphones\n",
    "\n",
    "Word type loss for CMU_destressed:\n",
    " - $133855 - 127799 = 6056$ word types lost due to unmodelable triphones\n",
    "\n",
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones\n",
    " \n",
    "Word type loss for NXT_swbd_destressed:\n",
    " - $15834 - 15338 = 496$ word types lost due to unmodelable triphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.642958Z",
     "start_time": "2019-09-12T23:13:47.639096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.649585Z",
     "start_time": "2019-09-12T23:13:47.644341Z"
    }
   },
   "outputs": [],
   "source": [
    "fisher_lm_vocab_fp = path.join(fisher_lm_dir, fisher_lm_vocab_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.666973Z",
     "start_time": "2019-09-12T23:13:47.656091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Filter LTR_NXT_swbd_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LTR_LM_filter_bundles = []\n",
    "for each_LTR_fp in LTR_CM_filtered:\n",
    "    bundle = dict()\n",
    "    LTR_descr = path.basename(each_LTR_fp)[:-4]\n",
    "    LM_V_descr = path.basename(fisher_lm_vocab_fp)[:-4]\n",
    "    bundle['l'] = each_LTR_fp\n",
    "    bundle['v'] = fisher_lm_vocab_fp\n",
    "    bundle['o'] = each_LTR_fp[:-4] + '_LM_filtered' + '.tsv'\n",
    "    bundle['nb_fp'] = path.join(path.dirname(each_LTR_fp),\n",
    "                                f'Filter {LTR_descr} against {LM_V_descr}' + '.ipynb')\n",
    "    bundle\n",
    "    LTR_LM_filter_bundles.append(bundle)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.671497Z",
     "start_time": "2019-09-12T23:13:47.668114Z"
    }
   },
   "outputs": [],
   "source": [
    "if '3b' in permittedSteps:\n",
    "    # takes about ~1m on wittgenstein\n",
    "    for each in LTR_LM_filter_bundles:\n",
    "        output_dir = path.dirname(each['o'])\n",
    "        ensure_dir_exists(output_dir)\n",
    "\n",
    "        progress_report(each['nb_fp'],\n",
    "                        dict(l = each['l'],\n",
    "                             v = each['v'],\n",
    "                             o = each['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Filter transcription lexicon by language model vocabulary.ipynb',\n",
    "        each['nb_fp'],\n",
    "        parameters=dict(l = each['l'],\n",
    "                        v = each['v'],\n",
    "                        o = each['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit less than half** of the triphone channel model-modelable `newdic` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:47.901638Z",
     "start_time": "2019-09-12T23:13:47.672570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "9412 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$17079 - 9412 = 7667$ lost\n",
    "\n",
    "$7667 / 17079 ≈ 0.45$ proportionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for newdic:\n",
    " - $19529 - 17079 = 2450$ word types lost due to unmodelable triphones\n",
    " - $17079 - 9412 = 7667$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{2450+7667}{19529} ≈ 51.8\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About three quarters** of the triphone channel model-modelable `CMU_destressed` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.132539Z",
     "start_time": "2019-09-12T23:13:47.903490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "33125 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$127799 - 33125 = 94674$ lost\n",
    "\n",
    "$94674 / 127799 ≈ 0.74$ proportionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for CMU_destressed:\n",
    " - $133855 - 127799 = 6056$ word types lost due to unmodelable triphones\n",
    " - $127799 - 33125 = 94674$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{6056+94674}{133855} ≈ 75.25\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit less than 20%** of the triphone channel model-modelable `Buckeye` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.358010Z",
     "start_time": "2019-09-12T23:13:48.133978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "6576 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7011 - 6576 = 435$ lost\n",
    "\n",
    "$435 / 7011 ≈ .06$ proportionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones\n",
    " - $7011 - 6576 = 435$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{435+988}{7999} ≈ 17.8\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About 15%** of the triphone channel model-modelable `NXT_swbd_destressed` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.601169Z",
     "start_time": "2019-09-12T23:13:48.359506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15318 LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv\n",
      "13246 LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$15338 - 13266 = 2072$ lost\n",
    "\n",
    "$2072 / 15338 ≈ 0.14$ proportionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for NXT_swbd_destressed:\n",
    " - $15834 - 15338 = 496$ word types lost due to unmodelable triphones\n",
    " - $15338 - 13266 = 2072$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{496+2072}{15834} ≈ 16.2\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting the filtered transcribed lexicon relations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.607960Z",
     "start_time": "2019-09-12T23:13:48.603190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered = list(map(lambda bundle: bundle['o'],\n",
    "                                       LTR_LM_filter_bundles))\n",
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Filter the conditioning events of channel distributions to only include $k$-factors contained in elements of a transcription lexicon's segmental wordforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3b**: `LTR_..._aligned_CM_filtered_LM_filtered.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.612567Z",
     "start_time": "2019-09-12T23:13:48.609320Z"
    }
   },
   "outputs": [],
   "source": [
    "#what channel models might you want to use with what lexicons?\n",
    "# there are 3x3 triphone CMs that are aligned with one of 3 LTRs and have one of 3 relevant pseudocount levels\n",
    "# For each of the 3 LTRs aligned with the gating data, there's exactly 1 `LTR...CM_filtered_LM_filtered.tsv` file\n",
    "# ∴ there are 3x3 triphone channel models to trim\n",
    "\n",
    "# Also, for each triphone CM, there are preview and postview distributions to trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.641298Z",
     "start_time": "2019-09-12T23:13:48.613982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    LTR_dir = path.dirname(each['LTR_fp'])\n",
    "    LTR_trimmed_fn = path.basename(each['LTR_fp']).split('_w_')[0] + '_CM_filtered_LM_filtered.tsv'\n",
    "    each['LTR_trimmed_fp'] = path.join(LTR_dir, LTR_trimmed_fn)\n",
    "    each['matching_trimmed_CMs'] = [path.join(path.dirname(fp),\n",
    "                                              LTR_trimmed_fn[:-4] + '_' + path.basename(fp))\n",
    "                                    for fp in each['matching_CMs']]\n",
    "    \n",
    "#     each['LTR_fp']\n",
    "    each['LTR_trimmed_fp']\n",
    "    each['matching_CMs']\n",
    "    each['matching_trimmed_CMs']\n",
    "#     each['trimmed LTR_fp']\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.670788Z",
     "start_time": "2019-09-12T23:13:48.642374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01 against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05 against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1 against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_LTR_CM_triples = []\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    assert len(each['matching_CMs']) == len(each['matching_trimmed_CMs'])\n",
    "    \n",
    "    for i in range(len(each['matching_CMs'])):\n",
    "        args = dict()\n",
    "        args['l'] = each['LTR_trimmed_fp']\n",
    "        args['c'] = each['matching_CMs'][i]\n",
    "        args['o'] = each['matching_trimmed_CMs'][i]\n",
    "        args['nb_fp'] = path.join(path.dirname(args['c']),\n",
    "                                  f\"Filter {path.dirname(args['c'])} against {path.basename(args['l'])[:-4]}.ipynb\")\n",
    "        args\n",
    "        trimmed_LTR_CM_triples.append(args)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.677262Z",
     "start_time": "2019-09-12T23:13:48.672132Z"
    }
   },
   "outputs": [],
   "source": [
    "if '3c' in permittedSteps:\n",
    "    # takes about 120s on wittgenstein\n",
    "    for each in trimmed_LTR_CM_triples:\n",
    "        output_dir = path.dirname(each['o'])\n",
    "        ensure_dir_exists(output_dir)\n",
    "    #     if not path.exists(output_dir):\n",
    "    #         print(f\"Creating output path '{output_dir}'\")\n",
    "    #         makedirs(output_dir)\n",
    "\n",
    "        progress_report(each['nb_fp'],\n",
    "                        dict(l = each['l'],\n",
    "                             c = each['c'],\n",
    "                             o = each['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Filter channel model by transcription lexicon.ipynb',\n",
    "        each['nb_fp'],\n",
    "        parameters=dict(l = each['l'],\n",
    "                        c = each['c'],\n",
    "                        o = each['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 2b**: `...pV_C`\n",
    " - **Step 3b**: `LTR_..._aligned_CM_filtered_LM_filtered.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.683899Z",
     "start_time": "2019-09-12T23:13:48.678569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.694465Z",
     "start_time": "2019-09-12T23:13:48.684918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_1_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Producing Fisher vocab in Buckeye following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_2_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Producing Fisher vocab in Buckeye following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_3_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Producing Fisher vocab in Buckeye following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_following_4_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Producing Fisher vocab in Buckeye following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Producing Fisher vocab in Buckeye preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Producing Fisher vocab in Buckeye preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Producing Fisher vocab in Buckeye preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'Buckeye',\n",
       "  'context_fn': 'buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'context_fp': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Producing Fisher vocab in Buckeye preceding contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Producing Fisher vocab in NXT swbd following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Producing Fisher vocab in NXT swbd following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Producing Fisher vocab in NXT swbd following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_rev_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Producing Fisher vocab in NXT swbd following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_2gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Producing Fisher vocab in NXT swbd preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_3gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Producing Fisher vocab in NXT swbd preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_4gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Producing Fisher vocab in NXT swbd preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'corpus': 'NXT_swbd',\n",
       "  'context_fn': 'nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fn': 'fisher_utterances_main_5gram.mmap',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Producing Fisher vocab in NXT swbd preceding contexts 5gram model contextual distributions.ipynb'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.702242Z",
     "start_time": "2019-09-12T23:13:48.696029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context_fp': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Producing Fisher vocab in Buckeye following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Producing Fisher vocab in Buckeye following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Producing Fisher vocab in Buckeye following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Producing Fisher vocab in Buckeye following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Producing Fisher vocab in Buckeye preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Producing Fisher vocab in Buckeye preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Producing Fisher vocab in Buckeye preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Producing Fisher vocab in Buckeye preceding contexts 5gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Producing Fisher vocab in NXT swbd following contexts 2gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Producing Fisher vocab in NXT swbd following contexts 3gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Producing Fisher vocab in NXT swbd following contexts 4gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_rev_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Producing Fisher vocab in NXT swbd following contexts 5gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_2gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Producing Fisher vocab in NXT swbd preceding contexts 2gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_3gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Producing Fisher vocab in NXT swbd preceding contexts 3gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Producing Fisher vocab in NXT swbd preceding contexts 4gram model contextual distributions.ipynb'},\n",
       " {'context_fp': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'lm_fp': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'LD_dir': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o_fn_stem': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_5gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Producing Fisher vocab in NXT swbd preceding contexts 5gram model contextual distributions.ipynb'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmap(partial(omit, keys=('corpus','context_fn', 'lm_fn')), LDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.712166Z",
     "start_time": "2019-09-12T23:13:48.703445Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD_projection_args = []\n",
    "for ld in LDs:\n",
    "    if ld['corpus'] == 'Buckeye':\n",
    "        l = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv'\n",
    "    elif ld['corpus'] == 'NXT_swbd':\n",
    "        l = 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv'\n",
    "    else:\n",
    "        raise Exception('Not currently supported...')\n",
    "    \n",
    "    lm_dir = path.dirname(ld['o'])\n",
    "    lm_fn = path.basename(ld['o'])\n",
    "    lm_stem = path.splitext(lm_fn)[0]\n",
    "    \n",
    "    l_fn = path.basename(l)\n",
    "    l_stem = path.splitext(l_fn)[0]\n",
    "    \n",
    "    o = path.join(lm_dir, lm_dir + '_projected_' + l_stem) + '.pV_C'\n",
    "    \n",
    "    projection_ab = {\n",
    "        'd':ld['o'] + '.pV_C',\n",
    "        'v':fisher_lm_vocab_fp,\n",
    "        'c':ld['c'],\n",
    "        'l':l,\n",
    "        'o':o,\n",
    "        'f':'True',\n",
    "        'nb_fp':path.join(lm_dir, \n",
    "                          'Filter ' + lm_stem + ' against ' + l_stem + '.ipynb')\n",
    "    }\n",
    "    LD_projection_args.append(projection_ab)\n",
    "\n",
    "LD_projection_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.717308Z",
     "start_time": "2019-09-12T23:13:48.713311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.721588Z",
     "start_time": "2019-09-12T23:13:48.718514Z"
    }
   },
   "outputs": [],
   "source": [
    "# buckeye_contexts\n",
    "# swbd2003_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.727471Z",
     "start_time": "2019-09-12T23:13:48.722613Z"
    }
   },
   "outputs": [],
   "source": [
    "# LD_projection_args = [\n",
    "#     {'d':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
    "#      'v':fisher_lm_vocab_fp,\n",
    "#      'c':buckeye_contexts,\n",
    "#      'l':'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
    "#      'o':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts' + '_projected_' + 'LTR_Buckeye' + '.pV_C',\n",
    "#      'f':'True',\n",
    "#      'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "#                        'Filter ' + 'LD_fisher_vocab_in_buckeye_contexts' + ' against ' + 'LTR_Buckeye_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "#     {'d':,\n",
    "#      'v':,\n",
    "#      'c':,\n",
    "#      'l':,\n",
    "#      'o':,\n",
    "#      'f':,\n",
    "#      'nb_fp':}\n",
    "# #     {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "# #      'v':fisher_lm_vocab_fp,\n",
    "# #      'c':swbd2003_contexts,\n",
    "# #      'l':'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "# #      'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_CMU_destressed' + '.pV_C',\n",
    "# #      'f':'True',\n",
    "# #      'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "# #                        'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "# #     {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "# #      'v':fisher_lm_vocab_fp,\n",
    "# #      'c':swbd2003_contexts,\n",
    "# #      'l':'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "# #      'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_newdic_destressed' + '.pV_C',\n",
    "# #      'f':'True',\n",
    "# #      'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "# #                        'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')}\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.738140Z",
     "start_time": "2019-09-12T23:13:48.728758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '3d' in permittedSteps:\n",
    "    # used to take about ~10-15m on wittgenstein:\n",
    "    #  ≤30s for Buckeye vocab + Buckeye contexts\n",
    "    #  ≈6.5-7m for CMU_destressed vocab in swbd2003 contexts\n",
    "    #  ≈3m for newdic_destressed vocab in swbd2003 contexts\n",
    "    \n",
    "    #takes 56.5m on wittgenstein\n",
    "    \n",
    "    for each in LD_projection_args:\n",
    "        output_dir = path.dirname(each['o'])\n",
    "        ensure_dir_exists(output_dir)\n",
    "\n",
    "        progress_report(each['nb_fp'],\n",
    "                        dict(d = each['d'],\n",
    "                             v = each['v'],\n",
    "                             c = each['c'],\n",
    "                             l = each['l'],\n",
    "                             o = each['o'],\n",
    "                             f = each['f']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
    "        each['nb_fp'],\n",
    "        parameters=dict(d = each['d'],\n",
    "                        v = each['v'],\n",
    "                        c = each['c'],\n",
    "                        l = each['l'],\n",
    "                        o = each['o'],\n",
    "                        f = each['f'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3b**: `LTR_..._aligned_CM_filtered_LM_filtered.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this is the step where word edge symbols are added to segmental wordform representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.745262Z",
     "start_time": "2019-09-12T23:13:48.739542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.784604Z",
     "start_time": "2019-09-12T23:13:48.746704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles = []\n",
    "for ltr_fp in LTR_CM_filtered_LM_filtered:\n",
    "    LTR_n = path.basename( ltr_fp )[:-4]\n",
    "    LTR_dir = path.dirname( ltr_fp )\n",
    "    pW_V_fp_bundles.append({'LTR_fp':ltr_fp,\n",
    "                           'pW_V_fp':ltr_fp[:-4],# + '.pW_V',\n",
    "                           'nb_fp':path.join(LTR_dir,'Define pW_V given {0}.ipynb'.format(LTR_n))})\n",
    "    pW_V_fp_bundles.append({'LTR_fp':ltr_fp,\n",
    "                           'pW_V_fp':ltr_fp[:-4] + '_trim',# + '.pW_V',\n",
    "                           'nb_fp':path.join(LTR_dir,'Define pW_V given {0}'.format(LTR_n) + '_trim' + '.ipynb'),\n",
    "                           'r':'False'})\n",
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.792199Z",
     "start_time": "2019-09-12T23:13:48.786083Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '3e' in permittedSteps:\n",
    "    # used to take about ~30s on wittgenstein\n",
    "    \n",
    "    #takes ≈90s on wittgenstein \n",
    "    for each in pW_V_fp_bundles:\n",
    "        pW_V_fp_output_dir = path.dirname(each['pW_V_fp'])\n",
    "        ensure_dir_exists(pW_V_fp_output_dir)\n",
    "        \n",
    "        if not overwrite and path.exists(each['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(each['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "\n",
    "        progress_report(each['nb_fp'],\n",
    "                        dict(l = each['LTR_fp'],\n",
    "                             o = each['pW_V_fp']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
    "        each['nb_fp'],\n",
    "        parameters=dict(l = each['LTR_fp'],\n",
    "                        o = each['pW_V_fp'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pre-calculate remaining forward model components and meta-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that none of these steps need actually be ordered with respect to each other: the ordering below is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Generate triphone lexicon distributions for every triphone channel model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3c**: `LTR_..._aligned_CM_filtered_LM_filtered_pY1X0X1X2.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.798216Z",
     "start_time": "2019-09-12T23:13:48.793338Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in listdir(a_dir)\n",
    "            if path.isdir(path.join(a_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:48.806385Z",
     "start_time": "2019-09-12T23:13:48.799260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs = get_immediate_subdirectories('.')\n",
    "len(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:51.994884Z",
     "start_time": "2019-09-12T23:13:48.807721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_model_fps = []\n",
    "for d in subdirs:\n",
    "    files = listdir(d)\n",
    "    is_triph_channel_model = lambda fn: 'pY1X0X1X2.json' in fn\n",
    "    CM_files = list(filter(is_triph_channel_model,\n",
    "                           files))\n",
    "    for CM_file in CM_files:\n",
    "        if 'old' not in d:\n",
    "            channel_model_fps.append(path.join(d, CM_file))\n",
    "len(channel_model_fps)\n",
    "channel_model_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.054622Z",
     "start_time": "2019-09-12T23:13:51.996447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Generating LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Generating LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Generating LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "triph_lex_bundles = []\n",
    "for cm_fp in channel_model_fps:\n",
    "    bundle = dict()\n",
    "    bundle['c'] = cm_fp\n",
    "    bundle['output_dir'] = path.dirname(cm_fp)\n",
    "    bundle['c_fn'] = path.basename(cm_fp)\n",
    "    bundle['o_fn_prefix'] = bundle['c_fn'].split('pY1X0X1X2')[0] + 'pX0X1X2'\n",
    "    bundle['o'] = path.join(bundle['output_dir'],\n",
    "                            bundle['o_fn_prefix'])\n",
    "    bundle['r'] = 'True' #set to 'False' and rerun when/if new channel model posterior calculation is running at acceptable speed and segmental analyses are practical\n",
    "    bundle['nb_fp'] = path.join(bundle['output_dir'],\n",
    "                                f\"Generating {bundle['c_fn'].split('pY1X0X1X2.json')[0][:-1]} uniform triphone lexicon dist.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    triph_lex_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.062223Z",
     "start_time": "2019-09-12T23:13:52.055996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '4a' in permittedSteps:\n",
    "    # used to take about ~1m on wittgenstein\n",
    "    \n",
    "    #takes 90s on wittgenstein\n",
    "    for bundle in triph_lex_bundles:\n",
    "        output_dir = bundle['output_dir']\n",
    "        if not path.exists(output_dir):\n",
    "            print(f\"Making output path {output_dir}\")\n",
    "            makedirs(output_dir)\n",
    "\n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "            \n",
    "        progress_report(bundle['nb_fp'],\n",
    "                        dict(c = bundle['c'],\n",
    "                             o = bundle['o'],\n",
    "                             r = bundle['r']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Generate triphone lexicon distribution from channel model.ipynb',\n",
    "        bundle['nb_fp'],\n",
    "        parameters=dict(c = bundle['c'],\n",
    "                        o = bundle['o'],\n",
    "                        r = bundle['r'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3e**: `...pW_V.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is comparable to the `Metadata` generation step in 2a above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.070178Z",
     "start_time": "2019-09-12T23:13:52.063584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fps = [each['pW_V_fp'] + '.pW_V.json' for each in pW_V_fp_bundles]\n",
    "pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.074758Z",
     "start_time": "2019-09-12T23:13:52.071579Z"
    }
   },
   "outputs": [],
   "source": [
    "# pW_V_fps = []\n",
    "# for d in subdirs:\n",
    "#     files = listdir(d)\n",
    "#     is_pW_V = lambda fn: 'pW_V.json' in fn\n",
    "#     pW_V_files = list(filter(is_pW_V,\n",
    "#                            files))\n",
    "#     for pW_V_file in pW_V_files:\n",
    "#         pW_V_fps.append(path.join(d, pW_V_file))\n",
    "# len(pW_V_fps)\n",
    "# pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.098227Z",
     "start_time": "2019-09-12T23:13:52.076300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'lex_name': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'lex_name': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'lex_name': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'lex_name': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'lex_name': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'lex_name': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'lex_name': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'o': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'lex_name': 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'o': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       " 'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Calculate word-prefix relation, Hamming distances, and k-cousin relation for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "lexicon_md_bundles = []\n",
    "for pW_V_fp in pW_V_fps:\n",
    "    \n",
    "    \n",
    "    bundle = dict()\n",
    "    bundle['p'] = pW_V_fp\n",
    "    bundle['lex_name'] = path.basename(pW_V_fp).split('.pW_V.json')[0]\n",
    "    bundle['o'] = path.join(path.dirname(pW_V_fp), path.basename(pW_V_fp).split('.pW_V.json')[0] )\n",
    "    \n",
    "    output_dir = path.dirname(pW_V_fp)\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path '{output_dir}'\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    bundle['nb_fp'] = path.join(path.dirname(pW_V_fp),\n",
    "                                f\"Calculate word-prefix relation, Hamming distances, and k-cousin relation for {bundle['lex_name']}.ipynb\")\n",
    "#                                 f\"Calculate prefix data, k-cousins, and k-spheres for {bundle['lex_name']}.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    lexicon_md_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.106424Z",
     "start_time": "2019-09-12T23:13:52.099602Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '4b' in permittedSteps:\n",
    "    #oldest runtimes...\n",
    "    # with J=-1, no CPU load, and no .npz export\n",
    "    #  - newdic CM_filtered_LM_filtered takes ~45m (~168m on Solomonoff)\n",
    "    #  - CMU CM_filtered_LM_filtered takes ~3.5-3.75h (14-15h? on Quine?)\n",
    "    # - Buckeye CM_filtered_LM_filtered takes ~20m (~80m on Quine)\n",
    "\n",
    "    #older runtimes...\n",
    "    # with J=-1, no CPU load, *and* .npz export\n",
    "    #  - newdic CM_filtered_LM_filtered takes ~45m\n",
    "    #  - CMU CM_filtered_LM_filtered takes ~20.5h\n",
    "    # - Buckeye CM_filtered_LM_filtered takes ~19.5h\n",
    "    # (.npz representations are up to 20x smaller on disk \n",
    "    #  and are significantly smaller in memory when loaded)\n",
    "\n",
    "    # with J=-1, no CPU load, *and* .npz export\n",
    "    #  - newdic CM_filtered_LM_filtered takes ~?m\n",
    "    #  - CMU CM_filtered_LM_filtered takes ~40h+ on wittgenstein\n",
    "    # - Buckeye CM_filtered_LM_filtered takes ~?h\n",
    "    # (.npz representations are up to 20x smaller on disk \n",
    "    #  and are significantly smaller in memory when loaded)\n",
    "    \n",
    "    #current runtime = 80m on wittgenstein\n",
    "    # newdic takes 5.5m\n",
    "    # CMU takes 60m / peak 55GB or 68GB mem usage\n",
    "    # Buckeye takes 3.16m\n",
    "    # NXT_swbd takes 9.3m\n",
    "    for bundle in lexicon_md_bundles:\n",
    "        \n",
    "        output_dir = path.dirname(bundle['o'])\n",
    "        if not path.exists(output_dir):\n",
    "            print(f\"Making output path {output_dir}\")\n",
    "            makedirs(output_dir)\n",
    "\n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        progress_report(bundle['nb_fp'],\n",
    "                        dict(p = bundle['p'],\n",
    "                             o = bundle['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Calculate word-prefix relation, Hamming distances, and k-cousin relation.ipynb',\n",
    "        bundle['nb_fp'],\n",
    "        parameters=dict(p = bundle['p'],\n",
    "                        o = bundle['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4c: Calculate the marginal probability $p(W|C)$ of each segmental wordform $w$ given $n$-gram contexts $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3e**: `pW_V` matrix\n",
    " - **Step 3d**: `LD_fisher_vocab_in...contexts_projected_LTR...pV_C.npy` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.112272Z",
     "start_time": "2019-09-12T23:13:52.107787Z"
    }
   },
   "outputs": [],
   "source": [
    "#gather pV_C, pW_V fp pairs as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.120212Z",
     "start_time": "2019-09-12T23:13:52.113574Z"
    }
   },
   "outputs": [],
   "source": [
    "def LTR_to_pW_Vs(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['LTR_fp'] == LTR_fp,\n",
    "                                   pW_V_fp_bundles))\n",
    "    matching_pW_V_fps = set(map(lambda bundle: bundle['pW_V_fp'],\n",
    "                                matching_bundles))\n",
    "    return matching_pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.127840Z",
     "start_time": "2019-09-12T23:13:52.121546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'},\n",
       " {'LTR_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'nb_fp': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb',\n",
       "  'r': 'False'}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.132605Z",
     "start_time": "2019-09-12T23:13:52.129154Z"
    }
   },
   "outputs": [],
   "source": [
    "def LTR_to_LD(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['l'] == LTR_fp,\n",
    "                                   LD_projection_args))\n",
    "    matching_pV_C_fps = set(map(lambda bundle: bundle['o'],\n",
    "                                matching_bundles))\n",
    "    return matching_pV_C_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.142556Z",
     "start_time": "2019-09-12T23:13:52.133943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_1_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_2_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_3_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_following_4_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Filter LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_1_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_2_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_3_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_Buckeye/buckeye_contexts_preceding_4_filtered.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_1_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_2_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_3_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_following_4_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_1_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_2_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_3_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'C_NXT_swbd/nxt_swbd_contexts_preceding_4_filtered.txt',\n",
       "  'l': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD_projection_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.151677Z",
     "start_time": "2019-09-12T23:13:52.143974Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C'),\n",
       " ('LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim',\n",
       "  'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C')}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_matched_pW_V_LD_pairs(LTR_fp):\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matching_pV_C_fps = LTR_to_LD(LTR_fp)\n",
    "    \n",
    "    return set(product(matching_pW_V_fps,\n",
    "                       matching_pV_C_fps))\n",
    "\n",
    "my_LTR_fps = list(map(lambda bundle: bundle['LTR_fp'],\n",
    "                      pW_V_fp_bundles))\n",
    "\n",
    "matched_pW_V_LD_pairs = union(map(get_matched_pW_V_LD_pairs,\n",
    "                                  my_LTR_fps))\n",
    "matched_pW_V_LD_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.249177Z",
     "start_time": "2019-09-12T23:13:52.153370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_4gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_3gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_trim.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_5gram_model.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WD_bundles = []\n",
    "for w,d in matched_pW_V_LD_pairs:\n",
    "    bundle = dict()\n",
    "    \n",
    "    LTR_key = path.basename(w)\n",
    "    LD_key = path.basename(d)\n",
    "    C_key = LD_key.split('LD_Fisher_vocab')[1].split('_projected_')[0]\n",
    "    \n",
    "    output_dir = path.dirname(d)\n",
    "    output_prefix = LTR_key + C_key + '.pW_C'\n",
    "    \n",
    "    bundle['d'] = d + '.npy'\n",
    "    bundle['w'] = w + '.pW_V.npz'\n",
    "    \n",
    "    bundle['o'] = path.join(output_dir, output_prefix)\n",
    "    \n",
    "    bundle['nb_fp'] = path.join(output_dir, f\"Calculate segmental wordform distribution for {LTR_key}{C_key.replace('_', ' ')}.ipynb\")\n",
    "    \n",
    "    bundle\n",
    "                                \n",
    "    trim_bundle = deepcopy(bundle)\n",
    "    trim_bundle['w'] = w + '_trim' + '.pW_V.npz'\n",
    "    trim_bundle\n",
    "    \n",
    "    WD_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.254627Z",
     "start_time": "2019-09-12T23:13:52.250151Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '4c' in permittedSteps:\n",
    "    # used to take ≈5-10m on wittgenstein with no background load\n",
    "    #  ≈8-10m for filtered LTR_CMU_destressed in swbd2003 contexts\n",
    "    #  ≤30s for filtered LTR_Buckeye in buckeye contexts\n",
    "    #  ≈2-3m for filtered LTR_newdic_destressed in swbd2003 contexts\n",
    "    \n",
    "    #now takes 18m on wittgenstein\n",
    "    for bundle in WD_bundles:\n",
    "        ensure_dir_exists(path.dirname(bundle['o']))\n",
    "\n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        progress_report(bundle['nb_fp'],\n",
    "                        dict(d = bundle['d'],\n",
    "                             w = bundle['w'],\n",
    "                             o = bundle['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
    "        bundle['nb_fp'],\n",
    "        parameters=dict(d = bundle['d'],\n",
    "                        w = bundle['w'],\n",
    "                        o = bundle['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4d: Define observation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3c**: `LTR_..._aligned_CM_filtered_LM_filtered_pY1X0X1X2.json`\n",
    " - **Step 3c**: `LTR_..._aligned_CM_filtered_LM_filtered_p3Y1X01.json`\n",
    " - **Step 3c**: `LTR_..._aligned_CM_filtered_LM_filtered_p6Y0X01.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.261245Z",
     "start_time": "2019-09-12T23:13:52.255584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'center': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'},\n",
       " {'center': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json'}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify trimmed center (i.e. triphone) channel model fps defined earlier\n",
    "trimmed_CM_bundles = [{'center':bundle['o']} for bundle in trimmed_LTR_CM_triples]\n",
    "trimmed_CM_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.265784Z",
     "start_time": "2019-09-12T23:13:52.262187Z"
    }
   },
   "outputs": [],
   "source": [
    "# add inferred (err, hardcoded-in-step-3c) fps for preview and postview distributions\n",
    "for bundle in trimmed_CM_bundles:\n",
    "    dirpath = path.dirname(bundle['center'])\n",
    "    processing_prefix = path.basename(bundle['center']).split('pY1X0X1X2.json')[0]\n",
    "    bundle['preview'] = path.join(dirpath, processing_prefix + 'p3Y1X01.json')\n",
    "    bundle['postview'] = path.join(dirpath, processing_prefix + 'p6Y0X01.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.290472Z",
     "start_time": "2019-09-12T23:13:52.266992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Calculate LTR_newdic_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Calculate LTR_newdic_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Calculate LTR_newdic_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Calculate LTR_CMU_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Calculate LTR_CMU_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Calculate LTR_CMU_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Calculate LTR_Buckeye_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Calculate LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Calculate LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p6Y0X01.json',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'r': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_p3Y1X01.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Calculate LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered observation distribution given channel models.ipynb'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_bundles = []\n",
    "for bundle in trimmed_CM_bundles:\n",
    "    new_bundle = dict()\n",
    "    \n",
    "    dirpath = path.dirname(bundle['center'])\n",
    "    processing_prefix = path.basename(bundle['center']).split('pY1X0X1X2.json')[0]\n",
    "    \n",
    "    new_bundle['l'] = bundle['postview']\n",
    "    new_bundle['c'] = bundle['center']\n",
    "    new_bundle['r'] = bundle['preview']\n",
    "    new_bundle['o'] = path.join(dirpath, processing_prefix + 'pC1X012')\n",
    "    \n",
    "#     pprintable_proc_pref = processing_prefix[:-1]\n",
    "    new_bundle['nb_fp'] = path.join(dirpath, f\"Calculate {processing_prefix[:-1]} observation distribution given channel models.ipynb\")\n",
    "    \n",
    "    new_bundle\n",
    "    observation_bundles.append(new_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.296137Z",
     "start_time": "2019-09-12T23:13:52.291411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if '4d' in permittedSteps:\n",
    "    # takes 5-6h on old sidious, iirc\n",
    "    # takes 2h 40m on new sidious \n",
    "    \n",
    "    # takes ≈100m on wittgenstein\n",
    "    # (peak mem usage is ≈100+GB but <120GB on cmu)\n",
    "    # ≈7.6m per newdic input\n",
    "    # ≈11.3m per CMU input\n",
    "    # ≈6m per Buckeye input\n",
    "    # ≈8.2m per NXT_swbd input\n",
    "    for bundle in observation_bundles:\n",
    "        ensure_dir_exists(path.dirname(bundle['o']))\n",
    "\n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        progress_report(bundle['nb_fp'],\n",
    "                        dict(l = bundle['l'],\n",
    "                             c = bundle['c'],\n",
    "                             r = bundle['r'],\n",
    "                             o = bundle['o']))\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "        'Calculate observation distribution given channel models.ipynb',\n",
    "        bundle['nb_fp'],\n",
    "        parameters=dict(l = bundle['l'],\n",
    "                        c = bundle['c'],\n",
    "                        r = bundle['r'],\n",
    "                        o = bundle['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4e: Define channel distributions on a set of segmental wordforms(+prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 3c**: `LTR_..._aligned_CM_filtered_LM_filtered_pY1X0X1X2.json`\n",
    " - **Step 3e**: `...pW_V.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.301829Z",
     "start_time": "2019-09-12T23:13:52.297079Z"
    }
   },
   "outputs": [],
   "source": [
    "# gather paired (....pW_V.json, ...pY1X0X1X2.json) p(W|V), p(Y_i|X_{i-1},X_i;X_{i+1}) distributions\n",
    "# output complete wordform channel models into the channel model directory with the same prefix that's on the triphone channel distribution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.362280Z",
     "start_time": "2019-09-12T23:13:52.303069Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_NXT_swbd_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'b': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pC1X012.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def LTR_to_pW_Vs(LTR_fp):\n",
    "#     matching_bundles = list(filter(lambda bundle: bundle['LTR_fp'] == LTR_fp,\n",
    "#                                    pW_V_fp_bundles))\n",
    "#     matching_pW_V_fps = set(map(lambda bundle: bundle['pW_V_fp'],\n",
    "#                                 matching_bundles))\n",
    "#     return matching_pW_V_fps\n",
    "\n",
    "def LTR_to_TCMs(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['l'] == LTR_fp,\n",
    "                                   trimmed_LTR_CM_triples))\n",
    "    matching_TCM_fps = set(map(lambda bundle: bundle['o'],\n",
    "                               matching_bundles))\n",
    "    return matching_TCM_fps\n",
    "\n",
    "def matched_pW_Vs_and_TCMs(LTR_fp):\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matching_TCM_fps = LTR_to_TCMs(LTR_fp)\n",
    "    return {'LW_V_fps':matching_pW_V_fps,\n",
    "            'TCM_fps':matching_TCM_fps}\n",
    "\n",
    "def get_matched_pairs(LTR_fp):\n",
    "    matching_TCM_fps = LTR_to_TCMs(LTR_fp)\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matched_pairs = set(product(matching_TCM_fps,\n",
    "                                matching_pW_V_fps))\n",
    "    return matched_pairs\n",
    "\n",
    "my_LTR_fps = list(map(lambda bundle: bundle['LTR_fp'],\n",
    "                      pW_V_fp_bundles))\n",
    "\n",
    "LCM_bundles = []\n",
    "for c,w in union(map(get_matched_pairs,\n",
    "                     my_LTR_fps)):\n",
    "#     if '_trim' in w:\n",
    "#         print(f'Skipping w = {w}')\n",
    "#         continue\n",
    "    \n",
    "    bundle = dict()\n",
    "    bundle['c'] = c\n",
    "    if '_trim' in w:\n",
    "        bundle['b'] = c.split('pY1X0X1X2.json')[0] + 'pC1X012.npy'\n",
    "    bundle['w'] = w + '.pW_V.json'\n",
    "    output_dir = path.dirname(c)\n",
    "    output_suffix = 'trim_' if '_trim' in w else ''\n",
    "    output_prefix = path.basename(c).split('pY1X0X1X2.json')[0] + output_suffix\n",
    "    bundle['o'] = path.join(output_dir, output_prefix)\n",
    "    \n",
    "    bundle['nb_fp'] = path.join(output_dir, f'Calculate wordform channel matrices for {path.basename(w)}.ipynb')\n",
    "    \n",
    "    bundle\n",
    "    LCM_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.365466Z",
     "start_time": "2019-09-12T23:13:52.363238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.376790Z",
     "start_time": "2019-09-12T23:13:52.366422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# skip_trim = False\n",
    "\n",
    "if '4e' in permittedSteps:\n",
    "    #old runtime info\n",
    "    # takes ~2m on wittgenstein with no background load and no .npz files\n",
    "    # takes ~12m on wittgenstein with no background load *and* .npz files\n",
    "    # takes ~15m on (new) sidious with no background load *and* .npz files *and* assertion checking\n",
    "    # takes ~20m on (new) sidious with no background load *and* .npz files *and* assertion checking *and* 'exact wordform' calculations\n",
    "    \n",
    "    #current runtime info\n",
    "    # ? on untrimmed inputs (wittgenstein)\n",
    "    #   ≈?m on newdic\n",
    "    #   ≈?m on Buckeye\n",
    "    #   ≈?m on CMU\n",
    "    #   ≈?m on NXT_swbd\n",
    "    # ≈160m on trimmed inputs (tarski)\n",
    "    #   ≈10m on newdic inputs\n",
    "    #   ≈6m on Buckeye inputs\n",
    "    #   ≈43m on CMU inputs\n",
    "    #   ≈14m on NXT_swbd inputs\n",
    "    for bundle in LCM_bundles:\n",
    "        ensure_dir_exists(path.dirname(bundle['o']))\n",
    "\n",
    "        if '_trim' in bundle['w'] and skip_trim:\n",
    "            bundle_w = bundle['w']\n",
    "            print(f'Skipping bundle containing w = {bundle_w}')\n",
    "            continue\n",
    "        \n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if '_trim' in bundle['w']:\n",
    "            progress_report(bundle['nb_fp'], \n",
    "                            dict(c = bundle['c'],\n",
    "                                 b = bundle['b'],\n",
    "                                 w = bundle['w'],\n",
    "                                 o = bundle['o']))\n",
    "            nb = pm.execute_notebook(\n",
    "                'Calculate segmental wordform and prefix channel matrices - OD.ipynb',\n",
    "                bundle['nb_fp'],\n",
    "                parameters=dict(c = bundle['c'],\n",
    "                                b = bundle['b'],\n",
    "                                w = bundle['w'],\n",
    "                                o = bundle['o'])\n",
    "            )\n",
    "        else:\n",
    "            progress_report(bundle['nb_fp'], \n",
    "                            dict(c = bundle['c'],\n",
    "                                 w = bundle['w'],\n",
    "                                 o = bundle['o']))\n",
    "            nb = pm.execute_notebook(\n",
    "                'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
    "                bundle['nb_fp'],\n",
    "                parameters=dict(c = bundle['c'],\n",
    "                                w = bundle['w'],\n",
    "                                o = bundle['o'])\n",
    "            )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate posterior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5a: Calculate $p(V|W, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 4c**: `pW_C` matrix\n",
    " - **Step 3e**: `pW_V` matrix\n",
    " - **Step 3d**: `LD_fisher_vocab_in...contexts_projected_LTR...pV_C.npy` matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) = \\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:13:52.383453Z",
     "start_time": "2019-09-12T23:13:52.378151Z"
    }
   },
   "outputs": [],
   "source": [
    "#gather aligned triples of filepaths defining\n",
    "# - $p(V|C)$\n",
    "# - $p(W|C)$\n",
    "# - $p(W|V)$\n",
    "#plus associated (and crucially appropriately ordered!) metadata detailing \n",
    "# - C\n",
    "# - V\n",
    "# - W\n",
    "# - the mapping between V and W\n",
    "# construct the output filename and location (probably in LD?)\n",
    "# construct output notebook filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T23:16:33.729975Z",
     "start_time": "2019-09-12T23:16:33.657077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd following contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 2gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_following_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_4gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_4gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_4gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 4gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_3gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_following_contexts_3gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_3gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye following contexts 3gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_5gram_model.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_following_contexts_5gram_model.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_following_contexts_5gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye following contexts 5gram model.ipynb'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posterior_WD_bundles = []\n",
    "for bundle in WD_bundles:\n",
    "    output_dir = path.dirname(bundle['d'])\n",
    "    LTR_key = path.basename(bundle['w']).split('.pW_V.npz')[0]\n",
    "    contexts_key = path.basename(bundle['o']).split('LM_filtered')[1].split('.pW_C')[0].replace('_', ' ')\n",
    "    output_base_prefix = LTR_key.replace('_trim', '') + contexts_key.replace(' ', '_') + '.pV_WC'\n",
    "    \n",
    "    \n",
    "    new_bundle = dict()\n",
    "    new_bundle['d'] = bundle['d']          #p(V|C) as .npy\n",
    "    new_bundle['w'] = bundle['w']          #p(W|V) as .npz\n",
    "    new_bundle['m'] = bundle['o'] + '.npy' #p(W|C) as .npy\n",
    "    \n",
    "    # c = arg pointing to file specifying C\n",
    "#     LM_dir = path.dirname(new_bundle['d'])\n",
    "#     LM_name = '_'.join(LM_dir.lower().split('_')[-2:])\n",
    "#     contexts_ext = '.txt'\n",
    "#     contexts_fn = 'LM_filtered_' + LM_name + contexts_ext\n",
    "#     contexts_fp = path.join(LM_dir, contexts_fn)\n",
    "#     new_bundle['c'] = contexts_fp\n",
    "    \n",
    "#     # v = arg pointing to file specifying V\n",
    "#     # l = arg pointing to file specifying W\n",
    "#     vlt_prefix = bundle['w'].split('.pW_V.npz')[0]\n",
    "#     vocabulary_fp = vlt_prefix + '_Orthographic_Wordforms' + '.txt'\n",
    "#     lexicon_fp = vlt_prefix + '_Transcriptions' + '.txt'\n",
    "#     LTR_fp = vlt_prefix + '.tsv'\n",
    "#     new_bundle['v'] = vocabulary_fp\n",
    "#     new_bundle['l'] = lexicon_fp\n",
    "#     new_bundle['t'] = LTR_fp\n",
    "    \n",
    "    new_bundle['o'] = path.join(output_dir, output_base_prefix)\n",
    "    new_bundle['nb_fp'] = path.join(output_dir, f'Calculate orthographic posterior given segmental wordform + context for {LTR_key.replace(\"_trim\", \"\")}{contexts_key}' + '.ipynb')\n",
    "    new_bundle\n",
    "    posterior_WD_bundles.append(new_bundle)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-12T23:18:30.428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start  @ 16:18:30\n",
      "Running notebook:\n",
      "\tCalculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 2gram model.ipynb\n",
      "Output directory:\n",
      "\tLD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model\n",
      "Arguments:\n",
      "{\n",
      " \"d\": \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy\",\n",
      " \"w\": \"LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz\",\n",
      " \"m\": \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy\",\n",
      " \"o\": \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89a9bec179547e7bcd3e9a5c8636bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'code',\n",
       "   'execution_count': 1,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.337260Z',\n",
       "     'start_time': '2019-07-16T19:53:06.311888Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:31.838652',\n",
       "     'end_time': '2019-09-12T23:18:31.885298',\n",
       "     'duration': 0.046646,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#Prints **all** console output, not just last item in cell \\nfrom IPython.core.interactiveshell import InteractiveShell\\nInteractiveShell.ast_node_interactivity = \"all\"'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:31.918985',\n",
       "     'end_time': '2019-09-12T23:18:31.953249',\n",
       "     'duration': 0.034264,\n",
       "     'status': 'completed'}},\n",
       "   'source': '**Eric Meinhardt / emeinhardt@ucsd.edu**'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'toc': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:31.981327',\n",
       "     'end_time': '2019-09-12T23:18:32.009208',\n",
       "     'duration': 0.027881,\n",
       "     'status': 'completed'}},\n",
       "   'source': '<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-load-data\" data-toc-modified-id=\"Imports-/-load-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / load data</a></span></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li></ul></div>'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.037942',\n",
       "     'end_time': '2019-09-12T23:18:32.066677',\n",
       "     'duration': 0.028735,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Overview'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.094695',\n",
       "     'end_time': '2019-09-12T23:18:32.122425',\n",
       "     'duration': 0.02773,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Given\\n  - a filepath $d$ to an `.npy` file defining $p(V|C)$ a distribution on orthographic wordforms $V$ given (orthographic) $n$-gram context $c \\\\in C$\\n  - a filepath $w$ to an `.npz` file defining $p(W|V)$ a distribution on full segmental wordforms $W$ given an orthographic wordform $v \\\\in V$\\n  - a filepath $m$ to an `.npy` file defining $p(W|C)$ a distribution on full segmental wordforms $W$ given an (orthographic) $n$-gram context $c \\\\in C$\\n  - an output filepath prefix $o$\\n\\nthis notebook calculates $p(V|W,C)$, i.e.\\n\\n$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nand exports it to $o$ as a `tiledb` sparse array.\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.151123',\n",
       "     'end_time': '2019-09-12T23:18:32.179382',\n",
       "     'duration': 0.028259,\n",
       "     'status': 'completed'}},\n",
       "   'source': '## Requirements'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.207770',\n",
       "     'end_time': '2019-09-12T23:18:32.235948',\n",
       "     'duration': 0.028178,\n",
       "     'status': 'completed'}},\n",
       "   'source': ' - `numpy`\\n - the `pydata` `sparse` package\\n - the `tiledb` package for high-dimensional, out-of-core, sparse matrix storage'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.264728',\n",
       "     'end_time': '2019-09-12T23:18:32.293139',\n",
       "     'duration': 0.028411,\n",
       "     'status': 'completed'}},\n",
       "   'source': '## Usage'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.438578Z',\n",
       "     'start_time': '2019-07-16T19:53:06.343693Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.321703',\n",
       "     'end_time': '2019-09-12T23:18:32.355291',\n",
       "     'duration': 0.033588,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#FIXME'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.388650',\n",
       "     'end_time': '2019-09-12T23:18:32.416838',\n",
       "     'duration': 0.028188,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Parameters'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 3,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.446673Z',\n",
       "     'start_time': '2019-07-16T19:53:06.442171Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.444909',\n",
       "     'end_time': '2019-09-12T23:18:32.478533',\n",
       "     'duration': 0.033624,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from os import getcwd, chdir, listdir, path, mkdir, makedirs'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 4,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.748859Z',\n",
       "     'start_time': '2019-07-16T19:53:06.449718Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.510378',\n",
       "     'end_time': '2019-09-12T23:18:32.744103',\n",
       "     'duration': 0.233725,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from boilerplate import *'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 5,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.757658Z',\n",
       "     'start_time': '2019-07-16T19:53:06.752232Z'},\n",
       "    'tags': ['parameters'],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.778613',\n",
       "     'end_time': '2019-09-12T23:18:32.813680',\n",
       "     'duration': 0.035067,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# Parameters\\n\\nd = ''\\n# d = 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy'\\n\\nw = ''\\n# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz'\\n\\nm = ''\\n# m = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy'\\n\\no = ''\\n# o = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pV_WC'\"},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {'tags': ['injected-parameters'],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.846448',\n",
       "     'end_time': '2019-09-12T23:18:32.880188',\n",
       "     'duration': 0.03374,\n",
       "     'status': 'completed'}},\n",
       "   'execution_count': 6,\n",
       "   'source': '# Parameters\\nd = \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy\"\\nw = \"LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz\"\\nm = \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy\"\\no = \"LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC\"\\n',\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 7,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.768566Z',\n",
       "     'start_time': '2019-07-16T19:53:06.760673Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.912110',\n",
       "     'end_time': '2019-09-12T23:18:32.945856',\n",
       "     'duration': 0.033746,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# Parameters\\n# d = \"LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C.npy\"\\n# w = \"LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz\"\\n# m = \"LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C.npy\"\\n# o = \"LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC\"\\n'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 8,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.780252Z',\n",
       "     'start_time': '2019-07-16T19:53:06.771566Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:32.978801',\n",
       "     'end_time': '2019-09-12T23:18:33.012937',\n",
       "     'duration': 0.034136,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'output_dir = path.dirname(o)\\nensure_dir_exists(output_dir)'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:33.044766',\n",
       "     'end_time': '2019-09-12T23:18:33.072472',\n",
       "     'duration': 0.027706,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Imports / load data'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 9,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.791359Z',\n",
       "     'start_time': '2019-07-16T19:53:06.786585Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:33.100959',\n",
       "     'end_time': '2019-09-12T23:18:33.134583',\n",
       "     'duration': 0.033624,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from itertools import starmap, chain'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 10,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.799175Z',\n",
       "     'start_time': '2019-07-16T19:53:06.794637Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:33.166431',\n",
       "     'end_time': '2019-09-12T23:18:33.199892',\n",
       "     'duration': 0.033461,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import numpy as np'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 11,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.024809Z',\n",
       "     'start_time': '2019-07-16T19:53:06.819749Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:33.234893',\n",
       "     'end_time': '2019-09-12T23:18:34.547564',\n",
       "     'duration': 1.312671,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import sparse'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 12,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.045352Z',\n",
       "     'start_time': '2019-07-16T19:53:08.028339Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:34.584219',\n",
       "     'end_time': '2019-09-12T23:18:34.628587',\n",
       "     'duration': 0.044368,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from tqdm import tqdm, tqdm_notebook, tqdm_gui'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 13,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.055475Z',\n",
       "     'start_time': '2019-07-16T19:53:08.048236Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:34.663252',\n",
       "     'end_time': '2019-09-12T23:18:34.700628',\n",
       "     'duration': 0.037376,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"#from joblib import Parallel, delayed\\nimport joblib as jl\\n\\nJ = -1\\nBACKEND = 'multiprocessing'\\n# BACKEND = 'loky'\\nV = 10\\nPREFER = 'processes'\\n# PREFER = 'threads'\\n\\ndef identity(x):\\n    return x\\n\\ndef par(gen_expr):\\n    return jl.Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 14,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:15.443845Z',\n",
       "     'start_time': '2019-07-16T19:53:08.058691Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:34.733891',\n",
       "     'end_time': '2019-09-12T23:18:34.944283',\n",
       "     'duration': 0.210392,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245, 4551)'},\n",
       "     'execution_count': 14},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 14},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.48222396'},\n",
       "     'execution_count': 14}],\n",
       "   'source': 'pV_C = np.load(d)\\npV_C.shape\\npV_C.dtype\\npV_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 15,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.061927Z',\n",
       "     'start_time': '2019-07-16T19:53:15.447121Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:34.977118',\n",
       "     'end_time': '2019-09-12T23:18:35.105734',\n",
       "     'duration': 0.128616,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pV_C, axis=0), np.ones(shape = np.sum(pV_C, axis=0).shape)  )'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 16,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.084381Z',\n",
       "     'start_time': '2019-07-16T19:53:21.065462Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:35.140537',\n",
       "     'end_time': '2019-09-12T23:18:35.231986',\n",
       "     'duration': 0.091449,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 13245)'},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.00031788'},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '7.802137785753296e-05'},\n",
       "     'execution_count': 16}],\n",
       "   'source': 'pW_V = sparse.load_npz(w)\\npW_V.shape\\npW_V.dtype\\npW_V.nbytes / 1e9\\npW_V.density'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 17,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.715481Z',\n",
       "     'start_time': '2019-07-16T19:53:21.088053Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:35.266418',\n",
       "     'end_time': '2019-09-12T23:18:37.160372',\n",
       "     'duration': 1.893954,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pW_V, axis=0).todense(), np.ones(shape = np.sum(pW_V, axis=0).todense().shape)  )'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 18,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:29.343027Z',\n",
       "     'start_time': '2019-07-16T19:53:21.718649Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.197040',\n",
       "     'end_time': '2019-09-12T23:18:37.388121',\n",
       "     'duration': 0.191081,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 4551)'},\n",
       "     'execution_count': 18},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 18},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.466641336'},\n",
       "     'execution_count': 18}],\n",
       "   'source': '#takes <=1 min\\npW_C = np.load(m)\\npW_C.shape\\npW_C.dtype\\npW_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 19,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.590331Z',\n",
       "     'start_time': '2019-07-16T19:53:29.346649Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.419779',\n",
       "     'end_time': '2019-09-12T23:18:37.555545',\n",
       "     'duration': 0.135766,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pW_C, axis=0), np.ones(shape = np.sum(pW_C, axis=0).shape)  )'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.592922',\n",
       "     'end_time': '2019-09-12T23:18:37.624054',\n",
       "     'duration': 0.031132,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Main calculation'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.654847',\n",
       "     'end_time': '2019-09-12T23:18:37.685340',\n",
       "     'duration': 0.030493,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nLet\\n - $d = p(V|C)$\\n - $w = p(W|V)$\\n - $m = p(W|C)$\\n - $o = p(V|W,C)$\\n\\n$o_{i,j,k} = \\\\frac{w_{j,i} d_{i,k}}{m_{j,k}}$\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 20,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.599447Z',\n",
       "     'start_time': '2019-07-16T19:53:34.593802Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.716672',\n",
       "     'end_time': '2019-09-12T23:18:37.753344',\n",
       "     'duration': 0.036672,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'num_orthWords, num_contexts = pV_C.shape\\nnum_segWords, num_orthWords2 = pW_V.shape\\nassert num_orthWords == num_orthWords2\\nnum_segWords2, num_contexts2 = pW_C.shape\\nassert num_segWords == num_segWords2\\nassert num_contexts == num_contexts'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 21,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.619226Z',\n",
       "     'start_time': '2019-07-16T19:53:34.602244Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.787164',\n",
       "     'end_time': '2019-09-12T23:18:37.827765',\n",
       "     'duration': 0.040601,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245, 12817, 4551)'},\n",
       "     'execution_count': 21},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'772,583,061,915'\"},\n",
       "     'execution_count': 21},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'6,180.66449532 GB'\"},\n",
       "     'execution_count': 21}],\n",
       "   'source': 'pV_WC_shape = (num_orthWords, num_segWords, num_contexts)\\npV_WC_shape\\nnum_cells = np.prod(pV_WC_shape); f\"{num_cells:,}\"\\nn_GB = (num_cells * 8) / 1e9; f\"{n_GB:,} GB\"'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 22,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.633386Z',\n",
       "     'start_time': '2019-07-16T19:53:34.627860Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.862804',\n",
       "     'end_time': '2019-09-12T23:18:37.901527',\n",
       "     'duration': 0.038723,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245,)'},\n",
       "     'execution_count': 22}],\n",
       "   'source': 'pW_V.data.shape'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.936048',\n",
       "     'end_time': '2019-09-12T23:18:37.967725',\n",
       "     'duration': 0.031677,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Given how large this number is, we *can't* calculate this naively, and given that $p(W|V)$ is *incredibly* sparse, we definitely don't *need* to.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 23,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.642853Z',\n",
       "     'start_time': '2019-07-16T19:53:34.637242Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:37.999884',\n",
       "     'end_time': '2019-09-12T23:18:38.037523',\n",
       "     'duration': 0.037639,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# def pV_WC_calc(v, w, c):\\n#     i = Vs_t\\n#     return pV_WC_calc_np(i,j,k)\\n\\ndef pV_WC_calc_np(i,j,k):\\n    numerator = pW_V[j,i]* pV_C[i,k]\\n    denominator = pW_C[j,k]\\n    return numerator / denominator'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 24,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.191796Z',\n",
       "     'start_time': '2019-07-16T19:53:34.645849Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.072805',\n",
       "     'end_time': '2019-09-12T23:18:38.299680',\n",
       "     'duration': 0.226875,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert not np.any(pW_C == 0) # if there are no 0s in pW_C, then we can take the element-wise inverse without anything blowing up\\ninv_pW_C = 1.0 / pW_C'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 25,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.203932Z',\n",
       "     'start_time': '2019-07-16T19:53:48.195411Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.339514',\n",
       "     'end_time': '2019-09-12T23:18:38.380213',\n",
       "     'duration': 0.040699,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 4551)'},\n",
       "     'execution_count': 25},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.466641336'},\n",
       "     'execution_count': 25}],\n",
       "   'source': 'inv_pW_C.shape\\ninv_pW_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 26,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.528451Z',\n",
       "     'start_time': '2019-07-16T19:53:48.206654Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.418032',\n",
       "     'end_time': '2019-09-12T23:18:38.468485',\n",
       "     'duration': 0.050453,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'del pW_C'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 27,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.536197Z',\n",
       "     'start_time': '2019-07-16T19:53:48.531924Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.506710',\n",
       "     'end_time': '2019-09-12T23:18:38.544254',\n",
       "     'duration': 0.037544,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# technically correct\\n# pV_WC = np.einsum('ji,ik,jk->ijk', [pW_V, pV_C, inv_pW_C])\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.580168',\n",
       "     'end_time': '2019-09-12T23:18:38.611867',\n",
       "     'duration': 0.031699,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Here's why we don't actually need to represent the full 3D matrix:\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 28,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.558331Z',\n",
       "     'start_time': '2019-07-16T19:53:48.539117Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.644353',\n",
       "     'end_time': '2019-09-12T23:18:38.688377',\n",
       "     'duration': 0.044024,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': 'array([[    0,     1,     2, ..., 12814, 12815, 12816],\\n       [ 5700,  5682,  5683, ..., 11990, 11994, 11993]])'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': 'array([1., 1., 1., ..., 1., 1., 1.])'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245,)'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 13245)'},\n",
       "     'execution_count': 28}],\n",
       "   'source': 'pW_V.coords\\npW_V.data\\npW_V.data.shape\\npW_V.shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 29,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.566623Z',\n",
       "     'start_time': '2019-07-16T19:53:48.561009Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.727280',\n",
       "     'end_time': '2019-09-12T23:18:38.765915',\n",
       "     'duration': 0.038635,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# H(W|V) = 0\\nassert pW_V.data.shape[0] == pW_V.shape[1]\\nassert np.array_equal(pW_V.data, np.ones(pW_V.data.shape))'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.803076',\n",
       "     'end_time': '2019-09-12T23:18:38.836029',\n",
       "     'duration': 0.032953,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"**Recall:**\\n\\n$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nLet\\n - $d = p(V|C)$\\n - $w = p(W|V)$\\n - $m = p(W|C)$\\n - $o = p(V|W,C)$\\n\\n$o_{i,j,k} = \\\\frac{w_{j,i} d_{i,k}}{m_{j,k}}$\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.869087',\n",
       "     'end_time': '2019-09-12T23:18:38.901996',\n",
       "     'duration': 0.032909,\n",
       "     'status': 'completed'}},\n",
       "   'source': '$w_{j,i}$ is 1 only at a certain set of coordinates and zero everywhere else, so therefore $o$ is non-zero only at corresponding coordinates: for every $(j,i)$ s.t. $w_{j,i} = 1$, there is a column of $d$ we want to divide by a column of $m$\\n$$o_{i,j,:} = \\\\frac{d_{i,:}}{m_{j,:}}$$'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 30,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.577673Z',\n",
       "     'start_time': '2019-07-16T19:53:48.569377Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:38.935412',\n",
       "     'end_time': '2019-09-12T23:18:38.978189',\n",
       "     'duration': 0.042777,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 13245)'},\n",
       "     'execution_count': 30},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245, 4551)'},\n",
       "     'execution_count': 30}],\n",
       "   'source': 'pW_V.shape\\npV_C.shape\\n# numerator = pW_V @ pV_C\\n# numerator.shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 31,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.586193Z',\n",
       "     'start_time': '2019-07-16T19:53:48.580327Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.016383',\n",
       "     'end_time': '2019-09-12T23:18:39.057246',\n",
       "     'duration': 0.040863,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245, 12817, 4551)'},\n",
       "     'execution_count': 31}],\n",
       "   'source': 'pV_WC_shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 32,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.598710Z',\n",
       "     'start_time': '2019-07-16T19:53:48.588988Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.095185',\n",
       "     'end_time': '2019-09-12T23:18:39.140428',\n",
       "     'duration': 0.045243,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '13245'},\n",
       "     'execution_count': 32},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '12817'},\n",
       "     'execution_count': 32},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '4551'},\n",
       "     'execution_count': 32}],\n",
       "   'source': 'pV_WC_shape[0]\\npV_WC_shape[1]\\npV_WC_shape[2]'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.179584',\n",
       "     'end_time': '2019-09-12T23:18:39.213383',\n",
       "     'duration': 0.033799,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"The cell below calculates one 'column' of $k$s at a time (as a `numpy array`), given $i$ and $j$.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 33,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.613373Z',\n",
       "     'start_time': '2019-07-16T19:53:48.601634Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.247801',\n",
       "     'end_time': '2019-09-12T23:18:39.296762',\n",
       "     'duration': 0.048961,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'pV_WC_coords_j = pW_V.coords[0]\\npV_WC_coords_i = pW_V.coords[1]\\n\\nij_pairs = tuple(zip(*(pV_WC_coords_i, pV_WC_coords_j)))\\n# ij_pairs = zip(*(pV_WC_coords_i, pV_WC_coords_j))\\ndel pV_WC_coords_j\\ndel pV_WC_coords_i\\n\\ndef calc_layer(i,j):\\n    return pV_C[i] * inv_pW_C[j]\\n# def calc_layer(ij_pair):\\n#     i, j = ij_pair\\n#     return pV_C[i] * inv_pW_C[j]\\n\\n# lazy_calc_layer = delayed(calc_layer)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 34,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.622022Z',\n",
       "     'start_time': '2019-07-16T19:53:48.615829Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.335384',\n",
       "     'end_time': '2019-09-12T23:18:39.380920',\n",
       "     'duration': 0.045536,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '13245'},\n",
       "     'execution_count': 34}],\n",
       "   'source': 'len(ij_pairs)'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.420913',\n",
       "     'end_time': '2019-09-12T23:18:39.455365',\n",
       "     'duration': 0.034452,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"The cell below calculates - as a `sparse COO` array - one 'block' of all relevant $j$s and $k$s given a single $i$.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 35,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.734951Z',\n",
       "     'start_time': '2019-07-16T19:53:48.721446Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.490424',\n",
       "     'end_time': '2019-09-12T23:18:39.534913',\n",
       "     'duration': 0.044489,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 4551)'},\n",
       "     'execution_count': 35}],\n",
       "   'source': \"# there is one block of cells for every orthographic word v_i\\n#  spanning (num_segwords x num_contexts)\\n\\n# block_shape = (1, num_segWords, num_contexts)\\nblock_shape = (num_segWords, num_contexts)\\nblock_shape\\n\\n# each block is *sparse*\\n\\n# @delayed\\ndef block_constructor(i):\\n    my_js = (ij_pair[1] for ij_pair in ij_pairs if ij_pair[0] == i)\\n#     my_nonempty_layers = (((i,j), calc_layer(i,j)) for j in my_js)\\n    \\n    my_DOK = sparse.DOK(shape = block_shape,\\n                        dtype = 'float64',\\n                        fill_value = 0.0)\\n    for j in my_js:\\n        my_DOK[j,:] = calc_layer(i,j)\\n#         my_DOK[0,j,:] = calc_layer(i,j)\\n#         my_DOK[i,j,:] = calc_layer(i,j)\\n#         my_DOK[0,j,:] = lazy_calc_layer(i,j)\\n    my_COO = sparse.COO(my_DOK)\\n#     return (i, my_COO)\\n    return my_COO\\n\\n#     #this is ≥2x slower - unlike the method above, \\n#     #it doesn't use slice assignments.\\n#     my_DOK_data = {(i,j,k):val\\n#                    for k,val in enumerate(calc_layer(i,j))}\\n#     my_DOK = sparse.DOK(shape = (1, num_segWords, num_contexts),\\n#                         data = my_DOK_data,\\n#                         dtype = 'float64',\\n#                         fill_value = 0.0)\\n#     return sparse.COO(my_DOK)\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 36,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.042265Z',\n",
       "     'start_time': '2019-07-16T19:53:48.997896Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:39.575855',\n",
       "     'end_time': '2019-09-12T23:18:40.286587',\n",
       "     'duration': 0.710732,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import tiledb'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 37,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.056566Z',\n",
       "     'start_time': '2019-07-16T19:53:49.046578Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.327317',\n",
       "     'end_time': '2019-09-12T23:18:40.370826',\n",
       "     'duration': 0.043509,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(13245, 12817, 4551)'},\n",
       "     'execution_count': 37},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(12817, 4551)'},\n",
       "     'execution_count': 37}],\n",
       "   'source': 'pV_WC_shape\\nblock_shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 38,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.410457',\n",
       "     'end_time': '2019-09-12T23:18:40.451042',\n",
       "     'duration': 0.040585,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# #Two block sizes that make sense for reading:\\n# # |V| x 1 x 1 = each distribution/conditioning event is a block\\n# # |V| x |W| x 1 = each context is a block\\n# dom_read = tiledb.Domain(tiledb.Dim(name=\"orthWord\", domain=(0, 9410), tile=9411, dtype=np.uint32),\\n#                          tiledb.Dim(name=\"segWord\", domain=(0, 9171), tile=9172, dtype=np.uint32),\\n#                          tiledb.Dim(name=\"context\", domain=(0, 106294), tile=1, dtype=np.uint32))\\n\\n# schema_read = tiledb.ArraySchema(domain=dom_read, sparse=True,\\n#                                  attrs=[tiledb.Attr(name=\"O\", dtype=np.float64)])'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 39,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.068069Z',\n",
       "     'start_time': '2019-07-16T19:53:49.059212Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.492063',\n",
       "     'end_time': '2019-09-12T23:18:40.537365',\n",
       "     'duration': 0.045302,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#Two block sizes that make sense for reading:\\n# |V| x 1 x 1 = each distribution/conditioning event is a block\\n# |V| x |W| x 1 = each context is a block\\ndom_read = tiledb.Domain(tiledb.Dim(name=\"orthWord\", domain=(0, pV_WC_shape[0]-1), tile=pV_WC_shape[0], dtype=np.uint32),\\n                         tiledb.Dim(name=\"segWord\", domain=(0, pV_WC_shape[1]-1), tile=pV_WC_shape[1], dtype=np.uint32),\\n                         tiledb.Dim(name=\"context\", domain=(0, pV_WC_shape[2]-1), tile=1, dtype=np.uint32))\\n\\nschema_read = tiledb.ArraySchema(domain=dom_read, sparse=True,\\n                                 attrs=[tiledb.Attr(name=\"O\", dtype=np.float64)])'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 40,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-17T22:00:25.160693Z',\n",
       "     'start_time': '2019-07-17T22:00:25.014309Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.581379',\n",
       "     'end_time': '2019-09-12T23:18:40.621979',\n",
       "     'duration': 0.0406,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# %rm -r LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC_read'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 41,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.078900Z',\n",
       "     'start_time': '2019-07-16T19:53:49.070669Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.660279',\n",
       "     'end_time': '2019-09-12T23:18:40.707880',\n",
       "     'duration': 0.047601,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 41}],\n",
       "   'source': 'o\\n# array_name_read = o + \"_read\"\\narray_name_read = o\\ntiledb.SparseArray.create(array_name_read, schema_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 42,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.089602Z',\n",
       "     'start_time': '2019-07-16T19:53:49.081773Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.757792',\n",
       "     'end_time': '2019-09-12T23:18:40.803703',\n",
       "     'duration': 0.045911,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"def block_transformer(i, constructed_block_set_jk_tile):\\n    I = np.array([i] * constructed_block_set_jk_tile.coords.shape[1])\\n    J = constructed_block_set_jk_tile.coords[0]\\n    K = constructed_block_set_jk_tile.coords[1]\\n    data = constructed_block_set_jk_tile.data\\n    return (I, J, K, data)\\n\\n# def block_writer(block_tuple, arr_name):\\n#     with tiledb.SparseArray(arr_name, mode='w') as A:\\ndef block_writer(block_tuple, A):\\n# #     with tiledb.SparseArray(arr_name, mode='w') as A:\\n        I, J, K, data = block_tuple\\n        A[I, J, K] = data\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 43,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.099762Z',\n",
       "     'start_time': '2019-07-16T19:53:49.091983Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.842880',\n",
       "     'end_time': '2019-09-12T23:18:40.885218',\n",
       "     'duration': 0.042338,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'config = tiledb.Config()\\n# config[\"sm.consolidation.steps\"] = 10\\nconfig[\"sm.consolidation.steps\"] = 600\\nconfig[\"sm.consolidation.step_min_frags\"] = 2\\nconfig[\"sm.consolidation.step_max_frags\"] = 20\\n# tiledb.consolidate(array_name, config)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 44,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T18:12:41.866355Z',\n",
       "     'start_time': '2019-07-16T17:48:50.400105Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:40.923144',\n",
       "     'end_time': '2019-09-12T23:18:40.964897',\n",
       "     'duration': 0.041753,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# # Each i index corresponds to a block; each \"block set\" is a sequence of ≈ pV_WC_shape[0]/4 indices\\n# block_sets = np.array_split(np.arange(pV_WC_shape[0]), 4)\\n\\n\\n# with tiledb.SparseArray(array_name_read, mode=\\'w\\') as A:\\n#     for block_set in tqdm(block_sets):\\n#         constructed_blocks = par(jl.delayed(block_constructor)(i) for i in block_set)\\n\\n#         block_tuples = jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_transformer)(i, constructed_block_tile)\\n#                                                                                             for i, constructed_block_tile in zip(block_set, constructed_blocks))\\n\\n#         jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_writer)(block_tuple, A) \\n#                                                                              for block_tuple in block_tuples);\\n# #         tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 45,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:40:39.820093Z',\n",
       "     'start_time': '2019-07-16T19:14:45.356814Z'},\n",
       "    'scrolled': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:18:41.003778',\n",
       "     'end_time': '2019-09-12T23:19:53.010059',\n",
       "     'duration': 72.006281,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r  0%|          | 0/4 [00:00<?, ?it/s]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0845s.) Setting batch_size=4.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:    0.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed:    2.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:    3.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:    3.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:    4.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1236 tasks      | elapsed:    4.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1352 tasks      | elapsed:    5.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1476 tasks      | elapsed:    5.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1600 tasks      | elapsed:    5.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1732 tasks      | elapsed:    6.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1864 tasks      | elapsed:    6.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2004 tasks      | elapsed:    7.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2144 tasks      | elapsed:    7.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2292 tasks      | elapsed:    8.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2440 tasks      | elapsed:    8.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2596 tasks      | elapsed:    9.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2752 tasks      | elapsed:    9.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2916 tasks      | elapsed:   10.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3312 out of 3312 | elapsed:   11.8s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    1.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3312 out of 3312 | elapsed:    1.7s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    1.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    1.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    2.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    2.5s\\n[Parallel(n_jobs=-1)]: Done 3312 out of 3312 | elapsed:    2.5s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 25%|██▌       | 1/4 [00:19<00:59, 19.80s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0796s.) Setting batch_size=4.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:    0.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    2.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:    2.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed:    2.9s\\n[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:    3.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:    3.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:    4.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1236 tasks      | elapsed:    4.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1352 tasks      | elapsed:    5.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1476 tasks      | elapsed:    5.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1600 tasks      | elapsed:    6.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1732 tasks      | elapsed:    6.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1864 tasks      | elapsed:    6.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2004 tasks      | elapsed:    7.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2144 tasks      | elapsed:    7.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2292 tasks      | elapsed:    8.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2440 tasks      | elapsed:    8.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2596 tasks      | elapsed:    9.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2752 tasks      | elapsed:   10.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2916 tasks      | elapsed:   10.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3248 out of 3311 | elapsed:   11.8s remaining:    0.2s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:   11.9s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    1.6s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    1.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    2.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    2.4s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 50%|█████     | 2/4 [00:36<00:37, 18.95s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.1031s.) Setting batch_size=2.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:    0.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 292 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 484 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 538 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 592 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 650 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 708 tasks      | elapsed:    2.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 770 tasks      | elapsed:    2.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:    3.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 898 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 964 tasks      | elapsed:    3.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1034 tasks      | elapsed:    3.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed:    4.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1178 tasks      | elapsed:    4.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1252 tasks      | elapsed:    4.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1330 tasks      | elapsed:    4.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1408 tasks      | elapsed:    5.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1490 tasks      | elapsed:    5.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1572 tasks      | elapsed:    5.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1658 tasks      | elapsed:    6.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1744 tasks      | elapsed:    6.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1834 tasks      | elapsed:    6.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1924 tasks      | elapsed:    7.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2018 tasks      | elapsed:    7.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2112 tasks      | elapsed:    7.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2210 tasks      | elapsed:    8.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2308 tasks      | elapsed:    8.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2410 tasks      | elapsed:    8.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2512 tasks      | elapsed:    9.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2618 tasks      | elapsed:    9.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2724 tasks      | elapsed:    9.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2834 tasks      | elapsed:   10.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2944 tasks      | elapsed:   10.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3058 tasks      | elapsed:   11.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3172 tasks      | elapsed:   11.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3248 out of 3311 | elapsed:   11.8s remaining:    0.2s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:   11.9s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    1.6s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    1.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    2.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    2.5s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    2.6s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    2.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    2.7s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    2.9s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 75%|███████▌  | 3/4 [00:54<00:18, 18.51s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0981s.) Setting batch_size=4.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:    0.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 520 tasks      | elapsed:    2.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:    2.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed:    2.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:    3.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    3.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:    3.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:    4.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1236 tasks      | elapsed:    4.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1352 tasks      | elapsed:    5.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1476 tasks      | elapsed:    5.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1600 tasks      | elapsed:    6.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1732 tasks      | elapsed:    6.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1864 tasks      | elapsed:    6.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2004 tasks      | elapsed:    7.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2144 tasks      | elapsed:    7.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2292 tasks      | elapsed:    8.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2440 tasks      | elapsed:    9.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2596 tasks      | elapsed:    9.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2752 tasks      | elapsed:   10.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2916 tasks      | elapsed:   10.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3248 out of 3311 | elapsed:   11.9s remaining:    0.2s\\n[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:   12.0s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    1.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    1.6s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1618 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 1677 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 1797 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 1921 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 1984 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 2049 tasks      | elapsed:    1.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2114 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 2181 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2248 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 2317 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 2457 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2528 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 2601 tasks      | elapsed:    2.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2674 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 2749 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 2824 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 2901 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:    2.5s\\n[Parallel(n_jobs=-1)]: Done 3057 tasks      | elapsed:    2.5s\\n[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    2.6s\\n[Parallel(n_jobs=-1)]: Done 3217 tasks      | elapsed:    2.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 3311 out of 3311 | elapsed:    2.7s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r100%|██████████| 4/4 [01:11<00:00, 18.25s/it]'},\n",
       "    {'output_type': 'stream', 'name': 'stderr', 'text': '\\n'}],\n",
       "   'source': '# Each i index corresponds to a block; each \"block set\" is a sequence of ≈ pV_WC_shape[0]/4 indices\\nblock_sets = np.array_split(np.arange(pV_WC_shape[0]), 4)\\n\\nfor block_set in tqdm(block_sets):\\n    constructed_blocks = par(jl.delayed(block_constructor)(i) for i in block_set)\\n\\n    block_tuples = jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_transformer)(i, constructed_block_tile)\\n                                                                                        for i, constructed_block_tile in zip(block_set, constructed_blocks))\\n    with tiledb.SparseArray(array_name_read, mode=\\'w\\') as A:\\n        jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_writer)(block_tuple, A) \\n                                                                             for block_tuple in block_tuples);\\n    tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 46,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.091792',\n",
       "     'end_time': '2019-09-12T23:19:53.194439',\n",
       "     'duration': 0.102647,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 46}],\n",
       "   'source': '# this last consolidation step should clean up any remaining fragments\\ntiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 47,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:57.922578Z',\n",
       "     'start_time': '2019-07-16T19:53:57.690268Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.282095',\n",
       "     'end_time': '2019-09-12T23:19:53.368290',\n",
       "     'duration': 0.086195,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# from random import choice\\n\\n# def randomCoords(arr_shape):\\n#     return tuple(np.random.randint(0,arr_shape[dim]) for dim in range(len(arr_shape)))\\n\\n# numRandomCoords = 10000\\n\\n# test_coords = [randomCoords(pV_WC_shape) for each in range(numRandomCoords)]'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 48,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:49:34.020498Z',\n",
       "     'start_time': '2019-07-16T19:44:44.071291Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.453158',\n",
       "     'end_time': '2019-09-12T23:19:53.546116',\n",
       "     'duration': 0.092958,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# %%timeit\\n\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n#     rand_coords = choice(test_coords)\\n# #     rand_coords\\n#     A[rand_coords]\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 49,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:59.764035Z',\n",
       "     'start_time': '2019-07-16T19:53:59.755300Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.633918',\n",
       "     'end_time': '2019-09-12T23:19:53.725364',\n",
       "     'duration': 0.091446,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# config'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 50,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T20:12:47.404606Z',\n",
       "     'start_time': '2019-07-16T20:12:47.396864Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.814161',\n",
       "     'end_time': '2019-09-12T23:19:53.904568',\n",
       "     'duration': 0.090407,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# config[\"sm.consolidation.steps\"] = 200'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 51,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:05:55.774087Z',\n",
       "     'start_time': '2019-07-16T20:22:34.864067Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:53.988254',\n",
       "     'end_time': '2019-09-12T23:19:54.075236',\n",
       "     'duration': 0.086982,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#10 steps, min_frags=2, max_frags=20 => ~1m20s per cell run, w/ <=200 fewer fragments per cell run\\n# => estimate of 16.66 min per 2k fragments\\n#80 steps, min_frags=2, max_frags=20 => ~4m7s per cell run\\n#100 steps, min_frags=2, max_frags=20 => ~4m53s per cell run, w/ 1900 fewer fragments per cell run\\n#200 steps, min_frags=2, max_frags=20 => ~8m20s per cell run, w/ 3800 fewer fragments per cell run\\n# tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 52,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:54.159909',\n",
       "     'end_time': '2019-09-12T23:19:54.250216',\n",
       "     'duration': 0.090307,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# %ls -l LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC_read/'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 53,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:07:11.463641Z',\n",
       "     'start_time': '2019-07-16T21:07:08.440777Z'},\n",
       "    'scrolled': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:54.334361',\n",
       "     'end_time': '2019-09-12T23:19:54.426173',\n",
       "     'duration': 0.091812,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# %%timeit\\n\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n#     rand_coords = choice(test_coords)\\n# #     rand_coords\\n#     A[rand_coords]\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:54.512077',\n",
       "     'end_time': '2019-09-12T23:19:54.596717',\n",
       "     'duration': 0.08464,\n",
       "     'status': 'completed'}},\n",
       "   'source': 'Consolidation of fragments leads to a nearly 1000x speed-up.'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 54,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:42:10.287333Z',\n",
       "     'start_time': '2019-07-16T21:42:10.278107Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:54.680844',\n",
       "     'end_time': '2019-09-12T23:19:54.780254',\n",
       "     'duration': 0.09941,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '12817'},\n",
       "     'execution_count': 54}],\n",
       "   'source': 'num_segWords'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 55,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:54.868231',\n",
       "     'end_time': '2019-09-12T23:19:55.035553',\n",
       "     'duration': 0.167322,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'name': 'stdout',\n",
       "     'text': 'Wrote metadata for \\n\\tLD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC\\n to \\n\\tLD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC_metadata.json\\n'}],\n",
       "   'source': 'pV_WC_md = {\\n    \\'V\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{d}\\\\n\\\\t{w}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_orthWords},\\n    \\'W\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{w}\\\\n\\\\t{m}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_segWords},\\n    \\'C\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{d}\\\\n\\\\t{m}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_contexts}\\n}\\n\\nmy_fp = o\\nexportMatrixMetadata(my_fp + \\'_metadata.json\\',\\n                     my_fp,\\n                     None,\\n                     pV_WC_md,\\n                     \\'Step 5a\\',\\n                     \\'Calculate orthographic posterior given segmental wordform + context\\',\\n                     {})'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 56,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-06-03T22:49:20.766312Z',\n",
       "     'start_time': '2019-06-03T22:49:20.730490Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:55.124960',\n",
       "     'end_time': '2019-09-12T23:19:55.253566',\n",
       "     'duration': 0.128606,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"['LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.hV_C_metadata.json',\\n 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC_metadata.json',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C_metadata.json',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC',\\n 'Producing Fisher vocab in NXT swbd preceding contexts 2gram model contextual distributions.ipynb',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pW_C_metadata.json',\\n 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.pV_C',\\n 'Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 2gram model.ipynb',\\n 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.pV_C_metadata.json',\\n 'Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 2gram model.ipynb',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC_metadata.json',\\n 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C_metadata.json',\\n 'Filter LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model against LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.ipynb',\\n 'fisher_vocabulary_main.txt',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC',\\n 'Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered in NXT swbd preceding contexts 2gram model.ipynb',\\n 'Calculate segmental wordform distribution for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim in NXT swbd preceding contexts 2gram model.ipynb',\\n 'LM_filtered_nxt_swbd_contexts_preceding_1_filtered.txt',\\n 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model.hV_C',\\n 'nxt_swbd_contexts_preceding_1_filtered.txt',\\n 'LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy']\"},\n",
       "     'execution_count': 56}],\n",
       "   'source': 'listdir(output_dir)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 57,\n",
       "   'metadata': {'ExecuteTime': {'start_time': '2019-07-16T21:45:56.446Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:55.344667',\n",
       "     'end_time': '2019-09-12T23:19:55.436586',\n",
       "     'duration': 0.091919,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# # pV_WC_temp = None\\n# temp_coords = None\\n# temp_data = None\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n# #     rand_coords = choice(test_coords)\\n# # #     rand_coords\\n# #     A[rand_coords]\\n# #     pV_WC_temp = A[:,:,:]\\n#     A[:,:,:]\\n#     temp_coords = A[:,:,:]['coords']\\n#     temp_data = A[:,:,:]['data']\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:55.525168',\n",
       "     'end_time': '2019-09-12T23:19:55.609849',\n",
       "     'duration': 0.084681,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': ''}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'name': 'python',\n",
       "   'version': '3.7.4',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'file_extension': '.py'},\n",
       "  'notify_time': '0',\n",
       "  'toc': {'base_numbering': 1,\n",
       "   'nav_menu': {},\n",
       "   'number_sections': True,\n",
       "   'sideBar': True,\n",
       "   'skip_h1_title': False,\n",
       "   'title_cell': 'Table of Contents',\n",
       "   'title_sidebar': 'Contents',\n",
       "   'toc_cell': True,\n",
       "   'toc_position': {},\n",
       "   'toc_section_display': True,\n",
       "   'toc_window_display': True},\n",
       "  'papermill': {'parameters': {'d': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       "    'w': 'LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       "    'm': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pW_C.npy',\n",
       "    'o': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_preceding_contexts_2gram_model.pV_WC'},\n",
       "   'environment_variables': {},\n",
       "   'version': '1.0.1',\n",
       "   'input_path': 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       "   'output_path': 'LD_Fisher_vocab_in_NXT_swbd_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd preceding contexts 2gram model.ipynb',\n",
       "   'start_time': '2019-09-12T23:18:30.591344',\n",
       "   'end_time': '2019-09-12T23:19:56.200824',\n",
       "   'duration': 85.60948,\n",
       "   'exception': None}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 2}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End  @ 16:19:56\n",
      "\n",
      "\n",
      "Start  @ 16:19:56\n",
      "Running notebook:\n",
      "\tCalculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 2gram model.ipynb\n",
      "Output directory:\n",
      "\tLD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model\n",
      "Arguments:\n",
      "{\n",
      " \"d\": \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy\",\n",
      " \"w\": \"LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz\",\n",
      " \"m\": \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy\",\n",
      " \"o\": \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564d6a307bd546909867ac0b6d8b6423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'code',\n",
       "   'execution_count': 1,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.337260Z',\n",
       "     'start_time': '2019-07-16T19:53:06.311888Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.503315',\n",
       "     'end_time': '2019-09-12T23:19:57.549410',\n",
       "     'duration': 0.046095,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#Prints **all** console output, not just last item in cell \\nfrom IPython.core.interactiveshell import InteractiveShell\\nInteractiveShell.ast_node_interactivity = \"all\"'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.582553',\n",
       "     'end_time': '2019-09-12T23:19:57.611556',\n",
       "     'duration': 0.029003,\n",
       "     'status': 'completed'}},\n",
       "   'source': '**Eric Meinhardt / emeinhardt@ucsd.edu**'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'toc': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.640058',\n",
       "     'end_time': '2019-09-12T23:19:57.668440',\n",
       "     'duration': 0.028382,\n",
       "     'status': 'completed'}},\n",
       "   'source': '<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Usage</a></span></li></ul></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Imports-/-load-data\" data-toc-modified-id=\"Imports-/-load-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports / load data</a></span></li><li><span><a href=\"#Main-calculation\" data-toc-modified-id=\"Main-calculation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main calculation</a></span></li></ul></div>'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.698545',\n",
       "     'end_time': '2019-09-12T23:19:57.727768',\n",
       "     'duration': 0.029223,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Overview'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.756508',\n",
       "     'end_time': '2019-09-12T23:19:57.784556',\n",
       "     'duration': 0.028048,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Given\\n  - a filepath $d$ to an `.npy` file defining $p(V|C)$ a distribution on orthographic wordforms $V$ given (orthographic) $n$-gram context $c \\\\in C$\\n  - a filepath $w$ to an `.npz` file defining $p(W|V)$ a distribution on full segmental wordforms $W$ given an orthographic wordform $v \\\\in V$\\n  - a filepath $m$ to an `.npy` file defining $p(W|C)$ a distribution on full segmental wordforms $W$ given an (orthographic) $n$-gram context $c \\\\in C$\\n  - an output filepath prefix $o$\\n\\nthis notebook calculates $p(V|W,C)$, i.e.\\n\\n$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nand exports it to $o$ as a `tiledb` sparse array.\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.813621',\n",
       "     'end_time': '2019-09-12T23:19:57.842345',\n",
       "     'duration': 0.028724,\n",
       "     'status': 'completed'}},\n",
       "   'source': '## Requirements'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.871559',\n",
       "     'end_time': '2019-09-12T23:19:57.900273',\n",
       "     'duration': 0.028714,\n",
       "     'status': 'completed'}},\n",
       "   'source': ' - `numpy`\\n - the `pydata` `sparse` package\\n - the `tiledb` package for high-dimensional, out-of-core, sparse matrix storage'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.929559',\n",
       "     'end_time': '2019-09-12T23:19:57.958717',\n",
       "     'duration': 0.029158,\n",
       "     'status': 'completed'}},\n",
       "   'source': '## Usage'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.438578Z',\n",
       "     'start_time': '2019-07-16T19:53:06.343693Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:57.988132',\n",
       "     'end_time': '2019-09-12T23:19:58.022824',\n",
       "     'duration': 0.034692,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#FIXME'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.055987',\n",
       "     'end_time': '2019-09-12T23:19:58.088774',\n",
       "     'duration': 0.032787,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Parameters'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 3,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.446673Z',\n",
       "     'start_time': '2019-07-16T19:53:06.442171Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.129789',\n",
       "     'end_time': '2019-09-12T23:19:58.388557',\n",
       "     'duration': 0.258768,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from os import getcwd, chdir, listdir, path, mkdir, makedirs'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 4,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.748859Z',\n",
       "     'start_time': '2019-07-16T19:53:06.449718Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.423206',\n",
       "     'end_time': '2019-09-12T23:19:58.694608',\n",
       "     'duration': 0.271402,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from boilerplate import *'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 5,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.757658Z',\n",
       "     'start_time': '2019-07-16T19:53:06.752232Z'},\n",
       "    'tags': ['parameters'],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.727781',\n",
       "     'end_time': '2019-09-12T23:19:58.763780',\n",
       "     'duration': 0.035999,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# Parameters\\n\\nd = ''\\n# d = 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy'\\n\\nw = ''\\n# w = 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz'\\n\\nm = ''\\n# m = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy'\\n\\no = ''\\n# o = 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pV_WC'\"},\n",
       "  {'cell_type': 'code',\n",
       "   'metadata': {'tags': ['injected-parameters'],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.801018',\n",
       "     'end_time': '2019-09-12T23:19:58.835580',\n",
       "     'duration': 0.034562,\n",
       "     'status': 'completed'}},\n",
       "   'execution_count': 6,\n",
       "   'source': '# Parameters\\nd = \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy\"\\nw = \"LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz\"\\nm = \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy\"\\no = \"LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC\"\\n',\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 7,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.768566Z',\n",
       "     'start_time': '2019-07-16T19:53:06.760673Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.867614',\n",
       "     'end_time': '2019-09-12T23:19:58.901084',\n",
       "     'duration': 0.03347,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# Parameters\\n# d = \"LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C.npy\"\\n# w = \"LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz\"\\n# m = \"LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C.npy\"\\n# o = \"LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC\"\\n'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 8,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.780252Z',\n",
       "     'start_time': '2019-07-16T19:53:06.771566Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:58.933695',\n",
       "     'end_time': '2019-09-12T23:19:58.970278',\n",
       "     'duration': 0.036583,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'output_dir = path.dirname(o)\\nensure_dir_exists(output_dir)'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:59.002682',\n",
       "     'end_time': '2019-09-12T23:19:59.030310',\n",
       "     'duration': 0.027628,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Imports / load data'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 9,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.791359Z',\n",
       "     'start_time': '2019-07-16T19:53:06.786585Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:59.058796',\n",
       "     'end_time': '2019-09-12T23:19:59.094111',\n",
       "     'duration': 0.035315,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from itertools import starmap, chain'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 10,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:06.799175Z',\n",
       "     'start_time': '2019-07-16T19:53:06.794637Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:59.127476',\n",
       "     'end_time': '2019-09-12T23:19:59.161433',\n",
       "     'duration': 0.033957,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import numpy as np'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 11,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.024809Z',\n",
       "     'start_time': '2019-07-16T19:53:06.819749Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:19:59.193872',\n",
       "     'end_time': '2019-09-12T23:20:00.067147',\n",
       "     'duration': 0.873275,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import sparse'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 12,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.045352Z',\n",
       "     'start_time': '2019-07-16T19:53:08.028339Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.101843',\n",
       "     'end_time': '2019-09-12T23:20:00.144234',\n",
       "     'duration': 0.042391,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'from tqdm import tqdm, tqdm_notebook, tqdm_gui'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 13,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:08.055475Z',\n",
       "     'start_time': '2019-07-16T19:53:08.048236Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.178047',\n",
       "     'end_time': '2019-09-12T23:20:00.216069',\n",
       "     'duration': 0.038022,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"#from joblib import Parallel, delayed\\nimport joblib as jl\\n\\nJ = -1\\nBACKEND = 'multiprocessing'\\n# BACKEND = 'loky'\\nV = 10\\nPREFER = 'processes'\\n# PREFER = 'threads'\\n\\ndef identity(x):\\n    return x\\n\\ndef par(gen_expr):\\n    return jl.Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 14,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:15.443845Z',\n",
       "     'start_time': '2019-07-16T19:53:08.058691Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.247620',\n",
       "     'end_time': '2019-09-12T23:20:00.343327',\n",
       "     'duration': 0.095707,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575, 2425)'},\n",
       "     'execution_count': 14},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 14},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.127555'},\n",
       "     'execution_count': 14}],\n",
       "   'source': 'pV_C = np.load(d)\\npV_C.shape\\npV_C.dtype\\npV_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 15,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.061927Z',\n",
       "     'start_time': '2019-07-16T19:53:15.447121Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.377775',\n",
       "     'end_time': '2019-09-12T23:20:00.440502',\n",
       "     'duration': 0.062727,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pV_C, axis=0), np.ones(shape = np.sum(pV_C, axis=0).shape)  )'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 16,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.084381Z',\n",
       "     'start_time': '2019-07-16T19:53:21.065462Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.475354',\n",
       "     'end_time': '2019-09-12T23:20:00.531921',\n",
       "     'duration': 0.056567,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 6575)'},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.0001578'},\n",
       "     'execution_count': 16},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.0001561524047470331'},\n",
       "     'execution_count': 16}],\n",
       "   'source': 'pW_V = sparse.load_npz(w)\\npW_V.shape\\npW_V.dtype\\npW_V.nbytes / 1e9\\npW_V.density'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 17,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:21.715481Z',\n",
       "     'start_time': '2019-07-16T19:53:21.088053Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.566500',\n",
       "     'end_time': '2019-09-12T23:20:00.928398',\n",
       "     'duration': 0.361898,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pW_V, axis=0).todense(), np.ones(shape = np.sum(pW_V, axis=0).todense().shape)  )'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 18,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:29.343027Z',\n",
       "     'start_time': '2019-07-16T19:53:21.718649Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:00.958870',\n",
       "     'end_time': '2019-09-12T23:20:01.040915',\n",
       "     'duration': 0.082045,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 2425)'},\n",
       "     'execution_count': 18},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"dtype('float64')\"},\n",
       "     'execution_count': 18},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.1242376'},\n",
       "     'execution_count': 18}],\n",
       "   'source': '#takes <=1 min\\npW_C = np.load(m)\\npW_C.shape\\npW_C.dtype\\npW_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 19,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.590331Z',\n",
       "     'start_time': '2019-07-16T19:53:29.346649Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.075049',\n",
       "     'end_time': '2019-09-12T23:20:01.140453',\n",
       "     'duration': 0.065404,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert np.allclose( np.sum(pW_C, axis=0), np.ones(shape = np.sum(pW_C, axis=0).shape)  )'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.178276',\n",
       "     'end_time': '2019-09-12T23:20:01.208920',\n",
       "     'duration': 0.030644,\n",
       "     'status': 'completed'}},\n",
       "   'source': '# Main calculation'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.239547',\n",
       "     'end_time': '2019-09-12T23:20:01.270065',\n",
       "     'duration': 0.030518,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nLet\\n - $d = p(V|C)$\\n - $w = p(W|V)$\\n - $m = p(W|C)$\\n - $o = p(V|W,C)$\\n\\n$o_{i,j,k} = \\\\frac{w_{j,i} d_{i,k}}{m_{j,k}}$\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 20,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.599447Z',\n",
       "     'start_time': '2019-07-16T19:53:34.593802Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.301525',\n",
       "     'end_time': '2019-09-12T23:20:01.339253',\n",
       "     'duration': 0.037728,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'num_orthWords, num_contexts = pV_C.shape\\nnum_segWords, num_orthWords2 = pW_V.shape\\nassert num_orthWords == num_orthWords2\\nnum_segWords2, num_contexts2 = pW_C.shape\\nassert num_segWords == num_segWords2\\nassert num_contexts == num_contexts'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 21,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.619226Z',\n",
       "     'start_time': '2019-07-16T19:53:34.602244Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.373328',\n",
       "     'end_time': '2019-09-12T23:20:01.415411',\n",
       "     'duration': 0.042083,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575, 6404, 2425)'},\n",
       "     'execution_count': 21},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'102,107,777,500'\"},\n",
       "     'execution_count': 21},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'816.86222 GB'\"},\n",
       "     'execution_count': 21}],\n",
       "   'source': 'pV_WC_shape = (num_orthWords, num_segWords, num_contexts)\\npV_WC_shape\\nnum_cells = np.prod(pV_WC_shape); f\"{num_cells:,}\"\\nn_GB = (num_cells * 8) / 1e9; f\"{n_GB:,} GB\"'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 22,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.633386Z',\n",
       "     'start_time': '2019-07-16T19:53:34.627860Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.451124',\n",
       "     'end_time': '2019-09-12T23:20:01.491346',\n",
       "     'duration': 0.040222,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575,)'},\n",
       "     'execution_count': 22}],\n",
       "   'source': 'pW_V.data.shape'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.529529',\n",
       "     'end_time': '2019-09-12T23:20:01.560629',\n",
       "     'duration': 0.0311,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Given how large this number is, we *can't* calculate this naively, and given that $p(W|V)$ is *incredibly* sparse, we definitely don't *need* to.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 23,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:34.642853Z',\n",
       "     'start_time': '2019-07-16T19:53:34.637242Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.592860',\n",
       "     'end_time': '2019-09-12T23:20:01.632200',\n",
       "     'duration': 0.03934,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# def pV_WC_calc(v, w, c):\\n#     i = Vs_t\\n#     return pV_WC_calc_np(i,j,k)\\n\\ndef pV_WC_calc_np(i,j,k):\\n    numerator = pW_V[j,i]* pV_C[i,k]\\n    denominator = pW_C[j,k]\\n    return numerator / denominator'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 24,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.191796Z',\n",
       "     'start_time': '2019-07-16T19:53:34.645849Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.670762',\n",
       "     'end_time': '2019-09-12T23:20:01.760412',\n",
       "     'duration': 0.08965,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'assert not np.any(pW_C == 0) # if there are no 0s in pW_C, then we can take the element-wise inverse without anything blowing up\\ninv_pW_C = 1.0 / pW_C'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 25,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.203932Z',\n",
       "     'start_time': '2019-07-16T19:53:48.195411Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.798670',\n",
       "     'end_time': '2019-09-12T23:20:01.838652',\n",
       "     'duration': 0.039982,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 2425)'},\n",
       "     'execution_count': 25},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '0.1242376'},\n",
       "     'execution_count': 25}],\n",
       "   'source': 'inv_pW_C.shape\\ninv_pW_C.nbytes / 1e9'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 26,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.528451Z',\n",
       "     'start_time': '2019-07-16T19:53:48.206654Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.875173',\n",
       "     'end_time': '2019-09-12T23:20:01.920548',\n",
       "     'duration': 0.045375,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'del pW_C'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 27,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.536197Z',\n",
       "     'start_time': '2019-07-16T19:53:48.531924Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:01.959595',\n",
       "     'end_time': '2019-09-12T23:20:01.997630',\n",
       "     'duration': 0.038035,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# technically correct\\n# pV_WC = np.einsum('ji,ik,jk->ijk', [pW_V, pV_C, inv_pW_C])\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.033140',\n",
       "     'end_time': '2019-09-12T23:20:02.064868',\n",
       "     'duration': 0.031728,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"Here's why we don't actually need to represent the full 3D matrix:\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 28,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.558331Z',\n",
       "     'start_time': '2019-07-16T19:53:48.539117Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.097597',\n",
       "     'end_time': '2019-09-12T23:20:02.141902',\n",
       "     'duration': 0.044305,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': 'array([[   0,    1,    2, ..., 6401, 6402, 6403],\\n       [2889, 2885, 2884, ..., 5935, 5943, 5869]])'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': 'array([1., 1., 1., ..., 1., 1., 1.])'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575,)'},\n",
       "     'execution_count': 28},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 6575)'},\n",
       "     'execution_count': 28}],\n",
       "   'source': 'pW_V.coords\\npW_V.data\\npW_V.data.shape\\npW_V.shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 29,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.566623Z',\n",
       "     'start_time': '2019-07-16T19:53:48.561009Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.179380',\n",
       "     'end_time': '2019-09-12T23:20:02.219415',\n",
       "     'duration': 0.040035,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# H(W|V) = 0\\nassert pW_V.data.shape[0] == pW_V.shape[1]\\nassert np.array_equal(pW_V.data, np.ones(pW_V.data.shape))'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.256220',\n",
       "     'end_time': '2019-09-12T23:20:02.289422',\n",
       "     'duration': 0.033202,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"**Recall:**\\n\\n$$p(\\\\hat{V} = v^*|\\\\hat{X}_0^f = x_0^{'f}, c) = \\\\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$$\\n\\nLet\\n - $d = p(V|C)$\\n - $w = p(W|V)$\\n - $m = p(W|C)$\\n - $o = p(V|W,C)$\\n\\n$o_{i,j,k} = \\\\frac{w_{j,i} d_{i,k}}{m_{j,k}}$\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.322421',\n",
       "     'end_time': '2019-09-12T23:20:02.355142',\n",
       "     'duration': 0.032721,\n",
       "     'status': 'completed'}},\n",
       "   'source': '$w_{j,i}$ is 1 only at a certain set of coordinates and zero everywhere else, so therefore $o$ is non-zero only at corresponding coordinates: for every $(j,i)$ s.t. $w_{j,i} = 1$, there is a column of $d$ we want to divide by a column of $m$\\n$$o_{i,j,:} = \\\\frac{d_{i,:}}{m_{j,:}}$$'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 30,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.577673Z',\n",
       "     'start_time': '2019-07-16T19:53:48.569377Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.388773',\n",
       "     'end_time': '2019-09-12T23:20:02.431281',\n",
       "     'duration': 0.042508,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 6575)'},\n",
       "     'execution_count': 30},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575, 2425)'},\n",
       "     'execution_count': 30}],\n",
       "   'source': 'pW_V.shape\\npV_C.shape\\n# numerator = pW_V @ pV_C\\n# numerator.shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 31,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.586193Z',\n",
       "     'start_time': '2019-07-16T19:53:48.580327Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.467628',\n",
       "     'end_time': '2019-09-12T23:20:02.509896',\n",
       "     'duration': 0.042268,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575, 6404, 2425)'},\n",
       "     'execution_count': 31}],\n",
       "   'source': 'pV_WC_shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 32,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.598710Z',\n",
       "     'start_time': '2019-07-16T19:53:48.588988Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.549289',\n",
       "     'end_time': '2019-09-12T23:20:02.594961',\n",
       "     'duration': 0.045672,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '6575'},\n",
       "     'execution_count': 32},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '6404'},\n",
       "     'execution_count': 32},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '2425'},\n",
       "     'execution_count': 32}],\n",
       "   'source': 'pV_WC_shape[0]\\npV_WC_shape[1]\\npV_WC_shape[2]'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.633680',\n",
       "     'end_time': '2019-09-12T23:20:02.667728',\n",
       "     'duration': 0.034048,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"The cell below calculates one 'column' of $k$s at a time (as a `numpy array`), given $i$ and $j$.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 33,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.613373Z',\n",
       "     'start_time': '2019-07-16T19:53:48.601634Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.702775',\n",
       "     'end_time': '2019-09-12T23:20:02.748670',\n",
       "     'duration': 0.045895,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'pV_WC_coords_j = pW_V.coords[0]\\npV_WC_coords_i = pW_V.coords[1]\\n\\nij_pairs = tuple(zip(*(pV_WC_coords_i, pV_WC_coords_j)))\\n# ij_pairs = zip(*(pV_WC_coords_i, pV_WC_coords_j))\\ndel pV_WC_coords_j\\ndel pV_WC_coords_i\\n\\ndef calc_layer(i,j):\\n    return pV_C[i] * inv_pW_C[j]\\n# def calc_layer(ij_pair):\\n#     i, j = ij_pair\\n#     return pV_C[i] * inv_pW_C[j]\\n\\n# lazy_calc_layer = delayed(calc_layer)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 34,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.622022Z',\n",
       "     'start_time': '2019-07-16T19:53:48.615829Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.786129',\n",
       "     'end_time': '2019-09-12T23:20:02.827541',\n",
       "     'duration': 0.041412,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '6575'},\n",
       "     'execution_count': 34}],\n",
       "   'source': 'len(ij_pairs)'},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.867895',\n",
       "     'end_time': '2019-09-12T23:20:02.902749',\n",
       "     'duration': 0.034854,\n",
       "     'status': 'completed'}},\n",
       "   'source': \"The cell below calculates - as a `sparse COO` array - one 'block' of all relevant $j$s and $k$s given a single $i$.\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 35,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:48.734951Z',\n",
       "     'start_time': '2019-07-16T19:53:48.721446Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:02.937312',\n",
       "     'end_time': '2019-09-12T23:20:02.986094',\n",
       "     'duration': 0.048782,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 2425)'},\n",
       "     'execution_count': 35}],\n",
       "   'source': \"# there is one block of cells for every orthographic word v_i\\n#  spanning (num_segwords x num_contexts)\\n\\n# block_shape = (1, num_segWords, num_contexts)\\nblock_shape = (num_segWords, num_contexts)\\nblock_shape\\n\\n# each block is *sparse*\\n\\n# @delayed\\ndef block_constructor(i):\\n    my_js = (ij_pair[1] for ij_pair in ij_pairs if ij_pair[0] == i)\\n#     my_nonempty_layers = (((i,j), calc_layer(i,j)) for j in my_js)\\n    \\n    my_DOK = sparse.DOK(shape = block_shape,\\n                        dtype = 'float64',\\n                        fill_value = 0.0)\\n    for j in my_js:\\n        my_DOK[j,:] = calc_layer(i,j)\\n#         my_DOK[0,j,:] = calc_layer(i,j)\\n#         my_DOK[i,j,:] = calc_layer(i,j)\\n#         my_DOK[0,j,:] = lazy_calc_layer(i,j)\\n    my_COO = sparse.COO(my_DOK)\\n#     return (i, my_COO)\\n    return my_COO\\n\\n#     #this is ≥2x slower - unlike the method above, \\n#     #it doesn't use slice assignments.\\n#     my_DOK_data = {(i,j,k):val\\n#                    for k,val in enumerate(calc_layer(i,j))}\\n#     my_DOK = sparse.DOK(shape = (1, num_segWords, num_contexts),\\n#                         data = my_DOK_data,\\n#                         dtype = 'float64',\\n#                         fill_value = 0.0)\\n#     return sparse.COO(my_DOK)\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 36,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.042265Z',\n",
       "     'start_time': '2019-07-16T19:53:48.997896Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.026377',\n",
       "     'end_time': '2019-09-12T23:20:03.091102',\n",
       "     'duration': 0.064725,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'import tiledb'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 37,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.056566Z',\n",
       "     'start_time': '2019-07-16T19:53:49.046578Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.134321',\n",
       "     'end_time': '2019-09-12T23:20:03.181984',\n",
       "     'duration': 0.047663,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6575, 6404, 2425)'},\n",
       "     'execution_count': 37},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '(6404, 2425)'},\n",
       "     'execution_count': 37}],\n",
       "   'source': 'pV_WC_shape\\nblock_shape'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 38,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.226732',\n",
       "     'end_time': '2019-09-12T23:20:03.273365',\n",
       "     'duration': 0.046633,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# #Two block sizes that make sense for reading:\\n# # |V| x 1 x 1 = each distribution/conditioning event is a block\\n# # |V| x |W| x 1 = each context is a block\\n# dom_read = tiledb.Domain(tiledb.Dim(name=\"orthWord\", domain=(0, 9410), tile=9411, dtype=np.uint32),\\n#                          tiledb.Dim(name=\"segWord\", domain=(0, 9171), tile=9172, dtype=np.uint32),\\n#                          tiledb.Dim(name=\"context\", domain=(0, 106294), tile=1, dtype=np.uint32))\\n\\n# schema_read = tiledb.ArraySchema(domain=dom_read, sparse=True,\\n#                                  attrs=[tiledb.Attr(name=\"O\", dtype=np.float64)])'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 39,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.068069Z',\n",
       "     'start_time': '2019-07-16T19:53:49.059212Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.316088',\n",
       "     'end_time': '2019-09-12T23:20:03.362852',\n",
       "     'duration': 0.046764,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#Two block sizes that make sense for reading:\\n# |V| x 1 x 1 = each distribution/conditioning event is a block\\n# |V| x |W| x 1 = each context is a block\\ndom_read = tiledb.Domain(tiledb.Dim(name=\"orthWord\", domain=(0, pV_WC_shape[0]-1), tile=pV_WC_shape[0], dtype=np.uint32),\\n                         tiledb.Dim(name=\"segWord\", domain=(0, pV_WC_shape[1]-1), tile=pV_WC_shape[1], dtype=np.uint32),\\n                         tiledb.Dim(name=\"context\", domain=(0, pV_WC_shape[2]-1), tile=1, dtype=np.uint32))\\n\\nschema_read = tiledb.ArraySchema(domain=dom_read, sparse=True,\\n                                 attrs=[tiledb.Attr(name=\"O\", dtype=np.float64)])'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 40,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-17T22:00:25.160693Z',\n",
       "     'start_time': '2019-07-17T22:00:25.014309Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.402823',\n",
       "     'end_time': '2019-09-12T23:20:03.443856',\n",
       "     'duration': 0.041033,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# %rm -r LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC_read'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 41,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.078900Z',\n",
       "     'start_time': '2019-07-16T19:53:49.070669Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.483184',\n",
       "     'end_time': '2019-09-12T23:20:03.560273',\n",
       "     'duration': 0.077089,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 41}],\n",
       "   'source': 'o\\n# array_name_read = o + \"_read\"\\narray_name_read = o\\ntiledb.SparseArray.create(array_name_read, schema_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 42,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.089602Z',\n",
       "     'start_time': '2019-07-16T19:53:49.081773Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.602591',\n",
       "     'end_time': '2019-09-12T23:20:03.646009',\n",
       "     'duration': 0.043418,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"def block_transformer(i, constructed_block_set_jk_tile):\\n    I = np.array([i] * constructed_block_set_jk_tile.coords.shape[1])\\n    J = constructed_block_set_jk_tile.coords[0]\\n    K = constructed_block_set_jk_tile.coords[1]\\n    data = constructed_block_set_jk_tile.data\\n    return (I, J, K, data)\\n\\n# def block_writer(block_tuple, arr_name):\\n#     with tiledb.SparseArray(arr_name, mode='w') as A:\\ndef block_writer(block_tuple, A):\\n# #     with tiledb.SparseArray(arr_name, mode='w') as A:\\n        I, J, K, data = block_tuple\\n        A[I, J, K] = data\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 43,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:49.099762Z',\n",
       "     'start_time': '2019-07-16T19:53:49.091983Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.691255',\n",
       "     'end_time': '2019-09-12T23:20:03.735391',\n",
       "     'duration': 0.044136,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': 'config = tiledb.Config()\\n# config[\"sm.consolidation.steps\"] = 10\\nconfig[\"sm.consolidation.steps\"] = 600\\nconfig[\"sm.consolidation.step_min_frags\"] = 2\\nconfig[\"sm.consolidation.step_max_frags\"] = 20\\n# tiledb.consolidate(array_name, config)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 44,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T18:12:41.866355Z',\n",
       "     'start_time': '2019-07-16T17:48:50.400105Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.804403',\n",
       "     'end_time': '2019-09-12T23:20:03.845950',\n",
       "     'duration': 0.041547,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# # Each i index corresponds to a block; each \"block set\" is a sequence of ≈ pV_WC_shape[0]/4 indices\\n# block_sets = np.array_split(np.arange(pV_WC_shape[0]), 4)\\n\\n\\n# with tiledb.SparseArray(array_name_read, mode=\\'w\\') as A:\\n#     for block_set in tqdm(block_sets):\\n#         constructed_blocks = par(jl.delayed(block_constructor)(i) for i in block_set)\\n\\n#         block_tuples = jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_transformer)(i, constructed_block_tile)\\n#                                                                                             for i, constructed_block_tile in zip(block_set, constructed_blocks))\\n\\n#         jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_writer)(block_tuple, A) \\n#                                                                              for block_tuple in block_tuples);\\n# #         tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 45,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:40:39.820093Z',\n",
       "     'start_time': '2019-07-16T19:14:45.356814Z'},\n",
       "    'scrolled': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:03.883375',\n",
       "     'end_time': '2019-09-12T23:20:40.512905',\n",
       "     'duration': 36.62953,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r  0%|          | 0/4 [00:00<?, ?it/s]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0617s.) Setting batch_size=6.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:    0.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 382 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 622 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 748 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 886 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 1174 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    3.3s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    0.5s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    1.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    1.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    1.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    2.1s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    2.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    2.3s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    2.5s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    2.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    2.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    3.0s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    3.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    3.3s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    3.5s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    3.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    3.8s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    4.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    4.1s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    4.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    4.4s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    4.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    4.8s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 25%|██▌       | 1/4 [00:09<00:27,  9.25s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0659s.) Setting batch_size=6.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:    0.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 382 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 622 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 748 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 886 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 1174 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    3.3s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    0.5s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    1.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    1.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    2.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    2.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    2.6s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    2.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    2.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    3.0s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    3.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    3.3s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    3.5s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    3.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    3.8s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    3.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    4.1s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 50%|█████     | 2/4 [00:19<00:18,  9.50s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0663s.) Setting batch_size=6.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:    0.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 382 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 622 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 748 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 886 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 1174 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    3.3s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    0.5s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    1.3s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    1.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    1.6s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    1.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    1.9s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    2.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    2.6s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    2.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    2.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    3.0s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    3.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    3.3s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    3.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    3.6s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    3.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    3.8s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    4.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed:    4.2s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r 75%|███████▌  | 3/4 [00:27<00:09,  9.25s/it]'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend MultiprocessingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0631s.) Setting batch_size=6.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:    0.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 382 tasks      | elapsed:    0.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 622 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 748 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 886 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 1174 tasks      | elapsed:    2.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1643 out of 1643 | elapsed:    3.2s finished\\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    0.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 1643 out of 1643 | elapsed:    0.5s finished\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\\n[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.0s\\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    0.1s\\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    0.2s\\n[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:    0.3s\\n[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.4s\\n[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:    0.4s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:    0.5s\\n[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.6s\\n[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:    0.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    0.7s\\n[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:    0.8s\\n[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    0.8s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:    0.9s\\n[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    1.0s\\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:    1.1s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    1.1s\\n[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:    1.2s\\n[Parallel(n_jobs=-1)]: Done 514 tasks      | elapsed:    1.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 549 tasks      | elapsed:    1.4s\\n[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    1.5s\\n[Parallel(n_jobs=-1)]: Done 621 tasks      | elapsed:    1.6s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 658 tasks      | elapsed:    1.7s\\n[Parallel(n_jobs=-1)]: Done 697 tasks      | elapsed:    1.8s\\n[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    1.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:    2.0s\\n[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:    2.0s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 861 tasks      | elapsed:    2.2s\\n[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:    2.3s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 949 tasks      | elapsed:    2.4s\\n[Parallel(n_jobs=-1)]: Done 994 tasks      | elapsed:    2.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1041 tasks      | elapsed:    2.6s\\n[Parallel(n_jobs=-1)]: Done 1088 tasks      | elapsed:    2.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:    2.8s\\n[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    2.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1237 tasks      | elapsed:    3.1s\\n[Parallel(n_jobs=-1)]: Done 1288 tasks      | elapsed:    3.2s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:    3.3s\\n[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:    3.5s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1449 tasks      | elapsed:    3.6s\\n[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    3.7s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1561 tasks      | elapsed:    3.9s\\n'},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '[Parallel(n_jobs=-1)]: Done 1643 out of 1643 | elapsed:    4.1s finished\\n'},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '[None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n None,\\n ...]'},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 45},\n",
       "    {'output_type': 'stream',\n",
       "     'name': 'stderr',\n",
       "     'text': '\\r100%|██████████| 4/4 [00:36<00:00,  9.04s/it]'},\n",
       "    {'output_type': 'stream', 'name': 'stderr', 'text': '\\n'}],\n",
       "   'source': '# Each i index corresponds to a block; each \"block set\" is a sequence of ≈ pV_WC_shape[0]/4 indices\\nblock_sets = np.array_split(np.arange(pV_WC_shape[0]), 4)\\n\\nfor block_set in tqdm(block_sets):\\n    constructed_blocks = par(jl.delayed(block_constructor)(i) for i in block_set)\\n\\n    block_tuples = jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_transformer)(i, constructed_block_tile)\\n                                                                                        for i, constructed_block_tile in zip(block_set, constructed_blocks))\\n    with tiledb.SparseArray(array_name_read, mode=\\'w\\') as A:\\n        jl.Parallel(n_jobs=J, backend=\\'threading\\', verbose=V, prefer=PREFER)(jl.delayed(block_writer)(block_tuple, A) \\n                                                                             for block_tuple in block_tuples);\\n    tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 46,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:40.578703',\n",
       "     'end_time': '2019-09-12T23:20:40.658413',\n",
       "     'duration': 0.07971,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'\"},\n",
       "     'execution_count': 46}],\n",
       "   'source': '# this last consolidation step should clean up any remaining fragments\\ntiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 47,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:57.922578Z',\n",
       "     'start_time': '2019-07-16T19:53:57.690268Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:40.731806',\n",
       "     'end_time': '2019-09-12T23:20:40.803556',\n",
       "     'duration': 0.07175,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# from random import choice\\n\\n# def randomCoords(arr_shape):\\n#     return tuple(np.random.randint(0,arr_shape[dim]) for dim in range(len(arr_shape)))\\n\\n# numRandomCoords = 10000\\n\\n# test_coords = [randomCoords(pV_WC_shape) for each in range(numRandomCoords)]'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 48,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:49:34.020498Z',\n",
       "     'start_time': '2019-07-16T19:44:44.071291Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:40.875412',\n",
       "     'end_time': '2019-09-12T23:20:40.949990',\n",
       "     'duration': 0.074578,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# %%timeit\\n\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n#     rand_coords = choice(test_coords)\\n# #     rand_coords\\n#     A[rand_coords]\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 49,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T19:53:59.764035Z',\n",
       "     'start_time': '2019-07-16T19:53:59.755300Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:41.019812',\n",
       "     'end_time': '2019-09-12T23:20:41.095182',\n",
       "     'duration': 0.07537,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# config'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 50,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T20:12:47.404606Z',\n",
       "     'start_time': '2019-07-16T20:12:47.396864Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:41.164374',\n",
       "     'end_time': '2019-09-12T23:20:41.234917',\n",
       "     'duration': 0.070543,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# config[\"sm.consolidation.steps\"] = 200'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 51,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:05:55.774087Z',\n",
       "     'start_time': '2019-07-16T20:22:34.864067Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:41.310672',\n",
       "     'end_time': '2019-09-12T23:20:41.386710',\n",
       "     'duration': 0.076038,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '#10 steps, min_frags=2, max_frags=20 => ~1m20s per cell run, w/ <=200 fewer fragments per cell run\\n# => estimate of 16.66 min per 2k fragments\\n#80 steps, min_frags=2, max_frags=20 => ~4m7s per cell run\\n#100 steps, min_frags=2, max_frags=20 => ~4m53s per cell run, w/ 1900 fewer fragments per cell run\\n#200 steps, min_frags=2, max_frags=20 => ~8m20s per cell run, w/ 3800 fewer fragments per cell run\\n# tiledb.consolidate(config, uri=array_name_read)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 52,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:43.286085',\n",
       "     'end_time': '2019-09-12T23:20:43.361147',\n",
       "     'duration': 0.075062,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': '# %ls -l LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC_read/'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 53,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:07:11.463641Z',\n",
       "     'start_time': '2019-07-16T21:07:08.440777Z'},\n",
       "    'scrolled': True,\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:43.432610',\n",
       "     'end_time': '2019-09-12T23:20:43.505228',\n",
       "     'duration': 0.072618,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# %%timeit\\n\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n#     rand_coords = choice(test_coords)\\n# #     rand_coords\\n#     A[rand_coords]\"},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:43.583354',\n",
       "     'end_time': '2019-09-12T23:20:43.656483',\n",
       "     'duration': 0.073129,\n",
       "     'status': 'completed'}},\n",
       "   'source': 'Consolidation of fragments leads to a nearly 1000x speed-up.'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 54,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-07-16T21:42:10.287333Z',\n",
       "     'start_time': '2019-07-16T21:42:10.278107Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:43.722206',\n",
       "     'end_time': '2019-09-12T23:20:43.799714',\n",
       "     'duration': 0.077508,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': '6404'},\n",
       "     'execution_count': 54}],\n",
       "   'source': 'num_segWords'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 55,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:43.870211',\n",
       "     'end_time': '2019-09-12T23:20:43.976367',\n",
       "     'duration': 0.106156,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'name': 'stdout',\n",
       "     'text': 'Wrote metadata for \\n\\tLD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC\\n to \\n\\tLD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC_metadata.json\\n'}],\n",
       "   'source': 'pV_WC_md = {\\n    \\'V\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{d}\\\\n\\\\t{w}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_orthWords},\\n    \\'W\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{w}\\\\n\\\\t{m}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_segWords},\\n    \\'C\\':{\\'from fp\\':f\"implicitly associated with two files:\\\\n\\\\t{d}\\\\n\\\\t{m}\",\\n         \\'changes\\':\\'None\\',\\n         \\'size\\':num_contexts}\\n}\\n\\nmy_fp = o\\nexportMatrixMetadata(my_fp + \\'_metadata.json\\',\\n                     my_fp,\\n                     None,\\n                     pV_WC_md,\\n                     \\'Step 5a\\',\\n                     \\'Calculate orthographic posterior given segmental wordform + context\\',\\n                     {})'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 56,\n",
       "   'metadata': {'ExecuteTime': {'end_time': '2019-06-03T22:49:20.766312Z',\n",
       "     'start_time': '2019-06-03T22:49:20.730490Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:44.048062',\n",
       "     'end_time': '2019-09-12T23:20:44.124842',\n",
       "     'duration': 0.07678,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'metadata': {},\n",
       "     'data': {'text/plain': \"['LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pV_WC',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pW_C_metadata.json',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C_metadata.json',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pV_WC_metadata.json',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC',\\n 'fisher_vocabulary_main.txt',\\n 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim in Buckeye preceding contexts 2gram model.ipynb',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.hV_C',\\n 'Producing Fisher vocab in Buckeye preceding contexts 2gram model contextual distributions.ipynb',\\n 'LM_filtered_buckeye_contexts_preceding_1_filtered.txt',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.pV_C_metadata.json',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC_metadata.json',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.pV_C',\\n 'buckeye_contexts_preceding_1_filtered.txt',\\n 'Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 2gram model.ipynb',\\n 'Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 2gram model.ipynb',\\n 'Filter LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy',\\n 'Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in Buckeye preceding contexts 2gram model.ipynb',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model.hV_C_metadata.json',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\\n 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy',\\n 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C_metadata.json']\"},\n",
       "     'execution_count': 56}],\n",
       "   'source': 'listdir(output_dir)'},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 57,\n",
       "   'metadata': {'ExecuteTime': {'start_time': '2019-07-16T21:45:56.446Z'},\n",
       "    'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:44.195228',\n",
       "     'end_time': '2019-09-12T23:20:44.268028',\n",
       "     'duration': 0.0728,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': \"# # pV_WC_temp = None\\n# temp_coords = None\\n# temp_data = None\\n# with tiledb.SparseArray(array_name_read, mode='r') as A:\\n# #     rand_coords = choice(test_coords)\\n# # #     rand_coords\\n# #     A[rand_coords]\\n# #     pV_WC_temp = A[:,:,:]\\n#     A[:,:,:]\\n#     temp_coords = A[:,:,:]['coords']\\n#     temp_data = A[:,:,:]['data']\"},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {'tags': [],\n",
       "    'papermill': {'exception': False,\n",
       "     'start_time': '2019-09-12T23:20:44.338936',\n",
       "     'end_time': '2019-09-12T23:20:44.409020',\n",
       "     'duration': 0.070084,\n",
       "     'status': 'completed'}},\n",
       "   'outputs': [],\n",
       "   'source': ''}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'name': 'python',\n",
       "   'version': '3.7.4',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'file_extension': '.py'},\n",
       "  'notify_time': '0',\n",
       "  'toc': {'base_numbering': 1,\n",
       "   'nav_menu': {},\n",
       "   'number_sections': True,\n",
       "   'sideBar': True,\n",
       "   'skip_h1_title': False,\n",
       "   'title_cell': 'Table of Contents',\n",
       "   'title_sidebar': 'Contents',\n",
       "   'toc_cell': True,\n",
       "   'toc_position': {},\n",
       "   'toc_section_display': True,\n",
       "   'toc_window_display': True},\n",
       "  'papermill': {'parameters': {'d': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model_projected_LTR_Buckeye_aligned_CM_filtered_LM_filtered.pV_C.npy',\n",
       "    'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim.pW_V.npz',\n",
       "    'm': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pW_C.npy',\n",
       "    'o': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/LTR_Buckeye_aligned_CM_filtered_LM_filtered_trim_in_Buckeye_preceding_contexts_2gram_model.pV_WC'},\n",
       "   'environment_variables': {},\n",
       "   'version': '1.0.1',\n",
       "   'input_path': 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       "   'output_path': 'LD_Fisher_vocab_in_Buckeye_preceding_contexts_2gram_model/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered trim in Buckeye preceding contexts 2gram model.ipynb',\n",
       "   'start_time': '2019-09-12T23:19:56.500347',\n",
       "   'end_time': '2019-09-12T23:20:44.882155',\n",
       "   'duration': 48.381808,\n",
       "   'exception': None}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 2}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End  @ 16:20:45\n",
      "\n",
      "\n",
      "Start  @ 16:20:45\n",
      "Running notebook:\n",
      "\tCalculate orthographic posterior given segmental wordform + context for LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered trim in NXT swbd following contexts 5gram model.ipynb\n",
      "Output directory:\n",
      "\tLD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model\n",
      "Arguments:\n",
      "{\n",
      " \"d\": \"LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model_projected_LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered.pV_C.npy\",\n",
      " \"w\": \"LTR_NXT_swbd_destressed_aligned_w_GD_AmE_destressed/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim.pW_V.npz\",\n",
      " \"m\": \"LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pW_C.npy\",\n",
      " \"o\": \"LD_Fisher_vocab_in_NXT_swbd_following_contexts_5gram_model/LTR_NXT_swbd_destressed_aligned_CM_filtered_LM_filtered_trim_in_NXT_swbd_following_contexts_5gram_model.pV_WC\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ba1d3e18914b6aadc1d8c3bf729160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NB for reasons I haven't tried to figure out, this cell dumps a bunch of notebook metadata\n",
    "# into the cell output...\n",
    "\n",
    "if '5a' in permittedSteps:\n",
    "    # takes ~360m for old sidious w/ no background load and J=-1\n",
    "    \n",
    "    # takes 135m for wittgenstein w/ no background load and J=-1\n",
    "    for bundle in posterior_WD_bundles:\n",
    "        output_dir = path.dirname(bundle['o'])\n",
    "        ensure_dir_exists(output_dir)\n",
    "\n",
    "        if not overwrite and path.exists(bundle['nb_fp']):\n",
    "#         if not overwrite and path.exists(path.join(ab['cm_dir'], ab['nb_output_name'])):\n",
    "            print('{0} already exists. Skipping...'.format(path.join(bundle['nb_fp'])))\n",
    "#             print('{0} already exists. Skipping...'.format(path.join(ab['cm_dir'], ab['nb_output_name'])))\n",
    "            endNote()\n",
    "            continue\n",
    "        \n",
    "        progress_report(bundle['nb_fp'],\n",
    "                        dict(d = bundle['d'],\n",
    "                             w = bundle['w'],\n",
    "                             m = bundle['m'],\n",
    "                             o = bundle['o']))\n",
    "\n",
    "        pm.execute_notebook(\n",
    "            'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
    "            bundle['nb_fp'],\n",
    "            parameters=dict(d = bundle['d'],\n",
    "                            w = bundle['w'],\n",
    "                            m = bundle['m'],\n",
    "#                             c = bundle['c'],\n",
    "#                             v = bundle['v'],\n",
    "#                             l = bundle['l'],\n",
    "#                             t = bundle['t'],\n",
    "                            o = bundle['o'])\n",
    "        )\n",
    "        endNote()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b: Calculate $p(\\hat{X}_0^f|X_0^f, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    " - **Step 4e**: `CM_AmE_destressed_aligned_w_LTR_..._pseudocount0.01/LTR_..._aligned_CM_filtered_LM_filtered_CMs_by_length_by_prefix_index.pickle` list of matrices\n",
    " - **Step 4c**: `LD_Fisher_vocab_in_..._contexts/LTR_..._aligned_CM_filtered_LM_filtered_in_..._contexts.pW_C.npy` matrix\n",
    " - **Step ?**: `LTR_..._aligned_w_GD_AmE_destressed` metadata directory\n",
    " - **Step 3e**: `LTR_..._aligned_w_GD_AmE_destressed/LTR_..._aligned_CM_filtered_LM_filtered.pW_V.json` dist (sanity check)\n",
    " - **Step 3c**: `CM_AmE_destressed_aligned_w_LTR_..._pseudocount0.01/LTR_..._aligned_CM_filtered_LM_filtered_pY1X0X1X2.json` dist (sanity check)\n",
    " - **Step ?**: `LD_Fisher_vocab_in_..._contexts/LM_filtered_..._contexts.txt` (sanity check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a choice of parameters $\\epsilon$ and $n$, and given\n",
    " - wordform channel matrices $p(Y_0^f|X_0^f)$\n",
    " - a contextual distribution on segmental wordforms $p(X_0^f|C)$\n",
    " - segmental lexicon metadata pre-calculating $k$-cousins/$k$-spheres up to $k=4$\n",
    " \n",
    "Calculate\n",
    "\n",
    "$$\\hat{p}(\\hat{X}_0^f = x_0^{'f}|X_0^f = x_0^{*f}, c) = \\frac{1}{n} \\sum\\limits_{y_0^f \\in S} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c)$$\n",
    " where \n",
    "  - edit distance $d(x_0^{'f}, x_0^{*f}) \\leq 4$\n",
    "  - $S = $ a set of $n$ samples from $p(Y_0^f|x_0^{*f})$. In practice an $n \\approx 50$ seems to result in estimates that are within $10^{-6}$ of the true estimate. \n",
    "  - $p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) = \\frac{p(y_0^f|x_0^{'f})p(x_0^{'f}|c)}{p(y_0^f | c)}$\n",
    "  - $p(y_0^f| c) = \\sum\\limits_{v', x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|v')p(v'|c) = \\sum\\limits_{x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:07:22.407390Z",
     "start_time": "2019-09-11T22:07:19.156Z"
    }
   },
   "outputs": [],
   "source": [
    "# where do wordform channel matrices live?\n",
    "#  where are they bundled?\n",
    "#   - LCM_bundles\n",
    "# where do contextual distributions on segmental wordforms live?\n",
    "#   where are they bundled?\n",
    "#   - WD_bundles\n",
    "# where do lexicon metadata live?\n",
    "#   where are they bundled?\n",
    "#   - lexicon_md_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:07:22.407875Z",
     "start_time": "2019-09-11T22:07:19.162Z"
    }
   },
   "outputs": [],
   "source": [
    "'Calculate segmental posterior given segmental wordform + context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:07:22.408444Z",
     "start_time": "2019-09-11T22:07:19.169Z"
    }
   },
   "outputs": [],
   "source": [
    "#plan:\n",
    "# 1. Check if there are components of the posterior calculation that are worth pre-calculating; if so, calculate them.\n",
    "#    - e.g. pW_C, pV_W-hat,C\n",
    "# 2. Next notebook should have a flag for calculating just p(V-hat = v* | V = v*) ∀v* vs. the full p(V-hat | V = v*) ∀v*\n",
    "#    - Consider facilitating SLURM cluster jobs by \n",
    "#       - making posterior calculation deterministic (unlike P4bnt2) \n",
    "#       - adding/supporting optional notebook arguments that specify which parts of the posterior dist to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5c: Calculate $p(\\hat{V} = v^*| V = v^*, C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T22:07:22.408932Z",
     "start_time": "2019-09-11T22:07:19.176Z"
    }
   },
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generating analysis measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
