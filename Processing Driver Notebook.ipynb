{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:39.850825Z",
     "start_time": "2019-06-04T22:25:39.847100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-we-want-to-calculate\" data-toc-modified-id=\"What-we-want-to-calculate-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What we <em>want</em> to calculate</a></span></li><li><span><a href=\"#What-we-can-calculate\" data-toc-modified-id=\"What-we-can-calculate-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>What we <em>can</em> calculate</a></span></li><li><span><a href=\"#The-structure-of-calculations\" data-toc-modified-id=\"The-structure-of-calculations-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>The structure of calculations</a></span></li><li><span><a href=\"#The-structure-of-the-pipeline\" data-toc-modified-id=\"The-structure-of-the-pipeline-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>The structure of the pipeline</a></span></li></ul></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Requirements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Python-environment\" data-toc-modified-id=\"Python-environment-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Python environment</a></span></li><li><span><a href=\"#Hardware-/-runtime-expectations\" data-toc-modified-id=\"Hardware-/-runtime-expectations-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Hardware / runtime expectations</a></span></li></ul></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Todo</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Step-0:-Check-for-foundational-files\" data-toc-modified-id=\"Step-0:-Check-for-foundational-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 0: Check for foundational files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-0a:-Check-for-gating-data\" data-toc-modified-id=\"Step-0a:-Check-for-gating-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 0a: Check for gating data</a></span></li><li><span><a href=\"#Step-0b:-Check-for-transcribed-lexicons\" data-toc-modified-id=\"Step-0b:-Check-for-transcribed-lexicons-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 0b: Check for transcribed lexicons</a></span></li><li><span><a href=\"#Step-0c:-Check-for-n-gram-contexts\" data-toc-modified-id=\"Step-0c:-Check-for-n-gram-contexts-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Step 0c: Check for n-gram contexts</a></span></li><li><span><a href=\"#Step-0d:-Check-for-language-model(s)\" data-toc-modified-id=\"Step-0d:-Check-for-language-model(s)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Step 0d: Check for language model(s)</a></span></li></ul></li><li><span><a href=\"#Step-1:-Segment-inventory-alignment\" data-toc-modified-id=\"Step-1:-Segment-inventory-alignment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 1: Segment inventory alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1a:-Define-inventory-alignment-projections\" data-toc-modified-id=\"Step-1a:-Define-inventory-alignment-projections-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Step 1a: Define inventory alignment projections</a></span></li><li><span><a href=\"#Step-1b:-Apply-inventory-alignment-projections\" data-toc-modified-id=\"Step-1b:-Apply-inventory-alignment-projections-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Step 1b: Apply inventory alignment projections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-projection-definitions\" data-toc-modified-id=\"Check-for-projection-definitions-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Check for projection definitions</a></span></li><li><span><a href=\"#How-are-inventory-alignment-projections-actually-applied?\" data-toc-modified-id=\"How-are-inventory-alignment-projections-actually-applied?-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>How are inventory alignment projections actually applied?</a></span></li><li><span><a href=\"#Apply-projection-definitions\" data-toc-modified-id=\"Apply-projection-definitions-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Apply projection definitions</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions\" data-toc-modified-id=\"Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 2: Generating channel and (orthographic) lexicon distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-2a:-Generating-channel-distributions-and-associated-metadata\" data-toc-modified-id=\"Step-2a:-Generating-channel-distributions-and-associated-metadata-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Step 2a: Generating channel distributions and associated metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Metadata</a></span></li><li><span><a href=\"#Channel-distributions\" data-toc-modified-id=\"Channel-distributions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Channel distributions</a></span></li></ul></li><li><span><a href=\"#Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)\" data-toc-modified-id=\"Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)</a></span></li></ul></li><li><span><a href=\"#Step-3:-Creating-combinable-models\" data-toc-modified-id=\"Step-3:-Creating-combinable-models-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 3: Creating combinable models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution\" data-toc-modified-id=\"Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution</a></span></li><li><span><a href=\"#Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary\" data-toc-modified-id=\"Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary</a></span></li><li><span><a href=\"#Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list\" data-toc-modified-id=\"Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list</a></span></li><li><span><a href=\"#Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms\" data-toc-modified-id=\"Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms</a></span></li><li><span><a href=\"#Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform\" data-toc-modified-id=\"Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform</a></span></li></ul></li><li><span><a href=\"#Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data\" data-toc-modified-id=\"Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 4: Pre-calculate remaining forward model components and meta-data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model\" data-toc-modified-id=\"Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Step 4a: Generate triphone lexicon distributions for every triphone channel model</a></span></li><li><span><a href=\"#Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon\" data-toc-modified-id=\"Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon</a></span></li><li><span><a href=\"#Step-4c:-Calculate-the-marginal-probability-$p(W|C)$-of-each-segmental-wordform-$w$-given-$n$-gram-contexts-$C$\" data-toc-modified-id=\"Step-4c:-Calculate-the-marginal-probability-$p(W|C)$-of-each-segmental-wordform-$w$-given-$n$-gram-contexts-$C$-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Step 4c: Calculate the marginal probability $p(W|C)$ of each segmental wordform $w$ given $n$-gram contexts $C$</a></span></li><li><span><a href=\"#Step-4d:-Define-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)\" data-toc-modified-id=\"Step-4d:-Define-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Step 4d: Define channel distributions on a set of segmental wordforms(+prefixes)</a></span></li></ul></li><li><span><a href=\"#Step-5:-Calculate-posterior-probabilities\" data-toc-modified-id=\"Step-5:-Calculate-posterior-probabilities-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Step 5: Calculate posterior probabilities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-5a:-Calculate-$p(V|W,-C)$\" data-toc-modified-id=\"Step-5a:-Calculate-$p(V|W,-C)$-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Step 5a: Calculate $p(V|W, C)$</a></span></li><li><span><a href=\"#Step-5b:-Calculate-$p(\\hat{X}_0^f|X_0^f,-C)$\" data-toc-modified-id=\"Step-5b:-Calculate-$p(\\hat{X}_0^f|X_0^f,-C)$-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Step 5b: Calculate $p(\\hat{X}_0^f|X_0^f, C)$</a></span></li><li><span><a href=\"#Step-5c:-Calculate-$p(\\hat{V}-=-v^*|-V-=-v^*,-C)$\" data-toc-modified-id=\"Step-5c:-Calculate-$p(\\hat{V}-=-v^*|-V-=-v^*,-C)$-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Step 5c: Calculate $p(\\hat{V} = v^*| V = v^*, C)$</a></span></li></ul></li><li><span><a href=\"#Step-6:-Generating-analysis-measures\" data-toc-modified-id=\"Step-6:-Generating-analysis-measures-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Step 6: Generating analysis measures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processing pipeline from \n",
    " - gating data\n",
    " - transcribed lexicon\n",
    " - a language model and (possibly empty) n-gram contexts\n",
    "\n",
    "to \n",
    " - channel distribution\n",
    " - lexicon distribution(s) (distributions over wordforms)\n",
    " - expected posterior distribution over intended wordform given what has been produced of what was intended.\n",
    " \n",
    " \n",
    "It describes what happens at each step, checks some pre- and post-conditions, describes what you, the user must do (if anything), and scripts some commands to automatically do the necessary processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "There are about one-and-a-half calculations that this processing pipeline enables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    " - $C = \\{c_0, c_1 ... c_{n-1}\\}$ denote a set of $n$-gram contexts (sequences of $\\leq (n-1)$ orthographic wordforms) drawn from a speech corpus (e.g. Buckeye or Switchboard).\n",
    " - $V = \\{v_0, v_1 ... v_V\\}$ denote a set of orthographic wordforms (a 'vocabulary') drawn from a speech corpus (the same corpus $C$ is drawn from).\n",
    " - $W = \\{w_0, w_1 ... w_W\\}$ denote a set of segmental wordforms ('transcribed wordforms'). Each element $w \\in W$ consists of a sequence $x_0 x_1 ... x_f = x_0^f$ of segments ('phonemes') drawn from a set of symbols $\\Sigma_x$.\n",
    " \n",
    "(All sets are finite, unless otherwise noted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - A language model describes $p(V|C)$.\n",
    " - A lexical database or speech corpus describes a relation $r$ between orthographic wordforms $V$ and segmental wordforms $W$, where $(v,w) \\in r$ iff $v$ can be produced as $w$.\n",
    " - Let $p(W|V)$ be defined as follows: $p(w|v) = \\frac{1}{r_{v}}$, where $r_v = |\\{w' | (v, w') \\in r\\}|$\n",
    " - Given that the $i$th segment that a speaker intended to produce is $x_i$, a triphone channel model describes $p(Y_i|x_{i-1}, x_i; x_{i+1})$, a distribution over received/perceived segments $\\Sigma_y$. This is estimated from diphone gating data. Note that it doesn't permit modeling insertions or deletions.\n",
    " - $p(Y_0^f|W) = p(Y_0^f|X_0^f)$ is a (channel) distribution over perceived (segmental) wordforms given an intended (segmental) wordform. It is completely defined by a choice of $p(W|V)$ and a choice of $p(Y_i|x_{i-1}, x_i; x_{i+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *forward model*, then defines a distribution $p(Y_0^f, X_0^f, V|C) = p(Y_0^f|X_0^f)p(X_0^f|V)p(V|C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we *want* to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in calculating the *expected posterior of a listener* over what orthographic wordform the speaker intended given what the speaker actually intended (taking the expectation over the speaker's actual intended segmental wordform, the perceived segmental wordform, and the listener's estimate of the speaker's intended segmental wordform):\n",
    "\n",
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, y_0^f, x_0^{'f}} p(\\hat{V} = v^*, \\hat{X}_0^f = x_0^{'f}, y_0^f, X_0^f = x_0^{*f}|V = v^*, c)$$\n",
    "\n",
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, y_0^f, x_0^{'f}} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) p(Y_0^f = y_0^f | X_0^f = x_0^{*f})p(X_0^f = x_0^{*f}|V = v^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "1. $p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) = \\frac{p(y_0^f|x_0^{'f})p(x_0^{'f}|c)}{p(y_0^f | c)}$\n",
    "2. $p(x_0^{'f}|c) = \\sum\\limits_{v'} p(x_0^{'f}|v')p(v'|c)$\n",
    "3. $p(y_0^f| c) = \\sum\\limits_{v', x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|v')p(v'|c) = \\sum\\limits_{x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|c)$\n",
    "4. $p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) = \\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we *can* calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Unfortunately, because of the enormous size of $Y_0^f$, exact marginalization over all $y_0^f$ is *not* feasible.\n",
    " 2. Fortunately, because, for any given $x_0^{*f}$, the fraction of $Y_0^f$ with non-negligible mass in $p(Y_0^f|x_0^{*f})$ is small, we can construct a Monte Carlo estimate of \n",
    " $$p(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) = \\sum\\limits_{y_0^f} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c) p(y_0^f|x_0^{*f})$$\n",
    " as\n",
    " $$\\hat{p}(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) = \\frac{1}{n} \\sum\\limits_{y_0^f \\in S} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c)$$\n",
    " where $S = $ a set of $n$ samples from $p(Y_0^f|x_0^{*f})$. In practice an $n \\approx 50$ seems to result in estimates that are within $10^{-6}$ of the true estimate. \n",
    " \n",
    " 3. Unfortunately, even with this approximation $p(\\hat{X}_0^f|X_0^{*f}, c)$ has about $10^8 - 10^9$ entries: too many to feasibly calculate exactly, especially across *all choices of $c$* as well.\n",
    " 4. However, because of the relative dispersion of wordforms, the relatively low overall rate of channel noise, and the information provided by sentential context, the fraction of $\\hat{X}_0^f$ assigned non-negligible mass in $p(\\hat{X}_0^f|x_0^{*f}, c)$ for any given $x_0^{*f}$ is relatively small, and largely concentrated on $x_0^{'f}$ that are within $k$ edits (substitutions) of $x_0^{*f}$ for small $k$. Accordingly, we can get an arbitrarily good approximation $\\hat{p}^{k}$ of $p(\\hat{X}_0^f|x_0^{*f}, c)$ by \n",
    "  - choosing an arbitrarily small threshold $\\epsilon$\n",
    "  - calculating\n",
    "$$p^k = \\{p(\\hat{X}_0^f = x_0^{'f}|x_0^{*f}, c) | x_0^{'f} \\text{ is within } k \\text{ edits of }x_0^{*f}\\}$$ for progressively increasing $k$, stopping when $1 - \\sum p_i^k \\leq \\epsilon$ and assigning $0$ probability to all $x_0^{'f}$ not associated with $p^k$.\n",
    "\n",
    "Note that this means that if we choose the same $\\epsilon$ for all $(x_0^{*f}, c)$ pairs, some segmental wordforms $x_0^f$ will have approximations involving higher or lower $k$ values than others, but that all will have distributions that are at least as accurate as some same minimum (determined by $\\epsilon$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\hat{V} = v^*|V = v^*, c) = \\sum\\limits_{x_0^{*f}, x_0^{'f}} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) p(\\hat{X}_0^f = x_0^{'f}| X_0^f = x_0^{*f}, c) p(X_0^f = x_0^{*f}|V = v^*)$$\n",
    "\n",
    "so $$p(\\hat{V} = v^*|V = v^*, c) \\approx \\sum\\limits_{x_0^{*f}, x_0^{'f}. d(x_0^{*f}, x_0^{'f}) \\leq k} p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) \\hat{p}(\\hat{X}_0^f = x_0^{'f}| X_0^f = x_0^{*f}, c) p(X_0^f = x_0^{*f}|V = v^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have the following distributions pre-computed (or as close to that as possible) as `numpy` arrays\n",
    " 1. $p(W|V) = p(X_0^f|V)$\n",
    " 2. $\\hat{p}(\\hat{X}_0^f|X_0^f, C)$\n",
    " 3. $p(Y_0^f|W)$\n",
    " 4. $p(W|C)$\n",
    " 5. $p(V|W, C)$\n",
    " \n",
    "so that we can efficiently calculate $p(\\hat{V} = v^*| V = v^*, c)$ for all pairs of $(v^*, c)$ and\n",
    "where \n",
    " - $p(\\hat{V} = v^*| V = v^*, c)$\n",
    " - $\\hat{p}(\\hat{X}_0^f|X_0^f, C)$, and \n",
    " - $p(V|W,C)$ \n",
    " \n",
    "involve decreasing amounts of non-trivial computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of the pipeline\n",
    "\n",
    "Given foundational files:\n",
    " - a language model $m$ over some orthographic vocabulary $V_m$\n",
    " - $n$-gram contexts $C$ taken from one or more speech corpora\n",
    " - a target orthographic vocabulary $V$ taken from each of the speech corpora that contexts are taken from\n",
    " - diphone gating data\n",
    " - one or more transcription relations $r$\n",
    " \n",
    "the first step in the processing pipeline involves aligning segmental inventories of gating data and transcription relations. Once that's been done, we can define triphone channel distributions on transcription-relation aligned gating data that can be applied to gating-data aligned transcribed wordforms. We also need to define $p(V_m|C)$ for each choice of $C$.\n",
    "\n",
    "Next, we create mutually combinable versions of transcription relations, channel distributions, and distributions over orthographic vocabularies - e.g.\n",
    " - we need to remove words from each $r$ that contain triphones that can't be modeled by the aligned triphone channel distribution\n",
    " - we need to remove words from each $r$ that aren't in the language model vocabulary $V_m$\n",
    " - $p(V_m|C)$ needs to be projected down to remove orthographic words that aren't in transcription relations and context sets $C$ need to be scrubbed of contexts containing orthographic wordforms not in the language model\n",
    "\n",
    "With all the atomic components of the model(/each possible combined model) constructed, we then pre-calculate remaining components of the forward model(s). Finally, we calculate components of the posterior. (The separation of this last step from *everything previous* facilitates parallel computation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Python environment\n",
    "This repository was developed using Python 3.6 with the following package requirements (upstream dependencies not included):\n",
    " - **Jupyter/notebook related packages**: `jupyter` `jupyter_contrib_nbextensions` `papermill`\n",
    "    - Notable notebook extensions used include `ExecuteTime` and `Table of Contents (2)`.\n",
    " - **Numeric computing and data processing**: `scipy` `numpy` `pandas` `tqdm` `joblib` `numba` `sparse` `pytorch` `torchvision`\n",
    " - **Plotting**: `matplotlib` `plotnine`\n",
    " - **Misc**: `funcy` `more_itertools`\n",
    "\n",
    "See `dev_environment.sh` for `conda` and `pip` commands to create an environment that has all of these packages. (Not all are available on conda, and not all of those that are available on conda are available from the main channel.)\n",
    "\n",
    "\n",
    "### Hardware / runtime expectations\n",
    "\n",
    "The last machine this repository was developed on (`wittgenstein`) has 32 cores. `joblib` is used extensively to parallelize data processing. On this machine, using `joblib` and often using nearly all of those cores:\n",
    "1. Step 1 takes about a minute.\n",
    "2. Step 2 takes about 3 hours.\n",
    "3. Step 3 takes about 15-20 minutes.\n",
    "4. Step 4 takes about 4.5 hours.\n",
    "\n",
    "To run these scripts comfortably and without modification, you will need about 120GB of memory and ≈250 GB of hard disk space. (Little or no attempt has been made to compress or avoid unnecessary file outputs except insofar as it creates representations that lead to usefully smaller memory loads or facilitate matrix-based compuations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "0. **Extensibility**: Step 3b (filtering transcribed lexicons against language model vocabularies) uses output filenames that won't scale if any transcription lexicon is used with more than one language model. (This also affects 3c.)\n",
    "1. **Extensibility/Modularity/Maintainability**: Every notebook that depends on the behavior or output of another notebook being a certain way should have that assumption flagged at the top of the notebook. \n",
    "   - **Most common example**: one of notebook $B$'s arguments is a *directory path* $d$, where it expects to find a specific set of files with certain fixed filenames (or with filenames that are derived in some way from one of $B$'s arguments - often $d$); this assumption is predicated on the behavior of some notebook $A$ earlier in the pipeline producing exactly some set of files in a common directory all with certain filenames.\n",
    "2. **Portability/Reproducibility**: For every file that this repository depends on that *isn't* tracked by the repository (e.g. processed versions of swbd2003, Buckeye, etc.), there should be *something* (e.g. a cell in this script) that lets the user identify where those files are located, and then said something copies them to wherever this repository is expecting to find them.\n",
    "3. **Portability/Reproducibility**: Check for and remove absolute paths in this and other files.\n",
    "4. **Portability/Reproducibility**: For platform independence (read: supporting windows users, I guess?) use `path.join` instead of manually choosing directory slashes...\n",
    "5. **Documentation**: \n",
    "   1. Make sure motivation section is up to date.\n",
    "   2. Math-y documentation in channel distribution and posterior distribution notebooks probably needs to be updated / at least have notation overhauled.\n",
    "   3. Go through notebooks used here and make sure `Overview` cells are accurate.\n",
    "   4. Go through notebooks used here and make sure Papermill-related `Usage` cells are filled in / accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.100370Z",
     "start_time": "2019-06-04T22:25:45.962065Z"
    }
   },
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.104384Z",
     "start_time": "2019-06-04T22:25:53.102121Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.185220Z",
     "start_time": "2019-06-04T22:25:53.105743Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.191796Z",
     "start_time": "2019-06-04T22:25:53.187540Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.805954Z",
     "start_time": "2019-06-04T22:25:53.193208Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from boilerplate import ensure_dir_exists, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.813757Z",
     "start_time": "2019-06-04T22:25:53.807191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd()\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.820620Z",
     "start_time": "2019-06-04T22:25:53.814646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'boilerplate.py',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " 'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'LM_Fisher',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'Calculate segmental posterior given segmental wordform + context.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'HMMs scratch.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
       " 'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_contents_0 = listdir()\n",
    "repo_contents_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Check for foundational files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    " - I assume all relevant transcriptions have been converted to Unicode IPA characters and that segment sequences have `.` as a separator. For each data source used here, the IPA alignment step is documented in a GitHub repository elsewhere. (While I don't *think* any script here depends on use of Unicode IPA symbols, I haven't - and won't - test that idea, and it is *absolutely required* that contiguous sequences of segments be separated by `.` in data files.)\n",
    " - Where language models and n-gram contexts (drawn from speech corpora) are referenced, each of these is assumed to have come from as is from other GitHub repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four steps below verify that foundational files assumed to be present by downstream notebooks are, in fact, present in the repository directory. If for some reason those files are not present, the processing pipeline will be aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0a: Check for gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.862637Z",
     "start_time": "2019-06-04T22:25:53.821479Z"
    }
   },
   "outputs": [],
   "source": [
    "AmE_gating_data_dir = 'GD_AmE'\n",
    "AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "assert path.exists(AmE_gating_data_dir), 'Gating data directory {0} does not exist.'.format(AmE_gating_data_dir)\n",
    "assert path.exists(AmE_GD_fp), 'Gating data file {0} does not exist.'.format(AmE_GD_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed gating data used here come from \n",
    " - https://github.com/emeinhardt/wmc2014-ipa\n",
    " \n",
    "See those repositories for information on how they were produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0b: Check for transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each transcribed lexicon `LEXNAME` should be in a folder (e.g. `LTR_LEXNAME`) containing a file `LTR_LEXNAME.tsv`. For documentation purposes, the source file and a notebook documenting the production of the `.tsv` file should, if practicable be included in the folder as well.\n",
    "   - A transcribed lexicon `LTR_....tsv` file should have two columns: `Orthographic_Wordform` and `Transcription`.\n",
    "   - NB: The `LTR_` prefix on transcribed lexicon data files and containing folders is simply a convention for organization and readability, but is not required or expected by any file or script in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions in the code below will only succeed if step 1 is complete for all transcribed lexicons listed for checking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:53.913705Z",
     "start_time": "2019-06-04T22:25:53.863597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 66.41it/s]\n"
     ]
    }
   ],
   "source": [
    "newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "# nxt_swbd_ltr_folder = \n",
    "\n",
    "LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder)\n",
    "LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder)\n",
    "\n",
    "for dirname in tqdm(LTR_folders_to_process):\n",
    "    assert path.exists(dirname), 'Transcribed lexicon directory {0} not found in repo directory'.format(dirname)\n",
    "    fname = path.join(dirname, dirname + '.tsv')\n",
    "    assert path.exists(fname), 'Transcribed lexicon {0} not found in repo directory'.format(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are transcribed lexicon relations made?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each was created by processing a transcription source (a lexical database, an annotated corpus, etc.). The processing step is described in other repositories:\n",
    " - https://github.com/emeinhardt/newdic-nettalk-ipa\n",
    " - https://github.com/emeinhardt/cmu-ipa\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    "\n",
    "Given the processed outputs of these repositories, the `Making a Transcribed Lexicon Relation - <LEXNAME>.ipynb` notebook in each `LTR_LEXNAME` folder describes how the homogeneous `.tsv` files downstream steps are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:55.220446Z",
     "start_time": "2019-06-04T22:25:55.212563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - newdic_destressed.ipynb',\n",
       " 'newdic_IPA.tsv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Making a Transcribed Lexicon Relation - CMU_destressed.ipynb',\n",
       " 'LTR_CMU_destressed.pW_V.json',\n",
       " 'LTR_CMU_destressed.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_CMU_destressed.pW_V.npz',\n",
       " 'cmudict-0.7b_IPA_destressed.tsv',\n",
       " 'LTR_CMU_destressed_Orthographic_Wordforms.txt',\n",
       " 'LTR_CMU_destressed_Transcriptions.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_Buckeye.tsv',\n",
       " 'buckeye_words_analysis_relation.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye.ipynb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname in LTR_folders_to_process:\n",
    "    listdir(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0c: Check for n-gram contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:56.701843Z",
     "start_time": "2019-06-04T22:25:56.698183Z"
    }
   },
   "outputs": [],
   "source": [
    "buckeye_contexts = 'buckeye_contexts.txt'\n",
    "swbd2003_contexts = 'swbd2003_contexts.txt'\n",
    "\n",
    "contexts = (buckeye_contexts, swbd2003_contexts)\n",
    "\n",
    "for c_fn in contexts:\n",
    "    assert path.exists(c_fn), \"N-gram contexts file {0} does not exist.\".format(c_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram context files are taken from\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    " \n",
    "See those repositories for more information on how the contexts were extracted. (*NB*: The context files are not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0d: Check for language model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:25:58.638965Z",
     "start_time": "2019-06-04T22:25:58.628758Z"
    }
   },
   "outputs": [],
   "source": [
    "fisher_lm_dir = 'LM_Fisher'\n",
    "fisher_lm_fn = 'fisher_utterances_main_4gram.mmap'\n",
    "fisher_lm_fp = path.join(fisher_lm_dir, fisher_lm_fn)\n",
    "\n",
    "fisher_lm_vocab_fn = 'fisher_vocabulary_main.txt'\n",
    "fisher_lm_vocab_fp = path.join(fisher_lm_dir, fisher_lm_vocab_fn)\n",
    "\n",
    "assert path.exists(fisher_lm_fp), 'Language model {0} not found'.format(fisher_lm_fp)\n",
    "assert path.exists(fisher_lm_vocab_fp), 'Language model vocabulary {0} not found'.format(fisher_lm_vocab_fp)\n",
    "\n",
    "fisher_lm_fps = {'lm':fisher_lm_fp, \n",
    "                 'vocab':fisher_lm_vocab_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (memory mapped) 4-gram language model is copied from the output of this repository:\n",
    " - https://github.com/emeinhardt/fisher-lm\n",
    " \n",
    "See that repository for more information. (*NB* Again, the language model file is not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment inventory alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Define inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment inventory of any given transcribed lexicon and the segment inventory of the gating data often do not line up. For the gating data to be usefully applied to a given lexicon of transcriptions, the strings in the (segmental) lexicon must contain only segments found in the gating data stimuli inventory.\n",
    "\n",
    "To ensure this happens, the notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` \n",
    " - takes as inputs \n",
    "     - a transcribed lexicon file path and a gating data file path\n",
    "     - a lexicon projection file path and a gating data projection file path\n",
    " - identifies the inventories of each and what symbols are relatively unique to the lexicon and the gating data\n",
    " - produces \n",
    "   - *a Jupyter notebook* for **you to open and finish by defining two projection functions** (i.e. Python dictionaries) to be applied to strings in the transcribed lexicon and to the gating data (one function for each). When you finish doing this (and set an export flag in the notebook to True and run the remainder of the notebook), this notebook will produce\n",
    "     - two *.json files storing these projections* according to the previously provided output file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.456543Z",
     "start_time": "2019-05-30T18:55:25.452481Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf LTR*_aligned_w_*\n",
    "# %rm -rf *\" alignment definition\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will only succeed if the American English gating data of Warner, McQueen, and Cutler (2014) is contained in the repo directory with a particular directory and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:03.419264Z",
     "start_time": "2019-06-04T22:26:03.415191Z"
    }
   },
   "outputs": [],
   "source": [
    "gating_data_folder = 'GD_AmE'\n",
    "gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "gating_data_fp = path.join(gating_data_folder, gating_data_fn)\n",
    "\n",
    "assert path.exists(gating_data_folder), 'AmE gating data folder {0} not found in repo directory'.format(gating_data_folder)\n",
    "assert path.exists(gating_data_fp), 'AmE gating data {0} not found in repo directory'.format(gating_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third cell below will create a notebook for alignment projection definitions for each of the transcribed lexicons from the previous step and the AmE gating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:05.085873Z",
     "start_time": "2019-06-04T22:26:05.081554Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeExtension(fp):\n",
    "    dir_name = path.dirname(fp)\n",
    "    file_name = path.basename(fp)\n",
    "    ext = file_name.split('.')[-1]\n",
    "    rest = '.'.join( file_name.split('.')[:-1] )\n",
    "    return path.join(dir_name, rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:05.252316Z",
     "start_time": "2019-06-04T22:26:05.243978Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment_arg_bundles = []\n",
    "for LTR_dirname in LTR_folders_to_process:\n",
    "    LTR_fn = LTR_dirname + '.tsv'\n",
    "    LTR_fp = path.join(LTR_dirname, LTR_fn)\n",
    "    \n",
    "    nb_output_name = 'GD_AmE-diphones - ' + LTR_dirname + ' alignment definition' + '.ipynb'\n",
    "    my_g = gating_data_fp\n",
    "    my_l = LTR_fp\n",
    "    my_s = 'destressed'\n",
    "    \n",
    "    gd_alignment_dn = 'GD_AmE_' + my_s + '_' + 'aligned_w_' + LTR_dirname\n",
    "    gd_alignment_fn = 'alignment_of_' + removeExtension(gating_data_fn) + '_w_' + LTR_dirname + '.json'\n",
    "    gd_alignment_fp = path.join(gd_alignment_dn, gd_alignment_fn)\n",
    "    if not path.exists(gd_alignment_dn):\n",
    "        makedirs(gd_alignment_dn)\n",
    "    my_gp = gd_alignment_fp\n",
    "    \n",
    "    ltr_alignment_dn = LTR_dirname + '_aligned_w_' + 'GD_AmE_' + my_s\n",
    "    ltr_alignment_fn = 'alignment_of_' + LTR_dirname + '_w_' + removeExtension(gating_data_fn) + '.json'\n",
    "    ltr_alignment_fp = path.join(ltr_alignment_dn, ltr_alignment_fn)\n",
    "    if not path.exists(ltr_alignment_dn):\n",
    "        makedirs(ltr_alignment_dn)\n",
    "    my_lp = ltr_alignment_fp\n",
    "    \n",
    "    \n",
    "    my_arg_bundle = OrderedDict({\n",
    "        'LTR_dirname':LTR_dirname,\n",
    "        'LTR_fn':LTR_fn,\n",
    "        'LTR_fp':LTR_fp,\n",
    "        'gd_alignment_dn':gd_alignment_dn,\n",
    "        'gd_alignment_fn':gd_alignment_fn,\n",
    "        'gd_alignment_fp':gd_alignment_fp,\n",
    "        'ltr_alignment_dn':ltr_alignment_dn,\n",
    "        'ltr_alignment_fn':ltr_alignment_fn,\n",
    "        'ltr_alignment_fp':ltr_alignment_fp,\n",
    "        'align_def_nb_output_name':nb_output_name,\n",
    "        'my_g':my_g,\n",
    "        'my_l':my_l,\n",
    "        'my_s':my_s,\n",
    "        'my_gp':my_gp,\n",
    "        'my_lp':my_lp,\n",
    "    })\n",
    "    alignment_arg_bundles.append(my_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.564555Z",
     "start_time": "2019-05-30T18:55:25.483841Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 37.26it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 37.65it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 37.35it/s]\u001b[A\n",
      " 23%|██▎       | 15/64 [00:00<00:01, 29.49it/s]\u001b[A\n",
      " 28%|██▊       | 18/64 [00:00<00:01, 29.46it/s]\u001b[A\n",
      " 34%|███▍      | 22/64 [00:00<00:01, 30.61it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:01<00:03, 11.30it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:02, 13.89it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 16.39it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:04<00:10,  2.74it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:07,  3.54it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:06<00:07,  3.06it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:06<00:05,  4.10it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:03,  5.52it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:06<00:02,  7.31it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:06<00:01,  9.25it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:00, 11.58it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:06<00:00, 14.10it/s]\u001b[A\n",
      " 94%|█████████▍| 60/64 [00:07<00:00, 16.52it/s]\u001b[A\n",
      " 98%|█████████▊| 63/64 [00:07<00:00, 18.73it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:10<00:20, 10.07s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 35.42it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 36.04it/s]\u001b[A\n",
      " 20%|██        | 13/64 [00:00<00:01, 37.00it/s]\u001b[A\n",
      " 27%|██▋       | 17/64 [00:00<00:01, 35.92it/s]\u001b[A\n",
      " 31%|███▏      | 20/64 [00:00<00:01, 33.21it/s]\u001b[A\n",
      " 38%|███▊      | 24/64 [00:00<00:01, 32.84it/s]\u001b[A\n",
      " 42%|████▏     | 27/64 [00:00<00:01, 22.29it/s]\u001b[A\n",
      " 48%|████▊     | 31/64 [00:01<00:01, 24.86it/s]\u001b[A\n",
      " 53%|█████▎    | 34/64 [00:01<00:01, 20.46it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:10,  2.51it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:09,  2.63it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:06<00:07,  2.81it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:04,  3.83it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:06<00:03,  5.12it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:06<00:01,  6.72it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:01,  8.75it/s]\u001b[A\n",
      " 91%|█████████ | 58/64 [00:07<00:00, 11.11it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:07<00:00, 13.10it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:20<00:10, 10.08s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 33.08it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 34.75it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 36.16it/s]\u001b[A\n",
      " 25%|██▌       | 16/64 [00:00<00:01, 35.28it/s]\u001b[A\n",
      " 30%|██▉       | 19/64 [00:00<00:01, 33.07it/s]\u001b[A\n",
      " 34%|███▍      | 22/64 [00:00<00:01, 31.18it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:00<00:01, 22.57it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 24.20it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 25.62it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:04<00:09,  2.93it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:06,  3.86it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:06,  3.61it/s]\u001b[A\n",
      " 67%|██████▋   | 43/64 [00:05<00:04,  4.88it/s]\u001b[A\n",
      " 72%|███████▏  | 46/64 [00:05<00:02,  6.50it/s]\u001b[A\n",
      " 77%|███████▋  | 49/64 [00:05<00:01,  8.42it/s]\u001b[A\n",
      " 81%|████████▏ | 52/64 [00:05<00:01, 10.47it/s]\u001b[A\n",
      " 86%|████████▌ | 55/64 [00:05<00:00, 12.50it/s]\u001b[A\n",
      " 91%|█████████ | 58/64 [00:06<00:00, 14.99it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:06<00:00, 17.40it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:29<00:00,  9.73s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~30s on wittgenstein\n",
    "for arg_bundle in tqdm(alignment_arg_bundles):\n",
    "    nb = pm.execute_notebook(\n",
    "        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "        arg_bundle['align_def_nb_output_name'],\n",
    "        parameters=dict(g = arg_bundle['my_g'], \n",
    "                        l = arg_bundle['my_l'], \n",
    "                        s = arg_bundle['my_s'], \n",
    "                        gp = arg_bundle['my_gp'], \n",
    "                        lp = arg_bundle['my_lp'])\n",
    "    )\n",
    "#     pm.execute_notebook(\n",
    "#        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "#        nb_output_name,\n",
    "#        parameters=dict(g = my_g, l = my_l, s = my_s, gp = my_gp, lp = my_lp)\n",
    "#     )\n",
    "    print(\"Finished creating alignment definition notebook '{0}'.\\nOpen and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\\n\".format(arg_bundle['align_def_nb_output_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Apply inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.568434Z",
     "start_time": "2019-05-30T18:55:54.566037Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf *\" alignment application \"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will succeed if you have run each of the previously produced notebooks correctly and produced a projection mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:10.941777Z",
     "start_time": "2019-06-04T22:26:10.839198Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    assert path.exists(args['gd_alignment_fp']), 'Gating data alignment projection mapping not found:\\n\\t{0}'.format(args['gd_alignment_fp'])\n",
    "    assert path.exists(args['ltr_alignment_fp']), 'Transcribed lexicon data alignment projection mapping not found:\\n\\t{0}'.format(args['ltr_alignment_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are inventory alignment projections actually applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Align transcriptions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below applies each pair of alignment projections to each matched pair of gating data and transcribed lexicon choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:21.158533Z",
     "start_time": "2019-06-04T22:26:21.152036Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    LTR_fn = args['LTR_fn']\n",
    "    \n",
    "#     my_pg = args['my_gp']\n",
    "#     my_g = args['my_g']\n",
    "    my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    args['my_og'] = my_og\n",
    "    \n",
    "#     my_pl = args['my_lp']\n",
    "#     my_l = args['my_l']\n",
    "    my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    args['my_ol'] = my_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:56:38.846995Z",
     "start_time": "2019-05-30T18:55:54.747961Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tLTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_newdic_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tLTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:03<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_CMU_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tLTR_Buckeye/LTR_Buckeye.tsv\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_Buckeye/LTR_Buckeye.tsv\n",
      "Result saved to\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~45s on wittgenstein\n",
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "#     LTR_fn = args['LTR_fn']\n",
    "    \n",
    "    my_pg = args['my_gp']\n",
    "    my_g = args['my_g']\n",
    "#     my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "#     my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "#     args['my_og'] = my_og\n",
    "    my_og = args['my_og']\n",
    "    print(\"Creating notebook '{0}' w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}\".format(args['align_apply_gd_nb_output_name'], my_pg, my_g, my_og))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_gd_nb_output_name'],\n",
    "        parameters=dict(p = my_pg,\n",
    "                        g = my_g,\n",
    "                        o = my_og)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tg = {1}\\nResult saved to\\n\\t{2}'.format(my_pg, my_g, my_og))\n",
    "    print(' ')\n",
    "    \n",
    "    my_pl = args['my_lp']\n",
    "    my_l = args['my_l']\n",
    "#     my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "#     my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "#     args['my_ol'] = my_ol\n",
    "    my_ol = args['my_ol']\n",
    "    print('Creating notebook {0} w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}'.format(args['align_apply_ltr_nb_output_name'], my_pg, my_l, my_ol))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_ltr_nb_output_name'],\n",
    "        parameters=dict(p = my_pl,\n",
    "                        l = my_l,\n",
    "                        o = my_ol)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tl = {1}\\nResult saved to\\n\\t{2}'.format(my_pl, my_l, my_ol))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generating channel and (orthographic) lexicon distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Generating channel distributions and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:32.327737Z",
     "start_time": "2019-06-04T22:26:32.211622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "%ls -d GD_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:32.403203Z",
     "start_time": "2019-06-04T22:26:32.398466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_folders = ('GD_AmE', ) + tuple(map(lambda ab: ab['gd_alignment_dn'], alignment_arg_bundles))\n",
    "gating_data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:32.548134Z",
     "start_time": "2019-06-04T22:26:32.543540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_fps = ('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',) + \\\n",
    "                  tuple(map(lambda ab: ab['my_og'], alignment_arg_bundles))\n",
    "gating_data_fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (for downstream convenience) we identify the $n$-phones (not) contained in and (not) constructible from each of the versions of the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** This is done by calling the notebook `Run n-phone analysis of gating data.ipynb`, passing it **the filepath to a gating data `.csv`/`.tsv` file** and **a path to an output directory** for the dozen or so files the notebook will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:36.999917Z",
     "start_time": "2019-05-30T18:56:38.976352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE/GD_AmE n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'GD_AmE n-phone analysis.ipynb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~2m on wittgenstein\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    gd_dir = path.dirname(gating_data_fp)\n",
    "    nb = pm.execute_notebook(\n",
    "        'Run n-phone analysis of gating data.ipynb',\n",
    "#         args['align_apply_ltr_nb_output_name'],\n",
    "        path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "        parameters=dict(g = gating_data_fp,\n",
    "                        o = gd_dir)\n",
    "    )\n",
    "    listdir(gd_dir)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the notebook `Producing channel distributions.ipynb` will create `.json` files defining (among other things) a uniphone and triphone channel distribution. It requires the following arguments to specify information about what kind of channel model to build and where to put it:\n",
    " - **a filepath** to a gating data file\n",
    " - **a directory** containing metadata indicating possible/impossible $n$-phones\n",
    " - **a string argument** (\"stressed\" or \"destressed\") indicating whether the distribution will be over a segment inventory with or without stress information\n",
    " - **a real valued**  smoothing parameter (a pseudocount to add to every channel outcome)\n",
    " - **an output directory** to write the channel model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:41.402468Z",
     "start_time": "2019-06-04T22:26:41.399557Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudocounts = (0, 0.01, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:42.502995Z",
     "start_time": "2019-06-04T22:26:42.495973Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_arg_bundles = []\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    metadata_dir = path.dirname(gating_data_fp)\n",
    "    s = \"destressed\"\n",
    "    channel_model_dir_stem = 'CM' + metadata_dir[2:]\n",
    "    \n",
    "    for pc in pseudocounts:\n",
    "        channel_model_dir_suffix = '_pseudocount' + str(pc)\n",
    "        if metadata_dir == 'GD_AmE':\n",
    "            channel_model_dir = channel_model_dir_stem + '_' + s + '_unaligned' + channel_model_dir_suffix\n",
    "        else:\n",
    "            channel_model_dir = channel_model_dir_stem + channel_model_dir_suffix\n",
    "        nb_output_name = 'Producing channel distributions from ' + metadata_dir + ', pc={0}'.format(pc) + '.ipynb'\n",
    "        new_arg_bundle = {'gating_data_fp':gating_data_fp,\n",
    "                          'metadata_dir':metadata_dir,\n",
    "                          's':s,\n",
    "                          'c':pc,\n",
    "                          'cm_dir':channel_model_dir,\n",
    "                          'nb_output_name':nb_output_name}\n",
    "        cm_arg_bundles.append(new_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:44.358791Z",
     "start_time": "2019-06-04T22:26:44.354919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gating_data_fp': 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'metadata_dir': 'GD_AmE',\n",
       " 's': 'destressed',\n",
       " 'c': 0,\n",
       " 'cm_dir': 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'nb_output_name': 'Producing channel distributions from GD_AmE, pc=0.ipynb'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arg_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:09.934163Z",
     "start_time": "2019-05-27T17:18:09.972176Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0/Producing channel distributions from GD_AmE, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.01/Producing channel distributions from GD_AmE, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.05/Producing channel distributions from GD_AmE, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [08:01<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.1/Producing channel distributions from GD_AmE, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:48<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:56<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:50<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:52<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:59<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:53<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:49<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:57<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:55<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~110m on wittgenstein\n",
    "for ab in cm_arg_bundles:\n",
    "    ensure_dir_exists(ab['cm_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing channel distributions.ipynb',\n",
    "    path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "    parameters=dict(g = ab['gating_data_fp'],\n",
    "                    m = ab['metadata_dir'],\n",
    "                    s = ab['s'],\n",
    "                    c = ab['c'],\n",
    "                    o = ab['cm_dir'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are channel distributions created?**\n",
    "\n",
    "Take a look at `Producing channel distributions.ipynb`. Besides removing stress information, there are some mathematically non-trivial details that go into defining both uniphone and triphone channel distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a language model $m$ (defined by a `.arpa` file or `kenlm` memory-mapped analogue) \n",
    " - a choice of n-gram contexts $C$ (a `.txt` file with one context per line)\n",
    " - a vocabulary $V$ (a `.txt` file with one word per line)\n",
    " - a (partial) output filepath $o$ / output filepath prefix $o$\n",
    " \n",
    "`Producing contextual distributions.ipynb` will write a serialized/memory-mapped `numpy` array to $o$.hV_C that defines $-log_2( p(V|C) )$ - slightly transformed output from `kenlm`. It will also write $p(V|C)$ to $o$.pV_C, and copy both $V$ and $C$ to the base directory specified by $o$. (In both cases, each column is associated with the distribution $p(V|c)$ for some $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:50.605114Z",
     "start_time": "2019-06-04T22:26:50.600515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('buckeye_contexts.txt', 'swbd2003_contexts.txt')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts\n",
    "fisher_lm_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:51.591733Z",
     "start_time": "2019-06-04T22:26:51.585185Z"
    }
   },
   "outputs": [],
   "source": [
    "LDs = [{'LD_dir':'LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':buckeye_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_Buckeye_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')},\n",
    "       {'LD_dir':'LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_swbd2003_contexts' + '/' + 'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':swbd2003_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_swbd2003_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')}\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T20:21:42.664184Z",
     "start_time": "2019-05-26T19:00:16.251722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/86 [00:00<00:03, 24.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 6/86 [00:00<00:03, 26.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 9/86 [00:00<00:02, 26.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 12/86 [00:00<00:02, 26.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 15/86 [00:00<00:02, 26.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 18/86 [00:00<00:02, 25.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 21/86 [00:00<00:03, 18.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 24/86 [00:01<00:03, 19.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 27/86 [00:01<00:02, 20.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 30/86 [00:01<00:02, 19.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/86 [00:01<00:02, 21.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 36/86 [00:01<00:02, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 39/86 [00:01<00:02, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 42/86 [00:01<00:02, 21.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/86 [00:02<00:01, 21.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 48/86 [00:02<00:01, 20.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 51/86 [00:02<00:01, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 54/86 [00:02<00:01, 22.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▋   | 57/86 [00:02<00:01, 21.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:02<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:21<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 61/86 [02:47<20:35, 49.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 62/86 [04:03<22:59, 57.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 63/86 [04:04<15:28, 40.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 64/86 [04:04<10:24, 28.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 65/86 [04:05<07:00, 20.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 66/86 [04:05<04:42, 14.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 67/86 [04:05<03:08,  9.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 68/86 [04:18<03:14, 10.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 69/86 [04:18<02:10,  7.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 70/86 [04:18<01:26,  5.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 71/86 [04:19<00:59,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 72/86 [04:26<01:08,  4.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 73/86 [04:26<00:45,  3.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 74/86 [04:26<00:29,  2.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 75/86 [04:27<00:21,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 76/86 [04:28<00:15,  1.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 77/86 [04:28<00:10,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 79/86 [04:28<00:05,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 81/86 [04:28<00:02,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 82/86 [04:28<00:01,  2.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 83/86 [05:05<00:34, 11.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 84/86 [06:19<01:00, 30.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 85/86 [06:20<00:21, 21.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 86/86 [06:20<00:00, 15.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/87 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/87 [00:00<00:03, 24.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 7/87 [00:00<00:03, 26.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 10/87 [00:00<00:02, 26.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 13/87 [00:00<00:02, 26.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 16/87 [00:00<00:03, 21.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 19/87 [00:00<00:03, 22.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 22/87 [00:01<00:03, 17.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 25/87 [00:01<00:03, 19.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 28/87 [00:01<00:03, 17.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 30/87 [00:01<00:03, 17.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/87 [00:01<00:02, 18.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 36/87 [00:01<00:02, 19.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 39/87 [00:01<00:02, 21.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 42/87 [00:02<00:02, 21.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/87 [00:02<00:01, 22.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 48/87 [00:02<00:01, 22.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 51/87 [00:02<00:01, 22.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 54/87 [00:02<00:01, 22.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 57/87 [00:02<00:01, 22.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:02<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:13<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 61/87 [16:04<2:05:03, 288.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 62/87 [38:25<4:11:48, 604.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 63/87 [38:26<2:49:14, 423.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▎  | 64/87 [38:26<1:53:33, 296.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 65/87 [38:26<1:16:02, 207.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 66/87 [38:26<50:49, 145.23s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 67/87 [38:26<33:53, 101.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 68/87 [42:50<47:35, 150.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 69/87 [42:58<32:19, 107.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 70/87 [43:05<21:58, 77.56s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 71/87 [44:09<19:35, 73.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 72/87 [45:28<18:44, 74.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 73/87 [50:38<33:55, 145.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 74/87 [50:38<22:05, 101.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 75/87 [50:38<14:16, 71.40s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 76/87 [50:43<09:26, 51.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▊ | 77/87 [50:43<06:00, 36.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 78/87 [50:43<03:47, 25.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 79/87 [50:44<02:21, 17.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 80/87 [50:44<01:28, 12.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 81/87 [50:44<00:53,  8.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 82/87 [50:45<00:31,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 83/87 [50:45<00:17,  4.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 84/87 [55:08<04:06, 82.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 85/87 [1:13:43<13:04, 392.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 86/87 [1:13:51<04:36, 276.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 87/87 [1:14:01<00:00, 196.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~80m on wittgenstein\n",
    "for ld in LDs:\n",
    "    output_dir = path.dirname(ld['LD_dir'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing contextual distributions.ipynb',\n",
    "#     'Producing ' + ld['LD_dir'].replace('_', ' ')[3:] + ' contextual distributions.ipynb',\n",
    "    ld['nb_fp'],\n",
    "    parameters=dict(m = ld['m'],\n",
    "                    v = ld['v'],\n",
    "                    c = ld['c'],\n",
    "                    o = ld['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating combinable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic problem:**\n",
    "\n",
    "1. **Channel model + transcribed lexicon relation**: Even after the gating data and a transcribed lexicon relation are defined over the same inventory, \n",
    " - the lexicon may contain triphones that are not in the stimuli triphones of a triphone channel model.\n",
    " - the channel distribution will contain triphones that cannot be found in the transcribed lexicon relation. (While the other steps here are strictly necessary, this is simply a practical step for making downstream computation faster.)\n",
    "2. **Language model + transcribed lexicon relation**: The orthographic vocabulary of a transcribed lexicon relation may contain wordforms not in an n-gram model's vocabulary. (We *don't* want to use the out-of-vocabulary estimate for those wordforms.)\n",
    "3.  **Contextual distributions + transcribed lexicon relation**: The contextual distributions from Step 3b above are defined over the *language model's* orthographic vocabulary, which will likely include wordforms that are not in the transcribed lexicon relation. We want to create modified forms of these distributions where we condition on the choice of an orthographic wordform that is in the transcribed lexicon relation.\n",
    "\n",
    "Once we have\n",
    " - a version $l'$ of the transcribed lexicon relation $l$ trimmed with respect to both the triphones of the channel model $c$ and the (orthographic) vocabulary of a language model $m$\n",
    " - a version $d'$ of the contextual distributions over $m$'s vocabulary (with respect to some set of n-gram contexts) $d$ trimmed to only define distributions over $l'$\n",
    " - a version $c'$ of the channel model $c$ trimmed with respect to a transcribed lexicon relation $l'$\n",
    " - a probability distribution over segmental wordforms given an orthographic wordform\n",
    " \n",
    "we will be able to combine everything together to calculate confusability of wordforms in corpus contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather relevant LTRs and their associated CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:26:59.317049Z",
     "start_time": "2019-06-04T22:26:59.311854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs = list(map(lambda ab: {'LTR_fp':ab['my_ol'],\n",
    "                                    'GD_fp':ab['my_og']},\n",
    "                        alignment_arg_bundles))\n",
    "list(map(lambda d: d['LTR_fp'],\n",
    "         aligned_LTRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:00.081220Z",
     "start_time": "2019-06-04T22:27:00.074569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CM_dirs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "#                                 'GD_fp':cab['gating_data_fp']},\n",
    "#                    filter(lambda cab: cab['c'] != 0, cm_arg_bundles)))\n",
    "# CM_dirs\n",
    "# CM_dirs[5]\n",
    "# # listdir(CM_dirs[0][])\n",
    "\n",
    "aligned_CMs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "                                    'GD_fp':cab['gating_data_fp']},\n",
    "                       filter(lambda cab: cab['c'] != 0 and 'aligned' in cab['gating_data_fp'], \n",
    "                              cm_arg_bundles)))\n",
    "aligned_CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:02.236030Z",
     "start_time": "2019-06-04T22:27:02.232251Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aligned_CMs(ltr_bundle):\n",
    "    matches = [cm_bundle for cm_bundle in aligned_CMs if cm_bundle['GD_fp'] == ltr_bundle['GD_fp']]\n",
    "    return list(map(lambda d: d['CM_fp'],\n",
    "                    matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:02.913121Z",
     "start_time": "2019-06-04T22:27:02.907924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs[0]['LTR_fp']\n",
    "\n",
    "#NB: all of these will have the same set of stimuli triphones\n",
    "#    ...which is all we care about here\n",
    "get_aligned_CMs(aligned_LTRs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:03.649076Z",
     "start_time": "2019-06-04T22:27:03.644274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs_and_CM = [{'LTR_fp':ltr['LTR_fp'],\n",
    "                        'matching_CMs':get_aligned_CMs(ltr)}\n",
    "                       for ltr in aligned_LTRs]\n",
    "aligned_LTRs_and_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:05.175639Z",
     "start_time": "2019-06-04T22:27:05.168404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "nb = LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    each['c'] = each['matching_CMs'][0]\n",
    "    each['l'] = each['LTR_fp']\n",
    "    o_dir = path.dirname(each['LTR_fp'])\n",
    "    o_fn = path.basename(each['LTR_fp']).split('w_')[0] + 'CM_filtered' + '.tsv'\n",
    "    each['o'] = path.join(o_dir, o_fn)\n",
    "    \n",
    "    nb_fn = 'Filter ' + o_fn.split('_aligned')[0] + ' against channel model.ipynb'\n",
    "    each['nb_fp'] = path.join(o_dir, nb_fn)\n",
    "    \n",
    "    print('c = {0}\\nl = {1}\\no = {2}\\nnb = {3}'.format(each['c'], each['l'], each['o'], each['nb_fp']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:43.618439Z",
     "start_time": "2019-05-30T19:00:28.488328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:02<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:04<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:02<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about 20s on wittgenstein\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter transcription lexicon by channel model.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** For this step, we will use shell commands provided by the `csvkit` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:08.405277Z",
     "start_time": "2019-06-04T22:27:08.400881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_fps\n",
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:10.629869Z",
     "start_time": "2019-06-04T22:27:10.623370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_fps = list(map(lambda pair: pair['LTR_fp'],\n",
    "                   aligned_LTRs))\n",
    "LTR_fps\n",
    "\n",
    "LTR_CM_filtered = list(map(lambda d: d['o'],\n",
    "                           aligned_LTRs_and_CM))\n",
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lexicon entries have unmodelable triphones? (We'll next check how many such lexicon entries aren't in the language model vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:13.539011Z",
     "start_time": "2019-06-04T22:27:13.048959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "133855 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "7999 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:13.994058Z",
     "start_time": "2019-06-04T22:27:13.541827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:15.485861Z",
     "start_time": "2019-06-04T22:27:15.481806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:15.648491Z",
     "start_time": "2019-06-04T22:27:15.635997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LTR_LM_filter_bundles = []\n",
    "for each_LTR_fp in LTR_CM_filtered:\n",
    "    bundle = dict()\n",
    "    LTR_descr = path.basename(each_LTR_fp)[:-4]\n",
    "    LM_V_descr = path.basename(fisher_lm_vocab_fp)[:-4]\n",
    "    bundle['l'] = each_LTR_fp\n",
    "    bundle['v'] = fisher_lm_vocab_fp\n",
    "    bundle['o'] = each_LTR_fp[:-4] + '_LM_filtered' + '.tsv'\n",
    "    bundle['nb_fp'] = path.join(path.dirname(each_LTR_fp),\n",
    "                                f'Filter {LTR_descr} against {LM_V_descr}' + '.ipynb')\n",
    "    bundle\n",
    "    LTR_LM_filter_bundles.append(bundle)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.279071Z",
     "start_time": "2019-05-30T19:00:57.183644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:44<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:02<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~1m on wittgenstein\n",
    "for each in LTR_LM_filter_bundles:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter transcription lexicon by language model vocabulary.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    v = each['v'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit less than half** of the triphone channel model-modelable `newdic` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:21.340319Z",
     "start_time": "2019-06-04T22:27:21.081972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "9412 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About three quarters** of the triphone channel model-modelable `CMU_destressed` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:22.081462Z",
     "start_time": "2019-06-04T22:27:21.761740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "33125 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Less than 10%** of the triphone channel model-modelable `Buckeye` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:24.877495Z",
     "start_time": "2019-06-04T22:27:24.633317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "6575 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones\n",
    " - $7011 - 6575 = 436$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{436+988}{7999} ≈ 17.8\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting the filtered transcribed lexicon relations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:25.416104Z",
     "start_time": "2019-06-04T22:27:25.411133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered = list(map(lambda bundle: bundle['o'],\n",
    "                                       LTR_LM_filter_bundles))\n",
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:29.626412Z",
     "start_time": "2019-06-04T22:27:29.624378Z"
    }
   },
   "outputs": [],
   "source": [
    "#what channel models might you want to use with what lexicons?\n",
    "# there are 3x3 triphone CMs that are aligned with one of 3 LTRs and have one of 3 relevant pseudocount levels\n",
    "# For each of the 3 LTRs aligned with the gating data, there's exactly 1 `LTR...CM_filtered_LM_filtered.tsv` file\n",
    "# ∴ there are 3x3 channel models to trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:30.524738Z",
     "start_time": "2019-06-04T22:27:30.502331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    LTR_dir = path.dirname(each['LTR_fp'])\n",
    "    LTR_trimmed_fn = path.basename(each['LTR_fp']).split('_w_')[0] + '_CM_filtered_LM_filtered.tsv'\n",
    "    each['LTR_trimmed_fp'] = path.join(LTR_dir, LTR_trimmed_fn)\n",
    "    each['matching_trimmed_CMs'] = [path.join(path.dirname(fp),\n",
    "                                              LTR_trimmed_fn[:-4] + '_' + path.basename(fp))\n",
    "                                    for fp in each['matching_CMs']]\n",
    "    \n",
    "#     each['LTR_fp']\n",
    "    each['LTR_trimmed_fp']\n",
    "    each['matching_CMs']\n",
    "    each['matching_trimmed_CMs']\n",
    "#     each['trimmed LTR_fp']\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:34.663182Z",
     "start_time": "2019-06-04T22:27:34.634748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_LTR_CM_triples = []\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    assert len(each['matching_CMs']) == len(each['matching_trimmed_CMs'])\n",
    "    \n",
    "    for i in range(len(each['matching_CMs'])):\n",
    "        args = dict()\n",
    "        args['l'] = each['LTR_trimmed_fp']\n",
    "        args['c'] = each['matching_CMs'][i]\n",
    "        args['o'] = each['matching_trimmed_CMs'][i]\n",
    "        args['nb_fp'] = path.join(path.dirname(args['c']),\n",
    "                                  f\"Filter {path.dirname(args['c'])} against {path.basename(args['l'])[:-4]}.ipynb\")\n",
    "        args\n",
    "        trimmed_LTR_CM_triples.append(args)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.285877Z",
     "start_time": "2019-05-30T19:01:55.022428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about 70s on wittgenstein\n",
    "for each in trimmed_LTR_CM_triples:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "#     if not path.exists(output_dir):\n",
    "#         print(f\"Creating output path '{output_dir}'\")\n",
    "#         makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter channel model by transcription lexicon.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:41.288143Z",
     "start_time": "2019-06-04T22:27:41.284606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:41.443548Z",
     "start_time": "2019-06-04T22:27:41.439589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LD_dir': 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       "  'o_fn_stem': 'LD_fisher_vocab_in_buckeye_contexts',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'buckeye_contexts.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_contexts/Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb'},\n",
       " {'LD_dir': 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       "  'o_fn_stem': 'LD_fisher_vocab_in_swbd2003_contexts',\n",
       "  'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'swbd2003_contexts.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:41.835849Z",
     "start_time": "2019-06-04T22:27:41.832373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:42.391525Z",
     "start_time": "2019-06-04T22:27:42.386976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buckeye_contexts.txt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'swbd2003_contexts.txt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckeye_contexts\n",
    "swbd2003_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:47.476669Z",
     "start_time": "2019-06-04T22:27:47.469286Z"
    }
   },
   "outputs": [],
   "source": [
    "LD_projection_args = [\n",
    "    {'d':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':buckeye_contexts,\n",
    "     'l':'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts' + '_projected_' + 'LTR_Buckeye' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_buckeye_contexts' + ' against ' + 'LTR_Buckeye_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "    {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':swbd2003_contexts,\n",
    "     'l':'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_CMU_destressed' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "    {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':swbd2003_contexts,\n",
    "     'l':'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_newdic_destressed' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T23:30:24.108534Z",
     "start_time": "2019-06-02T23:19:53.180487Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 5/42 [00:00<00:00, 44.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 11/42 [00:00<00:00, 48.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 14/42 [00:00<00:01, 27.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 19/42 [00:00<00:00, 30.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 22/42 [00:00<00:00, 26.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 25/42 [00:02<00:02,  6.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 27/42 [00:03<00:04,  3.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 29/42 [00:11<00:18,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 32/42 [00:12<00:11,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 33/42 [00:14<00:12,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 35/42 [00:14<00:06,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 36/42 [00:15<00:04,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 38/42 [00:15<00:02,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 40/42 [00:23<00:03,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 5/42 [00:00<00:00, 49.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 12/42 [00:00<00:00, 39.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 16/42 [00:00<00:00, 39.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 20/42 [00:00<00:00, 36.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 23/42 [00:00<00:00, 26.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 23/42 [00:10<00:00, 26.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 26/42 [00:12<00:19,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 29/42 [01:09<01:24,  6.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 32/42 [01:15<00:51,  5.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 33/42 [02:28<03:48, 25.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 34/42 [02:30<02:28, 18.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 36/42 [02:39<01:25, 14.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 37/42 [02:42<00:54, 10.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 39/42 [06:44<02:11, 43.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 41/42 [06:44<00:30, 30.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 42/42 [06:44<00:00,  9.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 6/42 [00:00<00:00, 56.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 12/42 [00:00<00:01, 20.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 17/42 [00:00<00:01, 24.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 20/42 [00:01<00:00, 25.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 23/42 [00:01<00:00, 26.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 26/42 [00:04<00:05,  2.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 26/42 [00:20<00:05,  2.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 29/42 [01:01<01:16,  5.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 32/42 [01:02<00:43,  4.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 33/42 [01:57<02:54, 19.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 34/42 [01:58<01:50, 13.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 36/42 [02:00<01:00, 10.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 37/42 [02:01<00:36,  7.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 39/42 [03:12<00:47, 15.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 42/42 [03:12<00:00, 11.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~10-15m on wittgenstein:\n",
    "#  ≤30s for Buckeye vocab + Buckeye contexts\n",
    "#  ≈6.5-7m for CMU_destressed vocab in swbd2003 contexts\n",
    "#  ≈3m for newdic_destressed vocab in swbd2003 contexts\n",
    "for each in LD_projection_args:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(d = each['d'],\n",
    "                    v = each['v'],\n",
    "                    c = each['c'],\n",
    "                    l = each['l'],\n",
    "                    o = each['o'],\n",
    "                    f = each['f'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:50.133019Z",
     "start_time": "2019-06-04T22:27:50.129510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:50.289972Z",
     "start_time": "2019-06-04T22:27:50.283091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles = []\n",
    "for ltr_fp in LTR_CM_filtered_LM_filtered:\n",
    "    LTR_n = path.basename( ltr_fp )[:-4]\n",
    "    LTR_dir = path.dirname( ltr_fp )\n",
    "    pW_V_fp_bundles.append({'LTR_fp':ltr_fp,\n",
    "                          'pW_V_fp':ltr_fp[:-4],# + '.pW_V',\n",
    "                          'nb_fp':path.join(LTR_dir,'Define pW_V given {0}.ipynb'.format(LTR_n))})\n",
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:42.869046Z",
     "start_time": "2019-05-30T19:03:01.337096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:07<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:23<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:04<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~30s on wittgenstein\n",
    "for each in pW_V_fp_bundles:\n",
    "    pW_V_fp_output_dir = path.dirname(each['pW_V_fp'])\n",
    "    ensure_dir_exists(pW_V_fp_output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['LTR_fp'],\n",
    "                    o = each['pW_V_fp'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pre-calculate remaining forward model components and meta-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that none of these steps need actually be ordered with respect to each other: the ordering below is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Generate triphone lexicon distributions for every triphone channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:55.171549Z",
     "start_time": "2019-06-04T22:27:55.167264Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in listdir(a_dir)\n",
    "            if path.isdir(path.join(a_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:55.328554Z",
     "start_time": "2019-06-04T22:27:55.299653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs = get_immediate_subdirectories('.')\n",
    "len(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:27:59.207876Z",
     "start_time": "2019-06-04T22:27:57.980439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'old/Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'old/IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_model_fps = []\n",
    "for d in subdirs:\n",
    "    files = listdir(d)\n",
    "    is_triph_channel_model = lambda fn: 'pY1X0X1X2.json' in fn\n",
    "    CM_files = list(filter(is_triph_channel_model,\n",
    "                           files))\n",
    "    for CM_file in CM_files:\n",
    "        channel_model_fps.append(path.join(d, CM_file))\n",
    "len(channel_model_fps)\n",
    "channel_model_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:28:02.966489Z",
     "start_time": "2019-06-04T22:28:02.913259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'old/Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'output_dir': 'old',\n",
       " 'c_fn': 'Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'Hammond-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'o': 'old/Hammond-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'nb_fp': 'old/Generating Hammond-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'old/IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'output_dir': 'old',\n",
       " 'c_fn': 'IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'IPhOD-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'o': 'old/IPhOD-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'nb_fp': 'old/Generating IPhOD-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "triph_lex_bundles = []\n",
    "for cm_fp in channel_model_fps:\n",
    "    bundle = dict()\n",
    "    bundle['c'] = cm_fp\n",
    "    bundle['output_dir'] = path.dirname(cm_fp)\n",
    "    bundle['c_fn'] = path.basename(cm_fp)\n",
    "    bundle['o_fn_prefix'] = bundle['c_fn'].split('pY1X0X1X2')[0] + 'pX0X1X2'\n",
    "    bundle['o'] = path.join(bundle['output_dir'],\n",
    "                            bundle['o_fn_prefix'])\n",
    "    bundle['nb_fp'] = path.join(bundle['output_dir'],\n",
    "                                f\"Generating {bundle['c_fn'].split('pY1X0X1X2.json')[0][:-1]} uniform triphone lexicon dist.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    triph_lex_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:04:56.478891Z",
     "start_time": "2019-05-30T19:03:43.559502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 32.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 33.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 36.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 36.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: old/Generating Hammond-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: old/Generating IPhOD-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 28.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 37.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~1m on wittgenstein\n",
    "for bundle in triph_lex_bundles:\n",
    "    output_dir = bundle['output_dir']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path {output_dir}\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Generate triphone lexicon distribution from channel model.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(c = bundle['c'],\n",
    "                    o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is comparable to the `Metadata` generation step in 2a above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:28:06.481588Z",
     "start_time": "2019-06-04T22:28:06.477557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fps = [each['pW_V_fp'] + '.pW_V.json' for each in pW_V_fp_bundles]\n",
    "pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:35:17.749791Z",
     "start_time": "2019-05-29T16:35:17.747869Z"
    }
   },
   "outputs": [],
   "source": [
    "# pW_V_fps = []\n",
    "# for d in subdirs:\n",
    "#     files = listdir(d)\n",
    "#     is_pW_V = lambda fn: 'pW_V.json' in fn\n",
    "#     pW_V_files = list(filter(is_pW_V,\n",
    "#                            files))\n",
    "#     for pW_V_file in pW_V_files:\n",
    "#         pW_V_fps.append(path.join(d, pW_V_file))\n",
    "# len(pW_V_fps)\n",
    "# pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T22:28:12.731509Z",
     "start_time": "2019-06-04T22:28:12.718212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "lexicon_md_bundles = []\n",
    "for pW_V_fp in pW_V_fps:\n",
    "    output_dir = bundle['o']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path '{output_dir}'\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    bundle = dict()\n",
    "    bundle['p'] = pW_V_fp\n",
    "    bundle['o'] = path.dirname(pW_V_fp)\n",
    "    bundle['lex_name'] = path.basename(pW_V_fp).split('.pW_V.json')[0]\n",
    "    bundle['nb_fp'] = path.join(bundle['o'],\n",
    "                                f\"Calculate prefix data, k-cousins, and k-spheres for {bundle['lex_name']}.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    lexicon_md_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.377297Z",
     "start_time": "2019-06-04T22:50:50.359428Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate prefix data, k-cousins, and k-spheres.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 171/171 [45:52<00:00,  1.95s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate prefix data, k-cousins, and k-spheres.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 171/171 [21:02:49<00:00, 67.45s/it]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate prefix data, k-cousins, and k-spheres.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 171/171 [19:26<00:00,  1.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with J=-1, heavy CPU load, and no .npz export\n",
    "#  - newdic CM_filtered_LM_filtered takes ~20-30m\n",
    "#  - CMU CM_filtered_LM_filtered takes ~3.5-3.75h\n",
    "# - Buckeye CM_filtered_LM_filtered takes ~10-15m\n",
    "\n",
    "# with J=-1, no CPU load, *and* .npz export\n",
    "#  - newdic CM_filtered_LM_filtered takes ~45m\n",
    "#  - CMU CM_filtered_LM_filtered takes ~20.5h\n",
    "# - Buckeye CM_filtered_LM_filtered takes ~19.5h\n",
    "# (.npz representations are up to 20x smaller on disk \n",
    "#  and are significantly smaller in memory when loaded)\n",
    "for bundle in lexicon_md_bundles:\n",
    "    output_dir = bundle['o']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path {output_dir}\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(p = bundle['p'],\n",
    "                    o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4c: Calculate the marginal probability $p(W|C)$ of each segmental wordform $w$ given $n$-gram contexts $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather pV_C, pW_V fp pairs as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.384542Z",
     "start_time": "2019-06-05T20:59:23.381032Z"
    }
   },
   "outputs": [],
   "source": [
    "def LTR_to_pW_Vs(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['LTR_fp'] == LTR_fp,\n",
    "                                   pW_V_fp_bundles))\n",
    "    matching_pW_V_fps = set(map(lambda bundle: bundle['pW_V_fp'],\n",
    "                                matching_bundles))\n",
    "    return matching_pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.424004Z",
     "start_time": "2019-06-05T20:59:23.386654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.430651Z",
     "start_time": "2019-06-05T20:59:23.426700Z"
    }
   },
   "outputs": [],
   "source": [
    "def LTR_to_LD(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['l'] == LTR_fp,\n",
    "                                   LD_projection_args))\n",
    "    matching_pV_C_fps = set(map(lambda bundle: bundle['o'],\n",
    "                                matching_bundles))\n",
    "    return matching_pV_C_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.436717Z",
     "start_time": "2019-06-05T20:59:23.431690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'buckeye_contexts.txt',\n",
       "  'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_contexts/Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'swbd2003_contexts.txt',\n",
       "  'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_CMU_destressed.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'swbd2003_contexts.txt',\n",
       "  'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C',\n",
       "  'f': 'True',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD_projection_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.444599Z",
     "start_time": "2019-06-05T20:59:23.437832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C'),\n",
       " ('LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_CMU_destressed.pV_C'),\n",
       " ('LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C')}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_matched_pW_V_LD_pairs(LTR_fp):\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matching_pV_C_fps = LTR_to_LD(LTR_fp)\n",
    "    \n",
    "    return set(product(matching_pW_V_fps,\n",
    "                       matching_pV_C_fps))\n",
    "\n",
    "my_LTR_fps = list(map(lambda bundle: bundle['LTR_fp'],\n",
    "                      pW_V_fp_bundles))\n",
    "\n",
    "matched_pW_V_LD_pairs = union(map(get_matched_pW_V_LD_pairs,\n",
    "                                  my_LTR_fps))\n",
    "matched_pW_V_LD_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T20:59:23.456810Z",
     "start_time": "2019-06-05T20:59:23.446112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C.npy',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Calculate segmental wordform distribution for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_contexts/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_CMU_destressed.pV_C.npy',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Calculate segmental wordform distribution for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WD_bundles = []\n",
    "for w,d in matched_pW_V_LD_pairs:\n",
    "    bundle = dict()\n",
    "    \n",
    "    LTR_key = path.basename(w)\n",
    "    LD_key = path.basename(d)\n",
    "    C_key = LD_key.split('LD_fisher_vocab')[1].split('_projected_')[0]\n",
    "    \n",
    "    output_dir = path.dirname(d)\n",
    "    output_prefix = LTR_key + C_key + '.pW_C'\n",
    "    \n",
    "    bundle['d'] = d + '.npy'\n",
    "    bundle['w'] = w + '.pW_V.npz'\n",
    "    \n",
    "    bundle['o'] = path.join(output_dir, output_prefix)\n",
    "    \n",
    "    bundle['nb_fp'] = path.join(output_dir, f\"Calculate segmental wordform distribution for {LTR_key}{C_key.replace('_', ' ')}.ipynb\")\n",
    "    \n",
    "    bundle\n",
    "    WD_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:10:56.356308Z",
     "start_time": "2019-06-05T20:59:23.457940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform distribution given corpus contexts.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Calculate segmental wordform distribution for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb\n",
      "100%|██████████| 30/30 [02:37<00:00, 10.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform distribution given corpus contexts.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Calculate segmental wordform distribution for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb\n",
      "100%|██████████| 30/30 [00:17<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform distribution given corpus contexts.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Calculate segmental wordform distribution for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb\n",
      "100%|██████████| 30/30 [08:27<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ≈5-10m on wittgenstein with no background load\n",
    "#  ≈8-10m for filtered LTR_CMU_destressed in swbd2003 contexts\n",
    "#  ≤30s for filtered LTR_Buckeye in buckeye contexts\n",
    "#  ≈2-3m for filtered LTR_newdic_destressed in swbd2003 contexts\n",
    "for bundle in WD_bundles:\n",
    "    ensure_dir_exists(path.dirname(bundle['o']))\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Calculate segmental wordform distribution given corpus contexts.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(d = bundle['d'],\n",
    "                    w = bundle['w'],\n",
    "                    o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4d: Define channel distributions on a set of segmental wordforms(+prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:10:56.360074Z",
     "start_time": "2019-06-05T21:10:56.358078Z"
    }
   },
   "outputs": [],
   "source": [
    "# gather paired (....pW_V.json, ...pY1X0X1X2.json) p(W|V), p(Y_i|X_{i-1},X_i;X_{i+1}) distributions\n",
    "# output complete wordform channel models into the channel model directory with the same prefix that's on the triphone channel distribution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:10:56.478510Z",
     "start_time": "2019-06-05T21:10:56.361678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def LTR_to_pW_Vs(LTR_fp):\n",
    "#     matching_bundles = list(filter(lambda bundle: bundle['LTR_fp'] == LTR_fp,\n",
    "#                                    pW_V_fp_bundles))\n",
    "#     matching_pW_V_fps = set(map(lambda bundle: bundle['pW_V_fp'],\n",
    "#                                 matching_bundles))\n",
    "#     return matching_pW_V_fps\n",
    "\n",
    "def LTR_to_TCMs(LTR_fp):\n",
    "    matching_bundles = list(filter(lambda bundle: bundle['l'] == LTR_fp,\n",
    "                                   trimmed_LTR_CM_triples))\n",
    "    matching_TCM_fps = set(map(lambda bundle: bundle['o'],\n",
    "                               matching_bundles))\n",
    "    return matching_TCM_fps\n",
    "\n",
    "def matched_pW_Vs_and_TCMs(LTR_fp):\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matching_TCM_fps = LTR_to_TCMs(LTR_fp)\n",
    "    return {'LW_V_fps':matching_pW_V_fps,\n",
    "            'TCM_fps':matching_TCM_fps}\n",
    "\n",
    "def get_matched_pairs(LTR_fp):\n",
    "    matching_TCM_fps = LTR_to_TCMs(LTR_fp)\n",
    "    matching_pW_V_fps = LTR_to_pW_Vs(LTR_fp)\n",
    "    matched_pairs = set(product(matching_TCM_fps,\n",
    "                                matching_pW_V_fps))\n",
    "    return matched_pairs\n",
    "\n",
    "my_LTR_fps = list(map(lambda bundle: bundle['LTR_fp'],\n",
    "                      pW_V_fp_bundles))\n",
    "\n",
    "LCM_bundles = []\n",
    "for c,w in union(map(get_matched_pairs,\n",
    "                     my_LTR_fps)):\n",
    "    bundle = dict()\n",
    "    bundle['c'] = c\n",
    "    bundle['w'] = w + '.pW_V.json'\n",
    "    output_dir = path.dirname(c)\n",
    "    output_prefix = path.basename(c).split('pY1X0X1X2.json')[0]\n",
    "    bundle['o'] = path.join(output_dir, output_prefix)\n",
    "    \n",
    "    bundle['nb_fp'] = path.join(output_dir, f'Calculate wordform channel matrices for {path.basename(w)}.ipynb')\n",
    "    \n",
    "    bundle\n",
    "    LCM_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:23:13.796165Z",
     "start_time": "2019-06-05T21:10:56.479794Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [03:09<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:22<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [03:01<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [03:09<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:21<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Calculate wordform channel matrices for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:21<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:28<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:28<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate segmental wordform and prefix channel matrices.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Calculate wordform channel matrices for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 86/86 [00:28<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~2m on wittgenstein with no background load and no .npz files\n",
    "# takes ~12m on wittgenstein with no background load *and* .npz files\n",
    "for bundle in LCM_bundles:\n",
    "    ensure_dir_exists(path.dirname(bundle['o']))\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Calculate segmental wordform and prefix channel matrices.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(c = bundle['c'],\n",
    "                    w = bundle['w'],\n",
    "                    o = bundle['o'],\n",
    "                    f = 'False')\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate posterior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5a: Calculate $p(V|W, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\hat{V} = v^*|\\hat{X}_0^f = x_0^{'f}, c) = \\frac{p(x_0^{'f}|v^*)p(v^*|c)}{p(x_0^{'f}|c)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:23:13.799710Z",
     "start_time": "2019-06-05T21:23:13.797657Z"
    }
   },
   "outputs": [],
   "source": [
    "#gather aligned triples of filepaths defining\n",
    "# - $p(V|C)$\n",
    "# - $p(W|C)$\n",
    "# - $p(W|V)$\n",
    "# construct the output filename and location (probably in LD?)\n",
    "# construct output notebook filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T21:23:13.815845Z",
     "start_time": "2019-06-05T21:23:13.800956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_newdic_destressed.pV_C.npy',\n",
       " 'w': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Calculate orthographic posterior given segmental wordform + context for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts_projected_LTR_Buckeye.pV_C.npy',\n",
       " 'w': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_Buckeye_contexts/LTR_Buckeye_aligned_CM_filtered_LM_filtered_in_buckeye_contexts.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_Buckeye_contexts/Calculate orthographic posterior given segmental wordform + context for LTR_Buckeye_aligned_CM_filtered_LM_filtered in buckeye contexts.ipynb'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts_projected_LTR_CMU_destressed.pV_C.npy',\n",
       " 'w': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.npz',\n",
       " 'm': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pW_C.npy',\n",
       " 'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_in_swbd2003_contexts.pV_WC',\n",
       " 'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Calculate orthographic posterior given segmental wordform + context for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posterior_WD_bundles = []\n",
    "for bundle in WD_bundles:\n",
    "    output_dir = path.dirname(bundle['d'])\n",
    "    LTR_key = path.basename(bundle['w']).split('.pW_V.npz')[0]\n",
    "    contexts_key = path.basename(bundle['o']).split('LM_filtered')[1].split('.pW_C')[0].replace('_', ' ')\n",
    "    output_base_prefix = LTR_key + contexts_key.replace(' ', '_') + '.pV_WC'\n",
    "    \n",
    "    new_bundle = dict()\n",
    "    new_bundle['d'] = bundle['d']          #p(V|C) as .npy\n",
    "    new_bundle['w'] = bundle['w']          #p(W|V) as .npz\n",
    "    new_bundle['m'] = bundle['o'] + '.npy' #p(W|C) as .npy\n",
    "    new_bundle['o'] = path.join(output_dir, output_base_prefix)\n",
    "    new_bundle['nb_fp'] = path.join(output_dir, f'Calculate orthographic posterior given segmental wordform + context for {LTR_key}{contexts_key}' + '.ipynb')\n",
    "    new_bundle\n",
    "    posterior_WD_bundles.append(new_bundle)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-06T21:58:51.551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate orthographic posterior given segmental wordform + context.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Calculate orthographic posterior given segmental wordform + context for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered in swbd2003 contexts.ipynb\n",
      "\n",
      "\n",
      "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 5/44 [00:00<00:00, 48.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 11/44 [00:00<00:00, 50.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 14/44 [00:01<00:05,  5.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 18/44 [00:04<00:07,  3.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 18/44 [00:16<00:07,  3.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 19/44 [01:20<09:35, 23.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 20/44 [01:20<06:27, 16.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 21/44 [02:35<12:59, 33.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 25/44 [02:35<07:30, 23.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 29/44 [02:39<04:12, 16.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 32/44 [02:39<02:21, 11.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 37/44 [02:39<00:57,  8.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 37/44 [02:56<00:57,  8.27s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# takes ?m for wittgenstein w/ no background load and J=-1\n",
    "for bundle in posterior_WD_bundles:\n",
    "    output_dir = path.dirname(bundle['o'])\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    pm.execute_notebook(\n",
    "    'Calculate orthographic posterior given segmental wordform + context.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(d = bundle['d'],\n",
    "                   w = bundle['w'],\n",
    "                   m = bundle['m'],\n",
    "                   o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b: Calculate $p(\\hat{X}_0^f|X_0^f, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a choice of parameters $\\epsilon$ and $n$, and given\n",
    " - wordform channel matrices $p(Y_0^f|X_0^f)$\n",
    " - a contextual distribution on segmental wordforms $p(X_0^f|C)$\n",
    " - segmental lexicon metadata pre-calculating $k$-cousins/$k$-spheres up to $k=4$\n",
    " \n",
    "Calculate\n",
    "\n",
    "$$\\hat{p}(\\hat{X}_0^f = x_0^{'f}|X_0^f = x_0^{*f}, c) = \\frac{1}{n} \\sum\\limits_{y_0^f \\in S} p(\\hat{X}_0^f = x_0^{'f}|y_0^f, c)$$\n",
    " where \n",
    "  - edit distance $d(x_0^{'f}, x_0^{*f}) \\leq 4$\n",
    "  - $S = $ a set of $n$ samples from $p(Y_0^f|x_0^{*f})$. In practice an $n \\approx 50$ seems to result in estimates that are within $10^{-6}$ of the true estimate. \n",
    "  - $p(\\hat{X}_0^f = x_0^{'f}|Y_0^f = y_0^f, c) = \\frac{p(y_0^f|x_0^{'f})p(x_0^{'f}|c)}{p(y_0^f | c)}$\n",
    "  - $p(y_0^f| c) = \\sum\\limits_{v', x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|v')p(v'|c) = \\sum\\limits_{x_0^{''f}} p(y_0^f|x_0^{''f})p(x_0^{''f}|c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do wordform channel matrices live?\n",
    "#  where are they bundled?\n",
    "#   - LCM_bundles\n",
    "# where do contextual distributions on segmental wordforms live?\n",
    "#   where are they bundled?\n",
    "#   - WD_bundles\n",
    "# where do lexicon metadata live?\n",
    "#   where are they bundled?\n",
    "#   - lexicon_md_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Calculate segmental posterior given segmental wordform + context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan:\n",
    "# 1. Check if there are components of the posterior calculation that are worth pre-calculating; if so, calculate them.\n",
    "#    - e.g. pW_C, pV_W-hat,C\n",
    "# 2. Next notebook should have a flag for calculating just p(V-hat = v* | V = v*) ∀v* vs. the full p(V-hat | V = v*) ∀v*\n",
    "#    - Consider facilitating SLURM cluster jobs by \n",
    "#       - making posterior calculation deterministic (unlike P4bnt2) \n",
    "#       - adding/supporting optional notebook arguments that specify which parts of the posterior dist to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5c: Calculate $p(\\hat{V} = v^*| V = v^*, C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generating analysis measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
