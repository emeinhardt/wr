{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:24.362415Z",
     "start_time": "2019-05-30T18:55:24.360024Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Requirements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Python-environment\" data-toc-modified-id=\"Python-environment-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Python environment</a></span></li><li><span><a href=\"#Hardware-/-runtime-expectations\" data-toc-modified-id=\"Hardware-/-runtime-expectations-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Hardware / runtime expectations</a></span></li></ul></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Todo</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Step-0:-Check-for-foundational-files\" data-toc-modified-id=\"Step-0:-Check-for-foundational-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 0: Check for foundational files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-0a:-Check-for-gating-data\" data-toc-modified-id=\"Step-0a:-Check-for-gating-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 0a: Check for gating data</a></span></li><li><span><a href=\"#Step-0b:-Check-for-transcribed-lexicons\" data-toc-modified-id=\"Step-0b:-Check-for-transcribed-lexicons-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 0b: Check for transcribed lexicons</a></span></li><li><span><a href=\"#Step-0c:-Check-for-n-gram-contexts\" data-toc-modified-id=\"Step-0c:-Check-for-n-gram-contexts-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Step 0c: Check for n-gram contexts</a></span></li><li><span><a href=\"#Step-0d:-Check-for-language-model(s)\" data-toc-modified-id=\"Step-0d:-Check-for-language-model(s)-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Step 0d: Check for language model(s)</a></span></li></ul></li><li><span><a href=\"#Step-1:-Segment-inventory-alignment\" data-toc-modified-id=\"Step-1:-Segment-inventory-alignment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 1: Segment inventory alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1a:-Define-inventory-alignment-projections\" data-toc-modified-id=\"Step-1a:-Define-inventory-alignment-projections-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Step 1a: Define inventory alignment projections</a></span></li><li><span><a href=\"#Step-1b:-Apply-inventory-alignment-projections\" data-toc-modified-id=\"Step-1b:-Apply-inventory-alignment-projections-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Step 1b: Apply inventory alignment projections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-projection-definitions\" data-toc-modified-id=\"Check-for-projection-definitions-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Check for projection definitions</a></span></li><li><span><a href=\"#How-are-inventory-alignment-projections-actually-applied?\" data-toc-modified-id=\"How-are-inventory-alignment-projections-actually-applied?-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>How are inventory alignment projections actually applied?</a></span></li><li><span><a href=\"#Apply-projection-definitions\" data-toc-modified-id=\"Apply-projection-definitions-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Apply projection definitions</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions\" data-toc-modified-id=\"Step-2:-Generating-channel-and-(orthographic)-lexicon-distributions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 2: Generating channel and (orthographic) lexicon distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-2a:-Generating-channel-distributions-and-associated-metadata\" data-toc-modified-id=\"Step-2a:-Generating-channel-distributions-and-associated-metadata-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Step 2a: Generating channel distributions and associated metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Metadata</a></span></li><li><span><a href=\"#Channel-distributions\" data-toc-modified-id=\"Channel-distributions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Channel distributions</a></span></li></ul></li><li><span><a href=\"#Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)\" data-toc-modified-id=\"Step-2b:-Generating-(contextual)-lexicon-distributions-(over-orthographic-vocabularies)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)</a></span></li></ul></li><li><span><a href=\"#Step-3:-Creating-combinable-models\" data-toc-modified-id=\"Step-3:-Creating-combinable-models-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 3: Creating combinable models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution\" data-toc-modified-id=\"Step-3a:-Filter-transcription-lexicons-to-only-include-words-that-can-be-modeled-by-a-given-channel-distribution-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution</a></span></li><li><span><a href=\"#Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary\" data-toc-modified-id=\"Step-3b:-Filter-transcription-lexicons-to-only-include-words-that-are-in-a-language-model's-vocabulary-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary</a></span></li><li><span><a href=\"#Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list\" data-toc-modified-id=\"Step-3c:-Filter-the-conditioning-events-of-channel-distributions-to-only-include-triphones-contained-in-a-transcription-lexicon's-segmental-wordform-list-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list</a></span></li><li><span><a href=\"#Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms\" data-toc-modified-id=\"Step-3d:-For-each-(filtered)-transcribed-lexicon-relation,-define-the-relevant-contextual-lexicon-distributions-over-orthographic-wordforms-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms</a></span></li><li><span><a href=\"#Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform\" data-toc-modified-id=\"Step-3e:-For-each-(filtered)-transcribed-lexicon-relation,-define-a-conditional-distribution-on-segmental-wordforms-given-an-orthographic-wordform-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform</a></span></li></ul></li><li><span><a href=\"#Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data\" data-toc-modified-id=\"Step-4:-Pre-calculate-remaining-forward-model-components-and-meta-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 4: Pre-calculate remaining forward model components and meta-data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model\" data-toc-modified-id=\"Step-4a:-Generate-triphone-lexicon-distributions-for-every-triphone-channel-model-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Step 4a: Generate triphone lexicon distributions for every triphone channel model</a></span></li><li><span><a href=\"#Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon\" data-toc-modified-id=\"Step-4b:-Pre-calculate-prefix-relation,-$k$-cousins,-and-$k$-spheres-for-each-segmental-lexicon-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon</a></span></li><li><span><a href=\"#Step-4c:-Define-incremental-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)\" data-toc-modified-id=\"Step-4c:-Define-incremental-channel-distributions-on-a-set-of-segmental-wordforms(+prefixes)-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Step 4c: Define incremental channel distributions on a set of segmental wordforms(+prefixes)</a></span></li></ul></li><li><span><a href=\"#Step-5:-Calculate-posterior-probabilities\" data-toc-modified-id=\"Step-5:-Calculate-posterior-probabilities-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Step 5: Calculate posterior probabilities</a></span></li><li><span><a href=\"#Step-6:-Generating-analysis-measures\" data-toc-modified-id=\"Step-6:-Generating-analysis-measures-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Step 6: Generating analysis measures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processing pipeline from \n",
    " - gating data\n",
    " - transcribed lexicon\n",
    " - a language model and (possibly empty) n-gram contexts\n",
    "\n",
    "to \n",
    " - channel distribution\n",
    " - lexicon distribution(s) (distributions over wordforms)\n",
    " - expected posterior distribution over intended wordform given what has been produced of what was intended.\n",
    " \n",
    " \n",
    "It describes what happens at each step, checks some pre- and post-conditions, describes what you, the user must do (if anything), and scripts some commands to automatically do the necessary processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "#FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Python environment\n",
    "This repository was developed using Python 3.6 with the following package requirements (upstream dependencies not included):\n",
    " - **Jupyter/notebook related packages**: `jupyter` `jupyter_contrib_nbextensions` `papermill`\n",
    "    - Notable notebook extensions used include `ExecuteTime` and `Table of Contents (2)`.\n",
    " - **Numeric computing and data processing**: `scipy` `numpy` `pandas` `tqdm` `joblib` `numba` `sparse` `pytorch` `torchvision`\n",
    " - **Plotting**: `matplotlib` `plotnine`\n",
    " - **Misc**: `funcy` `more_itertools`\n",
    "\n",
    "See `dev_environment.sh` for `conda` and `pip` commands to create an environment that has all of these packages. (Not all are available on conda, and not all of those that are available on conda are available from the main channel.)\n",
    "\n",
    "\n",
    "### Hardware / runtime expectations\n",
    "\n",
    "The last machine this repository was developed on (`wittgenstein`) has 32 cores. `joblib` is used extensively to parallelize data processing. On this machine, using `joblib` and often using nearly all of those cores:\n",
    "1. Step 1 takes about a minute.\n",
    "2. Step 2 takes about 3 hours.\n",
    "3. Step 3 takes about 15-20 minutes.\n",
    "\n",
    "To run these scripts without modification, you will need about 120GB of memory and a few hundred GB of available disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "0. **Extensibility**: Step 3b (filtering transcribed lexicons against language model vocabularies) uses output filenames that won't scale if any transcription lexicon is used with more than one language model. (This also affects 3c.)\n",
    "1. **Portability/Reproducibility**: For every file that this repository depends on that *isn't* tracked by the repository (e.g. processed versions of swbd2003, Buckeye, etc.), there should be *something* (e.g. a cell in this script) that lets the user identify where those files are located, and then said something copies them to wherever this repository is expecting to find them.\n",
    "2. **Portability/Reproducibility**: Check for and remove absolute paths in this and other files.\n",
    "3. **Portability/Reproducibility**: For platform independence (read: supporting windows users, I guess?) use `path.join` instead of manually choosing directory slashes...\n",
    "4. **Documentation**: \n",
    "   1. Fix motivation cell immediately above this one.\n",
    "   2. Math-y documentation in channel distribution and posterior distribution notebooks probably needs to be updated / at least have notation overhauled.\n",
    "   3. Go through notebooks used here and make sure `Overview` cells are accurate.\n",
    "   4. Go through notebooks used here and make sure Papermill-related `Usage` cells are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.385535Z",
     "start_time": "2019-05-30T18:55:24.366580Z"
    }
   },
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.389727Z",
     "start_time": "2019-05-30T18:55:25.387533Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.395787Z",
     "start_time": "2019-05-30T18:55:25.391130Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import getcwd, chdir, listdir, path, mkdir, makedirs\n",
    "\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.401424Z",
     "start_time": "2019-05-30T18:55:25.397573Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.412081Z",
     "start_time": "2019-05-30T18:55:25.403597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/wr'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = getcwd()\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.419204Z",
     "start_time": "2019-05-30T18:55:25.413370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probdist.py',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0',\n",
       " 'Generate triphone lexicon distribution from channel model.ipynb',\n",
       " 'swbd2003_contexts.txt',\n",
       " 'boilerplate.py',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb',\n",
       " 'LTR_Buckeye',\n",
       " '.gitignore',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'Run n-phone analysis of gating data.ipynb',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye',\n",
       " '__pycache__',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb',\n",
       " 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb',\n",
       " '1 initial directory setup.txt',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " '2a alignment_paths_and_cmds.sh',\n",
       " 'LM_Fisher',\n",
       " 'Filter channel model by transcription lexicon.ipynb',\n",
       " 'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
       " 'Align transcriptions.ipynb',\n",
       " 'Filter transcription lexicon by channel model.ipynb',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'Filter transcription lexicon by language model vocabulary.ipynb',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'LTR_CMU_destressed',\n",
       " 'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
       " 'buckeye_contexts.txt',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'Processing Driver Notebook.ipynb',\n",
       " 'old',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
       " 'GD_AmE',\n",
       " '.ipynb_checkpoints',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'string_utils.py',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'Producing channel distributions.ipynb',\n",
       " 'LTR_CMU_stressed',\n",
       " 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'Producing contextual distributions.ipynb',\n",
       " 'dev_environment.sh',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
       " '.git',\n",
       " 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb',\n",
       " 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb',\n",
       " 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb',\n",
       " 'LTR_newdic_destressed',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_contents_0 = listdir()\n",
    "repo_contents_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Check for foundational files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    " - I assume all relevant transcriptions have been converted to Unicode IPA characters and that segment sequences have `.` as a separator. For each data source used here, the IPA alignment step is documented in a GitHub repository elsewhere. (While I don't *think* any script here depends on use of Unicode IPA symbols, I haven't - and won't - test that idea, and it is *absolutely required* that contiguous sequences of segments be separated by `.` in data files.)\n",
    " - Where language models and n-gram contexts (drawn from speech corpora) are referenced, each of these is assumed to have come from as is from other GitHub repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four steps below verify that foundational files assumed to be present by downstream notebooks are, in fact, present in the repository directory. If for some reason those files are not present, the processing pipeline will be aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0a: Check for gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.424851Z",
     "start_time": "2019-05-30T18:55:25.421138Z"
    }
   },
   "outputs": [],
   "source": [
    "AmE_gating_data_dir = 'GD_AmE'\n",
    "AmE_gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "AmE_GD_fp = path.join(AmE_gating_data_dir, AmE_gating_data_fn)\n",
    "assert path.exists(AmE_gating_data_dir), 'Gating data directory {0} does not exist.'.format(AmE_gating_data_dir)\n",
    "assert path.exists(AmE_GD_fp), 'Gating data file {0} does not exist.'.format(AmE_GD_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed gating data used here come from \n",
    " - https://github.com/emeinhardt/wmc2014-ipa\n",
    " \n",
    "See those repositories for information on how they were produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0b: Check for transcribed lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Each transcribed lexicon `LEXNAME` should be in a folder (e.g. `LTR_LEXNAME`) containing a file `LTR_LEXNAME.tsv`. For documentation purposes, the source file and a notebook documenting the production of the `.tsv` file should, if practicable be included in the folder as well.\n",
    "   - A transcribed lexicon `LTR_....tsv` file should have two columns: `Orthographic_Wordform` and `Transcription`.\n",
    "   - NB: The `LTR_` prefix on transcribed lexicon data files and containing folders is simply a convention for organization and readability, but is not required or expected by any file or script in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions in the code below will only succeed if step 1 is complete for all transcribed lexicons listed for checking below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.434022Z",
     "start_time": "2019-05-30T18:55:25.426230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3586.92it/s]\n"
     ]
    }
   ],
   "source": [
    "newdic_destressed_ltr_folder = 'LTR_newdic_destressed'\n",
    "cmu_destressed_ltr_folder = 'LTR_CMU_destressed'\n",
    "cmu_stressed_ltr_folder = 'LTR_CMU_stressed'\n",
    "buckeye_ltr_folder = 'LTR_Buckeye'\n",
    "# nxt_swbd_ltr_folder = \n",
    "\n",
    "LTR_folders = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, cmu_stressed_ltr_folder, buckeye_ltr_folder)\n",
    "LTR_folders_to_process = (newdic_destressed_ltr_folder, cmu_destressed_ltr_folder, buckeye_ltr_folder)\n",
    "\n",
    "for dirname in tqdm(LTR_folders_to_process):\n",
    "    assert path.exists(dirname), 'Transcribed lexicon directory {0} not found in repo directory'.format(dirname)\n",
    "    fname = path.join(dirname, dirname + '.tsv')\n",
    "    assert path.exists(fname), 'Transcribed lexicon {0} not found in repo directory'.format(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are transcribed lexicon relations made?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each was created by processing a transcription source (a lexical database, an annotated corpus, etc.). The processing step is described in other repositories:\n",
    " - https://github.com/emeinhardt/newdic-nettalk-ipa\n",
    " - https://github.com/emeinhardt/cmu-ipa\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    "\n",
    "Given the processed outputs of these repositories, the `Making a Transcribed Lexicon Relation - <LEXNAME>.ipynb` notebook in each `LTR_LEXNAME` folder describes how the homogeneous `.tsv` files downstream steps are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.439457Z",
     "start_time": "2019-05-30T18:55:25.435018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed.tsv',\n",
       " 'Making a Transcribed Lexicon Relation - newdic_destressed.ipynb',\n",
       " 'newdic_IPA.tsv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Making a Transcribed Lexicon Relation - CMU_destressed.ipynb',\n",
       " 'LTR_CMU_destressed.pW_V.json',\n",
       " 'LTR_CMU_destressed.tsv',\n",
       " '.ipynb_checkpoints',\n",
       " 'LTR_CMU_destressed.pW_V.npz',\n",
       " 'cmudict-0.7b_IPA_destressed.tsv',\n",
       " 'LTR_CMU_destressed_Orthographic_Wordforms.txt',\n",
       " 'LTR_CMU_destressed_Transcriptions.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_Buckeye.tsv',\n",
       " 'buckeye_words_analysis_relation.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Making a Transcribed Lexicon Relation - Buckeye.ipynb']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname in LTR_folders_to_process:\n",
    "    listdir(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0c: Check for n-gram contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.443470Z",
     "start_time": "2019-05-30T18:55:25.440753Z"
    }
   },
   "outputs": [],
   "source": [
    "buckeye_contexts = 'buckeye_contexts.txt'\n",
    "swbd2003_contexts = 'swbd2003_contexts.txt'\n",
    "\n",
    "contexts = (buckeye_contexts, swbd2003_contexts)\n",
    "\n",
    "for c_fn in contexts:\n",
    "    assert path.exists(c_fn), \"N-gram contexts file {0} does not exist.\".format(c_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram context files are taken from\n",
    " - https://github.com/emeinhardt/buckeye-lm\n",
    " - https://github.com/emeinhardt/switchboard-lm\n",
    " \n",
    "See those repositories for more information on how the contexts were extracted. (*NB*: The context files are not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0d: Check for language model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.451271Z",
     "start_time": "2019-05-30T18:55:25.444818Z"
    }
   },
   "outputs": [],
   "source": [
    "fisher_lm_dir = 'LM_Fisher'\n",
    "fisher_lm_fn = 'fisher_utterances_main_4gram.mmap'\n",
    "fisher_lm_fp = path.join(fisher_lm_dir, fisher_lm_fn)\n",
    "\n",
    "fisher_lm_vocab_fn = 'fisher_vocabulary_main.txt'\n",
    "fisher_lm_vocab_fp = path.join(fisher_lm_dir, fisher_lm_vocab_fn)\n",
    "\n",
    "assert path.exists(fisher_lm_fp), 'Language model {0} not found'.format(fisher_lm_fp)\n",
    "assert path.exists(fisher_lm_vocab_fp), 'Language model vocabulary {0} not found'.format(fisher_lm_vocab_fp)\n",
    "\n",
    "fisher_lm_fps = {'lm':fisher_lm_fp, \n",
    "                 'vocab':fisher_lm_vocab_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (memory mapped) 4-gram language model is copied from the output of this repository:\n",
    " - https://github.com/emeinhardt/fisher-lm\n",
    " \n",
    "See that repository for more information. (*NB* Again, the language model file is not included in this repository both to avoid duplication and because of licensing restrictions: to recreate these contexts, you will need access to your own copy of the Buckeye and Switchboard corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment inventory alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Define inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment inventory of any given transcribed lexicon and the segment inventory of the gating data often do not line up. For the gating data to be usefully applied to a given lexicon of transcriptions, the strings in the (segmental) lexicon must contain only segments found in the gating data stimuli inventory.\n",
    "\n",
    "To ensure this happens, the notebook `Gating Data - Transcription Lexicon Alignment Maker.ipynb` \n",
    " - takes as inputs \n",
    "     - a transcribed lexicon file path and a gating data file path\n",
    "     - a lexicon projection file path and a gating data projection file path\n",
    " - identifies the inventories of each and what symbols are relatively unique to the lexicon and the gating data\n",
    " - produces \n",
    "   - *a Jupyter notebook* for **you to open and finish by defining two projection functions** (i.e. Python dictionaries) to be applied to strings in the transcribed lexicon and to the gating data (one function for each). When you finish doing this (and set an export flag in the notebook to True and run the remainder of the notebook), this notebook will produce\n",
    "     - two *.json files storing these projections* according to the previously provided output file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.456543Z",
     "start_time": "2019-05-30T18:55:25.452481Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf LTR*_aligned_w_*\n",
    "# %rm -rf *\" alignment definition\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will only succeed if the American English gating data of Warner, McQueen, and Cutler (2014) is contained in the repo directory with a particular directory and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.463248Z",
     "start_time": "2019-05-30T18:55:25.457823Z"
    }
   },
   "outputs": [],
   "source": [
    "gating_data_folder = 'GD_AmE'\n",
    "gating_data_fn = 'AmE-diphones-IPA-annotated-columns.csv'\n",
    "gating_data_fp = path.join(gating_data_folder, gating_data_fn)\n",
    "\n",
    "assert path.exists(gating_data_folder), 'AmE gating data folder {0} not found in repo directory'.format(gating_data_folder)\n",
    "assert path.exists(gating_data_fp), 'AmE gating data {0} not found in repo directory'.format(gating_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third cell below will create a notebook for alignment projection definitions for each of the transcribed lexicons from the previous step and the AmE gating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.470277Z",
     "start_time": "2019-05-30T18:55:25.464511Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeExtension(fp):\n",
    "    dir_name = path.dirname(fp)\n",
    "    file_name = path.basename(fp)\n",
    "    ext = file_name.split('.')[-1]\n",
    "    rest = '.'.join( file_name.split('.')[:-1] )\n",
    "    return path.join(dir_name, rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:25.482438Z",
     "start_time": "2019-05-30T18:55:25.471905Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment_arg_bundles = []\n",
    "for LTR_dirname in LTR_folders_to_process:\n",
    "    LTR_fn = LTR_dirname + '.tsv'\n",
    "    LTR_fp = path.join(LTR_dirname, LTR_fn)\n",
    "    \n",
    "    nb_output_name = 'GD_AmE-diphones - ' + LTR_dirname + ' alignment definition' + '.ipynb'\n",
    "    my_g = gating_data_fp\n",
    "    my_l = LTR_fp\n",
    "    my_s = 'destressed'\n",
    "    \n",
    "    gd_alignment_dn = 'GD_AmE_' + my_s + '_' + 'aligned_w_' + LTR_dirname\n",
    "    gd_alignment_fn = 'alignment_of_' + removeExtension(gating_data_fn) + '_w_' + LTR_dirname + '.json'\n",
    "    gd_alignment_fp = path.join(gd_alignment_dn, gd_alignment_fn)\n",
    "    if not path.exists(gd_alignment_dn):\n",
    "        makedirs(gd_alignment_dn)\n",
    "    my_gp = gd_alignment_fp\n",
    "    \n",
    "    ltr_alignment_dn = LTR_dirname + '_aligned_w_' + 'GD_AmE_' + my_s\n",
    "    ltr_alignment_fn = 'alignment_of_' + LTR_dirname + '_w_' + removeExtension(gating_data_fn) + '.json'\n",
    "    ltr_alignment_fp = path.join(ltr_alignment_dn, ltr_alignment_fn)\n",
    "    if not path.exists(ltr_alignment_dn):\n",
    "        makedirs(ltr_alignment_dn)\n",
    "    my_lp = ltr_alignment_fp\n",
    "    \n",
    "    \n",
    "    my_arg_bundle = OrderedDict({\n",
    "        'LTR_dirname':LTR_dirname,\n",
    "        'LTR_fn':LTR_fn,\n",
    "        'LTR_fp':LTR_fp,\n",
    "        'gd_alignment_dn':gd_alignment_dn,\n",
    "        'gd_alignment_fn':gd_alignment_fn,\n",
    "        'gd_alignment_fp':gd_alignment_fp,\n",
    "        'ltr_alignment_dn':ltr_alignment_dn,\n",
    "        'ltr_alignment_fn':ltr_alignment_fn,\n",
    "        'ltr_alignment_fp':ltr_alignment_fp,\n",
    "        'align_def_nb_output_name':nb_output_name,\n",
    "        'my_g':my_g,\n",
    "        'my_l':my_l,\n",
    "        'my_s':my_s,\n",
    "        'my_gp':my_gp,\n",
    "        'my_lp':my_lp,\n",
    "    })\n",
    "    alignment_arg_bundles.append(my_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.564555Z",
     "start_time": "2019-05-30T18:55:25.483841Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 37.26it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 37.65it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 37.35it/s]\u001b[A\n",
      " 23%|██▎       | 15/64 [00:00<00:01, 29.49it/s]\u001b[A\n",
      " 28%|██▊       | 18/64 [00:00<00:01, 29.46it/s]\u001b[A\n",
      " 34%|███▍      | 22/64 [00:00<00:01, 30.61it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:01<00:03, 11.30it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:02, 13.89it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 16.39it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:04<00:10,  2.74it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:07,  3.54it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:06<00:07,  3.06it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:06<00:05,  4.10it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:03,  5.52it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:06<00:02,  7.31it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:06<00:01,  9.25it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:00, 11.58it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:06<00:00, 14.10it/s]\u001b[A\n",
      " 94%|█████████▍| 60/64 [00:07<00:00, 16.52it/s]\u001b[A\n",
      " 98%|█████████▊| 63/64 [00:07<00:00, 18.73it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:10<00:20, 10.07s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 35.42it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 36.04it/s]\u001b[A\n",
      " 20%|██        | 13/64 [00:00<00:01, 37.00it/s]\u001b[A\n",
      " 27%|██▋       | 17/64 [00:00<00:01, 35.92it/s]\u001b[A\n",
      " 31%|███▏      | 20/64 [00:00<00:01, 33.21it/s]\u001b[A\n",
      " 38%|███▊      | 24/64 [00:00<00:01, 32.84it/s]\u001b[A\n",
      " 42%|████▏     | 27/64 [00:00<00:01, 22.29it/s]\u001b[A\n",
      " 48%|████▊     | 31/64 [00:01<00:01, 24.86it/s]\u001b[A\n",
      " 53%|█████▎    | 34/64 [00:01<00:01, 20.46it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:10,  2.51it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:09,  2.63it/s]\u001b[A\n",
      " 66%|██████▌   | 42/64 [00:06<00:07,  2.81it/s]\u001b[A\n",
      " 70%|███████   | 45/64 [00:06<00:04,  3.83it/s]\u001b[A\n",
      " 75%|███████▌  | 48/64 [00:06<00:03,  5.12it/s]\u001b[A\n",
      " 80%|███████▉  | 51/64 [00:06<00:01,  6.72it/s]\u001b[A\n",
      " 84%|████████▍ | 54/64 [00:06<00:01,  8.75it/s]\u001b[A\n",
      " 91%|█████████ | 58/64 [00:07<00:00, 11.11it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:07<00:00, 13.10it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:20<00:10, 10.08s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Gating Data - Transcription Lexicon Alignment Maker.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb\n",
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:01, 33.08it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:01, 34.75it/s]\u001b[A\n",
      " 19%|█▉        | 12/64 [00:00<00:01, 36.16it/s]\u001b[A\n",
      " 25%|██▌       | 16/64 [00:00<00:01, 35.28it/s]\u001b[A\n",
      " 30%|██▉       | 19/64 [00:00<00:01, 33.07it/s]\u001b[A\n",
      " 34%|███▍      | 22/64 [00:00<00:01, 31.18it/s]\u001b[A\n",
      " 39%|███▉      | 25/64 [00:00<00:01, 22.57it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 24.20it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 25.62it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:04<00:09,  2.93it/s]\u001b[A\n",
      " 58%|█████▊    | 37/64 [00:04<00:06,  3.86it/s]\u001b[A\n",
      " 62%|██████▎   | 40/64 [00:05<00:06,  3.61it/s]\u001b[A\n",
      " 67%|██████▋   | 43/64 [00:05<00:04,  4.88it/s]\u001b[A\n",
      " 72%|███████▏  | 46/64 [00:05<00:02,  6.50it/s]\u001b[A\n",
      " 77%|███████▋  | 49/64 [00:05<00:01,  8.42it/s]\u001b[A\n",
      " 81%|████████▏ | 52/64 [00:05<00:01, 10.47it/s]\u001b[A\n",
      " 86%|████████▌ | 55/64 [00:05<00:00, 12.50it/s]\u001b[A\n",
      " 91%|█████████ | 58/64 [00:06<00:00, 14.99it/s]\u001b[A\n",
      " 95%|█████████▌| 61/64 [00:06<00:00, 17.40it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:29<00:00,  9.73s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating alignment definition notebook 'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'.\n",
      "Open and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~30s on wittgenstein\n",
    "for arg_bundle in tqdm(alignment_arg_bundles):\n",
    "    nb = pm.execute_notebook(\n",
    "        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "        arg_bundle['align_def_nb_output_name'],\n",
    "        parameters=dict(g = arg_bundle['my_g'], \n",
    "                        l = arg_bundle['my_l'], \n",
    "                        s = arg_bundle['my_s'], \n",
    "                        gp = arg_bundle['my_gp'], \n",
    "                        lp = arg_bundle['my_lp'])\n",
    "    )\n",
    "#     pm.execute_notebook(\n",
    "#        'Gating Data - Transcription Lexicon Alignment Maker.ipynb',\n",
    "#        nb_output_name,\n",
    "#        parameters=dict(g = my_g, l = my_l, s = my_s, gp = my_gp, lp = my_lp)\n",
    "#     )\n",
    "    print(\"Finished creating alignment definition notebook '{0}'.\\nOpen and run the notebook, complete the projection definition, and run the remainder of the notebook (remembering to change the export flag to 'True').\\n\".format(arg_bundle['align_def_nb_output_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b: Apply inventory alignment projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will clear all existing alignment folders created using the code in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.568434Z",
     "start_time": "2019-05-30T18:55:54.566037Z"
    }
   },
   "outputs": [],
   "source": [
    "# %rm -rf *\" alignment application \"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will succeed if you have run each of the previously produced notebooks correctly and produced a projection mapping file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.741115Z",
     "start_time": "2019-05-30T18:55:54.572152Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    assert path.exists(args['gd_alignment_fp']), 'Gating data alignment projection mapping not found:\\n\\t{0}'.format(args['gd_alignment_fp'])\n",
    "    assert path.exists(args['ltr_alignment_fp']), 'Transcribed lexicon data alignment projection mapping not found:\\n\\t{0}'.format(args['ltr_alignment_fp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are inventory alignment projections actually applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Align transcriptions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply projection definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below applies each pair of alignment projections to each matched pair of gating data and transcribed lexicon choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:55:54.746800Z",
     "start_time": "2019-05-30T18:55:54.742520Z"
    }
   },
   "outputs": [],
   "source": [
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "    LTR_fn = args['LTR_fn']\n",
    "    \n",
    "#     my_pg = args['my_gp']\n",
    "#     my_g = args['my_g']\n",
    "    my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "    my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "    args['my_og'] = my_og\n",
    "    \n",
    "#     my_pl = args['my_lp']\n",
    "#     my_l = args['my_l']\n",
    "    my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "    my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "    args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "    args['my_ol'] = my_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:56:38.846995Z",
     "start_time": "2019-05-30T18:55:54.747961Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating notebook 'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_newdic_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json\n",
      "\tLTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_newdic_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_newdic_destressed/LTR_newdic_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_CMU_destressed/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json\n",
      "\tLTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb\n",
      "100%|██████████| 64/64 [00:03<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/alignment_of_LTR_CMU_destressed_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_CMU_destressed/LTR_CMU_destressed.tsv\n",
      "Result saved to\n",
      "\tLTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n",
      "Creating notebook 'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb' w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tGD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb\n",
      "100%|██████████| 64/64 [00:07<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = GD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "to\n",
      "\tg = GD_AmE/AmE-diphones-IPA-annotated-columns.csv\n",
      "Result saved to\n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv\n",
      " \n",
      "Creating notebook GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb w/ args p, g, o = \n",
      "\tGD_AmE_destressed_aligned_w_LTR_Buckeye/alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json\n",
      "\tLTR_Buckeye/LTR_Buckeye.tsv\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Align transcriptions.ipynb\n",
      "Output Notebook: GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb\n",
      "100%|██████████| 64/64 [00:02<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished applying alignment projection\n",
      "\tp = LTR_Buckeye_aligned_w_GD_AmE_destressed/alignment_of_LTR_Buckeye_w_AmE-diphones-IPA-annotated-columns.json\n",
      "to\n",
      "\tl = LTR_Buckeye/LTR_Buckeye.tsv\n",
      "Result saved to\n",
      "\tLTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~45s on wittgenstein\n",
    "for arg_bundle in alignment_arg_bundles:\n",
    "    args = arg_bundle\n",
    "#     LTR_fn = args['LTR_fn']\n",
    "    \n",
    "    my_pg = args['my_gp']\n",
    "    my_g = args['my_g']\n",
    "#     my_o_fn = 'GD_AmE-diphones' + '_aligned_w_' + removeExtension(LTR_fn) + '.tsv'\n",
    "#     my_og = path.join(args['gd_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_gd_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + 'AmE-diphones' + '.ipynb'\n",
    "#     args['my_og'] = my_og\n",
    "    my_og = args['my_og']\n",
    "    print(\"Creating notebook '{0}' w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}\".format(args['align_apply_gd_nb_output_name'], my_pg, my_g, my_og))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_gd_nb_output_name'],\n",
    "        parameters=dict(p = my_pg,\n",
    "                        g = my_g,\n",
    "                        o = my_og)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tg = {1}\\nResult saved to\\n\\t{2}'.format(my_pg, my_g, my_og))\n",
    "    print(' ')\n",
    "    \n",
    "    my_pl = args['my_lp']\n",
    "    my_l = args['my_l']\n",
    "#     my_o_fn = removeExtension(LTR_fn) + '_aligned_w_' + 'GD_AmE-diphones' + '.tsv'\n",
    "#     my_ol = path.join(args['ltr_alignment_dn'], my_o_fn)\n",
    "#     args['align_apply_ltr_nb_output_name'] = 'GD_AmE-diphones - ' + removeExtension(LTR_fn) + ' alignment application to ' + removeExtension(LTR_fn) + '.ipynb'\n",
    "#     args['my_ol'] = my_ol\n",
    "    my_ol = args['my_ol']\n",
    "    print('Creating notebook {0} w/ args p, g, o = \\n\\t{1}\\n\\t{2}\\n\\t{3}'.format(args['align_apply_ltr_nb_output_name'], my_pg, my_l, my_ol))\n",
    "    nb = pm.execute_notebook(\n",
    "        'Align transcriptions.ipynb',\n",
    "        args['align_apply_ltr_nb_output_name'],\n",
    "        parameters=dict(p = my_pl,\n",
    "                        l = my_l,\n",
    "                        o = my_ol)\n",
    "    )\n",
    "    print('Finished applying alignment projection\\n\\tp = {0}\\nto\\n\\tl = {1}\\nResult saved to\\n\\t{2}'.format(my_pl, my_l, my_ol))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generating channel and (orthographic) lexicon distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Generating channel distributions and associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:56:38.963886Z",
     "start_time": "2019-05-30T18:56:38.848286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mGD_AmE\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_Buckeye\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_CMU_destressed\u001b[0m/\r\n",
      " \u001b[01;34mGD_AmE_destressed_aligned_w_LTR_newdic_destressed\u001b[0m/\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment application to LTR_Buckeye.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_Buckeye alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment application to LTR_CMU_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_CMU_destressed alignment definition.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to AmE-diphones.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment application to LTR_newdic_destressed.ipynb'\r\n",
      "'GD_AmE-diphones - LTR_newdic_destressed alignment definition.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "%ls -d GD_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:56:38.968425Z",
     "start_time": "2019-05-30T18:56:38.965276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_folders = ('GD_AmE', ) + tuple(map(lambda ab: ab['gd_alignment_dn'], alignment_arg_bundles))\n",
    "gating_data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:56:38.975100Z",
     "start_time": "2019-05-30T18:56:38.969536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_data_fps = ('GD_AmE/AmE-diphones-IPA-annotated-columns.csv',) + \\\n",
    "                  tuple(map(lambda ab: ab['my_og'], alignment_arg_bundles))\n",
    "gating_data_fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (for downstream convenience) we identify the $n$-phones (not) contained in and (not) constructible from each of the versions of the gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** This is done by calling the notebook `Run n-phone analysis of gating data.ipynb`, passing it **the filepath to a gating data `.csv`/`.tsv` file** and **a path to an output directory** for the dozen or so files the notebook will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:36.999917Z",
     "start_time": "2019-05-30T18:56:38.976352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE/GD_AmE n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'GD_AmE n-phone analysis.ipynb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:27<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_newdic_destressed.json',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed n-phone analysis.ipynb',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['destressed response diphone-based constructible triphones.txt',\n",
       " 'GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed n-phone analysis.ipynb',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_CMU_destressed.json',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'destressed response diphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Run n-phone analysis of gating data.ipynb\n",
      "Output Notebook: GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb\n",
      "100%|██████████| 43/43 [00:26<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv',\n",
       " 'stressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphone-based constructible triphones.txt',\n",
       " 'stressed stimuli diphone-based constructible triphones.txt',\n",
       " 'GD_AmE_destressed_aligned_w_LTR_Buckeye n-phone analysis.ipynb',\n",
       " 'destressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli diphones.txt',\n",
       " 'destressed response diphone-based constructible triphones.txt',\n",
       " 'destressed stimuli illegal diphones.txt',\n",
       " 'alignment_of_AmE-diphones-IPA-annotated-columns_w_LTR_Buckeye.json',\n",
       " 'destressed response diphone-based illegal triphones.txt',\n",
       " 'destressed response uniphones.txt',\n",
       " 'stressed stimuli diphone-based illegal triphones.txt',\n",
       " 'stressed stimuli illegal diphones.txt',\n",
       " 'destressed stimuli uniphones.txt',\n",
       " 'destressed stimuli diphones.txt',\n",
       " 'destressed response illegal diphones.txt',\n",
       " 'destressed response diphones.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~2m on wittgenstein\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    gd_dir = path.dirname(gating_data_fp)\n",
    "    nb = pm.execute_notebook(\n",
    "        'Run n-phone analysis of gating data.ipynb',\n",
    "#         args['align_apply_ltr_nb_output_name'],\n",
    "        path.join(gd_dir, gd_dir) + \" n-phone analysis.ipynb\",\n",
    "        parameters=dict(g = gating_data_fp,\n",
    "                        o = gd_dir)\n",
    "    )\n",
    "    listdir(gd_dir)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the notebook `Producing channel distributions.ipynb` will create `.json` files defining (among other things) a uniphone and triphone channel distribution. It requires the following arguments to specify information about what kind of channel model to build and where to put it:\n",
    " - **a filepath** to a gating data file\n",
    " - **a directory** containing metadata indicating possible/impossible $n$-phones\n",
    " - **a string argument** (\"stressed\" or \"destressed\") indicating whether the distribution will be over a segment inventory with or without stress information\n",
    " - **a real valued**  smoothing parameter (a pseudocount to add to every channel outcome)\n",
    " - **an output directory** to write the channel model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:37.003699Z",
     "start_time": "2019-05-30T18:58:37.001357Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudocounts = (0, 0.01, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:37.014498Z",
     "start_time": "2019-05-30T18:58:37.004949Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_arg_bundles = []\n",
    "for gating_data_fp in gating_data_fps:\n",
    "    metadata_dir = path.dirname(gating_data_fp)\n",
    "    s = \"destressed\"\n",
    "    channel_model_dir_stem = 'CM' + metadata_dir[2:]\n",
    "    \n",
    "    for pc in pseudocounts:\n",
    "        channel_model_dir_suffix = '_pseudocount' + str(pc)\n",
    "        if metadata_dir == 'GD_AmE':\n",
    "            channel_model_dir = channel_model_dir_stem + '_' + s + '_unaligned' + channel_model_dir_suffix\n",
    "        else:\n",
    "            channel_model_dir = channel_model_dir_stem + channel_model_dir_suffix\n",
    "        nb_output_name = 'Producing channel distributions from ' + metadata_dir + ', pc={0}'.format(pc) + '.ipynb'\n",
    "        new_arg_bundle = {'gating_data_fp':gating_data_fp,\n",
    "                          'metadata_dir':metadata_dir,\n",
    "                          's':s,\n",
    "                          'c':pc,\n",
    "                          'cm_dir':channel_model_dir,\n",
    "                          'nb_output_name':nb_output_name}\n",
    "        cm_arg_bundles.append(new_arg_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:37.021129Z",
     "start_time": "2019-05-30T18:58:37.015541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gating_data_fp': 'GD_AmE/AmE-diphones-IPA-annotated-columns.csv',\n",
       " 'metadata_dir': 'GD_AmE',\n",
       " 's': 'destressed',\n",
       " 'c': 0,\n",
       " 'cm_dir': 'CM_AmE_destressed_unaligned_pseudocount0',\n",
       " 'nb_output_name': 'Producing channel distributions from GD_AmE, pc=0.ipynb'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_arg_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:06:09.934163Z",
     "start_time": "2019-05-27T17:18:09.972176Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0/Producing channel distributions from GD_AmE, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.01/Producing channel distributions from GD_AmE, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.05/Producing channel distributions from GD_AmE, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [08:01<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.1/Producing channel distributions from GD_AmE, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:48<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [03:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:56<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:50<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_newdic_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:52<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:59<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:53<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_CMU_destressed, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:47<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.ipynb\n",
      "100%|██████████| 189/189 [02:59<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.01.ipynb\n",
      "100%|██████████| 189/189 [07:49<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.05.ipynb\n",
      "100%|██████████| 189/189 [07:57<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing channel distributions.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Producing channel distributions from GD_AmE_destressed_aligned_w_LTR_Buckeye, pc=0.1.ipynb\n",
      "100%|██████████| 189/189 [07:55<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~110m on wittgenstein\n",
    "for ab in cm_arg_bundles:\n",
    "    if not path.exists(ab['cm_dir']):\n",
    "        mkdir(ab['cm_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing channel distributions.ipynb',\n",
    "    path.join(ab['cm_dir'], ab['nb_output_name']),\n",
    "    parameters=dict(g = ab['gating_data_fp'],\n",
    "                    m = ab['metadata_dir'],\n",
    "                    s = ab['s'],\n",
    "                    c = ab['c'],\n",
    "                    o = ab['cm_dir'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are channel distributions created?**\n",
    "\n",
    "Take a look at `Producing channel distributions.ipynb`. Besides removing stress information, there are some mathematically non-trivial details that go into defining both uniphone and triphone channel distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Generating (contextual) lexicon distributions (over orthographic vocabularies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given \n",
    " - a language model $m$ (defined by a `.arpa` file or `kenlm` memory-mapped analogue) \n",
    " - a choice of n-gram contexts $C$ (a `.txt` file with one context per line)\n",
    " - a vocabulary $V$ (a `.txt` file with one word per line)\n",
    " - a (partial) output filepath $o$ / output filepath prefix $o$\n",
    " \n",
    "`Producing contextual distributions.ipynb` will write a serialized/memory-mapped `numpy` array to $o$.hV_C that defines $-log_2( p(V|C) )$ - slightly transformed output from `kenlm`. It will also write $p(V|C)$ to $o$.pV_C, and copy both $V$ and $C$ to the base directory specified by $o$. (In both cases, each column is associated with the distribution $p(V|c)$ for some $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:37.027818Z",
     "start_time": "2019-05-30T18:58:37.022771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('buckeye_contexts.txt', 'swbd2003_contexts.txt')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts\n",
    "fisher_lm_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:58:37.034645Z",
     "start_time": "2019-05-30T18:58:37.029351Z"
    }
   },
   "outputs": [],
   "source": [
    "LDs = [{'LD_dir':'LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_Buckeye_contexts' + '/' + 'LD_fisher_vocab_in_buckeye_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':buckeye_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_Buckeye_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')},\n",
    "       {'LD_dir':'LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "        'o_fn_stem':'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'o':'LD_Fisher_vocab_in_swbd2003_contexts' + '/' + 'LD_fisher_vocab_in_swbd2003_contexts',\n",
    "        'm':fisher_lm_fps['lm'],\n",
    "        'v':fisher_lm_fps['vocab'],\n",
    "        'c':swbd2003_contexts,\n",
    "        'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts', \n",
    "                          'Producing ' + 'LD_Fisher_vocab_in_swbd2003_contexts'.replace('_', ' ')[3:] + ' contextual distributions.ipynb')}\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T20:21:42.664184Z",
     "start_time": "2019-05-26T19:00:16.251722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/86 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/86 [00:00<00:03, 24.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 6/86 [00:00<00:03, 26.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 9/86 [00:00<00:02, 26.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 12/86 [00:00<00:02, 26.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 15/86 [00:00<00:02, 26.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 18/86 [00:00<00:02, 25.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 21/86 [00:00<00:03, 18.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 24/86 [00:01<00:03, 19.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 27/86 [00:01<00:02, 20.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 30/86 [00:01<00:02, 19.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/86 [00:01<00:02, 21.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 36/86 [00:01<00:02, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 39/86 [00:01<00:02, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 42/86 [00:01<00:02, 21.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/86 [00:02<00:01, 21.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 48/86 [00:02<00:01, 20.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 51/86 [00:02<00:01, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 54/86 [00:02<00:01, 22.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▋   | 57/86 [00:02<00:01, 21.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:02<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 60/86 [00:21<00:01, 22.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 61/86 [02:47<20:35, 49.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 62/86 [04:03<22:59, 57.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 63/86 [04:04<15:28, 40.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 64/86 [04:04<10:24, 28.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 65/86 [04:05<07:00, 20.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 66/86 [04:05<04:42, 14.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 67/86 [04:05<03:08,  9.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 68/86 [04:18<03:14, 10.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 69/86 [04:18<02:10,  7.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 70/86 [04:18<01:26,  5.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 71/86 [04:19<00:59,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 72/86 [04:26<01:08,  4.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 73/86 [04:26<00:45,  3.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 74/86 [04:26<00:29,  2.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 75/86 [04:27<00:21,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 76/86 [04:28<00:15,  1.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 77/86 [04:28<00:10,  1.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 79/86 [04:28<00:05,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 81/86 [04:28<00:02,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 82/86 [04:28<00:01,  2.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 83/86 [05:05<00:34, 11.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 84/86 [06:19<01:00, 30.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 85/86 [06:20<00:21, 21.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 86/86 [06:20<00:00, 15.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Producing contextual distributions.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/87 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/87 [00:00<00:03, 24.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 7/87 [00:00<00:03, 26.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 10/87 [00:00<00:02, 26.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 13/87 [00:00<00:02, 26.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 16/87 [00:00<00:03, 21.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 19/87 [00:00<00:03, 22.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 22/87 [00:01<00:03, 17.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 25/87 [00:01<00:03, 19.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 28/87 [00:01<00:03, 17.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 30/87 [00:01<00:03, 17.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 33/87 [00:01<00:02, 18.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 36/87 [00:01<00:02, 19.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 39/87 [00:01<00:02, 21.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 42/87 [00:02<00:02, 21.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 45/87 [00:02<00:01, 22.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 48/87 [00:02<00:01, 22.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 51/87 [00:02<00:01, 22.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 54/87 [00:02<00:01, 22.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 57/87 [00:02<00:01, 22.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:02<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 60/87 [00:13<00:01, 22.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 61/87 [16:04<2:05:03, 288.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 62/87 [38:25<4:11:48, 604.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 63/87 [38:26<2:49:14, 423.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▎  | 64/87 [38:26<1:53:33, 296.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 65/87 [38:26<1:16:02, 207.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 66/87 [38:26<50:49, 145.23s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 67/87 [38:26<33:53, 101.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 68/87 [42:50<47:35, 150.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 69/87 [42:58<32:19, 107.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 70/87 [43:05<21:58, 77.56s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 71/87 [44:09<19:35, 73.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 72/87 [45:28<18:44, 74.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 73/87 [50:38<33:55, 145.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 74/87 [50:38<22:05, 101.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 75/87 [50:38<14:16, 71.40s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 76/87 [50:43<09:26, 51.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▊ | 77/87 [50:43<06:00, 36.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 78/87 [50:43<03:47, 25.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 79/87 [50:44<02:21, 17.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 80/87 [50:44<01:28, 12.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 81/87 [50:44<00:53,  8.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 82/87 [50:45<00:31,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 83/87 [50:45<00:17,  4.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 84/87 [55:08<04:06, 82.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 85/87 [1:13:43<13:04, 392.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 86/87 [1:13:51<04:36, 276.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 87/87 [1:14:01<00:00, 196.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes ~80m on wittgenstein\n",
    "for ld in LDs:\n",
    "    if not path.exists(ld['LD_dir']):\n",
    "        mkdir(ld['LD_dir'])\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Producing contextual distributions.ipynb',\n",
    "#     'Producing ' + ld['LD_dir'].replace('_', ' ')[3:] + ' contextual distributions.ipynb',\n",
    "    ld['nb_fp'],\n",
    "    parameters=dict(m = ld['m'],\n",
    "                    v = ld['v'],\n",
    "                    c = ld['c'],\n",
    "                    o = ld['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating combinable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic problem:**\n",
    "\n",
    "1. **Channel model + transcribed lexicon relation**: Even after the gating data and a transcribed lexicon relation are defined over the same inventory, \n",
    " - the lexicon may contain triphones that are not in the stimuli triphones of a triphone channel model.\n",
    " - the channel distribution will contain triphones that cannot be found in the transcribed lexicon relation. (While the other steps here are strictly necessary, this is simply a practical step for making downstream computation faster.)\n",
    "2. **Language model + transcribed lexicon relation**: The orthographic vocabulary of a transcribed lexicon relation may contain wordforms not in an n-gram model's vocabulary. (We *don't* want to use the out-of-vocabulary estimate for those wordforms.)\n",
    "3.  **Contextual distributions + transcribed lexicon relation**: The contextual distributions from Step 3b above are defined over the *language model's* orthographic vocabulary, which will likely include wordforms that are not in the transcribed lexicon relation. We want to create modified forms of these distributions where we condition on the choice of an orthographic wordform that is in the transcribed lexicon relation.\n",
    "\n",
    "Once we have\n",
    " - a version $l'$ of the transcribed lexicon relation $l$ trimmed with respect to both the triphones of the channel model $c$ and the (orthographic) vocabulary of a language model $m$\n",
    " - a version $d'$ of the contextual distributions over $m$'s vocabulary (with respect to some set of n-gram contexts) $d$ trimmed to only define distributions over $l'$\n",
    " - a version $c'$ of the channel model $c$ trimmed with respect to a transcribed lexicon relation $l'$\n",
    " - a probability distribution over segmental wordforms given an orthographic wordform\n",
    " \n",
    "we will be able to combine everything together to calculate confusability of wordforms in corpus contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Filter transcription lexicons to only include words that can be modeled by a given channel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather relevant LTRs and their associated CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:33.734661Z",
     "start_time": "2019-05-30T18:59:33.731309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs = list(map(lambda ab: {'LTR_fp':ab['my_ol'],\n",
    "                                    'GD_fp':ab['my_og']},\n",
    "                        alignment_arg_bundles))\n",
    "list(map(lambda d: d['LTR_fp'],\n",
    "         aligned_LTRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:35.019556Z",
     "start_time": "2019-05-30T18:59:35.013982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_newdic_destressed/GD_AmE-diphones_aligned_w_LTR_newdic_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_CMU_destressed/GD_AmE-diphones_aligned_w_LTR_CMU_destressed.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'},\n",
       " {'CM_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       "  'GD_fp': 'GD_AmE_destressed_aligned_w_LTR_Buckeye/GD_AmE-diphones_aligned_w_LTR_Buckeye.tsv'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CM_dirs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "#                                 'GD_fp':cab['gating_data_fp']},\n",
    "#                    filter(lambda cab: cab['c'] != 0, cm_arg_bundles)))\n",
    "# CM_dirs\n",
    "# CM_dirs[5]\n",
    "# # listdir(CM_dirs[0][])\n",
    "\n",
    "aligned_CMs = list(map(lambda cab: {'CM_fp':path.join(cab['cm_dir'],'pY1X0X1X2.json'),\n",
    "                                    'GD_fp':cab['gating_data_fp']},\n",
    "                       filter(lambda cab: cab['c'] != 0 and 'aligned' in cab['gating_data_fp'], \n",
    "                              cm_arg_bundles)))\n",
    "aligned_CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:38.001954Z",
     "start_time": "2019-05-30T18:59:37.997662Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aligned_CMs(ltr_bundle):\n",
    "    matches = [cm_bundle for cm_bundle in aligned_CMs if cm_bundle['GD_fp'] == ltr_bundle['GD_fp']]\n",
    "    return list(map(lambda d: d['CM_fp'],\n",
    "                    matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:38.452568Z",
     "start_time": "2019-05-30T18:59:38.448408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs[0]['LTR_fp']\n",
    "\n",
    "#NB: all of these will have the same set of stimuli triphones\n",
    "#    ...which is all we care about here\n",
    "get_aligned_CMs(aligned_LTRs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:39.797702Z",
     "start_time": "2019-05-30T18:59:39.793423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv',\n",
       "  'matching_CMs': ['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       "   'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_LTRs_and_CM = [{'LTR_fp':ltr['LTR_fp'],\n",
    "                        'matching_CMs':get_aligned_CMs(ltr)}\n",
    "                       for ltr in aligned_LTRs]\n",
    "aligned_LTRs_and_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T18:59:42.595473Z",
     "start_time": "2019-05-30T18:59:42.590148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "nb = LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      " \n",
      "c = CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json\n",
      "l = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n",
      "o = LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "nb = LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    each['c'] = each['matching_CMs'][0]\n",
    "    each['l'] = each['LTR_fp']\n",
    "    o_dir = path.dirname(each['LTR_fp'])\n",
    "    o_fn = path.basename(each['LTR_fp']).split('w_')[0] + 'CM_filtered' + '.tsv'\n",
    "    each['o'] = path.join(o_dir, o_fn)\n",
    "    \n",
    "    nb_fn = 'Filter ' + o_fn.split('_aligned')[0] + ' against channel model.ipynb'\n",
    "    each['nb_fp'] = path.join(o_dir, nb_fn)\n",
    "    \n",
    "    print('c = {0}\\nl = {1}\\no = {2}\\nnb = {3}'.format(each['c'], each['l'], each['o'], each['nb_fp']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:43.618439Z",
     "start_time": "2019-05-30T19:00:28.488328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:02<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:04<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by channel model.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye against channel model.ipynb\n",
      "100%|██████████| 27/27 [00:02<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about 20s on wittgenstein\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    if not path.exists(path.dirname(each['o'])):\n",
    "        mkdir(path.dirname(each['o']))\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter transcription lexicon by channel model.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filter transcription lexicons to only include words that are in a language model's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** For this step, we will use shell commands provided by the `csvkit` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:44.139485Z",
     "start_time": "2019-05-30T19:00:44.136043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       " 'vocab': 'LM_Fisher/fisher_vocabulary_main.txt'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_fps\n",
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:44.923146Z",
     "start_time": "2019-05-30T19:00:44.918830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_fps = list(map(lambda pair: pair['LTR_fp'],\n",
    "                   aligned_LTRs))\n",
    "LTR_fps\n",
    "\n",
    "LTR_CM_filtered = list(map(lambda d: d['o'],\n",
    "                           aligned_LTRs_and_CM))\n",
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lexicon entries have unmodelable triphones? (We'll next check how many such lexicon entries aren't in the language model vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:50.594758Z",
     "start_time": "2019-05-30T19:00:50.248147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "133855 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
      "7999 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_w_GD_AmE-diphones.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_w_GD_AmE-diphones.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:51.371018Z",
     "start_time": "2019-05-30T19:00:51.030190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:53.649347Z",
     "start_time": "2019-05-30T19:00:53.646335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:00:53.896515Z",
     "start_time": "2019-05-30T19:00:53.888307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv',\n",
       " 'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye_aligned_CM_filtered against fisher_vocabulary_main.ipynb'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LTR_LM_filter_bundles = []\n",
    "for each_LTR_fp in LTR_CM_filtered:\n",
    "    bundle = dict()\n",
    "    LTR_descr = path.basename(each_LTR_fp)[:-4]\n",
    "    LM_V_descr = path.basename(fisher_lm_vocab_fp)[:-4]\n",
    "    bundle['l'] = each_LTR_fp\n",
    "    bundle['v'] = fisher_lm_vocab_fp\n",
    "    bundle['o'] = each_LTR_fp[:-4] + '_LM_filtered' + '.tsv'\n",
    "    bundle['nb_fp'] = path.join(path.dirname(each_LTR_fp),\n",
    "                                f'Filter {LTR_descr} against {LM_V_descr}' + '.ipynb')\n",
    "    bundle\n",
    "    LTR_LM_filter_bundles.append(bundle)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.279071Z",
     "start_time": "2019-05-30T19:00:57.183644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Filter LTR_newdic_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Filter LTR_CMU_destressed_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:44<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter transcription lexicon by language model vocabulary.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Filter LTR_Buckeye_aligned_CM_filtered against fisher_vocabulary_main.ipynb\n",
      "100%|██████████| 25/25 [00:02<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~1m on wittgenstein\n",
    "for each in LTR_LM_filter_bundles:\n",
    "    if not path.exists(path.dirname(each['o'])):\n",
    "        mkdir(path.dirname(each['o']))\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter transcription lexicon by language model vocabulary.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    v = each['v'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit less than half** of the triphone channel model-modelable `newdic` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.507153Z",
     "start_time": "2019-05-30T19:01:54.280981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17079 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
      "9412 LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About three quarters** of the triphone channel model-modelable `CMU_destressed` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.741697Z",
     "start_time": "2019-05-30T19:01:54.509277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127799 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
      "33125 LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Less than 10%** of the triphone channel model-modelable `Buckeye` lexicon **isn't** in the LM vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.969650Z",
     "start_time": "2019-05-30T19:01:54.743655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7011 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
      "6575 LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered.tsv\n",
    "!wc -l LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word type loss for Buckeye:\n",
    " - $7999 - 7011 = 988$ word types lost due to unmodelable triphones\n",
    " - $7011 - 6575 = 436$ further word types lost due orthographic wordforms not being in the language model vocabulary\n",
    " - $\\frac{436+988}{7999} ≈ 17.8\\%$ of word types lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting the filtered transcribed lexicon relations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.977847Z",
     "start_time": "2019-05-30T19:01:54.973014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered = list(map(lambda bundle: bundle['o'],\n",
    "                                       LTR_LM_filter_bundles))\n",
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Filter the conditioning events of channel distributions to only include triphones contained in a transcription lexicon's segmental wordform list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:46:18.061069Z",
     "start_time": "2019-05-27T19:46:18.058861Z"
    }
   },
   "outputs": [],
   "source": [
    "#what channel models might you want to use with what lexicons?\n",
    "# there are 3x3 triphone CMs that are aligned with one of 3 LTRs and have one of 3 relevant pseudocount levels\n",
    "# For each of the 3 LTRs aligned with the gating data, there's exactly 1 `LTR...CM_filtered_LM_filtered.tsv` file\n",
    "# ∴ there are 3x3 channel models to trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:54.997202Z",
     "start_time": "2019-05-30T19:01:54.979290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for each in aligned_LTRs_and_CM:\n",
    "    LTR_dir = path.dirname(each['LTR_fp'])\n",
    "    LTR_trimmed_fn = path.basename(each['LTR_fp']).split('_w_')[0] + '_CM_filtered_LM_filtered.tsv'\n",
    "    each['LTR_trimmed_fp'] = path.join(LTR_dir, LTR_trimmed_fn)\n",
    "    each['matching_trimmed_CMs'] = [path.join(path.dirname(fp),\n",
    "                                              LTR_trimmed_fn[:-4] + '_' + path.basename(fp))\n",
    "                                    for fp in each['matching_CMs']]\n",
    "    \n",
    "#     each['LTR_fp']\n",
    "    each['LTR_trimmed_fp']\n",
    "    each['matching_CMs']\n",
    "    each['matching_trimmed_CMs']\n",
    "#     each['trimmed LTR_fp']\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:01:55.020868Z",
     "start_time": "2019-05-30T19:01:54.998408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_LTR_CM_triples = []\n",
    "for each in aligned_LTRs_and_CM:\n",
    "    assert len(each['matching_CMs']) == len(each['matching_trimmed_CMs'])\n",
    "    \n",
    "    for i in range(len(each['matching_CMs'])):\n",
    "        args = dict()\n",
    "        args['l'] = each['LTR_trimmed_fp']\n",
    "        args['c'] = each['matching_CMs'][i]\n",
    "        args['o'] = each['matching_trimmed_CMs'][i]\n",
    "        args['nb_fp'] = path.join(path.dirname(args['c']),\n",
    "                                  f\"Filter {path.dirname(args['c'])} against {path.basename(args['l'])[:-4]}.ipynb\")\n",
    "        args\n",
    "        trimmed_LTR_CM_triples.append(args)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.285877Z",
     "start_time": "2019-05-30T19:01:55.022428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1 against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1 against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter channel model by transcription lexicon.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Filter CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1 against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about 70s on wittgenstein\n",
    "for each in trimmed_LTR_CM_triples:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Creating output path '{output_dir}'\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter channel model by transcription lexicon.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['l'],\n",
    "                    c = each['c'],\n",
    "                    o = each['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: For each (filtered) transcribed lexicon relation, define the relevant contextual lexicon distributions over orthographic wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.289674Z",
     "start_time": "2019-05-30T19:03:01.287031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.300808Z",
     "start_time": "2019-05-30T19:03:01.290533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LD_dir': 'LD_Fisher_vocab_in_Buckeye_contexts',\n",
       "  'o_fn_stem': 'LD_fisher_vocab_in_buckeye_contexts',\n",
       "  'o': 'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'buckeye_contexts.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_Buckeye_contexts/Producing Fisher vocab in Buckeye contexts contextual distributions.ipynb'},\n",
       " {'LD_dir': 'LD_Fisher_vocab_in_swbd2003_contexts',\n",
       "  'o_fn_stem': 'LD_fisher_vocab_in_swbd2003_contexts',\n",
       "  'o': 'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts',\n",
       "  'm': 'LM_Fisher/fisher_utterances_main_4gram.mmap',\n",
       "  'v': 'LM_Fisher/fisher_vocabulary_main.txt',\n",
       "  'c': 'swbd2003_contexts.txt',\n",
       "  'nb_fp': 'LD_Fisher_vocab_in_swbd2003_contexts/Producing Fisher vocab in swbd2003 contexts contextual distributions.ipynb'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.306677Z",
     "start_time": "2019-05-30T19:03:01.302190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_Fisher/fisher_vocabulary_main.txt'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_lm_vocab_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.313493Z",
     "start_time": "2019-05-30T19:03:01.308238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buckeye_contexts.txt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'swbd2003_contexts.txt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckeye_contexts\n",
    "swbd2003_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.321166Z",
     "start_time": "2019-05-30T19:03:01.315057Z"
    }
   },
   "outputs": [],
   "source": [
    "LD_projection_args = [\n",
    "    {'d':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':buckeye_contexts,\n",
    "     'l':'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_Buckeye_contexts/LD_fisher_vocab_in_buckeye_contexts' + '_projected_' + 'LTR_Buckeye' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_Buckeye_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_buckeye_contexts' + ' against ' + 'LTR_Buckeye_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "    {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':swbd2003_contexts,\n",
    "     'l':'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_CMU_destressed' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')},\n",
    "    {'d':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts.pV_C',\n",
    "     'v':fisher_lm_vocab_fp,\n",
    "     'c':swbd2003_contexts,\n",
    "     'l':'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
    "     'o':'LD_Fisher_vocab_in_swbd2003_contexts/LD_fisher_vocab_in_swbd2003_contexts' + '_projected_' + 'LTR_newdic_destressed' + '.pV_C',\n",
    "     'f':'True',\n",
    "     'nb_fp':path.join('LD_Fisher_vocab_in_swbd2003_contexts',\n",
    "                       'Filter ' + 'LD_fisher_vocab_in_swbd2003_contexts' + ' against ' + 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered' + '.ipynb')}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:47:00.624460Z",
     "start_time": "2019-05-28T21:32:15.823720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_Buckeye_contexts/Filter LD_fisher_vocab_in_buckeye_contexts against LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 5/36 [00:00<00:00, 44.40it/s]\u001b[A\n",
      " 28%|██▊       | 10/36 [00:00<00:00, 44.49it/s]\u001b[A\n",
      " 42%|████▏     | 15/36 [00:00<00:00, 44.21it/s]\u001b[A\n",
      " 50%|█████     | 18/36 [00:00<00:01, 12.48it/s]\u001b[A\n",
      " 58%|█████▊    | 21/36 [00:01<00:01, 14.62it/s]\u001b[A\n",
      " 67%|██████▋   | 24/36 [00:04<00:04,  2.60it/s]\u001b[A\n",
      " 75%|███████▌  | 27/36 [00:16<00:12,  1.43s/it]\u001b[A\n",
      " 83%|████████▎ | 30/36 [00:17<00:07,  1.17s/it]\u001b[A\n",
      " 86%|████████▌ | 31/36 [00:20<00:08,  1.60s/it]\u001b[A\n",
      " 92%|█████████▏| 33/36 [00:27<00:06,  2.26s/it]\u001b[A\n",
      "100%|██████████| 36/36 [00:28<00:00,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█         | 4/36 [00:00<00:00, 35.15it/s]\u001b[A\n",
      " 25%|██▌       | 9/36 [00:00<00:00, 37.54it/s]\u001b[A\n",
      " 39%|███▉      | 14/36 [00:00<00:00, 39.86it/s]\u001b[A\n",
      " 50%|█████     | 18/36 [00:00<00:00, 20.40it/s]\u001b[A\n",
      " 58%|█████▊    | 21/36 [00:01<00:00, 15.82it/s]\u001b[A\n",
      " 64%|██████▍   | 23/36 [00:09<00:17,  1.34s/it]\u001b[A\n",
      " 69%|██████▉   | 25/36 [00:18<00:24,  2.23s/it]\u001b[A\n",
      " 75%|███████▌  | 27/36 [01:38<02:02, 13.58s/it]\u001b[A\n",
      " 78%|███████▊  | 28/36 [01:38<01:16,  9.57s/it]\u001b[A\n",
      " 83%|████████▎ | 30/36 [01:46<00:47,  7.92s/it]\u001b[A\n",
      " 92%|█████████▏| 33/36 [09:32<03:58, 79.48s/it]\u001b[A\n",
      " 94%|█████████▍| 34/36 [09:33<01:51, 55.89s/it]\u001b[A\n",
      " 97%|█████████▋| 35/36 [09:33<00:39, 39.18s/it]\u001b[A\n",
      "100%|██████████| 36/36 [09:34<00:00, 27.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Filter contextual lexicon distribution by transcription lexicon.ipynb\n",
      "Output Notebook: LD_Fisher_vocab_in_swbd2003_contexts/Filter LD_fisher_vocab_in_swbd2003_contexts against LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 6/36 [00:00<00:00, 52.79it/s]\u001b[A\n",
      " 33%|███▎      | 12/36 [00:00<00:00, 52.65it/s]\u001b[A\n",
      " 50%|█████     | 18/36 [00:00<00:00, 19.84it/s]\u001b[A\n",
      " 58%|█████▊    | 21/36 [00:01<00:00, 19.11it/s]\u001b[A\n",
      " 67%|██████▋   | 24/36 [00:04<00:04,  2.41it/s]\u001b[A\n",
      " 67%|██████▋   | 24/36 [00:24<00:04,  2.41it/s]\u001b[A\n",
      " 75%|███████▌  | 27/36 [01:07<00:59,  6.60s/it]\u001b[A\n",
      " 78%|███████▊  | 28/36 [01:08<00:37,  4.70s/it]\u001b[A\n",
      " 83%|████████▎ | 30/36 [01:10<00:21,  3.57s/it]\u001b[A\n",
      " 86%|████████▌ | 31/36 [02:10<01:43, 20.67s/it]\u001b[A\n",
      " 92%|█████████▏| 33/36 [03:48<01:27, 29.14s/it]\u001b[A\n",
      " 94%|█████████▍| 34/36 [03:48<00:41, 20.52s/it]\u001b[A\n",
      " 97%|█████████▋| 35/36 [03:49<00:14, 14.55s/it]\u001b[A\n",
      "100%|██████████| 36/36 [03:50<00:00, 10.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~15m on wittgenstein\n",
    "for each in LD_projection_args:\n",
    "    output_dir = path.dirname(each['o'])\n",
    "    if not path.exists(output_dir):\n",
    "        print(\"Making output path '{0}'\".format(output_dir))\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Filter contextual lexicon distribution by transcription lexicon.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(d = each['d'],\n",
    "                    v = each['v'],\n",
    "                    c = each['c'],\n",
    "                    l = each['l'],\n",
    "                    o = each['o'],\n",
    "                    f = each['f'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: For each (filtered) transcribed lexicon relation, define a conditional distribution on segmental wordforms given an orthographic wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.327293Z",
     "start_time": "2019-05-30T19:03:01.322204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTR_CM_filtered_LM_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:01.335534Z",
     "start_time": "2019-05-30T19:03:01.328176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LTR_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'},\n",
       " {'LTR_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.tsv',\n",
       "  'pW_V_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       "  'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fp_bundles = []\n",
    "for ltr_fp in LTR_CM_filtered_LM_filtered:\n",
    "    LTR_n = path.basename( ltr_fp )[:-4]\n",
    "    LTR_dir = path.dirname( ltr_fp )\n",
    "    pW_V_fp_bundles.append({'LTR_fp':ltr_fp,\n",
    "                          'pW_V_fp':ltr_fp[:-4],# + '.pW_V',\n",
    "                          'nb_fp':path.join(LTR_dir,'Define pW_V given {0}.ipynb'.format(LTR_n))})\n",
    "pW_V_fp_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:42.869046Z",
     "start_time": "2019-05-30T19:03:01.337096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:07<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Define pW_V given LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:23<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Define a conditional distribution on segmental wordforms given an orthographic one.ipynb\n",
      "Output Notebook: LTR_Buckeye_aligned_w_GD_AmE_destressed/Define pW_V given LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 61/61 [00:04<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~30s on wittgenstein\n",
    "for each in pW_V_fp_bundles:\n",
    "    pW_V_fp_output_dir = path.dirname(each['pW_V_fp'])\n",
    "    if not path.exists(pW_V_fp_output_dir):\n",
    "        mkdir(pW_V_fp_output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Define a conditional distribution on segmental wordforms given an orthographic one.ipynb',\n",
    "    each['nb_fp'],\n",
    "    parameters=dict(l = each['LTR_fp'],\n",
    "                    o = each['pW_V_fp'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pre-calculate remaining forward model components and meta-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Generate triphone lexicon distributions for every triphone channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:42.873616Z",
     "start_time": "2019-05-30T19:03:42.870676Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in listdir(a_dir)\n",
    "            if path.isdir(path.join(a_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T19:03:42.894147Z",
     "start_time": "2019-05-30T19:03:42.876131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs = get_immediate_subdirectories('.')\n",
    "len(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:01:44.780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'old/Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'old/IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_model_fps = []\n",
    "for d in subdirs:\n",
    "    files = listdir(d)\n",
    "    is_triph_channel_model = lambda fn: 'pY1X0X1X2.json' in fn\n",
    "    CM_files = list(filter(is_triph_channel_model,\n",
    "                           files))\n",
    "    for CM_file in CM_files:\n",
    "        channel_model_fps.append(path.join(d, CM_file))\n",
    "len(channel_model_fps)\n",
    "channel_model_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:01:46.452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'old/Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'output_dir': 'old',\n",
       " 'c_fn': 'Hammond-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'Hammond-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'o': 'old/Hammond-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'nb_fp': 'old/Generating Hammond-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'old/IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'output_dir': 'old',\n",
       " 'c_fn': 'IPhOD-aligned_destressed_pseudocount0.001 pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'IPhOD-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'o': 'old/IPhOD-aligned_destressed_pseudocount0.001 pX0X1X2',\n",
       " 'nb_fp': 'old/Generating IPhOD-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05',\n",
       " 'c_fn': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05',\n",
       " 'c_fn': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/LTR_Buckeye_aligned_CM_filtered_LM_filtered_pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': 'CM_AmE_destressed_unaligned_pseudocount0.05/pY1X0X1X2.json',\n",
       " 'output_dir': 'CM_AmE_destressed_unaligned_pseudocount0.05',\n",
       " 'c_fn': 'pY1X0X1X2.json',\n",
       " 'o_fn_prefix': 'pX0X1X2',\n",
       " 'o': 'CM_AmE_destressed_unaligned_pseudocount0.05/pX0X1X2',\n",
       " 'nb_fp': 'CM_AmE_destressed_unaligned_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "triph_lex_bundles = []\n",
    "for cm_fp in channel_model_fps:\n",
    "    bundle = dict()\n",
    "    bundle['c'] = cm_fp\n",
    "    bundle['output_dir'] = path.dirname(cm_fp)\n",
    "    bundle['c_fn'] = path.basename(cm_fp)\n",
    "    bundle['o_fn_prefix'] = bundle['c_fn'].split('pY1X0X1X2')[0] + 'pX0X1X2'\n",
    "    bundle['o'] = path.join(bundle['output_dir'],\n",
    "                            bundle['o_fn_prefix'])\n",
    "    bundle['nb_fp'] = path.join(bundle['output_dir'],\n",
    "                                f\"Generating {bundle['c_fn'].split('pY1X0X1X2.json')[0][:-1]} uniform triphone lexicon dist.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    triph_lex_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:01:48.860Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.05/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 32.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.1/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_newdic_destressed_pseudocount0.01/Generating LTR_newdic_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 33.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.01/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 36.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.01/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 36.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: old/Generating Hammond-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: old/Generating IPhOD-aligned_destressed_pseudocount0.001 uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.1/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_CMU_destressed_pseudocount0.05/Generating LTR_CMU_destressed_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 28.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:01<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_aligned_w_LTR_Buckeye_pseudocount0.05/Generating LTR_Buckeye_aligned_CM_filtered_LM_filtered uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:00<00:00, 37.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Generate triphone lexicon distribution from channel model.ipynb\n",
      "Output Notebook: CM_AmE_destressed_unaligned_pseudocount0.05/Generating  uniform triphone lexicon dist.ipynb\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about ~1m on wittgenstein\n",
    "for bundle in triph_lex_bundles:\n",
    "    output_dir = bundle['output_dir']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path {output_dir}\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Generate triphone lexicon distribution from channel model.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(c = bundle['c'],\n",
    "                    o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Pre-calculate prefix relation, $k$-cousins, and $k$-spheres for each segmental lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is comparable to the `Metadata` generation step in 2a above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:01:51.267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pW_V_fps = [each['pW_V_fp'] + '.pW_V.json' for each in pW_V_fp_bundles]\n",
    "pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:35:17.749791Z",
     "start_time": "2019-05-29T16:35:17.747869Z"
    }
   },
   "outputs": [],
   "source": [
    "# pW_V_fps = []\n",
    "# for d in subdirs:\n",
    "#     files = listdir(d)\n",
    "#     is_pW_V = lambda fn: 'pW_V.json' in fn\n",
    "#     pW_V_files = list(filter(is_pW_V,\n",
    "#                            files))\n",
    "#     for pW_V_file in pW_V_files:\n",
    "#         pW_V_fps.append(path.join(d, pW_V_file))\n",
    "# len(pW_V_fps)\n",
    "# pW_V_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:01:55.551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making output path 'CM_AmE_destressed_unaligned_pseudocount0.05/pX0X1X2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_newdic_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_CMU_destressed_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/LTR_Buckeye_aligned_CM_filtered_LM_filtered.pW_V.json',\n",
       " 'o': 'LTR_Buckeye_aligned_w_GD_AmE_destressed',\n",
       " 'lex_name': 'LTR_Buckeye_aligned_CM_filtered_LM_filtered',\n",
       " 'nb_fp': 'LTR_Buckeye_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_Buckeye_aligned_CM_filtered_LM_filtered.ipynb'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "lexicon_md_bundles = []\n",
    "for pW_V_fp in pW_V_fps:\n",
    "    output_dir = bundle['o']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path '{output_dir}'\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    bundle = dict()\n",
    "    bundle['p'] = pW_V_fp\n",
    "    bundle['o'] = path.dirname(pW_V_fp)\n",
    "    bundle['lex_name'] = path.basename(pW_V_fp).split('.pW_V.json')[0]\n",
    "    bundle['nb_fp'] = path.join(bundle['o'],\n",
    "                                f\"Calculate prefix data, k-cousins, and k-spheres for {bundle['lex_name']}.ipynb\")\n",
    "    bundle\n",
    "    print(' ')\n",
    "    lexicon_md_bundles.append(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-30T19:03:19.553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate prefix data, k-cousins, and k-spheres.ipynb\n",
      "Output Notebook: LTR_newdic_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_newdic_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      "100%|██████████| 126/126 [23:14<00:00,  1.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  Calculate prefix data, k-cousins, and k-spheres.ipynb\n",
      "Output Notebook: LTR_CMU_destressed_aligned_w_GD_AmE_destressed/Calculate prefix data, k-cousins, and k-spheres for LTR_CMU_destressed_aligned_CM_filtered_LM_filtered.ipynb\n",
      " 40%|████      | 51/126 [48:01<1:40:54, 80.73s/it]  "
     ]
    }
   ],
   "source": [
    "# with J=-1 and heavy CPU load\n",
    "#  - newdic CM_filtered_LM_filtered takes ~30m\n",
    "#  - CMU CM_filtered_LM_filtered takes ~3h41m to get to 60% (54/90)\n",
    "# - Buckeye CM_filtered_LM_filtered takes ~15m\n",
    "for bundle in lexicon_md_bundles:\n",
    "    output_dir = bundle['o']\n",
    "    if not path.exists(output_dir):\n",
    "        print(f\"Making output path {output_dir}\")\n",
    "        makedirs(output_dir)\n",
    "    \n",
    "    nb = pm.execute_notebook(\n",
    "    'Calculate prefix data, k-cousins, and k-spheres.ipynb',\n",
    "    bundle['nb_fp'],\n",
    "    parameters=dict(p = bundle['p'],\n",
    "                    o = bundle['o'])\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4c: Define incremental channel distributions on a set of segmental wordforms(+prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan:\n",
    "# 1. Backtrack from the bottom of P4bnt2 to figure out \n",
    "#  - what function call is actually used to calculate likelihoods $p(Y_0^k|X_0^k)$\n",
    "#  - what data structures that function calls\n",
    "# 2. Create a notebook that \n",
    "#  - takes some kind of file specifying a set of segmental wordforms (or prefixes)\n",
    "#  - an output directory\n",
    "#  - outputs data structures representing channel matrices for each length (use np.save/load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan:\n",
    "# 1. Check if there are components of the posterior calculation that are worth pre-calculating; if so, calculate them.\n",
    "#    - e.g. pW_C, pV_W-hat,C\n",
    "# 2. Next notebook should have a flag for calculating just p(V-hat = v* | V = v*) ∀v* vs. the full p(V-hat | V = v*) ∀v*\n",
    "#    - Consider facilitating SLURM cluster jobs by \n",
    "#       - making posterior calculation deterministic (unlike P4bnt2) \n",
    "#       - adding/supporting optional notebook arguments that specify which parts of the posterior dist to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generating analysis measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
