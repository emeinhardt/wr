{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:44:23.450095Z",
     "start_time": "2018-12-04T03:44:23.443175Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#About-this-notebook\" data-toc-modified-id=\"About-this-notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>About this notebook</a></span></li><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Diphone-gating-data\" data-toc-modified-id=\"Diphone-gating-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Diphone gating data</a></span></li><li><span><a href=\"#Hammond's-newdic\" data-toc-modified-id=\"Hammond's-newdic-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Hammond's newdic</a></span></li><li><span><a href=\"#IPhOD\" data-toc-modified-id=\"IPhOD-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>IPhOD</a></span></li></ul></li><li><span><a href=\"#Inventory-exploration\" data-toc-modified-id=\"Inventory-exploration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Inventory exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gating-data\" data-toc-modified-id=\"Gating-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Gating data</a></span></li><li><span><a href=\"#Hammond's-newdict\" data-toc-modified-id=\"Hammond's-newdict-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Hammond's newdict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparison-of-inventories-between-Hammond's-newdic-and-the-diphone-gating-data\" data-toc-modified-id=\"Comparison-of-inventories-between-Hammond's-newdic-and-the-diphone-gating-data-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Comparison of inventories between Hammond's newdic and the diphone gating data</a></span></li></ul></li><li><span><a href=\"#Lexicon---IPhOD\" data-toc-modified-id=\"Lexicon---IPhOD-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Lexicon - IPhOD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparison-of-IPhOD-inventory-and-diphone-gating-data\" data-toc-modified-id=\"Comparison-of-IPhOD-inventory-and-diphone-gating-data-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Comparison of IPhOD inventory and diphone gating data</a></span></li></ul></li></ul></li><li><span><a href=\"#Processing-pipeline-sketch\" data-toc-modified-id=\"Processing-pipeline-sketch-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Processing pipeline sketch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-todo-tally:\" data-toc-modified-id=\"Global-todo-tally:-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Global todo tally:</a></span><ul class=\"toc-item\"><li><span><a href=\"#New-capabilities\" data-toc-modified-id=\"New-capabilities-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>New capabilities</a></span></li><li><span><a href=\"#Open-questions\" data-toc-modified-id=\"Open-questions-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Open questions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook author:** emeinhardt@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook goal:** Examine the inventories of diphone gating and lexicon representations and cite the relevant portions of each paper necessary to determine the gist of what needs to be done to align gating data with representations of the lexicon for use in a word recognition model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:** The goal is to construct a word recognition model that can map from a segmental transcription of the incrementally produced prefix of a speaker's intended wordform to a listener's beliefs about what the speaker's actual intended wordform is. In this model task, the speaker chooses a wordform according to its prior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Sources:**\n",
    "This requires a lexicon of transcribed wordforms and the ability to assign a prior probability to each wordform, as well as a model of listening noise. The diphone gating data of Warner et al. (2014) provides the noise model for (Southwestern) American English. \n",
    "\n",
    "Its inventory is based on the transcriptions from a dictionary of transcriptions of unclear origins provided by Mike Hammond ('newdic.txt'); as far as I can tell, its transcriptions are from the same source as the NetTalk Corpus and what I think is the Hoosier Mental Lexicon (Hammond's dictionary also shares word frequency estiamtes with what I think is the HML) -- some digitally transcribed English dictionary from the 60s or 70s. There are about 20k entries in Hammond's newdic.\n",
    "\n",
    "The Irvine Phonotactic Online Dictionary ('IPhOD') contains those transcriptions from the CMU pronouncing dictionary (many of whose transcriptions were also taken from some dictionary and whose other transcription sources are unvetted and undocumented) that could be aligned with orthographic word frequency estimates from the SUBTLEX_US database. There are 40-50k entries in IPhOD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concrete Problem:** The inventory of segment symbols used in the model of the lexicon needs to be aligned with the inventory of the noise model. This notebook investigates what the inventories of each dataset are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notational convention:** I will often use '.'s to indicate boundaries between segments (not e.g. syllable structure), because the alternative that ought to permit normal notation (using unicode tie bars to represent diphthongs and affricates) looks terrible on my machine (and everyone else's too, by default) and the trouble it takes to get them to render properly isn't worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:44:28.233379Z",
     "start_time": "2018-12-04T03:44:28.219174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A diphthong with tie bar: ɔi͡\n",
      "An affricate with tie bar: tʃ ͡\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' tʃ ͡'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use e.g. \n",
    "#  http://westonruter.github.io/ipa-chart/keyboard/\n",
    "#  https://linguistlist.org/unicode/ipa.html\n",
    "#  http://www.internationalphoneticalphabet.org/ipa-charts/ipa-symbols-with-unicode-decimal-and-hex-codes/\n",
    "# to construct a two-segment pair to put a tie bar over.\n",
    "print('A diphthong with tie bar: ɔi͡')\n",
    "print('An affricate with tie bar: tʃ ͡')\n",
    "' tʃ ͡'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, I am running Python 3.6.5, Jupyter Notebook 5.5.0, and otherwise defaults associated with Anaconda 5.2. More specifically, this notebook assumes the current working directory contains\n",
    " - a copy of the dataset and annotations available from https://linguistics.arizona.edu/dpl/english_diphones (NOTE: Natasha Warner has just - as of early 2018 - changed the website, and links to the data are currently broken)\n",
    " - a copy of Hammond's mysterious 'newdic' transcribed lexicon of English http://dingo.sbs.arizona.edu/~hammond/lsasummer11/newdic\n",
    " - a copy of the data associated with IPhOD (available from http://www.iphod.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:31.390323Z",
     "start_time": "2018-11-18T20:17:31.385280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/c2-jn'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:33.579743Z",
     "start_time": "2018-11-18T20:17:33.463855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diphones-raw-data.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls diphones-raw-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:34.998645Z",
     "start_time": "2018-11-18T20:17:34.882264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words.txt  \u001b[0m\u001b[01;31mIPhODv2.0_REALS.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%ls IPhOD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:39.717231Z",
     "start_time": "2018-11-18T20:17:39.601898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:43.004536Z",
     "start_time": "2018-11-18T20:17:42.996020Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diphone gating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do the data describe?** Each row in the diphones data file corresponds to a trial in a gating task: experimenters made audio recordings of the production of all (or nearly all) phonotactically licit diphones in (south)western American English embedded in nonsense contexts of at most a few speech sounds to either side -- e.g. one full stimulus nonsense word produced could be [ɑ.p.tʃ.ə], in which the vowels at either end are 'nonsense' context and [p.tʃ] is the diphone. For each such sound segment sequence, one recording was chosen. Six gating task stimuli were then created, each covering an increasing portion of the original recording (up to one of six 'gates')\n",
    " - one from the start of the recording to 1/3 of the way through the production of the first segment of the diphone\n",
    " - one from the start of the recording to 2/3 of the way through the production of the first segment of the diphone\n",
    " - one from the start of the recording to 3/3 of the way through the production of the first segment of the diphone\n",
    " - one from the start of the recording to 1/3 of the way through the production of the second segment of the diphone\n",
    " - one from the start of the recording to 2/3 of the way through the production of the second segment of the diphone\n",
    " - one from the start of the recording to 3/3 of the way through the production of the second segment of the diphone\n",
    "In a given trial, experiment participants listen to one of the six possible gatings of some sound segment sequence containing a diphone and then reported what they thought the full diphone was that was being produced in the recording in question. \n",
    "\n",
    "Each row indicates various properties of the stimulus and the participant's response.\n",
    "\n",
    "For more details, see Warner et al. (2014), the earlier paper on the Dutch diphone gating experiment (cited in Warner et al., 2014), and the documentation accompanying the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:47.198356Z",
     "start_time": "2018-11-18T20:17:47.192529Z"
    }
   },
   "outputs": [],
   "source": [
    "diphoneDataInFilename = \"diphones-raw-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:50.227391Z",
     "start_time": "2018-11-18T20:17:50.222079Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDiphoneGatingTrials(filename, print_fields = True):\n",
    "    '''\n",
    "    Opens filename in the current working directory and returns the trials as a \n",
    "    list of dictionaries, plus the fieldnames in the order present in the file.\n",
    "    '''\n",
    "    diphone_fields = []\n",
    "    diphoneTrials = []\n",
    "    diphoneDataInFilename = filename\n",
    "    with open(diphoneDataInFilename, newline='') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        diphone_fields = my_reader.fieldnames\n",
    "        if print_fields:\n",
    "            print(\"fieldnames: {0}\".format(diphone_fields))\n",
    "        for row in my_reader:\n",
    "            #print(row)\n",
    "            diphoneTrials.append(row)\n",
    "    return {'trials': diphoneTrials, 'fields':diphone_fields}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:53.439417Z",
     "start_time": "2018-11-18T20:17:52.053576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fieldnames: ['Subject', 'Diph_num', 'Diph_name', 'Sylltype', 'SoundFile', 'Prec_context', 'gate', 'four_gate', 'seg1_stress', 'seg2_stress', 'CorrAns1', 'CorrAns2', 'Resp1', 'Resp2', 'Seg1Accur', 'Seg2Accur', 'Prec_context_binary', 'wrong_preccontext', 'replacedSeg1Data', 'replacedSeg2Data']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Subject', '1'),\n",
       "             ('Diph_num', '1'),\n",
       "             ('Diph_name', 'CC'),\n",
       "             ('Sylltype', 'CC'),\n",
       "             ('SoundFile', '0001_CC_CC-g1-beeped.wav'),\n",
       "             ('Prec_context', 'ah'),\n",
       "             ('gate', '1'),\n",
       "             ('four_gate', '0'),\n",
       "             ('seg1_stress', '2'),\n",
       "             ('seg2_stress', '2'),\n",
       "             ('CorrAns1', 'ch'),\n",
       "             ('CorrAns2', 'ch'),\n",
       "             ('Resp1', 'ch'),\n",
       "             ('Resp2', 't'),\n",
       "             ('Seg1Accur', '1'),\n",
       "             ('Seg2Accur', '0'),\n",
       "             ('Prec_context_binary', '1'),\n",
       "             ('wrong_preccontext', ' '),\n",
       "             ('replacedSeg1Data', ' '),\n",
       "             ('replacedSeg2Data', ' ')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diphoneTrials = getDiphoneGatingTrials(diphoneDataInFilename)['trials']\n",
    "diphoneTrials[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T18:16:21.728673Z",
     "start_time": "2018-03-28T18:16:21.722765Z"
    },
    "collapsed": true
   },
   "source": [
    "## Hammond's newdic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the data**: Recall that the inventory of the gating experiment is based on a dictionary of transcriptions of unclear origins provided by Mike Hammond ('newdic.txt'); as far as I can tell, its transcriptions are from the same source as the NetTalk Corpus and what I think is the Hoosier Mental Lexicon (Hammond's dictionary also shares word frequency estiamtes with what I think is the HML) -- some digitally transcribed English dictionary from the 60s or 70s. There are about 20k entries in Hammond's newdic. I don't currently understand exactly how to interpret the columns I've labeled 'stressInfoA' and 'stressInfoB', but that's also not currently important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:17:57.396285Z",
     "start_time": "2018-11-18T20:17:57.391031Z"
    }
   },
   "outputs": [],
   "source": [
    "hammond_fn = \"Hammond's mysterious newdic.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:00.408500Z",
     "start_time": "2018-11-18T20:18:00.293961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hammond's mysterious newdic.txt\"\r\n"
     ]
    }
   ],
   "source": [
    "%ls Hammond*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:06.552445Z",
     "start_time": "2018-11-18T20:18:06.485267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'x'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdic_raw = []\n",
    "fieldnames = ['Transcription', 'stressInfoA', 'stressInfoB', 'Orthography', 'Frequency', 'PoSs']\n",
    "with open(hammond_fn) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t', fieldnames=fieldnames)\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        newdic_raw.append(row)\n",
    "\n",
    "newdic_raw[0]\n",
    "len(newdic_raw[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPhOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the data**: The Irvine Phonotactic Online Dictionary ('IPhOD') contains those transcriptions from the CMU pronouncing dictionary (many of whose transcriptions were also taken from some dictionary and whose other transcription sources are unvetted and undocumented) that could be aligned with orthographic word frequency estimates from the SUBTLEX_US database. (There are also a variety of columns indicating things like phonotactic probability neighborhood density useful for psycholinguistic experiments.)\n",
    "\n",
    "There are 40-50k entries in IPhOD; note that each one corresponds to a pronunciation, but the same orthographic word may have multiple transcriptions (rows) associated with it -- homophones that are also homographs and pronunciation variants of the same 'word' are indistinguishable using only the resources of the dataset alone.\n",
    "\n",
    "See the IPhOD website (link near the top) for more explanation of the cryptic column abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:10.574140Z",
     "start_time": "2018-11-18T20:18:10.454594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPhOD2_Words.txt  \u001b[0m\u001b[01;31mIPhODv2.0_REALS.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%ls IPhOD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:21.138649Z",
     "start_time": "2018-11-18T20:18:21.133758Z"
    }
   },
   "outputs": [],
   "source": [
    "#%cd IPhODv2.0_REALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:26.929198Z",
     "start_time": "2018-11-18T20:18:26.130185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Indx', '1'),\n",
       "             ('Word', 'a'),\n",
       "             ('UnTrn', 'AH'),\n",
       "             ('StTrn', 'AH0'),\n",
       "             ('NSyll', '1'),\n",
       "             ('NPhon', '1'),\n",
       "             ('unsDENS', '26'),\n",
       "             ('unsFDEN', '150377.45'),\n",
       "             ('unsLDEN', '91.03'),\n",
       "             ('unsCDEN', '136251'),\n",
       "             ('strDENS', '21'),\n",
       "             ('strFDEN', '145035.32'),\n",
       "             ('strLDEN', '77.56'),\n",
       "             ('strCDEN', '114167'),\n",
       "             ('unsBPAV', '0'),\n",
       "             ('unsFBPAV', '0'),\n",
       "             ('unsLBPAV', '0'),\n",
       "             ('unsCBPAV', '0'),\n",
       "             ('strBPAV', '0'),\n",
       "             ('strFBPAV', '0'),\n",
       "             ('strLBPAV', '0'),\n",
       "             ('strCBPAV', '0'),\n",
       "             ('unsTPAV', '0'),\n",
       "             ('unsFTPAV', '0'),\n",
       "             ('unsLTPAV', '0'),\n",
       "             ('unsCTPAV', '0'),\n",
       "             ('strTPAV', '0'),\n",
       "             ('strFTPAV', '0'),\n",
       "             ('strLTPAV', '0'),\n",
       "             ('strCTPAV', '0'),\n",
       "             ('unsPOSPAV', '0.04449866'),\n",
       "             ('unsFPOSPAV', '0.07192868'),\n",
       "             ('unsLPOSPAV', '0.03999447'),\n",
       "             ('unsCPOSPAV', '0.04159586'),\n",
       "             ('strPOSPAV', '0.03588891'),\n",
       "             ('strFPOSPAV', '0.06061931'),\n",
       "             ('strLPOSPAV', '0.03338777'),\n",
       "             ('strCPOSPAV', '0.03349570'),\n",
       "             ('unsLCPOSPAV', '0.07692308'),\n",
       "             ('unsFLCPOSPAV', '0.22760866'),\n",
       "             ('unsLLCPOSPAV', '0.12570126'),\n",
       "             ('unsCLCPOSPAV', '0.15987477'),\n",
       "             ('strLCPOSPAV', '0.06666667'),\n",
       "             ('strFLCPOSPAV', '0.21988359'),\n",
       "             ('strLLCPOSPAV', '0.07556101'),\n",
       "             ('strCLCPOSPAV', '0.10933994'),\n",
       "             ('SFreq', '20415.27'),\n",
       "             ('SCDcnt', '8382'),\n",
       "             ('', '')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPhOD_raw_filename = 'IPhOD2_Words.txt'\n",
    "\n",
    "lexicon_raw = []\n",
    "with open(IPhOD_raw_filename) as csvfile:\n",
    "    my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in my_reader:\n",
    "        #print(row)\n",
    "        lexicon_raw.append(row)\n",
    "\n",
    "lexicon_raw[0]\n",
    "len(lexicon_raw[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:18:35.992033Z",
     "start_time": "2018-11-18T20:18:35.985789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Indx', '2'),\n",
       "             ('Word', 'a'),\n",
       "             ('UnTrn', 'EY'),\n",
       "             ('StTrn', 'EY1'),\n",
       "             ('NSyll', '1'),\n",
       "             ('NPhon', '1'),\n",
       "             ('unsDENS', '44'),\n",
       "             ('unsFDEN', '89743.84'),\n",
       "             ('unsLDEN', '116.4'),\n",
       "             ('unsCDEN', '145321'),\n",
       "             ('strDENS', '44'),\n",
       "             ('strFDEN', '89743.84'),\n",
       "             ('strLDEN', '116.4'),\n",
       "             ('strCDEN', '145321'),\n",
       "             ('unsBPAV', '0'),\n",
       "             ('unsFBPAV', '0'),\n",
       "             ('unsLBPAV', '0'),\n",
       "             ('unsCBPAV', '0'),\n",
       "             ('strBPAV', '0'),\n",
       "             ('strFBPAV', '0'),\n",
       "             ('strLBPAV', '0'),\n",
       "             ('strCBPAV', '0'),\n",
       "             ('unsTPAV', '0'),\n",
       "             ('unsFTPAV', '0'),\n",
       "             ('unsLTPAV', '0'),\n",
       "             ('unsCTPAV', '0'),\n",
       "             ('strTPAV', '0'),\n",
       "             ('strFTPAV', '0'),\n",
       "             ('strLTPAV', '0'),\n",
       "             ('strCTPAV', '0'),\n",
       "             ('unsPOSPAV', '0.00333062'),\n",
       "             ('unsFPOSPAV', '0.02151505'),\n",
       "             ('unsLPOSPAV', '0.00425179'),\n",
       "             ('unsCPOSPAV', '0.00473232'),\n",
       "             ('strPOSPAV', '0.00286029'),\n",
       "             ('strFPOSPAV', '0.02042537'),\n",
       "             ('strLPOSPAV', '0.00398874'),\n",
       "             ('strCPOSPAV', '0.00458059'),\n",
       "             ('unsLCPOSPAV', '0.07692308'),\n",
       "             ('unsFLCPOSPAV', '0.21993981'),\n",
       "             ('unsLLCPOSPAV', '0.09186536'),\n",
       "             ('unsCLCPOSPAV', '0.11115314'),\n",
       "             ('strLCPOSPAV', '0.06666667'),\n",
       "             ('strFLCPOSPAV', '0.21993981'),\n",
       "             ('strLLCPOSPAV', '0.09186536'),\n",
       "             ('strCLCPOSPAV', '0.11115314'),\n",
       "             ('SFreq', '20415.27'),\n",
       "             ('SCDcnt', '8382'),\n",
       "             ('', '')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_raw[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The empty fieldname in both lexicon entries/rows above is a result of IPhOD having trailing tabs on every line - it can be ignored.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:14.226163Z",
     "start_time": "2018-11-18T20:20:14.220233Z"
    }
   },
   "outputs": [],
   "source": [
    "sound_fields = ['Prec_context', 'CorrAns1', 'CorrAns2', 'Resp1', 'Resp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:18.513789Z",
     "start_time": "2018-11-18T20:20:18.510892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 123, 'Job': 'clerk'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_dict(the_dict, keys_to_keep):\n",
    "    new_dict = {key:the_dict[key] for key in the_dict.keys() if key in keys_to_keep}\n",
    "    return new_dict\n",
    "project_dict({'Name':'Joe','ID':123,'Job':'clerk'},['Job','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:23.514986Z",
     "start_time": "2018-11-18T20:20:22.768879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_values(['ah', 'ch', 'ch', 'ch', 't']),\n",
       " dict_values(['ah', 'ch', 'ch', 'uh', 't']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 't']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 't']),\n",
       " dict_values(['ah', 'ch', 'ch', 'k', 't']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 'uu']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 'uh']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 'k']),\n",
       " dict_values(['ah', 'ch', 'ch', 't', 'ch']),\n",
       " dict_values(['ah', 'ch', 'ch', 'ch', 'h'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundsInDiphoneMatrix = set()\n",
    "def getSounds(row):\n",
    "    return project_dict(row, sound_fields).values()\n",
    "soundsByRow = [getSounds(row) for row in diphoneTrials]\n",
    "soundsByRow[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:28.262496Z",
     "start_time": "2018-11-18T20:20:28.132917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'l', 'th', 'er', ' ', 'ah', 'r', 'ng', 'd', 'oy', 'X', 'z', 'p', 'uh', 'oh', 'eh', 'm', 'f', 'sh', 'ahb', 'F', 'ee', 'x', 'w', 'oo', 'j', 't', 'v', 'ay', 'n', 'b', 'dh', 'zh', 'ae', 's', 'aye', 'L', 'ih', 'ch', 'uu', 'ow', 'g', 'h', 'k'}\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "for eachSoundSet in soundsByRow:\n",
    "    soundsInDiphoneMatrix.update(eachSoundSet)\n",
    "print(soundsInDiphoneMatrix)\n",
    "print(len(soundsInDiphoneMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the links below, the text of Warner et al. (2014) and earlier papers on the Dutch diphone gating data, plus the raw data set documentation, I've identified the relation between the inventory of the (English) diphone gating data experiment and IPA. Of note:\n",
    " - the speaker who produced the audio recordings for the experiment speaks a dialect of English that has the caught-cot merger.\n",
    " - Warner et al. have coded the low-back vowel resulting from the merger with 'a', the IPA symbol for the low *front* vowel that begins two of the diphthongs of American English; in IPA/in dealing with the merger and other data, this should be rendered with the glyph 'ɑ' rather than that of 'a' \n",
    " - the only consonant to have a distinguished syllabic version is [l] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:33.031502Z",
     "start_time": "2018-11-18T20:20:33.029196Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɸ\n",
      "ɸ\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Relevant:\n",
    "# https://www.wikiwand.com/en/IPA_Extensions\n",
    "# http://www.phon.ucl.ac.uk/home/wells/ipa-unicode.htm\n",
    "# http://westonruter.github.io/ipa-chart/keyboard/\n",
    "# https://docs.python.org/3/howto/unicode.html\n",
    "# http://stackoverflow.com/questions/33583485/ipa-to-arpabet-python\n",
    "# http://unicode-table.com/en/#control-character\n",
    "\n",
    "#Demonstration of unicode and Python playing nice\n",
    "print(\"\\u0278\") \n",
    "print(\"ɸ\")\n",
    "print(\"ɸ\" == \"\\u0278\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:37.917731Z",
     "start_time": "2018-11-18T20:20:37.900966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('x', 'ə'), ('d', 'd'), ('z', 'z'), ('g', 'g'), ('X', 'ɚ'), ('eh', 'ɛ'), ('p', 'p'), ('t', 't'), ('ay', 'eɪ'), ('oo', 'u'), ('uu', 'ʊ'), ('h', 'h'), ('y', 'j'), ('f', 'f'), ('uh', 'ʌ'), ('ow', 'aʊ'), ('ng', 'ŋ'), ('ae', 'æ'), ('dh', 'ð'), ('ch', 'tʃ'), ('k', 'k'), ('L', 'l̩'), ('w', 'w'), ('m', 'm'), ('th', 'θ'), ('s', 's'), ('l', 'l'), ('r', 'r'), ('ee', 'i'), ('er', 'ɚ'), ('sh', 'ʃ'), ('oy', 'ɔɪ'), ('ih', 'ɪ'), ('j', 'dʒ'), ('ah', 'ɑ'), ('v', 'v'), ('n', 'n'), ('aye', 'aɪ'), ('oh', 'oʊ'), ('zh', 'ʒ'), ('F', 'ɾ'), ('b', 'b')}\n",
      "42\n",
      "θ\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "# arpabet = ['AO','AA','IY','UW','EH','IH','UH','AH','AX','AE','EY','AY','OW','AW','OY','ER','P','B','T','D','K','G','CH','JH','F','V','TH','DH','S','Z','SH','ZH','HH','M','EM','N','EN','NG','ENG','L','EL','R','DX','NX','Y','W','Q']\n",
    "\n",
    "# Commented out lines indicate symbols in the diphone gating data (and their corresponding IPA symbols) that are not in the inventory used by Warner et al.  \n",
    "diphoneUnicodeIPArelation = set([ \\\n",
    "# ('', 'ɔ'), #Speaker for Warner et al is from Arizona -- ɔ is not in her inventory\n",
    "# ('ah', 'a'), #contrary to the raw dataset explanation, this is the low *back* vowel resulting from the cot-caught merger, not the low *front* vowel (IPA 'a') that some diphthongs start with...\n",
    "('ah', 'ɑ'), \n",
    "('ee', 'i'),\n",
    "('oo', 'u'),\n",
    "('eh', 'ɛ'),\n",
    "('ih', 'ɪ'),\n",
    "('uu', 'ʊ'),\n",
    "('uh', 'ʌ'),\n",
    "('x', 'ə'),\n",
    "('ae', 'æ'),\n",
    "('ay', 'eɪ'),\n",
    "('aye', 'aɪ'),\n",
    "('oh', 'oʊ'),\n",
    "('ow', 'aʊ'),\n",
    "('oy', 'ɔɪ'),\n",
    "('er', 'ɚ'),     # two symbols mapping to the same sound - unstressed r-colored schwa\n",
    "('X', 'ɚ'),      # two symbols mapping to the same sound - stressed r-colored schwa\n",
    "('p', 'p'),\n",
    "('b', 'b'),\n",
    "('t', 't'),\n",
    "('d', 'd'),\n",
    "('k', 'k'),\n",
    "('g', 'g'),\n",
    "('ch', 'tʃ'),\n",
    "('j', 'dʒ'),\n",
    "('f', 'f'),\n",
    "('v', 'v'),\n",
    "('th', 'θ'),\n",
    "('dh', 'ð'),\n",
    "('s', 's'),\n",
    "('z', 'z'),\n",
    "('sh', 'ʃ'),\n",
    "('zh', 'ʒ'),\n",
    "('h', 'h'),\n",
    "('m', 'm'),\n",
    "('n', 'n'),\n",
    "('ng', 'ŋ'),\n",
    "('l', 'l'),\n",
    "('L', 'l̩'),\n",
    "('r', 'r'),\n",
    "('F', 'ɾ'),\n",
    "('y', 'j'),\n",
    "('w', 'w'),\n",
    "])\n",
    "print(diphoneUnicodeIPArelation) \n",
    "print(len(diphoneUnicodeIPArelation))\n",
    "def diphoneToUnicodeIPA(diphoneSymbol):\n",
    "    mapping = dict(diphoneUnicodeIPArelation)\n",
    "    #print(mapping)\n",
    "    return mapping[diphoneSymbol]\n",
    "print(diphoneToUnicodeIPA('th'))\n",
    "def invertMapping(mydict): \n",
    "    return dict([[val, key] for key,val in mydict.items()])\n",
    "def unicodeIPAToDiphone(unicodeIPAsymbol):\n",
    "    return invertMapping( dict(diphoneUnicodeIPArelation) )[unicodeIPAsymbol]\n",
    "print(unicodeIPAToDiphone('θ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assert statement below succeeds, it means every transcribed sound observed in the diphone data at hand is, in fact, documented in Table 1 (pp. 2-3) of http://www.u.arizona.edu/~nwarner/Raw_data_explanation.pdf and therefore in the relation defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:42.559458Z",
     "start_time": "2018-11-18T20:20:42.523040Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a14a7d9f9da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiphoneSound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiphoneUnicodeIPArelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdiphoneSound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoundsInDiphoneMatrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(all([diphoneSound in list(map(lambda pair: pair[0],diphoneUnicodeIPArelation)) for diphoneSound in soundsInDiphoneMatrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...what transcribed sounds (as I'm capturing them) aren't in the mapping to IPA I've constructed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:48.630924Z",
     "start_time": "2018-11-18T20:20:48.624878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y',\n",
       " 'l',\n",
       " 'th',\n",
       " 'er',\n",
       " 'ah',\n",
       " 'r',\n",
       " 'ng',\n",
       " 'd',\n",
       " 'oy',\n",
       " 'X',\n",
       " 'z',\n",
       " 'p',\n",
       " 'uh',\n",
       " 'oh',\n",
       " 'eh',\n",
       " 'm',\n",
       " 'f',\n",
       " 'sh',\n",
       " 'F',\n",
       " 'ee',\n",
       " 'x',\n",
       " 'w',\n",
       " 'oo',\n",
       " 'j',\n",
       " 't',\n",
       " 'v',\n",
       " 'ay',\n",
       " 'n',\n",
       " 'b',\n",
       " 'dh',\n",
       " 'zh',\n",
       " 'ae',\n",
       " 's',\n",
       " 'aye',\n",
       " 'L',\n",
       " 'ih',\n",
       " 'ch',\n",
       " 'uu',\n",
       " 'ow',\n",
       " 'g',\n",
       " 'h',\n",
       " 'k']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[diphoneSound for diphoneSound in soundsInDiphoneMatrix if diphoneSound in list(map(lambda pair: pair[0],diphoneUnicodeIPArelation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:53.367728Z",
     "start_time": "2018-11-18T20:20:53.365051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'ahb']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[diphoneSound for diphoneSound in soundsInDiphoneMatrix if diphoneSound not in list(map(lambda pair: pair[0],diphoneUnicodeIPArelation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:20:57.961253Z",
     "start_time": "2018-11-18T20:20:57.958519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ɑ'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diphoneToUnicodeIPA('ah')\n",
    "diphoneToUnicodeIPA('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ahb' is a proceeding context (viz. 'ɑb'); ' ' is also a preceding context in some cases. I'll extend the mapping/functions I wrote to handle these two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:21:02.589883Z",
     "start_time": "2018-11-18T20:21:02.569519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('x', 'ə'), ('d', 'd'), ('z', 'z'), ('g', 'g'), ('X', 'ɚ'), ('eh', 'ɛ'), ('p', 'p'), ('t', 't'), ('ay', 'eɪ'), ('ahb', 'ɑb'), ('oo', 'u'), ('uu', 'ʊ'), ('h', 'h'), ('y', 'j'), ('f', 'f'), ('uh', 'ʌ'), ('ow', 'aʊ'), ('ng', 'ŋ'), ('ae', 'æ'), ('dh', 'ð'), ('ch', 'tʃ'), ('k', 'k'), ('L', 'l̩'), ('w', 'w'), ('m', 'm'), ('th', 'θ'), ('s', 's'), ('l', 'l'), ('r', 'r'), ('ee', 'i'), ('er', 'ɚ'), ('sh', 'ʃ'), ('oy', 'ɔɪ'), ('ih', 'ɪ'), ('j', 'dʒ'), ('ah', 'ɑ'), (' ', ' '), ('v', 'v'), ('n', 'n'), ('aye', 'aɪ'), ('oh', 'oʊ'), ('zh', 'ʒ'), ('F', 'ɾ'), ('b', 'b')}\n",
      "44\n",
      "θ\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "# arpabet = ['AO','AA','IY','UW','EH','IH','UH','AH','AX','AE','EY','AY','OW','AW','OY','ER','P','B','T','D','K','G','CH','JH','F','V','TH','DH','S','Z','SH','ZH','HH','M','EM','N','EN','NG','ENG','L','EL','R','DX','NX','Y','W','Q']\n",
    "\n",
    "# Commented out lines indicate symbols in the diphone gating data (and their corresponding IPA symbols) that are not in the inventory used by Warner et al.  \n",
    "diphoneUnicodeIPArelation = set([ \\\n",
    "# ('AO', 'ɔ'), #Speaker for Warner et al is from Arizona -- ɔ is not in her inventory\n",
    "(' ', ' '),\n",
    "# ('ahb', 'ab'),\n",
    "('ahb', 'ɑb'),\n",
    "# ('ah', 'a'), #contrary to the raw dataset explanation, this is the low *back* vowel resulting from the cot-caught merger, not the low *front* vowel (IPA 'a') that some diphthongs start with...\n",
    "('ah', 'ɑ'),\n",
    "('ee', 'i'),\n",
    "('oo', 'u'),\n",
    "('eh', 'ɛ'),\n",
    "('ih', 'ɪ'),\n",
    "('uu', 'ʊ'),\n",
    "('uh', 'ʌ'),\n",
    "('x', 'ə'),\n",
    "('ae', 'æ'),\n",
    "('ay', 'eɪ'),\n",
    "('aye', 'aɪ'),\n",
    "('oh', 'oʊ'),\n",
    "('ow', 'aʊ'),\n",
    "('oy', 'ɔɪ'),\n",
    "('er', 'ɚ'),     # two symbols mapping to the same sound - unstressed r-colored schwa\n",
    "('X', 'ɚ'),      # two symbols mapping to the same sound - stressed r-colored schwa\n",
    "('p', 'p'),\n",
    "('b', 'b'),\n",
    "('t', 't'),\n",
    "('d', 'd'),\n",
    "('k', 'k'),\n",
    "('g', 'g'),\n",
    "('ch', 'tʃ'),\n",
    "('j', 'dʒ'),\n",
    "('f', 'f'),\n",
    "('v', 'v'),\n",
    "('th', 'θ'),\n",
    "('dh', 'ð'),\n",
    "('s', 's'),\n",
    "('z', 'z'),\n",
    "('sh', 'ʃ'),\n",
    "('zh', 'ʒ'),\n",
    "('h', 'h'),\n",
    "('m', 'm'),\n",
    "# ('EM', 'm̩'),   #\n",
    "('n', 'n'),\n",
    "# ('EN', 'n̩'),   #\n",
    "('ng', 'ŋ'),\n",
    "# ('ENG', 'ŋ̩'),  #\n",
    "('l', 'l'),\n",
    "('L', 'l̩'),\n",
    "('r', 'r'),\n",
    "('F', 'ɾ'),\n",
    "# ('NX', 'ɾ̃'),   #\n",
    "('y', 'j'),\n",
    "('w', 'w'),\n",
    "# ('Q', 'ʔ')     #\n",
    "])\n",
    "print(diphoneUnicodeIPArelation) \n",
    "print(len(diphoneUnicodeIPArelation))\n",
    "def diphoneToUnicodeIPA(diphoneSymbol):\n",
    "    mapping = dict(diphoneUnicodeIPArelation)\n",
    "    #print(mapping)\n",
    "    return mapping[diphoneSymbol]\n",
    "print(diphoneToUnicodeIPA('th'))\n",
    "def invertMapping(mydict): \n",
    "    return dict([[val, key] for key,val in mydict.items()])\n",
    "def unicodeIPAToDiphone(unicodeIPAsymbol):\n",
    "    return invertMapping( dict(diphoneUnicodeIPArelation) )[unicodeIPAsymbol]\n",
    "print(unicodeIPAToDiphone('θ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the assertion again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:21:07.182779Z",
     "start_time": "2018-11-18T20:21:07.180591Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(all([diphoneSound in list(map(lambda pair: pair[0],diphoneUnicodeIPArelation)) for diphoneSound in soundsInDiphoneMatrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah. Now we can convert characters to UTF-8 IPA symbols for comparison with lexicon representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:21:11.783649Z",
     "start_time": "2018-11-18T20:21:11.781358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ʌ', 'ɔɪ', 'ɛ', 'l', 'aʊ', 'g', ' ', 'aɪ', 'ɑ', 'r', 'tʃ', 'ɚ', 'ŋ', 'd', 'ɪ', 'z', 'p', 'ʃ', 'ʊ', 'l̩', 'm', 'f', 'ð', 'eɪ', 'ʒ', 'i', 'ə', 'w', 'j', 't', 'v', 'ɾ', 'oʊ', 'n', 'b', 'dʒ', 's', 'ɑb', 'u', 'θ', 'æ', 'h', 'k'}\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "soundsInDiphoneMatrix_IPA = set(map(diphoneToUnicodeIPA, soundsInDiphoneMatrix))\n",
    "print(soundsInDiphoneMatrix_IPA)\n",
    "print(len(soundsInDiphoneMatrix_IPA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hammond's newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:10.382306Z",
     "start_time": "2018-11-18T20:22:10.376420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Transcription', 'x'),\n",
       "             ('stressInfoA', '_'),\n",
       "             ('stressInfoB', 'S1'),\n",
       "             ('Orthography', 'a'),\n",
       "             ('Frequency', '23178'),\n",
       "             ('PoSs', '(N IA VB PP)')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdic_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:15.210160Z",
     "start_time": "2018-11-18T20:22:15.196679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTrn(entry):\n",
    "    return entry['Transcription']\n",
    "\n",
    "def stringToTuple(s):\n",
    "    return tuple(list(s))\n",
    "\n",
    "def getSounds(entry):\n",
    "    trn = getTrn(entry)\n",
    "    trnTple = stringToTuple(trn)\n",
    "    return trnTple\n",
    "\n",
    "soundsByWord = list(map(getSounds, newdic_raw))\n",
    "len(soundsByWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:19.862971Z",
     "start_time": "2018-11-18T20:22:19.849755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundsetsByWord = list(map(set, soundsByWord))\n",
    "len(soundsetsByWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:24.475834Z",
     "start_time": "2018-11-18T20:22:24.465509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'J', 'W', 'Y', 'l', '^', 'g', 'k', 'r', '|', 'd', 'X', 'z', 'p', 'T', 'E', 'm', 'f', 'R', 'x', 'i', '@', 'w', 'G', 'C', 't', 'v', 'U', 'n', 'S', 'b', 'a', 'O', 's', 'L', 'M', 'D', 'u', 'N', 'Z', 'o', 'e', 'I', 'h', 'c'}\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "soundsInNewDic = reduce(set.union, soundsetsByWord)\n",
    "len(soundsInNewDic)\n",
    "print(soundsInNewDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:29.080738Z",
     "start_time": "2018-11-18T20:22:29.076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19528\n",
      "19528\n"
     ]
    }
   ],
   "source": [
    "orthWordList = [entry['Orthography'] for entry in newdic_raw]\n",
    "print(len(orthWordList))\n",
    "orthWords = set(orthWordList)\n",
    "print(len(orthWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:33.727592Z",
     "start_time": "2018-11-18T20:22:33.717718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19528"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "orthWordCounter = Counter(orthWords)\n",
    "uniqueOrthWords = [orthWord for orthWord in orthWords if orthWordCounter[orthWord] == 1]\n",
    "duplicatedOrthWords = [orthWord for orthWord in orthWords if orthWordCounter[orthWord] > 1]\n",
    "len(uniqueOrthWords)\n",
    "len(duplicatedOrthWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every orthographic word is associated with exactly one phonological description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:50.929007Z",
     "start_time": "2018-11-18T20:22:38.386237Z"
    }
   },
   "outputs": [],
   "source": [
    "orthToEntries = {orthword:[entry for entry in newdic_raw if entry['Orthography'] == orthword] for orthword in orthWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:22:55.635197Z",
     "start_time": "2018-11-18T20:22:55.630459Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(all(map(lambda orthword: len(orthToEntries[orthword]) == 1, orthToEntries )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:00.326482Z",
     "start_time": "2018-11-18T20:23:00.321457Z"
    }
   },
   "outputs": [],
   "source": [
    "orthToEntry = {orthword:orthToEntries[orthword][0] for orthword in orthWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:05.088414Z",
     "start_time": "2018-11-18T20:23:05.081293Z"
    }
   },
   "outputs": [],
   "source": [
    "orthToTrn = {orthword:orthToEntry[orthword]['Transcription'] for orthword in orthWords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the relation between symbols in Hammond's newdic and the diphone gating dataset, based on comparing IPA transcriptions in IPhOD with those in Hammond's newdic. Of note:\n",
    " - the inventory of Hammond's newdic distinguishes stressed and unstressed versions of some sounds with distinct symbols (/ə/, /ɚ/, /ɪ/)\n",
    " - the inventory of Hammond's newdic distinguishes syllabic and nonsyllabic versions of some consonants with distinct symbols (/n/, /m/, /l/)\n",
    " - Hammond's newdic preserves the cot-caught distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:09.921545Z",
     "start_time": "2018-11-18T20:23:09.903864Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hammond_diphone_inventory_relation = [\n",
    " ('h','h'),\n",
    " ('S','sh'),\n",
    " ('p','p'),\n",
    " ('x','x'), #schwa\n",
    " ('m','m'),\n",
    " ('y','y'),\n",
    " ('v','v'),\n",
    " ('^','uh'),\n",
    " ('o','oh'),\n",
    " ('u','oo'),\n",
    " ('I','ih'),\n",
    " ('G','ng'),\n",
    " ('N','n'), #syllabic n isn't represented in the diphone inventory\n",
    " ('|','ih'), #as near as I can tell, this is unstressed/reduced 'ɪ'\n",
    " ('Z','zh'),\n",
    " ('L','L'), #syllabic l\n",
    " ('M', 'm'), #syllabic m isn't represented in the diphone inventory\n",
    " ('i','ee'),\n",
    " ('r','r'),\n",
    " ('g','g'),\n",
    " ('O','oy'),\n",
    " ('T','th'),\n",
    " ('n','n'),\n",
    " ('J','j'),\n",
    " ('d','d'),\n",
    " ('k','k'),\n",
    " ('W','ow'),\n",
    " ('f','f'),\n",
    " ('D','dh'),\n",
    " ('U','uu'),\n",
    " ('z','z'),\n",
    " ('Y','aye'),\n",
    " ('b','b'),\n",
    " ('X','er'), #r-colored schwa - unstressed\n",
    " ('a','a'),\n",
    " ('s','s'),\n",
    " ('e','ay'),\n",
    " ('C','ch'),\n",
    " ('t','t'),\n",
    " ('R','er'), #r-colored schwa - stressed\n",
    " ('E','eh'),\n",
    " ('w','w'),\n",
    " ('l','l'),\n",
    " ('@','ae'),\n",
    " ('c','ah') #ɔ as in kɔt ('caught') isn't actually in the diphone gating data segment inventory\n",
    "]\n",
    "def hammondToDiphone(hammond_symb):\n",
    "    relevant_relations = [rel for rel in hammond_diphone_inventory_relation if rel[0] == hammond_symb]\n",
    "    diph_symbs = set([rel[1] for rel in relevant_relations])\n",
    "    return diph_symbs\n",
    "def diphoneToHammond(diph_symb):\n",
    "    relevant_relations = [rel for rel in hammond_diphone_inventory_relation if rel[1] == diph_symb]\n",
    "    hammond_symbs = set([rel[0] for rel in relevant_relations])\n",
    "    return hammond_symbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the relation between symbols in Hammond's newdic and the IPA, based on comparing IPA transcriptions in IPhOD with those in Hammond's newdic. Of note:\n",
    " - the inventory of Hammond's newdic distinguishes stressed and unstressed versions of some sounds with distinct symbols (/ə/, /ɚ/, /ɪ/)\n",
    " - the inventory of Hammond's newdic distinguishes syllabic and nonsyllabic versions of some consonants with distinct symbols (/n/, /m/, /l/)\n",
    " - Hammond's newdic preserves the cot-caught distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:14.720410Z",
     "start_time": "2018-11-18T20:23:14.703472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "hammond_IPA_relation = [\n",
    " ('h', 'h'),\n",
    " ('S', 'ʃ'),\n",
    " ('p', 'p'),\n",
    " ('x', 'ə'),\n",
    " ('m', 'm'),\n",
    " ('y', 'j'),\n",
    " ('v', 'v'),\n",
    " ('^', 'ʌ'),\n",
    " ('o', 'oʊ'),\n",
    " ('u', 'u'),\n",
    " ('I', 'ɪ'),\n",
    " ('G', 'ŋ'),\n",
    " ('N', 'ṇ'), #sllabic n is NOT in diphone gating inventory\n",
    " ('|', 'ɪ'), #as near as I can tell, this is unstressed/reduced 'ɪ'\n",
    " ('Z', 'ʒ'),\n",
    " ('L', 'l̩'),\n",
    " ('M', 'ṃ'), #syllabic m is NOT in diphone gating inventory\n",
    " ('i', 'i'),\n",
    " ('r', 'r'),\n",
    " ('g', 'g'),\n",
    " ('O', 'ɔɪ'),\n",
    " ('T', 'θ'),\n",
    " ('n', 'n'),\n",
    " ('J', 'dʒ'),\n",
    " ('d', 'd'),\n",
    " ('k', 'k'),\n",
    " ('W', 'aʊ'),\n",
    " ('f', 'f'),\n",
    " ('D', 'ð'),\n",
    " ('U', 'ʊ'),\n",
    " ('z', 'z'),\n",
    " ('Y', 'aɪ'),\n",
    " ('b', 'b'),\n",
    " ('X', 'ɚ'), #r-colored schwa - stressed\n",
    " ('a', 'ɑ'),\n",
    " ('s', 's'),\n",
    " ('e', 'eɪ'),\n",
    " ('C', 'tʃ'),\n",
    " ('t', 't'),\n",
    " ('R', 'ɚ'), #r-colored schwa - UNstressed\n",
    " ('E', 'ɛ'),\n",
    " ('w', 'w'),\n",
    " ('l', 'l'),\n",
    " ('@', 'æ'),\n",
    " ('c', 'ɔ')] #ɔ is NOT in diphone gating data inventory\n",
    "def hammondToUnicodeIPA(diphoneSymbol):\n",
    "    mapping = dict(hammond_IPA_relation)\n",
    "    #print(mapping)\n",
    "    return mapping[diphoneSymbol]\n",
    "print(hammondToUnicodeIPA('T'))\n",
    "def invertMapping(mydict): \n",
    "    return dict([[val, key] for key,val in mydict.items()])\n",
    "def unicodeIPAToHammond(unicodeIPAsymbol):\n",
    "    return invertMapping( dict(hammond_IPA_relation) )[unicodeIPAsymbol]\n",
    "print(unicodeIPAToHammond('θ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:19.499018Z",
     "start_time": "2018-11-18T20:23:19.496161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "{'ʌ', 'ɔɪ', 'ɛ', 'l', 'aʊ', 'ɔ', 'g', 'aɪ', 'ɑ', 'r', 'tʃ', 'ɚ', 'd', 'ŋ', 'ɪ', 'z', 'p', 'ʃ', 'ʊ', 'l̩', 'm', 'f', 'ð', 'eɪ', 'ʒ', 'i', 'ə', 'w', 'j', 'ṃ', 't', 'v', 'ṇ', 'oʊ', 'n', 'b', 'dʒ', 's', 'u', 'θ', 'æ', 'h', 'k'}\n"
     ]
    }
   ],
   "source": [
    "soundsInHammondsNewDic_IPA = set(map(hammondToUnicodeIPA, soundsInNewDic))\n",
    "print(len(soundsInHammondsNewDic_IPA))\n",
    "print(soundsInHammondsNewDic_IPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of inventories between Hammond's newdic and the diphone gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:24.370107Z",
     "start_time": "2018-11-18T20:23:24.366908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ɔ', 'ṃ', 'ṇ'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundsMissingFromDiphoneData = soundsInHammondsNewDic_IPA - soundsInDiphoneMatrix_IPA\n",
    "soundsMissingFromDiphoneData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:29.343938Z",
     "start_time": "2018-11-18T20:23:29.340557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', 'ɑb', 'ɾ'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundsMissingFromHammond = soundsInDiphoneMatrix_IPA - soundsInHammondsNewDic_IPA\n",
    "soundsMissingFromHammond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of Hammond's newdic for transcriptions/the lexicon alongside the diphone gating data for channel/noise distributions minimally requires:\n",
    " 1. merging syllabic m into m in Hammond's newdic.\n",
    " 2. merging syllabic n into n in Hammond's newdic.\n",
    " 3. merging 'ɔ' into 'ɑ' in Hammond's newdic.\n",
    " 4. merging 'ɾ' into t (or d) in the diphone gating data.\n",
    "\n",
    "Since both other syllabic consonants in Hammond's newdic are unrepresentable, I'm going to merge syllabic l into l in both Hammond's newdic and the gating data for parsimony, yielding the following set of alignment steps:\n",
    " 1. merging syllabic m into m in Hammond's newdic.\n",
    " 2. merging syllabic n into n in Hammond's newdic.\n",
    " 3. merging 'ɔ' into 'ɑ' in Hammond's newdic.\n",
    " 4. merging 'ɾ' into t (or d) in the diphone gating data.\n",
    " 5. merging syllabic l into l in Hammond's newdic.\n",
    " 6. merging syllabic l into l in the diphone gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWO NOTES ON PROSODY:\n",
    " 1. The inventory of newdic incorporates stress information that is suppressed when rendered only in terms of IPA segments.\n",
    " 2. The other prosodic annotations of Hammond's newdic/various other versions of the same data source (Hoosier mental lexicon, NETTALK) contains two pieces of information about each transcription\n",
    "     - one is the sequence of stresses associated with each vowel (\\_ = unstressed, ' = moderate stress, \\` = high stress).\n",
    "     - I don't understand what the second one is yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon - IPhOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:34.294750Z",
     "start_time": "2018-11-18T20:23:34.292327Z"
    }
   },
   "outputs": [],
   "source": [
    "#All we want are arpabet symbols.\n",
    "desired_fieldnames = [\n",
    "#                       'Word',  #orthographic representation\n",
    "                      'UnTrn', #unstressed CMU pronouncing dictionary transcription\n",
    "#                       'StTrn', #stressed CMU pronouncing dictionary transcription\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:39.213087Z",
     "start_time": "2018-11-18T20:23:39.099288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'UnTrn': 'AH'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'UnTrn': 'AH.P.R.AA.K.S.AH.M.AH.T'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPhOD_slim = [project_dict(row, desired_fieldnames) for row in lexicon_raw]\n",
    "len(IPhOD_slim)\n",
    "IPhOD_slim[0]\n",
    "IPhOD_slim[2304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:44.046273Z",
     "start_time": "2018-11-18T20:23:44.041803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AH']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['AH', 'P', 'R', 'AA', 'K', 'S', 'AH', 'M', 'AH', 'T']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPhOD_slim[0]['UnTrn'].split('.')\n",
    "IPhOD_slim[2304]['UnTrn'].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:49.241656Z",
     "start_time": "2018-11-18T20:23:49.183897Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AH'},\n",
       " {'EY'},\n",
       " {'AH', 'EY', 'IH', 'L', 'P', 'R', 'T'},\n",
       " {'AA', 'D', 'K', 'R', 'V'},\n",
       " {'AH', 'EH', 'N', 'R'},\n",
       " {'AE', 'B'},\n",
       " {'B', 'EY', 'IY'},\n",
       " {'AE', 'AH', 'B', 'K'},\n",
       " {'AE', 'AH', 'B', 'K', 'S'},\n",
       " {'AE', 'AH', 'B', 'IY', 'L', 'N', 'OW'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getArpabetSymbols(row):\n",
    "    return row['UnTrn'].split('.')\n",
    "\n",
    "arpabetSymbolsByWord = list(map(lambda r: set(getArpabetSymbols(r)), IPhOD_slim))\n",
    "arpabetSymbolsByWord[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:54.273531Z",
     "start_time": "2018-11-18T20:23:54.246757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CH', 'EY', 'W', 'Y', 'V', 'IY', 'JH', 'IH', 'SH', 'AY', 'T', 'HH', 'P', 'R', 'F', 'K', 'B', 'G', 'OY', 'AH', 'ER', 'AO', 'UW', 'S', 'DH', 'AE', 'AA', 'UH', 'L', 'D', 'M', 'AW', 'N', 'OW', 'Z', 'NG', 'ZH', 'EH', 'TH'}\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "arpabetSymbolsInIPhOD = set()\n",
    "for eachSymbolSet in arpabetSymbolsByWord:\n",
    "    arpabetSymbolsInIPhOD.update(eachSymbolSet)\n",
    "print(arpabetSymbolsInIPhOD)\n",
    "print(len(arpabetSymbolsInIPhOD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:23:59.188623Z",
     "start_time": "2018-11-18T20:23:59.184628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CH', 'EM', 'EY', 'W', 'Y', 'NX', 'V', 'IY', 'JH', 'IH', 'SH', 'AY', 'T', 'HH', 'P', 'R', 'F', 'ENG', 'AX', 'B', 'K', 'G', 'OY', 'AH', 'ER', 'AO', 'UW', 'S', 'DH', 'AE', 'AA', 'UH', 'EN', 'M', 'D', 'L', 'AW', 'EL', 'OW', 'Z', 'N', 'NG', 'ZH', 'EH', 'TH', 'DX', 'Q'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpabet = set(['AO','AA','IY','UW','EH','IH','UH','AH','AX','AE','EY','AY','OW','AW','OY','ER','P','B','T','D','K','G','CH','JH','F','V','TH','DH','S','Z','SH','ZH','HH','M','EM','N','EN','NG','ENG','L','EL','R','DX','NX','Y','W','Q'])\n",
    "print(arpabet)\n",
    "len(arpabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:24:04.074999Z",
     "start_time": "2018-11-18T20:24:04.071461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AX', 'DX', 'EL', 'EM', 'EN', 'ENG', 'NX', 'Q'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpabet = set(arpabet)\n",
    "missingFromIphod = arpabet - arpabetSymbolsInIPhOD\n",
    "missingFromIphod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:24:09.111401Z",
     "start_time": "2018-11-18T20:24:09.108133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingFromArpabet = arpabetSymbolsInIPhOD - arpabet\n",
    "missingFromArpabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So IPhOD doesn't use all of the sounds in the arpabet, but it also doesn't use any extra sounds that are NOT in the arpabet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should I be worried about the sounds NOT in IPhOD?\n",
    "\n",
    " - #AX is a schwa - this is potentially a problem.\n",
    " - #DX is a tap - this is unfortunate but not a showstopper; all taps are dental stops in the CMU transcription.\n",
    " - #EL is syllabic dark l.\n",
    " - #EM is syllabic m.\n",
    " - #EN is syllabic n.\n",
    " - #ENG is syllabic engma.\n",
    " - #NX is a nasalized tap.\n",
    " - #Q is a glottal stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:14.647596Z",
     "start_time": "2018-11-18T20:25:14.642819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɸ\n",
      "ɸ\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u0278\")\n",
    "print(\"ɸ\")\n",
    "print(\"ɸ\" == \"\\u0278\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:20.035916Z",
     "start_time": "2018-11-18T20:25:20.016876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('AA', 'ɑ'),\n",
       " ('AE', 'æ'),\n",
       " ('AH', 'ʌ'),\n",
       " ('AO', 'ɔ'),\n",
       " ('AW', 'aʊ'),\n",
       " ('AX', 'ə'),\n",
       " ('AY', 'aɪ'),\n",
       " ('B', 'b'),\n",
       " ('CH', 'tʃ'),\n",
       " ('D', 'd'),\n",
       " ('DH', 'ð'),\n",
       " ('DX', 'ɾ'),\n",
       " ('EH', 'ɛ'),\n",
       " ('EL', 'l̩'),\n",
       " ('EM', 'm̩'),\n",
       " ('EN', 'n̩'),\n",
       " ('ENG', 'ŋ̩'),\n",
       " ('ER', 'ɚ'),\n",
       " ('EY', 'eɪ'),\n",
       " ('F', 'f'),\n",
       " ('G', 'g'),\n",
       " ('HH', 'h'),\n",
       " ('IH', 'ɪ'),\n",
       " ('IY', 'i'),\n",
       " ('JH', 'dʒ'),\n",
       " ('K', 'k'),\n",
       " ('L', 'l'),\n",
       " ('M', 'm'),\n",
       " ('N', 'n'),\n",
       " ('NG', 'ŋ'),\n",
       " ('NX', 'ɾ̃'),\n",
       " ('OW', 'oʊ'),\n",
       " ('OY', 'ɔɪ'),\n",
       " ('P', 'p'),\n",
       " ('Q', 'ʔ'),\n",
       " ('R', 'r'),\n",
       " ('S', 's'),\n",
       " ('SH', 'ʃ'),\n",
       " ('T', 't'),\n",
       " ('TH', 'θ'),\n",
       " ('UH', 'ʊ'),\n",
       " ('UW', 'u'),\n",
       " ('V', 'v'),\n",
       " ('W', 'w'),\n",
       " ('Y', 'j'),\n",
       " ('Z', 'z'),\n",
       " ('ZH', 'ʒ')}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpabetUnicodeIPArelation = set([ \\\n",
    "('AO', 'ɔ'),\n",
    "('AA', 'ɑ'),\n",
    "('IY', 'i'),\n",
    "('UW', 'u'),\n",
    "('EH', 'ɛ'),\n",
    "('IH', 'ɪ'),\n",
    "('UH', 'ʊ'),\n",
    "('AH', 'ʌ'),\n",
    "('AX', 'ə'),\n",
    "('AE', 'æ'),\n",
    "('EY', 'eɪ'),\n",
    "('AY', 'aɪ'),\n",
    "('OW', 'oʊ'),\n",
    "('AW', 'aʊ'),\n",
    "('OY', 'ɔɪ'),\n",
    "('ER', 'ɚ'),\n",
    "('P', 'p'),\n",
    "('B', 'b'),\n",
    "('T', 't'),\n",
    "('D', 'd'),\n",
    "('K', 'k'),\n",
    "('G', 'g'),\n",
    "('CH', 'tʃ'),\n",
    "('JH', 'dʒ'),\n",
    "('F', 'f'),\n",
    "('V', 'v'),\n",
    "('TH', 'θ'),\n",
    "('DH', 'ð'),\n",
    "('S', 's'),\n",
    "('Z', 'z'),\n",
    "('SH', 'ʃ'),\n",
    "('ZH', 'ʒ'),\n",
    "('HH', 'h'),\n",
    "('M', 'm'),\n",
    "('EM', 'm̩'),\n",
    "('N', 'n'),\n",
    "('EN', 'n̩'),\n",
    "('NG', 'ŋ'),\n",
    "('ENG', 'ŋ̩'),\n",
    "('L', 'l'),\n",
    "('EL', 'l̩'),\n",
    "('R', 'r'),\n",
    "('DX', 'ɾ'),\n",
    "('NX', 'ɾ̃'),\n",
    "('Y', 'j'),\n",
    "('W', 'w'),\n",
    "('Q', 'ʔ')\n",
    "])\n",
    "arpabetUnicodeIPArelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:24.902323Z",
     "start_time": "2018-11-18T20:25:24.897984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'θ'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'TH'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arpabetToUnicodeIPA(arpabetSymbol):\n",
    "    mapping = dict(arpabetUnicodeIPArelation)\n",
    "    #print(mapping)\n",
    "    return mapping[arpabetSymbol]\n",
    "arpabetToUnicodeIPA('TH')\n",
    "\n",
    "def invertMapping(mydict): \n",
    "    return dict([[val, key] for key,val in mydict.items()])\n",
    "def unicodeIPAToArpabet(unicodeIPAsymbol):\n",
    "    return invertMapping( dict(arpabetUnicodeIPArelation) )[unicodeIPAsymbol]\n",
    "unicodeIPAToArpabet('θ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:29.809060Z",
     "start_time": "2018-11-18T20:25:29.806567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpabetSymbolsInIPhOD_IPA = set(map(arpabetToUnicodeIPA, arpabetSymbolsInIPhOD))\n",
    "len(arpabetSymbolsInIPhOD_IPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of IPhOD inventory and diphone gating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:34.685176Z",
     "start_time": "2018-11-18T20:25:34.683315Z"
    }
   },
   "outputs": [],
   "source": [
    "iphod_inventory = arpabetSymbolsInIPhOD_IPA\n",
    "gating_inventory = soundsInDiphoneMatrix_IPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:39.530227Z",
     "start_time": "2018-11-18T20:25:39.527871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ɔ'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_iphod_NOT_in_gating = iphod_inventory - gating_inventory\n",
    "in_iphod_NOT_in_gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ɔ' should indeed be missing from the gating data, as noted previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T20:25:45.464941Z",
     "start_time": "2018-11-18T20:25:45.459485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', 'l̩', 'ɑb', 'ə', 'ɾ'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_gating_NOT_in_iphod = gating_inventory - iphod_inventory\n",
    "in_gating_NOT_in_iphod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ' ' and 'ab' are hacks I introduced (and documented) above; they are not concerning.\n",
    " - syllabic l, schwa, and taps are indeed documented as missing from CMU pronouncing dictionary representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of IPhOD for transcriptions/the lexicon alongside the diphone gating data for channel/noise distributions requires:\n",
    " 1. merging 'ɔ' into 'ɑ' in IPhOD.\n",
    " 2. mapping at least unstressed 'ʌ' to 'ə' in IPhOD.\n",
    " 3. merging syllabic l into l the diphone gating data.\n",
    " 4. merging 'ɾ' into t (or d) in the diphone gating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pipeline sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **IPA**: To be more accessibly inspectable and easier to work with and analyze, all transcriptions in IPhOD, Hammond's newdic, and the gating data should be transformed to IPA symbols.\n",
    "\n",
    "2. **Alignment**: For either lexicon (IPhOD vs. Hammond's newdic) to be useful with the coarticulation/noise data from Warner et al. (2014), they need to utilize a common inventory of segments.\n",
    "  - Use of IPhOD for transcriptions/the lexicon alongside the diphone gating data for channel/noise distributions requires:\n",
    "    1. merging 'ɔ' into 'ɑ' in IPhOD.\n",
    "    2. mapping at least unstressed 'ʌ' to 'ə' in IPhOD.\n",
    "    3. merging syllabic l into l the diphone gating data.\n",
    "    4. merging 'ɾ' into t (or d) in the diphone gating data.\n",
    " -  Use of Hammond's newdic for transcriptions/the lexicon alongside the diphone gating data for channel/noise distributions requires:\n",
    "    1. merging syllabic m into m in Hammond's newdic.\n",
    "    2. merging syllabic n into n in Hammond's newdic.\n",
    "    3. merging 'ɔ' into 'ɑ' in Hammond's newdic.\n",
    "    4. merging 'ɾ' into t (or d) in the diphone gating data.\n",
    "    5. merging syllabic l into l in Hammond's newdic.\n",
    "    6. merging syllabic l into l in the diphone gating data. \n",
    "\n",
    "3. **Transformation into model inputs**: The word recognition model needs \n",
    " - a prior distribution over phonological wordforms, defined from (the aligned version of) the chosen lexicon.\n",
    " - a coarticulation/noise model, defined from (the aligned version of) the gating data, plus smoothing.\n",
    "\n",
    "These two model inputs are designed to be used with minimal modification by an implemented word recognition model, documented in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the three stages of processing, there are two notebooks ('1a', '2a', '3a', '1b', '2b', '3b'), one for processing the gating data, and one for processing the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global todo tally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T03:36:46.389735Z",
     "start_time": "2018-03-29T03:36:46.206442Z"
    }
   },
   "source": [
    "### New capabilities\n",
    " \n",
    " 1. **Support generating random lexicons with less structure than a given real one** - Notebooks 3b\n",
    "  - E.g. it would be nice to have a prior distribution over wordforms using the same inventory as the aligned version of IPhOD or the aligned version of Hammond's newdic and with e.g. same uniphone distribution, but with no phonotactics, or e.g. with similar phonotactics but with different levels of 'clumpiness' among (e.g. morphologically) related forms relative to the actual lexicon.\n",
    " \n",
    " 3. **Support prosodically-annotated segmental representations** - Notebooks 3a (+ 1b, 2b, 3b + Model Notebooks)\n",
    "  1. First goal: Modify Notebook 3a to define/export channel distributions that include vowel stress information instead of just segmental identities.\n",
    "     - May help identify and explain anomalies in the confusion data / repercussions of alignment choices.\n",
    "     - *Low priority*, unless it becomes clear (as seems to have been the case in the past in some cases) that ignoring vowel stress is the cause of strange results in the context of pursuing other questions.\n",
    "  2. Second goal: Be able to use channel distributions that incorporate stress information in the word recognition model.\n",
    "     - May help increase model performance, naturalness of its behavior; \n",
    "     - Requires non-trivial modification of scripts processing the lexicon:\n",
    "        1. Reprocessing Hammond's newdic in Processing Notebook 1a so that stress values are interleaved (as in IPhOD and the gating data); because the inventory of Hammond's newdic incorporates stress information into the original inventory, this may involve changing the mapping into IPA+stress (for vowels/maybe syllabic consonants, anyway).\n",
    "           - This \"maybe\" should involve checking that the stress level suggested by the symbol and the stress annotation given are always compatible, and resolving discrepancies as needed.\n",
    "        2. Producing lexicon distributions in Processing Notebook 3b that include stress information.\n",
    "        3. Modifying the word recognition model to deal with how the channel output distribution’s sample space is strictly coarser grained than that of the input. (Gating trial stimuli have stress annotations, but gating trial responses do *not*.)\n",
    "     - *Lowest priority* currently. Requires a lot of work; unless e.g. enough channel distributions with vs. without stress are dramatically different, it's not clear what would make the work worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open questions\n",
    "\n",
    "1. **Handling the lack of schwas in IPhOD** - Notebooks 2a, 2b\n",
    " - All vowels in IPhOD are annotated with one of a few levels of stress. There are no transcriptions containing 'ə'. Deleting trial data with stimuli or responses containing 'ə' is not a good idea. To avoid distorting the channel distribution, it seems reasonable to instead change the lexicon and convert unstressed ('stress level 0') 'ʌ's (42% of all unstressed vowel tokens in IPhOD) to 'ə's; it might not be unreasonable to merge unstressed 'i' (12.5% of all unstressed vowel tokens in IPhOD) or 'ɪ' (24.5% of all unstressed vowel tokens in IPhOD) into 'ə' as well. \n",
    " \n",
    "   - At stake here is the accuracy and usefulness of the word recognition model -- the question is whether 12.5+24.5 = 37% more of all unstressed vowel tokens in IPhOD should be moved from two segments into a third ('ə'), making schwa almost twice as common as it is with the current merger of unstressed 'ʌ' into 'ə'. Given the empirical ubiquity of vowel reduction in English and the practical benefit of capturing some aspects of richer (and more complicated) prosodic annotation with very little work, this might be a simple and justifiable way to avoid outliers or anomalously extreme predictions (most likely finding words harder to recognize than they actually are.\n",
    "   \n",
    "   - *Easy to implement.* Ask Marc about whether this is a no brainer / reasonable before comparing word recognition performance (e.g. an estimate of average surprisal during word recognition over the whole lexicon) more rigorously/exhaustively.\n",
    "\n",
    "2. **Good things to know about the gating data** - Notebook 2a\n",
    "   1. **Features and Errors**: Import a feature matrix from Phonological Corpus Tools (or my version of Riggle's feature chart); use it to get a finer grained description about errors in terms of features. This may be useful for identifying, understanding, and deciding what to do with outliers when e.g. considering the problem immediately below.\n",
    "       - This would be tremendously useful for investigating all kinds of questions about the gating data, the performance of the word recognition model, making interpretable graphs (for trivially organizing symbols by feature/natural class), and relating the confusability data to hypothetical phonological constraints (perceptibility constraints \"projected from the 'P-map'\").\n",
    "       - *High priority.*\n",
    "   2. **What is the impact of inventory alignments (and ignoring vowel stress) on the channel distribution?**: When segment types get remarkably easier or harder to identify after aligning the gating data with a lexicon in some way, we may have caused a net-undesirable change with respect to the goal of modeling word recognition (e.g. maybe certain words now are judged substantially harder to accurately recognize than they actually are), or at least one that we should be able to identify and investigate. It would be nice to have some relatively general measures to compare channel distributions before vs. after at least some alignments - e.g. I could, for each segment, take the KL divergence between a segment's uniphone channel distribution when unaligned vs. when aligned for IPhOD or when aligned for IPhOD vs. aligned for Hammond's newdic; ranking segments by that difference and looking at the distribution of differences could point out which segments are most affected by the alignment changes and prompt further investigation. (Recall that the difference between the unaligned trials and the newdic-aligned trials is that taps in the trial data have been rewritten as [t], and that the difference between the newdic-aligned trials and the IPhOD-aligned trials is that syllabic l in the trial data has been rewritten as [l]; note also that no outputs of Notebook 3a take stress or prosody into account, currently.) \n",
    "   \n",
    "      - For example: taps are currently merged into [t]s, but -- if I recall correctly -- taps are actually relatively easily confusable with both [t]s and [d]s, unlike either [t]s or [d]s, meaning the merger of taps into [t] in trial data undesirably makes [t]s seem more confusable with [d]s than they actually are. Maybe instead of merging all taps into [t]s, I should exclude all trials with taps in the stimuli and convert taps in the responses of remaining trials evenly but randomly into [t]s and [d]s. \n",
    "      - *Moderate priority.* Could plausibly become higher priority if I encounter issues or anomalies involving taps, low back vowels, l, or unstressed/reduced vowels, or if I have more information (as a result of e.g. integrating features).\n",
    "\n",
    "3. **Choice of lexicon & exact problem formulation** - Notebooks 2a, 2b\n",
    " - Q: Is there any reason to prefer Hammond's newdic over IPhOD or vice versa for any relevant scientific question or practical issue?\n",
    "    - A: IPhOD has ≈2-3x the number of phonological wordforms as Hammond's newdic. Consequences:\n",
    "    - *Higher expected surprisal*, *more informative top-down expectations*. The prior with IPhOD (relative to the prior over Hammond's newdic/the portion of it alignable with SUBTLEX word frequencies) probably contains a larger tail of infrequent words that will increase expected surprisal. Acoustic noise should have less effect on average using the IPhOD prior; in general, there may also plausibly be *some* relatively infrequent words where top-down expectations and a larger, denser lexicon make them *more likely* to be confused relative to a prior based on Hammond's newdic.\n",
    "    - IPhOD has multiple phonological variants for the same orthographic word, unlike Hammond's newdic.\n",
    "      1. *Neighborhood density and confusability.* Between simply having more words and specifically allowing multiple phonological variants per orthographic word, I expect the average phonological neighborhood to be denser than in Hammond's newdic, increasing the average confusability of each wordform.\n",
    "      2. *Phonological wordform vs. orthographic wordform vs. psycholinguistic lemma.* Without any kind of model of the relative frequency of different variants of an orthographic word (as one might get from the data of a transcribed corpus of natural speech - e.g. the Buckeye corpus), I split the IPhOD-provided SUBTLEX_US frequency of an orthographic wordform equally among its phonological variants. If the modeled problem is taken to be about the speaker choosing an intended phonological wordform (rather than an orthographic one or a 'lemma' - plausibly operationalizable here as a part-of-speech and a subset of all the phonological variants associated with an orthographic word), then the prior probability of the speaker's specific intended phonological variant will be low (perhaps seemingly artificially so) and so will the listener's expected degree of belief in the speaker's actual intended variant. This could be addressed by slightly altering the exact formulation of the modeled problem."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "188px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
